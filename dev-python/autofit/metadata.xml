<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>PyAutoFit: Classy Probabilistic Programming===========================================.. |binder| image:: https://mybinder.org/badge_logo.svg   :target: https://mybinder.org/v2/gh/Jammy2211/autofit_workspace/HEAD.. |RTD| image:: https://readthedocs.org/projects/pyautofit/badge/?version=latest    :target: https://pyautofit.readthedocs.io/en/latest/?badge=latest    :alt: Documentation Status.. |Tests| image:: https://github.com/rhayes777/PyAutoFit/actions/workflows/main.yml/badge.svg   :target: https://github.com/rhayes777/PyAutoFit/actions.. |Build| image:: https://github.com/rhayes777/PyAutoBuild/actions/workflows/release.yml/badge.svg   :target: https://github.com/rhayes777/PyAutoBuild/actions.. |JOSS| image:: https://joss.theoj.org/papers/10.21105/joss.02550/status.svg   :target: https://doi.org/10.21105/joss.02550|binder| |Tests| |Build| |RTD| |JOSS|`Installation Guide &lt;https://pyautofit.readthedocs.io/en/latest/installation/overview.html&gt;`_ |`readthedocs &lt;https://pyautofit.readthedocs.io/en/latest/index.html&gt;`_ |`Introduction on Binder &lt;https://mybinder.org/v2/gh/Jammy2211/autofit_workspace/release?filepath=notebooks/overview/overview_1_the_basics.ipynb&gt;`_ |`HowToFit &lt;https://pyautofit.readthedocs.io/en/latest/howtofit/howtofit.html&gt;`_**PyAutoFit** is a Python based probabilistic programming language for model fitting and Bayesian inferenceof large datasets.The basic **PyAutoFit** API allows us a user to quickly compose a probabilistic model and fit it to data via alog likelihood function, using a range of non-linear search algorithms (e.g. MCMC, nested sampling).Users can then set up **PyAutoFit** scientific workflow, which enables streamlined modeling of smalldatasets with tools to scale up to large datasets.**PyAutoFit** supports advanced statistical methods, mostnotably `a big data framework for Bayesian hierarchical analysis &lt;https://pyautofit.readthedocs.io/en/latest/features/graphical.html&gt;`_.Getting Started---------------The following links are useful for new starters:- `The PyAutoFit readthedocs &lt;https://pyautofit.readthedocs.io/en/latest&gt;`_, which includes an `installation guide &lt;https://pyautofit.readthedocs.io/en/latest/installation/overview.html&gt;`_ and an overview of **PyAutoFit**'s core features.- `The introduction Jupyter Notebook on Binder &lt;https://mybinder.org/v2/gh/Jammy2211/autofit_workspace/release?filepath=notebooks/overview/overview_1_the_basics.ipynb&gt;`_, where you can try **PyAutoFit** in a web browser (without installation).- `The autofit_workspace GitHub repository &lt;https://github.com/Jammy2211/autofit_workspace&gt;`_, which includes example scripts and the `HowToFit Jupyter notebook lectures &lt;https://github.com/Jammy2211/autofit_workspace/tree/master/notebooks/howtofit&gt;`_ which give new users a step-by-step introduction to **PyAutoFit**.Support-------Support for installation issues, help with Fit modeling and using **PyAutoFit** is available by`raising an issue on the GitHub issues page &lt;https://github.com/rhayes777/PyAutoFit/issues&gt;`_.We also offer support on the **PyAutoFit** `Slack channel &lt;https://pyautoFit.slack.com/&gt;`_, where we also provide thelatest updates on **PyAutoFit**. Slack is invitation-only, so if you'd like to join sendan `email &lt;https://github.com/Jammy2211&gt;`_ requesting an invite.HowToFit--------For users less familiar with Bayesian inference and scientific analysis you may wish to read throughthe **HowToFits** lectures. These teach you the basic principles of Bayesian inference, with thecontent pitched at undergraduate level and above.A complete overview of the lectures `is provided on the HowToFit readthedocs page &lt;https://pyautofit.readthedocs.io/en/latest/howtofit/howtofit.htmll&gt;`_API Overview------------To illustrate the **PyAutoFit** API, we use an illustrative toy model of fitting a one-dimensional Gaussian tonoisy 1D data. Here's the ``data`` (black) and the model (red) we'll fit:.. image:: https://raw.githubusercontent.com/rhayes777/PyAutoFit/master/files/toy_model_fit.png  :width: 400We define our model, a 1D Gaussian by writing a Python class using the format below:.. code-block:: python    class Gaussian:        def __init__(            self,            centre=0.0,        # &lt;- PyAutoFit recognises these            normalization=0.1, # &lt;- constructor arguments are            sigma=0.01,        # &lt;- the Gaussian's parameters.        ):            self.centre = centre            self.normalization = normalization            self.sigma = sigma        &quot;&quot;&quot;        An instance of the Gaussian class will be available during model fitting.        This method will be used to fit the model to data and compute a likelihood.        &quot;&quot;&quot;        def model_data_1d_via_xvalues_from(self, xvalues):            transformed_xvalues = xvalues - self.centre            return (self.normalization / (self.sigma * (2.0 * np.pi) ** 0.5)) * \                    np.exp(-0.5 * (transformed_xvalues / self.sigma) ** 2.0)**PyAutoFit** recognises that this Gaussian may be treated as a model component whose parameters can be fitted for viaa non-linear search like `emcee &lt;https://github.com/dfm/emcee&gt;`_.To fit this Gaussian to the ``data`` we create an Analysis object, which gives **PyAutoFit** the ``data`` and a``log_likelihood_function`` describing how to fit the ``data`` with the model:.. code-block:: python    class Analysis(af.Analysis):        def __init__(self, data, noise_map):            self.data = data            self.noise_map = noise_map        def log_likelihood_function(self, instance):            &quot;&quot;&quot;            The 'instance' that comes into this method is an instance of the Gaussian class            above, with the parameters set to values chosen by the non-linear search.            &quot;&quot;&quot;            print(&quot;Gaussian Instance:&quot;)            print(&quot;Centre = &quot;, instance.centre)            print(&quot;normalization = &quot;, instance.normalization)            print(&quot;Sigma = &quot;, instance.sigma)            &quot;&quot;&quot;            We fit the ``data`` with the Gaussian instance, using its            &quot;model_data_1d_via_xvalues_from&quot; function to create the model data.            &quot;&quot;&quot;            xvalues = np.arange(self.data.shape[0])            model_data = instance.model_data_1d_via_xvalues_from(xvalues=xvalues)            residual_map = self.data - model_data            chi_squared_map = (residual_map / self.noise_map) ** 2.0            log_likelihood = -0.5 * sum(chi_squared_map)            return log_likelihoodWe can now fit our model to the ``data`` using a non-linear search:.. code-block:: python    model = af.Model(Gaussian)    analysis = Analysis(data=data, noise_map=noise_map)    emcee = af.Emcee(nwalkers=50, nsteps=2000)    result = emcee.fit(model=model, analysis=analysis)The ``result`` contains information on the model-fit, for example the parameter samples, maximum log likelihoodmodel and marginalized probability density functions.</longdescription>
</pkgmetadata>