<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>=================pytorch-optimizer=================+--------------+------------------------------------------+| Build        | |workflow| |Documentation Status|        |+--------------+------------------------------------------+| Quality      | |codecov| |black| |ruff|                 |+--------------+------------------------------------------+| Package      | |PyPI version| |PyPI pyversions|         |+--------------+------------------------------------------+| Status       | |PyPi download| |PyPi month download|    |+--------------+------------------------------------------+| License      | |apache|                                 |+--------------+------------------------------------------+| **pytorch-optimizer** is optimizer &amp; lr scheduler collections in PyTorch.| I just re-implemented (speed &amp; memory tweaks, plug-ins) the algorithm while based on the original paper. Also, It includes useful and practical optimization ideas.| Currently, 55 optimizers, 6 lr schedulers are supported!|| Highly inspired by `pytorch-optimizer &lt;https://github.com/jettify/pytorch-optimizer&gt;`__.Getting Started---------------For more, see the `documentation &lt;https://pytorch-optimizers.readthedocs.io/en/latest/&gt;`__.Most optimizers are under MIT or Apache 2.0 license, but a few optimizers like `Fromage`, `Nero` have BY-NC-SA 4.0 license, which is non-commercial.So, please double-check the license before using it at your work.Installation~~~~~~~~~~~~::    $ pip3 install -U pytorch-optimizerIf there's a version issue when installing the package, try with `--no-deps` option.::    $ pip3 install -U --no-deps pytorch-optimizerSimple Usage~~~~~~~~~~~~::    from pytorch_optimizer import AdamP    model = YourModel()    optimizer = AdamP(model.parameters())    # or you can use optimizer loader, simply passing a name of the optimizer.    from pytorch_optimizer import load_optimizer    model = YourModel()    opt = load_optimizer(optimizer='adamp')    optimizer = opt(model.parameters())Also, you can load the optimizer via `torch.hub`::    import torch    model = YourModel()    opt = torch.hub.load('kozistr/pytorch_optimizer', 'adamp')    optimizer = opt(model.parameters())If you want to build the optimizer with parameters &amp; configs, there's `create_optimizer()` API.::    from pytorch_optimizer import create_optimizer    optimizer = create_optimizer(        model,        'adamp',        lr=1e-3,        weight_decay=1e-3,        use_gc=True,        use_lookahead=True,    )Supported Optimizers--------------------You can check the supported optimizers with below code.::    from pytorch_optimizer import get_supported_optimizers    supported_optimizers = get_supported_optimizers()+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Optimizer    | Description                                                                                       | Official Code                                                                     | Paper                                                                                         |                                                              Citation                                                |+==============+===================================================================================================+===================================================================================+===============================================================================================+======================================================================================================================+| AdaBelief    | *Adapting Step-sizes by the Belief in Observed Gradients*                                         | `github &lt;https://github.com/juntang-zhuang/Adabelief-Optimizer&gt;`__                | `https://arxiv.org/abs/2010.07468 &lt;https://arxiv.org/abs/2010.07468&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2020arXiv201007468Z/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaBound     | *Adaptive Gradient Methods with Dynamic Bound of Learning Rate*                                   | `github &lt;https://github.com/Luolc/AdaBound/blob/master/adabound/adabound.py&gt;`__   | `https://openreview.net/forum?id=Bkg3g2R9FX &lt;https://openreview.net/forum?id=Bkg3g2R9FX&gt;`__   | `cite &lt;https://github.com/Luolc/AdaBound#citing&gt;`__                                                                  |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaHessian   | *An Adaptive Second Order Optimizer for Machine Learning*                                         | `github &lt;https://github.com/amirgholami/adahessian&gt;`__                            | `https://arxiv.org/abs/2006.00719 &lt;https://arxiv.org/abs/2006.00719&gt;`__                       | `cite &lt;https://github.com/amirgholami/adahessian#citation&gt;`__                                                        |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdamD        | *Improved bias-correction in Adam*                                                                |                                                                                   | `https://arxiv.org/abs/2110.10828 &lt;https://arxiv.org/abs/2110.10828&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2021arXiv211010828S/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdamP        | *Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights*                    | `github &lt;https://github.com/clovaai/AdamP&gt;`__                                     | `https://arxiv.org/abs/2006.08217 &lt;https://arxiv.org/abs/2006.08217&gt;`__                       | `cite &lt;https://github.com/clovaai/AdamP#how-to-cite&gt;`__                                                              |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| diffGrad     | *An Optimization Method for Convolutional Neural Networks*                                        | `github &lt;https://github.com/shivram1987/diffGrad&gt;`__                              | `https://arxiv.org/abs/1909.11015v3 &lt;https://arxiv.org/abs/1909.11015v3&gt;`__                   | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv190911015D/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| MADGRAD      | *A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic*                          | `github &lt;https://github.com/facebookresearch/madgrad&gt;`__                          | `https://arxiv.org/abs/2101.11075 &lt;https://arxiv.org/abs/2101.11075&gt;`__                       | `cite &lt;https://github.com/facebookresearch/madgrad#tech-report&gt;`__                                                   |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| RAdam        | *On the Variance of the Adaptive Learning Rate and Beyond*                                        | `github &lt;https://github.com/LiyuanLucasLiu/RAdam&gt;`__                              | `https://arxiv.org/abs/1908.03265 &lt;https://arxiv.org/abs/1908.03265&gt;`__                       | `cite &lt;https://github.com/LiyuanLucasLiu/RAdam#citation&gt;`__                                                          |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Ranger       | *a synergistic optimizer combining RAdam and LookAhead, and now GC in one optimizer*              | `github &lt;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer&gt;`__          | `https://bit.ly/3zyspC3 &lt;https://bit.ly/3zyspC3&gt;`__                                           | `cite &lt;https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer#citing-this-work&gt;`__                              |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Ranger21     | *a synergistic deep learning optimizer*                                                           | `github &lt;https://github.com/lessw2020/Ranger21&gt;`__                                | `https://arxiv.org/abs/2106.13731 &lt;https://arxiv.org/abs/2106.13731&gt;`__                       | `cite &lt;https://github.com/lessw2020/Ranger21#referencing-this-work&gt;`__                                               |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Lamb         | *Large Batch Optimization for Deep Learning*                                                      | `github &lt;https://github.com/cybertronai/pytorch-lamb&gt;`__                          | `https://arxiv.org/abs/1904.00962 &lt;https://arxiv.org/abs/1904.00962&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv190400962Y/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Shampoo      | *Preconditioned Stochastic Tensor Optimization*                                                   | `github &lt;https://github.com/moskomule/shampoo.pytorch&gt;`__                         | `https://arxiv.org/abs/1802.09568 &lt;https://arxiv.org/abs/1802.09568&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2018arXiv180209568G/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Nero         | *Learning by Turning: Neural Architecture Aware Optimisation*                                     | `github &lt;https://github.com/jxbz/nero&gt;`__                                         | `https://arxiv.org/abs/2102.07227 &lt;https://arxiv.org/abs/2102.07227&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2021arXiv210207227L/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Adan         | *Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models*                          | `github &lt;https://github.com/sail-sg/Adan&gt;`__                                      | `https://arxiv.org/abs/2208.06677 &lt;https://arxiv.org/abs/2208.06677&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2022arXiv220806677X/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Adai         | *Disentangling the Effects of Adaptive Learning Rate and Momentum*                                | `github &lt;https://github.com/zeke-xie/adaptive-inertia-adai&gt;`__                    | `https://arxiv.org/abs/2006.15815 &lt;https://arxiv.org/abs/2006.15815&gt;`__                       | `cite &lt;https://github.com/zeke-xie/adaptive-inertia-adai#citing&gt;`__                                                  |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| SAM          | *Sharpness-Aware Minimization*                                                                    | `github &lt;https://github.com/davda54/sam&gt;`__                                       | `https://arxiv.org/abs/2010.01412 &lt;https://arxiv.org/abs/2010.01412&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2020arXiv201001412F/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| ASAM         | *Adaptive Sharpness-Aware Minimization*                                                           | `github &lt;https://github.com/davda54/sam&gt;`__                                       | `https://arxiv.org/abs/2102.11600 &lt;https://arxiv.org/abs/2102.11600&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2021arXiv210211600K/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| GSAM         | *Surrogate Gap Guided Sharpness-Aware Minimization*                                               | `github &lt;https://github.com/juntang-zhuang/GSAM&gt;`__                               | `https://openreview.net/pdf?id=edONMAnhLu- &lt;https://openreview.net/pdf?id=edONMAnhLu-&gt;`__     | `cite &lt;https://github.com/juntang-zhuang/GSAM#citation&gt;`__                                                           |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| D-Adaptation | *Learning-Rate-Free Learning by D-Adaptation*                                                     | `github &lt;https://github.com/facebookresearch/dadaptation&gt;`__                      | `https://arxiv.org/abs/2301.07733 &lt;https://arxiv.org/abs/2301.07733&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2023arXiv230107733D/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaFactor    | *Adaptive Learning Rates with Sublinear Memory Cost*                                              | `github &lt;https://github.com/DeadAt0m/adafactor-pytorch&gt;`__                        | `https://arxiv.org/abs/1804.04235 &lt;https://arxiv.org/abs/1804.04235&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2018arXiv180404235S/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Apollo       | *An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization*   | `github &lt;https://github.com/XuezheMax/apollo&gt;`__                                  | `https://arxiv.org/abs/2009.13586 &lt;https://arxiv.org/abs/2009.13586&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2020arXiv200913586M/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| NovoGrad     | *Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks*      | `github &lt;https://github.com/lonePatient/NovoGrad-pytorch&gt;`__                      | `https://arxiv.org/abs/1905.11286 &lt;https://arxiv.org/abs/1905.11286&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv190511286G/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Lion         | *Symbolic Discovery of Optimization Algorithms*                                                   | `github &lt;https://github.com/google/automl/tree/master/lion&gt;`__                    | `https://arxiv.org/abs/2302.06675 &lt;https://arxiv.org/abs/2302.06675&gt;`__                       | `cite &lt;https://github.com/google/automl/tree/master/lion#citation&gt;`__                                                |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Ali-G        | *Adaptive Learning Rates for Interpolation with Gradients*                                        | `github &lt;https://github.com/oval-group/ali-g&gt;`__                                  | `https://arxiv.org/abs/1906.05661 &lt;https://arxiv.org/abs/1906.05661&gt;`__                       | `cite &lt;https://github.com/oval-group/ali-g#adaptive-learning-rates-for-interpolation-with-gradients&gt;`__              |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| SM3          | *Memory-Efficient Adaptive Optimization*                                                          | `github &lt;https://github.com/google-research/google-research/tree/master/sm3&gt;`__   | `https://arxiv.org/abs/1901.11150 &lt;https://arxiv.org/abs/1901.11150&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv190111150A/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaNorm      | *Adaptive Gradient Norm Correction based Optimizer for CNNs*                                      | `github &lt;https://github.com/shivram1987/AdaNorm&gt;`__                               | `https://arxiv.org/abs/2210.06364 &lt;https://arxiv.org/abs/2210.06364&gt;`__                       | `cite &lt;https://github.com/shivram1987/AdaNorm/tree/main#citation&gt;`__                                                 |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| RotoGrad     | *Gradient Homogenization in Multitask Learning*                                                   | `github &lt;https://github.com/adrianjav/rotograd&gt;`__                                | `https://openreview.net/pdf?id=T8wHz4rnuGL &lt;https://openreview.net/pdf?id=T8wHz4rnuGL&gt;`__     | `cite &lt;https://github.com/adrianjav/rotograd#citing&gt;`__                                                              |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| A2Grad       | *Optimal Adaptive and Accelerated Stochastic Gradient Descent*                                    | `github &lt;https://github.com/severilov/A2Grad_optimizer&gt;`__                        | `https://arxiv.org/abs/1810.00553 &lt;https://arxiv.org/abs/1810.00553&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2018arXiv181000553D/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AccSGD       | *Accelerating Stochastic Gradient Descent For Least Squares Regression*                           | `github &lt;https://github.com/rahulkidambi/AccSGD&gt;`__                               | `https://arxiv.org/abs/1704.08227 &lt;https://arxiv.org/abs/1704.08227&gt;`__                       | `cite &lt;https://github.com/rahulkidambi/AccSGD#citation&gt;`__                                                           |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| SGDW         | *Decoupled Weight Decay Regularization*                                                           | `github &lt;https://github.com/loshchil/AdamW-and-SGDW&gt;`__                           | `https://arxiv.org/abs/1711.05101 &lt;https://arxiv.org/abs/1711.05101&gt;`__                       | `cite &lt;https://github.com/loshchil/AdamW-and-SGDW#contact&gt;`__                                                        |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| ASGD         | *Adaptive Gradient Descent without Descent*                                                       | `github &lt;https://github.com/ymalitsky/adaptive_GD&gt;`__                             | `https://arxiv.org/abs/1910.09529 &lt;https://arxiv.org/abs/1910.09529&gt;`__                       | `cite &lt;https://github.com/ymalitsky/adaptive_GD#reference&gt;`__                                                        |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Yogi         | *Adaptive Methods for Nonconvex Optimization*                                                     |                                                                                   | `NIPS 2018 &lt;https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization&gt;`__ | `cite &lt;https://proceedings.neurips.cc/paper_files/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html&gt;`__ |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| SWATS        | *Improving Generalization Performance by Switching from Adam to SGD*                              |                                                                                   | `https://arxiv.org/abs/1712.07628 &lt;https://arxiv.org/abs/1712.07628&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2017arXiv171207628S/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Fromage      | *On the distance between two neural networks and the stability of learning*                       | `github &lt;https://github.com/jxbz/fromage&gt;`__                                      | `https://arxiv.org/abs/2002.03432 &lt;https://arxiv.org/abs/2002.03432&gt;`__                       | `cite &lt;https://github.com/jxbz/fromage#citation&gt;`__                                                                  |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| MSVAG        | *Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients*                       | `github &lt;https://github.com/lballes/msvag&gt;`__                                     | `https://arxiv.org/abs/1705.07774 &lt;https://arxiv.org/abs/1705.07774&gt;`__                       | `cite &lt;https://github.com/lballes/msvag#citation&gt;`__                                                                 |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaMod       | *An Adaptive and Momental Bound Method for Stochastic Learning*                                   | `github &lt;https://github.com/lancopku/AdaMod&gt;`__                                   | `https://arxiv.org/abs/1910.12249 &lt;https://arxiv.org/abs/1910.12249&gt;`__                       | `cite &lt;https://github.com/lancopku/AdaMod#citation&gt;`__                                                               |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AggMo        | *Aggregated Momentum: Stability Through Passive Damping*                                          | `github &lt;https://github.com/AtheMathmo/AggMo&gt;`__                                  | `https://arxiv.org/abs/1804.00325 &lt;https://arxiv.org/abs/1804.00325&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2018arXiv180400325L/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| QHAdam       | *Quasi-hyperbolic momentum and Adam for deep learning*                                            | `github &lt;https://github.com/facebookresearch/qhoptim&gt;`__                          | `https://arxiv.org/abs/1810.06801 &lt;https://arxiv.org/abs/1810.06801&gt;`__                       | `cite &lt;https://github.com/facebookresearch/qhoptim#reference&gt;`__                                                     |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| PID          | *A PID Controller Approach for Stochastic Optimization of Deep Networks*                          | `github &lt;https://github.com/tensorboy/PIDOptimizer&gt;`__                            | `CVPR 18 &lt;http://www4.comp.polyu.edu.hk/~cslzhang/paper/CVPR18_PID.pdf&gt;`__                    | `cite &lt;https://github.com/tensorboy/PIDOptimizer#citation&gt;`__                                                        |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Gravity      | *a Kinematic Approach on Optimization in Deep Learning*                                           | `github &lt;https://github.com/dariush-bahrami/gravity.optimizer&gt;`__                 | `https://arxiv.org/abs/2101.09192 &lt;https://arxiv.org/abs/2101.09192&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2021arXiv210109192B/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaSmooth    | *An Adaptive Learning Rate Method based on Effective Ratio*                                       |                                                                                   | `https://arxiv.org/abs/2204.00825v1 &lt;https://arxiv.org/abs/2204.00825v1&gt;`__                   | `cite &lt;https://ui.adsabs.harvard.edu/abs/2022arXiv220400825L/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| SRMM         | *Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates* | `github &lt;https://github.com/HanbaekLyu/SRMM&gt;`__                                   | `https://arxiv.org/abs/2201.01652 &lt;https://arxiv.org/abs/2201.01652&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2022arXiv220101652L/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AvaGrad      | *Domain-independent Dominance of Adaptive Methods*                                                | `github &lt;https://github.com/lolemacs/avagrad&gt;`__                                  | `https://arxiv.org/abs/1912.01823 &lt;https://arxiv.org/abs/1912.01823&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv191201823S/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| PCGrad       | *Gradient Surgery for Multi-Task Learning*                                                        | `github &lt;https://github.com/tianheyu927/PCGrad&gt;`__                                | `https://arxiv.org/abs/2001.06782 &lt;https://arxiv.org/abs/2001.06782&gt;`__                       | `cite &lt;https://github.com/tianheyu927/PCGrad#reference&gt;`__                                                           |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AMSGrad      | *On the Convergence of Adam and Beyond*                                                           |                                                                                   | `https://openreview.net/pdf?id=ryQu7f-RZ &lt;https://openreview.net/pdf?id=ryQu7f-RZ&gt;`__         | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv190409237R/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Lookahead    | *k steps forward, 1 step back*                                                                    | `github &lt;https://github.com/pytorch/examples/tree/main/imagenet&gt;`__               | `https://arxiv.org/abs/1907.08610 &lt;https://arxiv.org/abs/1907.08610&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv190708610Z/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| PNM          | *Manipulating Stochastic Gradient Noise to Improve Generalization*                                | `github &lt;https://github.com/zeke-xie/Positive-Negative-Momentum&gt;`__               | `https://arxiv.org/abs/2103.17182 &lt;https://arxiv.org/abs/2103.17182&gt;`__                       | `cite &lt;https://github.com/zeke-xie/Positive-Negative-Momentum#citing&gt;`__                                             |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| GC           | *Gradient Centralization*                                                                         | `github &lt;https://github.com/Yonghongwei/Gradient-Centralization&gt;`__               | `https://arxiv.org/abs/2004.01461 &lt;https://arxiv.org/abs/2004.01461&gt;`__                       | `cite &lt;https://github.com/Yonghongwei/Gradient-Centralization#citation&gt;`__                                           |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AGC          | *Adaptive Gradient Clipping*                                                                      | `github &lt;https://github.com/deepmind/deepmind-research/tree/master/nfnets&gt;`__     | `https://arxiv.org/abs/2102.06171 &lt;https://arxiv.org/abs/2102.06171&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2021arXiv210206171B/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Stable WD    | *Understanding and Scheduling Weight Decay*                                                       | `github &lt;https://github.com/zeke-xie/stable-weight-decay-regularization&gt;`__       | `https://arxiv.org/abs/2011.11152 &lt;https://arxiv.org/abs/2011.11152&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2020arXiv201111152X/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Softplus T   | *Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM*                           |                                                                                   | `https://arxiv.org/abs/1908.00700 &lt;https://arxiv.org/abs/1908.00700&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv190800700T/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Un-tuned w/u | *On the adequacy of untuned warmup for adaptive optimization*                                     |                                                                                   | `https://arxiv.org/abs/1910.04209 &lt;https://arxiv.org/abs/1910.04209&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2019arXiv191004209M/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Norm Loss    | *An efficient yet effective regularization method for deep neural networks*                       |                                                                                   | `https://arxiv.org/abs/2103.06583 &lt;https://arxiv.org/abs/2103.06583&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2021arXiv210306583G/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaShift     | *Decorrelation and Convergence of Adaptive Learning Rate Methods*                                 | `github &lt;https://github.com/MichaelKonobeev/adashift&gt;`__                          | `https://arxiv.org/abs/1810.00143v4 &lt;https://arxiv.org/abs/1810.00143v4&gt;`__                   | `cite &lt;https://ui.adsabs.harvard.edu/abs/2018arXiv181000143Z/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaDelta     | *An Adaptive Learning Rate Method*                                                                |                                                                                   | `https://arxiv.org/abs/1212.5701v1 &lt;https://arxiv.org/abs/1212.5701v1&gt;`__                     | `cite &lt;https://ui.adsabs.harvard.edu/abs/2012arXiv1212.5701Z/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Amos         | *An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale*                 | `github &lt;https://github.com/google-research/jestimator&gt;`__                        | `https://arxiv.org/abs/2210.11693 &lt;https://arxiv.org/abs/2210.11693&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2022arXiv221011693T/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| SignSGD      | *Compressed Optimisation for Non-Convex Problems*                                                 | `github &lt;https://github.com/jxbz/signSGD&gt;`__                                      | `https://arxiv.org/abs/1802.04434 &lt;https://arxiv.org/abs/1802.04434&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2018arXiv180204434B/exportcitation&gt;`__                                      |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| AdaHessian   | *An Adaptive Second Order Optimizer for Machine Learning*                                         | `github &lt;https://github.com/amirgholami/adahessian&gt;`__                            | `https://arxiv.org/abs/2006.00719 &lt;https://arxiv.org/abs/2006.00719&gt;`__                       | `cite &lt;https://github.com/amirgholami/adahessian#citation&gt;`__                                                        |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Sophia       | *A Scalable Stochastic Second-order Optimizer for Language Model Pre-training*                    | `github &lt;https://github.com/Liuhong99/Sophia&gt;`__                                  | `https://arxiv.org/abs/2305.14342 &lt;https://arxiv.org/abs/2305.14342&gt;`__                       | `cite &lt;https://github.com/Liuhong99/Sophia&gt;`__                                                                       |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Prodigy      | *An Expeditiously Adaptive Parameter-Free Learner*                                                | `github &lt;https://github.com/konstmish/prodigy&gt;`__                                 | `https://arxiv.org/abs/2306.06101 &lt;https://arxiv.org/abs/2306.06101&gt;`__                       | `cite &lt;https://github.com/konstmish/prodigy#how-to-cite&gt;`__                                                          |+--------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+Supported LR Scheduler----------------------You can check the supported learning rate schedulers with below code.::    from pytorch_optimizer import get_supported_lr_schedulers    supported_lr_schedulers = get_supported_lr_schedulers()+------------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| LR Scheduler     | Description                                                                                       | Official Code                                                                     | Paper                                                                                         |                                                              Citation                                                |+==================+===================================================================================================+===================================================================================+===============================================================================================+======================================================================================================================+| Explore-Exploit  | *Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule*                   |                                                                                   | `https://arxiv.org/abs/2003.03977 &lt;https://arxiv.org/abs/2003.03977&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2020arXiv200303977I/exportcitation&gt;`__                                      |+------------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+| Chebyshev        | *Acceleration via Fractal Learning Rate Schedules*                                                |                                                                                   | `https://arxiv.org/abs/2103.01338 &lt;https://arxiv.org/abs/2103.01338&gt;`__                       | `cite &lt;https://ui.adsabs.harvard.edu/abs/2021arXiv210301338A/exportcitation&gt;`__                                      |+------------------+---------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+Useful Resources----------------Several optimization ideas to regularize &amp; stabilize the training. Mostof the ideas are applied in ``Ranger21`` optimizer.Also, most of the captures are taken from ``Ranger21`` paper.+------------------------------------------+---------------------------------------------+--------------------------------------------+| `Adaptive Gradient Clipping`_            | `Gradient Centralization`_                  | `Softplus Transformation`_                 |+------------------------------------------+---------------------------------------------+--------------------------------------------+| `Gradient Normalization`_                | `Norm Loss`_                                | `Positive-Negative Momentum`_              |+------------------------------------------+---------------------------------------------+--------------------------------------------+| `Linear learning rate warmup`_           | `Stable weight decay`_                      | `Explore-exploit learning rate schedule`_  |+------------------------------------------+---------------------------------------------+--------------------------------------------+| `Lookahead`_                             | `Chebyshev learning rate schedule`_         | `(Adaptive) Sharpness-Aware Minimization`_ |+------------------------------------------+---------------------------------------------+--------------------------------------------+| `On the Convergence of Adam and Beyond`_ | `Improved bias-correction in Adam`_         | `Adaptive Gradient Norm Correction`_       |+------------------------------------------+---------------------------------------------+--------------------------------------------+Adaptive Gradient Clipping--------------------------| This idea originally proposed in ``NFNet (Normalized-Free Network)`` paper.| ``AGC (Adaptive Gradient Clipping)`` clips gradients based on the ``unit-wise ratio of gradient norms to parameter norms``.-  code : `github &lt;https://github.com/deepmind/deepmind-research/tree/master/nfnets&gt;`__-  paper : `arXiv &lt;https://arxiv.org/abs/2102.06171&gt;`__Gradient Centralization-----------------------+-----------------------------------------------------------------------------------------------------------------+| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/gradient_centralization.png  |+-----------------------------------------------------------------------------------------------------------------+``Gradient Centralization (GC)`` operates directly on gradients by centralizing the gradient to have zero mean.-  code : `github &lt;https://github.com/Yonghongwei/Gradient-Centralization&gt;`__-  paper : `arXiv &lt;https://arxiv.org/abs/2004.01461&gt;`__Softplus Transformation-----------------------By running the final variance denom through the softplus function, it lifts extremely tiny values to keep them viable.-  paper : `arXiv &lt;https://arxiv.org/abs/1908.00700&gt;`__Gradient Normalization----------------------Norm Loss---------+---------------------------------------------------------------------------------------------------+| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/norm_loss.png  |+---------------------------------------------------------------------------------------------------+-  paper : `arXiv &lt;https://arxiv.org/abs/2103.06583&gt;`__Positive-Negative Momentum--------------------------+--------------------------------------------------------------------------------------------------------------------+| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/positive_negative_momentum.png  |+--------------------------------------------------------------------------------------------------------------------+-  code : `github &lt;https://github.com/zeke-xie/Positive-Negative-Momentum&gt;`__-  paper : `arXiv &lt;https://arxiv.org/abs/2103.17182&gt;`__Linear learning rate warmup---------------------------+----------------------------------------------------------------------------------------------------------+| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/linear_lr_warmup.png  |+----------------------------------------------------------------------------------------------------------+-  paper : `arXiv &lt;https://arxiv.org/abs/1910.04209&gt;`__Stable weight decay-------------------+-------------------------------------------------------------------------------------------------------------+| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/stable_weight_decay.png  |+-------------------------------------------------------------------------------------------------------------+-  code : `github &lt;https://github.com/zeke-xie/stable-weight-decay-regularization&gt;`__-  paper : `arXiv &lt;https://arxiv.org/abs/2011.11152&gt;`__Explore-exploit learning rate schedule--------------------------------------+---------------------------------------------------------------------------------------------------------------------+| .. image:: https://raw.githubusercontent.com/kozistr/pytorch_optimizer/main/assets/explore_exploit_lr_schedule.png  |+---------------------------------------------------------------------------------------------------------------------+-  code : `github &lt;https://github.com/nikhil-iyer-97/wide-minima-density-hypothesis&gt;`__-  paper : `arXiv &lt;https://arxiv.org/abs/2003.03977&gt;`__Lookahead---------| ``k`` steps forward, 1 step back. ``Lookahead`` consisting of keeping an exponential moving average of the weights that is| updated and substituted to the current weights every ``k_{lookahead}`` steps (5 by default).Chebyshev learning rate schedule--------------------------------Acceleration via Fractal Learning Rate Schedules.(Adaptive) Sharpness-Aware Minimization---------------------------------------| Sharpness-Aware Minimization (SAM) simultaneously minimizes loss value and loss sharpness.| In particular, it seeks parameters that lie in neighborhoods having uniformly low loss.On the Convergence of Adam and Beyond-------------------------------------| Convergence issues can be fixed by endowing such algorithms with 'long-term memory' of past gradients.Improved bias-correction in Adam--------------------------------| With the default bias-correction, Adam may actually make larger than requested gradient updates early in training.Adaptive Gradient Norm Correction---------------------------------| Correcting the norm of gradient in each iteration based on the adaptive training history of gradient norm.Citation--------Please cite original authors of optimization algorithms. If you use this software, please cite it as below.Or you can get from &quot;cite this repository&quot; button.::    @software{Kim_pytorch_optimizer_Optimizer_and_2022,        author = {Kim, Hyeongchan},        month = {1},        title = {{pytorch_optimizer: optimizer and lr scheduler collections in PyTorch}},        version = {1.0.0},        year = {2022}    }Author------Hyeongchan Kim / `@kozistr &lt;http://kozistr.tech/about&gt;`__.. |workflow| image:: https://github.com/kozistr/pytorch_optimizer/actions/workflows/ci.yml/badge.svg?branch=main.. |Documentation Status| image:: https://readthedocs.org/projects/pytorch-optimizers/badge/?version=latest   :target: https://pytorch-optimizers.readthedocs.io/en/latest/?badge=latest.. |PyPI version| image:: https://badge.fury.io/py/pytorch-optimizer.svg   :target: https://badge.fury.io/py/pytorch-optimizer.. |PyPi download| image:: https://pepy.tech/badge/pytorch-optimizer   :target: https://pepy.tech/project/pytorch-optimizer.. |PyPi month download| image:: https://pepy.tech/badge/pytorch-optimizer/month   :target: https://pepy.tech/project/pytorch-optimizer.. |PyPI pyversions| image:: https://img.shields.io/pypi/pyversions/pytorch-optimizer.svg   :target: https://pypi.python.org/pypi/pytorch-optimizer/.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg.. |ruff| image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v1.json   :target: https://github.com/charliermarsh/ruff.. |codecov| image:: https://codecov.io/gh/kozistr/pytorch_optimizer/branch/main/graph/badge.svg?token=L4K00EA0VD   :target: https://codecov.io/gh/kozistr/pytorch_optimizer.. |apache| image:: https://img.shields.io/badge/License-Apache_2.0-blue.svg   :target: https://opensource.org/licenses/Apache-2.0</longdescription>
</pkgmetadata>