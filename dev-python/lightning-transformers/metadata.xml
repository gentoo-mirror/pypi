<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Deprecation notice ðŸ”’**This repository has been archived (read-only) on Nov 21, 2022**. Thanks to everyone who contributed to `lightning-transformers`, we feel it's time to move on.:hugs: Transformers can **already be easily trained using the Lightning :zap: Trainer**. Here's a recent example from the community: &lt;https://sachinruk.github.io/blog/deep-learning/2022/11/07/t5-for-grammar-correction.html&gt;. Note that there are **no limitations or workarounds**, things just work out of the box.The `lightning-transformers` repo explored the possibility to provide task-specific modules and pre-baked defaults, at the cost of introducing extra abstractions. In the spirit of keeping ourselves focused, these abstractions are not something we wish to continue supporting.If you liked `lightning-transformers` and want to continue developing it in the future, feel free to fork the repo and choose another name for the project.______________________________________________________________________&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/Lightning-AI/lightning-transformers/raw/0.2.5/docs/source/_static/images/logo.png&quot; width=&quot;500px&quot;&gt;**Flexible components pairing :hugs: Transformers with [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) :zap:**______________________________________________________________________&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://lightning-transformers.readthedocs.io/&quot;&gt;Docs&lt;/a&gt; â€¢  &lt;a href=&quot;#community&quot;&gt;Community&lt;/a&gt;&lt;/p&gt;______________________________________________________________________&lt;/div&gt;## Installation```bashpip install lightning-transformers```&lt;details&gt;&lt;summary&gt;From Source&lt;/summary&gt;```bashgit clone https://github.com/PyTorchLightning/lightning-transformers.gitcd lightning-transformerspip install .```&lt;/details&gt;______________________________________________________________________## What is Lightning-TransformersLightning Transformers provides `LightningModules`, `LightningDataModules` and `Strategies` to use :hugs: Transformers with the [PyTorch Lightning Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html).## Quick Recipes#### Train [bert-base-cased](https://huggingface.co/bert-base-cased) on the [CARER](https://huggingface.co/datasets/emotion) emotion dataset using the Text Classification task.```pythonimport pytorch_lightning as plfrom transformers import AutoTokenizerfrom lightning_transformers.task.nlp.text_classification import (    TextClassificationDataModule,    TextClassificationTransformer,)tokenizer = AutoTokenizer.from_pretrained(    pretrained_model_name_or_path=&quot;bert-base-cased&quot;)dm = TextClassificationDataModule(    batch_size=1,    dataset_name=&quot;emotion&quot;,    max_length=512,    tokenizer=tokenizer,)model = TextClassificationTransformer(    pretrained_model_name_or_path=&quot;bert-base-cased&quot;, num_labels=dm.num_classes)trainer = pl.Trainer(accelerator=&quot;auto&quot;, devices=&quot;auto&quot;, max_epochs=1)trainer.fit(model, dm)```#### Train a pre-trained [mt5-base](https://huggingface.co/google/mt5-base) backbone on the [WMT16](https://huggingface.co/datasets/wmt16) dataset using the Translation task.```pythonimport pytorch_lightning as plfrom transformers import AutoTokenizerfrom lightning_transformers.task.nlp.translation import (    TranslationTransformer,    WMT16TranslationDataModule,)tokenizer = AutoTokenizer.from_pretrained(    pretrained_model_name_or_path=&quot;google/mt5-base&quot;)model = TranslationTransformer(    pretrained_model_name_or_path=&quot;google/mt5-base&quot;,    n_gram=4,    smooth=False,    val_target_max_length=142,    num_beams=None,    compute_generate_metrics=True,)dm = WMT16TranslationDataModule(    # WMT translation datasets: ['cs-en', 'de-en', 'fi-en', 'ro-en', 'ru-en', 'tr-en']    dataset_config_name=&quot;ro-en&quot;,    source_language=&quot;en&quot;,    target_language=&quot;ro&quot;,    max_source_length=128,    max_target_length=128,    padding=&quot;max_length&quot;,    tokenizer=tokenizer,)trainer = pl.Trainer(accelerator=&quot;auto&quot;, devices=&quot;auto&quot;, max_epochs=1)trainer.fit(model, dm)```Lightning Transformers supports a bunch of :hugs: tasks and datasets. See the [documentation](https://lightning-transformers.readthedocs.io/en/latest/).## Billion Parameter Model Support### Big Model InferenceIt's really easy to enable large model support for the pre-built LightningModule :hugs: tasks.Below is an example to enable automatic model partitioning (across CPU/GPU and even leveraging disk space) to run text generation using a 6B parameter model.```pythonimport torchfrom accelerate import init_empty_weightsfrom transformers import AutoTokenizerfrom lightning_transformers.task.nlp.language_modeling import (    LanguageModelingTransformer,)with init_empty_weights():    model = LanguageModelingTransformer(        pretrained_model_name_or_path=&quot;EleutherAI/gpt-j-6B&quot;,        tokenizer=AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;),        low_cpu_mem_usage=True,        device_map=&quot;auto&quot;,  # automatically partitions the model based on the available hardware.    )output = model.generate(&quot;Hello, my name is&quot;, device=torch.device(&quot;cuda&quot;))print(model.tokenizer.decode(output[0].tolist()))```For more information see [Big Transformers Model Inference](https://lightning-transformers.readthedocs.io/en/latest/features/large_model.html).### Big Model Training with DeepSpeedBelow is an example of how you can train a 6B parameter transformer model using Lightning Transformers and DeepSpeed.```pythonimport pytorch_lightning as plfrom transformers import AutoTokenizerfrom lightning_transformers.task.nlp.language_modeling import (    LanguageModelingDataModule,    LanguageModelingTransformer,)tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=&quot;gpt2&quot;)model = LanguageModelingTransformer(    pretrained_model_name_or_path=&quot;EleutherAI/gpt-j-6B&quot;,    tokenizer=AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;),    deepspeed_sharding=True,  # defer initialization of the model to shard/load pre-train weights)dm = LanguageModelingDataModule(    batch_size=1,    dataset_name=&quot;wikitext&quot;,    dataset_config_name=&quot;wikitext-2-raw-v1&quot;,    tokenizer=tokenizer,)trainer = pl.Trainer(    accelerator=&quot;gpu&quot;,    devices=&quot;auto&quot;,    strategy=&quot;deepspeed_stage_3&quot;,    precision=16,    max_epochs=1,)trainer.fit(model, dm)```For more information see [DeepSpeed Training with Big Transformers Models](https://lightning-transformers.readthedocs.io/en/latest/features/large_model_training.html) or the [Model Parallelism](https://pytorch-lightning.readthedocs.io/en/latest/advanced/model_parallel.html#fully-sharded-training) documentation.## ContributePull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.Please make sure to update tests as appropriate.## CommunityFor help or questions, join our huge community on [Slack](https://www.pytorchlightning.ai/community)!</longdescription>
</pkgmetadata>