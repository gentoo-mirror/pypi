<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[//]: # (Logo:)&lt;div align=&quot;center&quot;&gt;https://user-images.githubusercontent.com/7593028/188328887-1b6cda72-2f41-439e-ae23-75dd3489bc24.mp4# PySR: High-Performance Symbolic Regression in Python&lt;/div&gt;PySR uses evolutionary algorithms to search for symbolic expressions which optimize a particular objective.&lt;div align=&quot;center&quot;&gt;| **Docs** | **colab** | **pip** | **conda** | **Stats** ||---|---|---|---|---||[![Documentation](https://github.com/MilesCranmer/PySR/actions/workflows/docs.yml/badge.svg)](https://astroautomata.com/PySR/)|[![Colab](https://img.shields.io/badge/colab-notebook-yellow)](https://colab.research.google.com/github/MilesCranmer/PySR/blob/master/examples/pysr_demo.ipynb)|[![PyPI version](https://badge.fury.io/py/pysr.svg)](https://badge.fury.io/py/pysr)|[![Conda Version](https://img.shields.io/conda/vn/conda-forge/pysr.svg)](https://anaconda.org/conda-forge/pysr)|&lt;div align=&quot;center&quot;&gt;pip: [![Downloads](https://pepy.tech/badge/pysr)](https://badge.fury.io/py/pysr)&lt;br&gt;conda: [![Anaconda-Server Badge](https://anaconda.org/conda-forge/pysr/badges/downloads.svg)](https://anaconda.org/conda-forge/pysr)&lt;/div&gt;|&lt;/div&gt;(pronounced like *py* as in python, and then *sur* as in surface)If you find PySR useful, please cite it using the citation information given in [CITATION.md](https://github.com/MilesCranmer/PySR/blob/master/CITATION.md).If you've finished a project with PySR, please submit a PR to showcase your work on the [Research Showcase page](https://astroautomata.com/PySR/papers)!&lt;div align=&quot;center&quot;&gt;### Test status| **Linux** | **Windows** | **macOS (intel)** ||---|---|---||[![Linux](https://github.com/MilesCranmer/PySR/actions/workflows/CI.yml/badge.svg)](https://github.com/MilesCranmer/PySR/actions/workflows/CI.yml)|[![Windows](https://github.com/MilesCranmer/PySR/actions/workflows/CI_Windows.yml/badge.svg)](https://github.com/MilesCranmer/PySR/actions/workflows/CI_Windows.yml)|[![macOS](https://github.com/MilesCranmer/PySR/actions/workflows/CI_mac.yml/badge.svg)](https://github.com/MilesCranmer/PySR/actions/workflows/CI_mac.yml)|| **Docker** | **Conda** | **Coverage** ||[![Docker](https://github.com/MilesCranmer/PySR/actions/workflows/CI_docker.yml/badge.svg)](https://github.com/MilesCranmer/PySR/actions/workflows/CI_docker.yml)|[![conda-forge](https://github.com/MilesCranmer/PySR/actions/workflows/CI_conda_forge.yml/badge.svg)](https://github.com/MilesCranmer/PySR/actions/workflows/CI_conda_forge.yml)|[![Coverage Status](https://coveralls.io/repos/github/MilesCranmer/PySR/badge.svg?branch=master&amp;service=github)](https://coveralls.io/github/MilesCranmer/PySR)|&lt;/div&gt;PySR is built on an extremely optimized pure-Julia backend: [SymbolicRegression.jl](https://github.com/MilesCranmer/SymbolicRegression.jl).Symbolic regression is a very interpretable machine learning algorithmfor low-dimensional problems: these tools search equation spaceto find algebraic relations that approximate a dataset.One can alsoextend these approaches to higher-dimensionalspaces by using a neural network as proxy, as explained in[2006.11287](https://arxiv.org/abs/2006.11287), where we applyit to N-body problems. Here, one essentially usessymbolic regression to convert a neural netto an analytic equation. Thus, these tools simultaneously presentan explicit and powerful way to interpret deep models.*Backstory:*Previously, we have used[eureqa](https://www.creativemachineslab.com/eureqa.html),which is a very efficient and user-friendly tool. However,eureqa is GUI-only, doesn't allow for user-definedoperators, has no distributed capabilities,and has become proprietary (and recently been merged into an onlineservice). Thus, the goalof this package is to have an open-source symbolic regression toolas efficient as eureqa, while also exposing a configurablepython interface.# Installation&lt;div align=&quot;center&quot;&gt;| pip - **recommended** &lt;br&gt; (works everywhere) | conda &lt;br&gt;(Linux and Intel-based macOS) | docker &lt;br&gt;(if all else fails) ||---|---|---|| 1. [Install Julia](https://julialang.org/downloads/)&lt;br&gt;2. Then, run: `pip install -U pysr`&lt;br&gt;3. Finally, to install Julia packages:&lt;br&gt;`python3 -c 'import pysr; pysr.install()'` | `conda install -c conda-forge pysr` | 1. Clone this repo.&lt;br&gt;2. `docker build -t pysr .`&lt;br&gt;Run with:&lt;br&gt;`docker run -it --rm pysr ipython`&lt;/div&gt;Common issues tend to be related to Python not finding Julia.To debug this, try running `python3 -c 'import os; print(os.environ[&quot;PATH&quot;])'`.If none of these folders contain your Julia binary, then you need to add Julia's `bin` folder to your `PATH` environment variable.**Running PySR on macOS with an M1 processor:** you should use the pip version, and make sure to get the Julia binary for ARM/M-series processors.# IntroductionYou might wish to try the interactive tutorial [here](https://colab.research.google.com/github/MilesCranmer/PySR/blob/master/examples/pysr_demo.ipynb), which uses the notebook in `examples/pysr_demo.ipynb`.In practice, I highly recommend using IPython rather than Jupyter, as the printing is much nicer.Below is a quick demo here which you can paste into a Python runtime.First, let's import numpy to generate some test data:```pythonimport numpy as npX = 2 * np.random.randn(100, 5)y = 2.5382 * np.cos(X[:, 3]) + X[:, 0] ** 2 - 0.5```We have created a dataset with 100 datapoints, with 5 features each.The relation we wish to model is $2.5382 \cos(x_3) + x_0^2 - 0.5$.Now, let's create a PySR model and train it.PySR's main interface is in the style of scikit-learn:```pythonfrom pysr import PySRRegressormodel = PySRRegressor(    niterations=40,  # &lt; Increase me for better results    binary_operators=[&quot;+&quot;, &quot;*&quot;],    unary_operators=[        &quot;cos&quot;,        &quot;exp&quot;,        &quot;sin&quot;,        &quot;inv(x) = 1/x&quot;,        # ^ Custom operator (julia syntax)    ],    extra_sympy_mappings={&quot;inv&quot;: lambda x: 1 / x},    # ^ Define operator for SymPy as well    loss=&quot;loss(prediction, target) = (prediction - target)^2&quot;,    # ^ Custom loss function (julia syntax))```This will set up the model for 40 iterations of the search code, which contains hundreds of thousands of mutations and equation evaluations.Let's train this model on our dataset:```pythonmodel.fit(X, y)```Internally, this launches a Julia process which will do a multithreaded search for equations to fit the dataset.Equations will be printed during training, and once you are satisfied, you mayquit early by hitting 'q' and then \&lt;enter\&gt;.After the model has been fit, you can run `model.predict(X)`to see the predictions on a given dataset using the automatically-selected expression,or, for example, `model.predict(X, 3)` to see the predictions of the 3rd equation.You may run:```pythonprint(model)```to print the learned equations:```pythonPySRRegressor.equations_ = [   pick     score                                           equation       loss  complexity0        0.000000                                          4.4324794  42.354317           11        1.255691                                          (x0 * x0)   3.437307           32        0.011629                          ((x0 * x0) + -0.28087974)   3.358285           53        0.897855                              ((x0 * x0) + cos(x3))   1.368308           64        0.857018                ((x0 * x0) + (cos(x3) * 2.4566472))   0.246483           85  &gt;&gt;&gt;&gt;       inf  (((cos(x3) + -0.19699033) * 2.5382123) + (x0 *...   0.000000          10]```This arrow in the `pick` column indicates which equation is currently selected by your`model_selection` strategy for prediction.(You may change `model_selection` after `.fit(X, y)` as well.)`model.equations_` is a pandas DataFrame containing all equations, including callable format(`lambda_format`),SymPy format (`sympy_format` - which you can also get with `model.sympy()`), and even JAX and PyTorch format(both of which are differentiable - which you can get with `model.jax()` and `model.pytorch()`).Note that `PySRRegressor` stores the state of the last search, and will restart from where you left off the next time you call `.fit()`, assuming you have set `warm_start=True`.This will cause problems if significant changes are made to the search parameters (like changing the operators). You can run `model.reset()` to reset the state.You will notice that PySR will save two files: `hall_of_fame...csv` and `hall_of_fame...pkl`.The csv file is a list of equations and their losses, and the pkl file is a saved state of the model.You may load the model from the `pkl` file with:```pythonmodel = PySRRegressor.from_file(&quot;hall_of_fame.2022-08-10_100832.281.pkl&quot;)```There are several other useful features such as denoising (e.g., `denoising=True`),feature selection (e.g., `select_k_features=3`).For examples of these and other features, see the [examples page](https://astroautomata.com/PySR/examples).For a detailed look at more options, see the [options page](https://astroautomata.com/PySR/options).You can also see the full API at [this page](https://astroautomata.com/PySR/api).There are also tips for tuning PySR on [this page](https://astroautomata.com/PySR/tuning).## Detailed ExampleThe following code makes use of as many PySR features as possible.Note that is just a demonstration of features and you should not use this example as-is.For details on what each parameter does, check out the [API page](https://astroautomata.com/PySR/api/).```pythonmodel = PySRRegressor(    procs=4,    populations=8,    # ^ 2 populations per core, so one is always running.    population_size=50,    # ^ Slightly larger populations, for greater diversity.    ncyclesperiteration=500,     # ^ Generations between migrations.    niterations=10000000,  # Run forever    early_stop_condition=(        &quot;stop_if(loss, complexity) = loss &lt; 1e-6 &amp;&amp; complexity &lt; 10&quot;        # Stop early if we find a good and simple equation    ),    timeout_in_seconds=60 * 60 * 24,    # ^ Alternatively, stop after 24 hours have passed.    maxsize=50,    # ^ Allow greater complexity.    maxdepth=10,    # ^ But, avoid deep nesting.    binary_operators=[&quot;*&quot;, &quot;+&quot;, &quot;-&quot;, &quot;/&quot;],    unary_operators=[&quot;square&quot;, &quot;cube&quot;, &quot;exp&quot;, &quot;cos2(x)=cos(x)^2&quot;],    constraints={        &quot;/&quot;: (-1, 9),        &quot;square&quot;: 9,        &quot;cube&quot;: 9,        &quot;exp&quot;: 9,    },    # ^ Limit the complexity within each argument.    # &quot;inv&quot;: (-1, 9) states that the numerator has no constraint,    # but the denominator has a max complexity of 9.    # &quot;exp&quot;: 9 simply states that `exp` can only have    # an expression of complexity 9 as input.    nested_constraints={        &quot;square&quot;: {&quot;square&quot;: 1, &quot;cube&quot;: 1, &quot;exp&quot;: 0},        &quot;cube&quot;: {&quot;square&quot;: 1, &quot;cube&quot;: 1, &quot;exp&quot;: 0},        &quot;exp&quot;: {&quot;square&quot;: 1, &quot;cube&quot;: 1, &quot;exp&quot;: 0},    },    # ^ Nesting constraints on operators. For example,    # &quot;square(exp(x))&quot; is not allowed, since &quot;square&quot;: {&quot;exp&quot;: 0}.    complexity_of_operators={&quot;/&quot;: 2, &quot;exp&quot;: 3},    # ^ Custom complexity of particular operators.    complexity_of_constants=2,    # ^ Punish constants more than variables    select_k_features=4,    # ^ Train on only the 4 most important features    progress=True,    # ^ Can set to false if printing to a file.    weight_randomize=0.1,    # ^ Randomize the tree much more frequently    cluster_manager=None,    # ^ Can be set to, e.g., &quot;slurm&quot;, to run a slurm    # cluster. Just launch one script from the head node.    precision=64,    # ^ Higher precision calculations.    warm_start=True,    # ^ Start from where left off.    turbo=True,    # ^ Faster evaluation (experimental)    julia_project=None,    # ^ Can set to the path of a folder containing the    # &quot;SymbolicRegression.jl&quot; repo, for custom modifications.    update=False,    # ^ Don't update Julia packages    extra_sympy_mappings={&quot;cos2&quot;: lambda x: sympy.cos(x)**2},    # extra_torch_mappings={sympy.cos: torch.cos},    # ^ Not needed as cos already defined, but this    # is how you define custom torch operators.    # extra_jax_mappings={sympy.cos: &quot;jnp.cos&quot;},    # ^ For JAX, one passes a string.)```# DockerYou can also test out PySR in Docker, withoutinstalling it locally, by running the following command inthe root directory of this repo:```bashdocker build -t pysr .```This builds an image called `pysr` for your system's architecture,which also contains IPython.You can then run this with:```bashdocker run -it --rm -v &quot;$PWD:/data&quot; pysr ipython```which will link the current directory to the container's `/data` directoryand then launch ipython.If you have issues building for your system's architecture,you can emulate another architecture by including `--platform linux/amd64`,before the `build` and `run` commands.</longdescription>
</pkgmetadata>