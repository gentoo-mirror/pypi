<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>.. -*- mode: rst -*-|Azure|_ |CirrusCI|_ |Codecov|_ |CircleCI|_ |Nightly wheels|_ |Black|_ |PythonVersion|_ |PyPi|_ |DOI|_ |Benchmark|_.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main.. _Azure: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&amp;branchName=main.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield.. _CircleCI: https://circleci.com/gh/scikit-learn/scikit-learn.. |CirrusCI| image:: https://img.shields.io/cirrus/github/scikit-learn/scikit-learn/main?label=Cirrus%20CI.. _CirrusCI: https://cirrus-ci.com/github/scikit-learn/scikit-learn/main.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9.. _Codecov: https://codecov.io/gh/scikit-learn/scikit-learn.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=schedule.. _`Nightly wheels`: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule.. |PythonVersion| image:: https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10-blue.. _PythonVersion: https://pypi.org/project/scikit-learn/.. |PyPi| image:: https://img.shields.io/pypi/v/scikit-learn.. _PyPi: https://pypi.org/project/scikit-learn.. |Black| image:: https://img.shields.io/badge/code%20style-black-000000.svg.. _Black: https://github.com/psf/black.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg.. _DOI: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue.. _`Benchmark`: https://scikit-learn.org/scikit-learn-benchmarks/.. |PythonMinVersion| replace:: 3.8.. |NumPyMinVersion| replace:: 1.17.3.. |SciPyMinVersion| replace:: 1.5.0.. |JoblibMinVersion| replace:: 1.1.1.. |ThreadpoolctlMinVersion| replace:: 2.0.0.. |MatplotlibMinVersion| replace:: 3.1.3.. |Scikit-ImageMinVersion| replace:: 0.16.2.. |PandasMinVersion| replace:: 1.0.5.. |SeabornMinVersion| replace:: 0.9.0.. |PytestMinVersion| replace:: 5.3.1.. |PlotlyMinVersion| replace:: 5.10.0=================Scikit-learn-tree=================``scikit-learn-tree`` is an alias of scikit-learn, released under the namespace `sklearn_fork`. It is a maintained fork of scikit-learn, which advances the tree submodule, while staying in-linewith changes from upstream scikit-learn. It is an exact stand-in for ``sklearn_fork`` in package imports, but isreleased under the name ``scikit-learn-tree`` to avoid confusion.It is currently maintained by a team of volunteers.The upstream package **scikit-learn** is a Python module for machine learning built on top ofSciPy and is distributed under the 3-Clause BSD license. Refer to their website for all documentationneeds: https://scikit-learn.org.Why a fork?-----------Currently, the scikit-learn tree submodule is difficult to extend. Requests to modularizeand improve the extensibility of the code is currently unsupported, or may take a long time.The desire for advanced tree models that also leverage the robustness of scikit-learn is desirable.However, &quot;hard-forking&quot; via copy/pasting the explicit Python/Cython code into another tree packagealtogether is undesirable because it results in a tree codebase that is inherently differentand not compatible with ``scikit-learn``. For example, `quantile-forests &lt;https://github.com/zillow/quantile-forest&gt;`_,and `EconML &lt;https://github.com/py-why/EconML&gt;`_ do this, and their current tree submodulescannot take advantage of improvements made in upstream ``scikit-learn``.An example of seamless integration would be `scikit-survival &lt;https://github.com/sebp/scikit-survival&gt;`_, whichonly needs to implement a subclass of the Cython ``Criterion`` oject in their code to enable survival trees.Maintaining a &quot;soft-fork&quot; of ``scikit-learn`` in the form of a repository fork allows us to developa separate package that serves as a stand-in for ``sklearn_fork`` in any package, extends the tree submoduleand can also be synced with upstream changes in ``scikit-learn``. This enables this fork to alwaystake advantage of improvements made in ``scikit-learn`` main upstream, while providing a customizabletree API.Installation------------Dependencies~~~~~~~~~~~~scikit-learn-tree requires:- Python (&gt;= |PythonMinVersion|)- NumPy (&gt;= |NumPyMinVersion|)- SciPy (&gt;= |SciPyMinVersion|)- joblib (&gt;= |JoblibMinVersion|)- threadpoolctl (&gt;= |ThreadpoolctlMinVersion|)============================Installing scikit-learn-tree============================Scikit-learn-tree is a maintained fork of scikit-learn, which extends thetree submodule in a few ways documented in `fork_changelog`_. We release versions of scikit-learn-tree in an analagous fashion toscikit-learn main. Due to maintenance resources, we only release on PyPiand recommend therefore installing with ``pip``.There are different ways to install scikit-learn-tree:  * Install the latest official release `install_fork_release`_. This    is the best approach for most users. It will provide a stable version    and pre-built packages are available for most platforms.      * Building the package from source `install_source`_. This is best for users who want the    latest-and-greatest features and aren't afraid of running    brand-new code. This is also needed for users who wish to contribute to the    project... _install_fork_release:Installing the latest release-----------------------------We release wheels for common distributions and this is thus installable via pip.    pip install scikit-learn-treeThis will install ``scikit-learn-tree`` under the namespace of ``sklearn_fork``, which thencan be used as a stand-in for any package that relies on the public API of ``sklearn_fork``.For example, any usage of ``scikit-learn`` is preserved with ``scikit-learn-tree``  &gt;&gt;&gt; # the sklearn_fork installed is that of scikit-learn-tree and is equivalent to scikit-learn  &gt;&gt;&gt; from sklearn_fork.ensemble import RandomForestClassifier  &gt;&gt;&gt; clf = RandomForestClassifier(random_state=0)  &gt;&gt;&gt; X = [[ 1,  2,  3],  # 2 samples, 3 features  ...      [11, 12, 13]]  &gt;&gt;&gt; y = [0, 1]  # classes of each sample  &gt;&gt;&gt; clf.fit(X, y)  RandomForestClassifier(random_state=0).. _install_source:Building from source--------------------If you are a developer and are interested in helping maintain, or add some newfeatures to the fork, the building from source instructions are exactly the sameas that of scikit-learn main, so please refer to `scikit-learn documentation &lt;https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge&gt;`_for instructions on building from source.===========Development-----------We welcome new contributors of all experience levels, specifically to maintain the fork.Any contributions that make sure our fork is &quot;better in-line&quot; with scikit-learn upstream,or improves the tree submodule in anyway will be appreciated.The scikit-learn community goals are to be helpful, welcoming, and effective. The`Development Guide &lt;https://scikit-learn.org/stable/developers/index.html&gt;`_has detailed information about contributing code, documentation, tests, andmore. We've included some basic information in this README.=========================.. _fork_changelog:Major Changes of the Fork-------------------------The purpose of this page is to illustrate some of the main features that``scikit-learn-tree`` provides compared to ``scikit-learn``. It assumes aan understanding of core package ``scikit-learn`` and also decision treesmodels. Please refer to our installation instructions `install_fork_release`_ for installing ``scikit-learn-tree``.Scikit-learn-tree though operates as a stand-in for upstream ``scikit-learn``.It is used in packages exactly the same way and will support all featuresin the corresponding version of ``scikit-learn``. For example, if youare interested in features of ``scikit-learn`` in v1.2.2 for ``NearestNeighbors`` algorithm,then if ``scikit-learn-tree`` has a version release of v1.2.2, then it will haveall those features. The breaking API changes will be with respect to anything in the ``tree`` submodule,and related Forest ensemble models. See below for a detailed list of breaking changes.See: https://scikit-learn.org/ for documentation on scikit-learn main.Our Philosophy--------------Our design philosophy with this fork of ``scikit-learn`` is to maintain as few changesas possible, such that incorporating upstream changes into the fork requires minimal effort.Candidate changes and PRs accepted into the fork are those that:- improve compatability with upstream ``scikit-learn`` main- enable improved extensibility of tree modelsDecision tree generalizations-----------------------------``Scikit-learn`` provides an axis-aligned `sklearn_fork.tree.DecisionTreeClassifier &lt;https://scikit-learn.org/stable/modules/generated/sklearn_fork.tree.DecisionTreeClassifier.html&gt;`_decision tree model (classifier and regressor), which has a few fundamental limitationsthat prevent 3rd parties from utilizing the existing class, without forking a largeamount of copy/pasted Python and Cython code. We highlight those limitations hereand then describe how we generalize that limitation.Cython Internal Private API:Note, the Cython API for scikit-learn is still not a publicly supported API, so it maychange without warning.- leaf and split nodes: These nodes are treated the same way and there is no internal  API for setting them differently. Quantile trees and causal trees inherently generalize  how leaf nodes are set.- Criterion class: The criterion class currently assumes a supervised learning interface.  - Our fix: We implement a ``BaseCriterion`` object that provides an abstract API for unsupervised criterion.- Splitter class: The splitter clas currently assumes a supervised learning interface and  does not provide a way of generalizing the way split candidates are proposed.  - Our fix: We implement a ``BaseSplitter`` object that provides an abstract API for unsupervised splitters and also implement an API to allow generalizations of the ``SplitRecord`` struct and ``Splitter.node_split`` function. For example, this enables oblique splits to be considered.- Tree class: The tree class currently assumes a supervised learning interface and does not  provide a way of generalizing the type of tree.  - Our fix: We implementa ``BaseTree`` object that provides an abstract API for general tree models and also implement an API that allows generalization of the type of tree. For example, oblique trees are trivially implementable as an extension now.- stopping conditions for splitter: Currently, the ``Splitter.node_split`` function has various  stopping conditions for the splitter based on hyperparameters. It is plausible that these conditions  may be extended. For example, in causal trees, one may want the splitter to also account for  a minimal degree of heterogeneity (i.e. variance) in its children nodes. Python API:- ``sklearn_fork.tree.BaseDecisionTree`` assumes the underlying tree model is supervised: The ``y``  parameter is required to be passed in, which is not necessary for general tree-based models.  For example, an unsupervised tree may pass in ``y=None``.  - Our fix: We fix this API, so the ``BaseDecisionTree`` is subclassable by unsupervised tree models that do not require ``y`` to be defined.- ``sklearn_fork.tree.BaseDecisionTree`` does not provide a way to generalize the ``Criterion``, ``Splitter``  and ``Tree`` Cython classes used: The current codebase requires users to define custom  criterion and/or splitters outside the instantiation of the ``BaseDecisionTree``. This prevents  users from generalizing the ``Criterion`` and ``Splitter`` and creating a neat Python API wrapper.  Moreover, the ``Tree`` class is not customizable.  - Our fix: We internally implement a private function to actually build the entire tree, ``BaseDecisionTree._build_tree``, which can be overridden in subclasses that customize the criterion, splitter, or tree, or any combination of them.- ``sklearn_fork.ensemble.BaseForest`` and its subclass algorithms are slow when ``n_samples`` is very high. Binning  features into a histogram, which is the basis of &quot;LightGBM&quot; and &quot;HistGradientBoostingClassifier&quot; is a computational  trick that can both significantly increase runtime efficiency, but also help prevent overfitting in trees, since  the sorting in &quot;BestSplitter&quot; is done on bins rather than the continuous feature values. This would enable  random forests and their variants to scale to millions of samples.  - Our fix: We added a ``max_bins=None`` keyword argument to the ``BaseForest`` class, and all its subclasses. The default behavior is no binning. The current implementation is not necessarily efficient. There are several improvements to be made. See below.Overall, the existing tree models, such as `sklearn_fork.tree.DecisionTreeClassifier &lt;https://scikit-learn.org/stable/modules/generated/sklearn_fork.tree.DecisionTreeClassifier.html&gt;`_and `sklearn_fork.ensemble.RandomForestClassifier &lt;https://scikit-learn.org/stable/modules/generated/sklearn_fork.ensemble.RandomForestClassifier.html#sklearn_fork.ensemble.RandomForestClassifier&gt;`_ all work exactly the same as theywould in ``scikit-learn`` main, but these extensions enable 3rd-party packages to extendthe Cython/Python API easily.Roadmap-------There are several improvements that can be made in this fork. Primarily, the binning featurepromises to make Random Forests and their variants ultra-fast. However, the binning needsto be implemented in a similar fashion to ``HistGradientBoostingClassifier``, which passesin the binning thresholds throughout the tree construction step, such that the split nodesstore the actual numerical value of the bin rather than the &quot;bin index&quot;. This requiresmodifying the tree Cython code to take in a ``binning_thresholds`` parameter that is partof the ``_BinMapper`` fitted class. This also allows us not to do any binning during prediction/applytime because the tree already stores the &quot;numerical&quot; threshold value we would want to applyto any incoming ``X`` that is not binned.Besides that modification, the tree and splitter need to be able to handle not just ``np.float32``data (the type for X normally in Random Forests), but also ``uint8`` data (the type for X when itis binned in to e.g. 255 bins). This would not only save RAM since ``uint8`` storage of millionsof samples would result in many GB saved, but also improved runtime.So in summary, the Cython code of the tree submodule needs to take in an extra parameter forthe binning thresholds if binning occurs and also be able to handle ``X`` being of dtype ``uint8``.Afterwards, Random Forests will have fully leveraged the binning feature.Something to keep in mind is that upstream scikit-learn is actively working on incorporatingmissing-value handling and categorical handling into Random Forests.Next steps----------We have briefly covered how the tree submodule has changed with respect to ``scikit-learn``.This enables packages to leverage these changes in developing more complex tree modelsthat may, or may not eventually be PRed into ``scikit-learn``. For example,- `scikit-tree &lt;https://docs.neurodata.io/scikit-tree/dev/index.html&gt;`_ is a scikit-learn  compatible package for more complex and advanced tree models.If you are developing tree models, we encourage you to take a look at that package, orif you have suggestions to make the tree submodule of our fork, ``scikit-learn-tree``more </longdescription>
</pkgmetadata>