<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># FastAPI AsyncPGFastAPI integration for AsyncPG## NarrativeFirst of all, so sorry for my poor english. I will be so happy,if someone pushes a PR correcting all my english mistakes. AnywayI will try to do my best.Looking at fastapi ecosystem seems like everyone is trying to integratefastapi with orms, but from my experience working with rawsql I'm so productive.If you think a bit around, your real model layer, is the schema on yourdb (you can add abastractions on top of it), but what endsis your data, and these are tables, columns and rows.Also, sql, it's one of the best things I learnedbecause it's something that always is there.On another side, postgresql it's robust and rock solid,thousands of projects depend on it, and use it as their storage layer.AsyncPG it's a crazy fast postgresql driverwritten from scratch.FastAPI seems like a clean, and developer productive approach to webframeworks. It's crazy how well it integrates with OpenAPI,and how easy makes things to a developer to move on.## Integrationfastapi_asyncpg trys to integrate fastapi and asyncpg in an idiomatic way.fastapi_asyncpg when configured exposes two injectable providers tofastapi path functions, can use:- `db.connection` : it's just a raw connection picked from the pool,  that it's auto released when pathfunction ends, this is mostly  merit of the DI system around fastapi.- `db.transaction`: the same, but wraps the pathfuncion on a transaction  this is more or less the same than the `atomic` decorator from Django.  also `db.atomic` it's aliased```pythonfrom fastapi import FastAPIfrom fastapi import Dependsfrom fastapi_asyncpg import configure_asyncpgapp = FastAPI()# we need to pass the fastapi app to make use of lifespan asgi eventsdb = configure_asyncpg(app, &quot;postgresql://postgres:postgres@localhost/db&quot;)@db.on_initasync def initialization(conn):    # you can run your db initialization code here    await conn.execute(&quot;SELECT 1&quot;)@app.get(&quot;/&quot;)async def get_content(db=Depends(db.connection)):    rows = await db.fetch(&quot;SELECT wathever FROM tablexxx&quot;)    return [dict(r) for r in rows]@app.post(&quot;/&quot;)async def mutate_something_compled(db=Depends(db.atomic))    await db.execute()    await db.execute()    # if something fails, everyting is rolleback, you know all or nothing```And there's also an `initialization` callable on the main factory function.That can be used like in flask to initialize whatever you need on the db.The `initialization` is called right after asyncpg stablishes a connection,and before the app fully boots. (Some projects use this as a poor migrationrunner, not the best practice if you are deploying multipleinstances of the app).## TestingFor testing we use [pytest-docker-fixtures](https://pypi.org/project/pytest-docker-fixtures/), it requires docker on the host machine or on whatever CI you use(seems like works as expected with github actions)It works, creating a container for the session and exposing it as pytest fixture.It's a good practice to run tests with a real database, andpytest-docker-fixtures make it's so easy. As a bonus, all fixtures run on a CI.We use Jenkins witht docker and docker, but also seems like travis and github actionsalso work.The fixture needs to be added to the pytest plugins `conftest.py` file.on conftest.py```pythonpytest_plugins = [    &quot;pytest_docker_fixtures&quot;,]```With this in place, we can just yield a pg fixture```pythonfrom pytest_docker_fixtures import images# image params can be configured from hereimages.configure(    &quot;postgresql&quot;, &quot;postgres&quot;, &quot;11.1&quot;, env={&quot;POSTGRES_DB&quot;: &quot;test_db&quot;})# and then on our test we have a pg container running# ready to recreate our dbasync def test_pg(pg):    host, port = pg    dsn = f&quot;postgresql://postgres@{host}:{port}/test_db&quot;    await asyncpg.Connect(dsn=dsn)    # let's go```With this in place, we can just create our own pytest.fixture that_patches_ the app dsn to make it work with our custom createdcontainer.````pythonfrom .app import app, dbfrom async_asgi_testclient import TestClientimport pytestpytestmark = pytest.mark.asyncio@pytest.fixtureasync def asgi_app(pg)    host, port = pg    dsn = f&quot;postgresql://postgres@{host}:{port}/test_db&quot;    # here we patch the dsn for the db    # con_opts: are also accessible    db.dsn = dsn    yield app, dbasync def test_something(asgi_app):    app, db = asgi_app    async with db.pool.acquire() as db:        # setup your test state    # this context manager handlers lifespan events    async with TestClient(app) as client:        res = await client.request(&quot;/&quot;)```Anyway if the application will grow, to multiples subpackages,and apps, we trend to build the main app as a factory, thatcreates it, something like:```pythonfrom fastapi_asyncpg import configure_asyncpgfrom apppackage import settingsimport venusiandef make_asgi_app(settings):    app = FastAPI()    db = configure_asyncpg(settings.DSN)    scanner = venusian.Scanner(app=app)    venusian.scan(theapp)    return app````Then on the fixture, we just need, to factorze and app from our function```pythonfrom .factory import make_asgi_appfrom async_asgi_testclient import TestClientimport pytestpytestmark = pytest.mark.asyncio@pytest.fixtureasync def asgi_app(pg)    host, port = pg    dsn = f&quot;postgresql://postgres@{host}:{port}/test_db&quot;    app = make_asgi_app({&quot;dsn&quot;: dsn})    # ther's a pointer on the pool into app.state    yield appasync def test_something(asgi_app):    app = asgi_app    pool = app.state.pool    async with db.pool.acquire() as db:        # setup your test state    # this context manager handlers lifespan events    async with TestClient(app) as client:        res = await client.request(&quot;/&quot;)```There's also another approach exposed and used on [tests](tests/test_db.py),that exposes a single connection to the test and rolls back changes on end.We use this approach on a large project (500 tables per schema andmultiples schemas), and seems like it speeds up a bit test creation.This approach is what [Databases](https://www.encode.io/databases/) it's using.Feel free to follow the tests to see if it feets better.## ExtrasThere are some utility functions I daily use with asyncpg that helps mespeed up some sql operations like, they are all on sql.py, and mostly areself documented. They are in use on tests.### Authors`fastapi_asyncpg` was written by `Jordi collell &lt;jordic@gmail.com&gt;`\_.</longdescription>
</pkgmetadata>