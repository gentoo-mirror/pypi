<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Aboutnewslinkrss generates RSS feeds for websites that do not provide their own.It works by loading URLs with lists of articles, looking for links matchingregular expressions, and optionally visiting them to get more informationand even processing these target pages with XPath and CSS Selectors ifnecessary. It is basically a purpose specific crawler or scraper.The results are printed as a RSS feed to stdout or optionally to a file. Thesimplest way to use it is just configure your **local** feed reader, like[Liferea](https://lzone.de/liferea/) or [Newsboat](https://newsboat.org/), touse a &quot;command&quot; source and pass the correct command line arguments to generatea suitable feed -- this allows you to centralize the configuration in thereader itself and let it handle update times, etc.Run `newslinkrss --help` for the complete list of command line options.# Intended audience (and a rant)This script is mostly intended to technically versed people using some kindof Unix operating system (e.g. Linux). I was planning to write a detaileddocumentation but I just gave up. There is no much hope of making it friendlyto casual users when every use case requires complex command lines, abuse ofregular expressions, strftime strings, XPath, CSS Selectors, etc.## The RantEverything would be just easier if websites simply provided clean and completeRSS or Atom feeds, but these sites are becoming rarer every day. Most sitesassume we want to follow them through social media (or that we *use* socialmedia!) and expect us to give away our privacy and submit ourselves totracking and personal data collection in exchange for timelinesalgorithmically optimized to improve &quot;engagement&quot; with advertisers.I'm still resisting and wrote lots of feed scrapers/filters in the last 18 orso years; newslinkrss is one that replaced several of these ad-hoc filters bycentralizing some very common pieces of code and it is polished enough to bepublished.# Installation## Installing from PyPIThis is the simplest installation method and will deliver the latest **stable**version of the program. To install in your `$HOME` directory, just type:    pip3 install newslinkrssThis assumes pip3 is installed and configured (which usually happens bydefault in Linux distributions intended for general use).## Installing from the Git repositoryThis installation process is recommended for developers and people wantingthe newest version. For that, just clone the Git repository and install orupdate the program with:    pip3 install -U .You may want to do this in a virtual environment so it won't interfere withother user-level or system-wide components and make experimentation withdevelopment versions easier. In this case, just do:    python3 -m venv my-venv    . my-venv/bin/activate    pip install -U .newslinkrss depends on a few libraries, this will ensure all them are alsoinstalled correctly.## Installing from the Snapnewslinkrss is also available as [a Snap](https://snapcraft.io/newslinkrss).This makes easy to always have the latest version, updated automatically, anddo not requires creating Python virtual environments or touching user orsystem level Python modules. To install in any Snap-enabled distribution, run:    sudo snap install newslinkrssThis will install and follow to the latest stable version; if you want toalways have the latest development version, freshly built from the Git masterbranch, install it from the &quot;edge&quot; channel with:    sudo snap install newslinkrss --edge# Usage examplesnewslinkrss is lacking a lot of documentation, but the following examplescan show a bit of what it can and can not do. To make understanding easier,examples uses mostly the long, verbose, form of some options even whenabbreviations are available. A complete list of options, abbreviations,default values, etc. can be read by typing `newslinkrss --help`.### Simplest caseThe simplest use case is just load a website, select all links matching apattern and expose a feed using the text of that link as description andsetting the publish date to the date and time that the command was run. Forexample, to generate a feed from site https://www.jaraguadosul.sc.gov.br/noticias.php ,collecting all links with the substring `/news/` in the URL, use:    newslinkrss -p '.+/news/.+' https://www.jaraguadosul.sc.gov.br/noticias.phpIt won't generate a good feed (the limitation with the dates being thebiggest issue) but we will go back to this example later. It is alreadymore practical than checking this government website manually, however.### Following pagesTo improve the situation exposed in the previous example, we may want toget information that it not is available from the URL or anchor text.Option `--follow` (shortcut `-f`) will make newslinkrss load the candidatetarget page and look for more data there. By default, it automaticallycaptures the title of the page as the title of the feed entry, keeps thesummary from anchor text, and loads author information and the pagepublishing and update dates and times from the page metadata (**if** thisinformation is available in some common format, like[Open Graph](https://ogp.me/ ) or Twitter cards.Reuters [killed](https://news.ycombinator.com/item?id=23576022) its RSS feedsin mid 2020, so let's take them as an example and use newslinkrss to bringthe feed back to life and right into our news readers. Our criteria will be:- First we must find every link that appears as plain HTML on the front page:  https://www.reuters.com/ There is an infinite scroll and more links are  added periodically with JavaScript, but we can just ignore this and poll  the page more frequently, giving enough chance to capture them;- We want to be very selective with filtering so we only get the current news  and do not waste time downloading things like section listings, utility  pages, links for other domains, etc. By looking at the URLs of the news  pages, we can notice that all of them follow a pattern similar to:  https://www.reuters.com/world/europe/eu-proposes-create-solidarity-fund-ukraines-basic-needs-2022-03-18/  Notice they all are in domain &quot;https://www.reuters.com/&quot;, have at least one  section (&quot;/world/europe/&quot;) that is followed by part of the title and the  publish date. This format is really great, as it allows us to ignore  anything that does not look like a news article on the spot. So, a good  pattern will be `'https://www.reuters.com/.+/.+\d{4}-\d{2}-\d{2}.+'`;- newslinkrss deduplicates URLs automatically, so we don't need to worry if  we end up capturing the same link twice;- Target pages have Open Graph meta tags, so by just following them we can  get accurate publish dates and times with no extra effort. Better yet, as  we know that **all** news pages that we want have them, we can also instruct  newslinkrss to ignore any page without a valid date, preventing any non-news  article, captured by accident, from appearing in our feed. This is done  with option `--require-dates`;- All page titles are something like &quot;The actual headline | Reuters&quot;. This  format is nice for a website but not so for feed items: the &quot;| Reuters&quot;  part is not only redundant (the feed title already tells us about the  source of the article) but also noisy, as it makes scanning through the  headlines harder. newslinkrss has an option `--title-regex` for exactly  this use case of cleaning up redundant text from titles. It accepts a  regular expression with a single capture group; if the expression matches,  the text from the group will be used as the title, otherwise the original  text will be used (so we don't loose titles if something changes at the  source, for example). For this case, a good choice would be  `--title-regex '(.+)\s+\|'` .- When following pages we must be **very careful** as we do not want to  abuse the website or get stuck downloading gigabytes of data. newslinkrss  has several options to prevent these problems, all with sensible default  values, but we should check if they work for every use case. At first we  must limit number of links to be followed with option `--max-links` (the  default value is 50, so it is OK for this so we can just omit the option  for now), then we may use option `--max-page-length` to only load the  first 512 kB of data from the every followed link, and stop processing a  page after after a few seconds with option `--http-timeout` (the default  value is 2 s, so we can omit this option for now too);- Cookies will be remembered among these requests, but they will only be  kept in memory and forgotten once the program finishes (there is an  option `--no-cookies` if this behavior becomes a problem for a particular  source).So, our syntax for this will be:    newslinkrss \        --follow \        -p 'https://www.reuters.com/.+/.+\d{4}-\d{2}-\d{2}.+' \        --max-page-length 512 \        --require-dates \        --title-regex '(.+)\s+\|' \        https://www.reuters.com/### Generating complete feedsComplete feeds are the ones which include the full text of the article withit, instead of just a summary and a link. They are really nice as we canread everything in the news aggregator itself. A good item body should bemostly static and clean HTML (so no scripts, interactive content, aggressiveformatting, etc.) leaving everything else to the aggregator to handle.So let's extend the previous example from Reuters website: as we are alreadyfollowing and downloading the links, there is no much extra work to generatethe full feed from it. Option `--with-body` will copy the entire contents ofthe &quot;body&quot; element from the page into the feed, just removing a few obviouslyunwanted tags (scripts, forms, etc.).Including the entire body works for every case, but for this site we canfilter a bit more and pick only actual text of the news article, ignoringunwanted noise like menus, sidebars, links to other news, etc. Running aquick &quot;inspect element&quot; in Firefox shows us that there is a single &quot;article&quot;element in the pages and that it has the text we want. newslinkrss allowsusing both XPath expressions and CSS Selectors to pick particular elementsfrom the DOM and, for this case, we choose XPath by using option`--body-xpath '//article'` — sometimes CSS Selectors are easier andcleaner, if it appears that you are struggling too much with a particularXPath expression, try using a CSS Selector with `--body-csss` instead.So, the updated syntax will be:    newslinkrss \        --follow \        -p 'https://www.reuters.com/.+/.+\d{4}-\d{2}-\d{2}.+' \        --max-page-length 512 \        --with-body \        --body-xpath '//article' \        --require-dates \        --title-regex '(.+)\s+\|' \        https://www.reuters.com/And now we have our feed!The body for this example is very simple but the selectors (both XPath andCSSS) are surprisingly powerful. They can return any number of elements inany order, so you can create a readable item body from whatever exists inthe source page by carefully picking elements in the right order (XPathoperator &quot;|&quot; and CSSS &quot;,&quot; are your friends!).### Cleaning body elements (and handling bad CSS class names)If we look at the bodies captured with the previous Reuters example, we willnotice that (at least in July 2023 when this text is being written) there arestill some unwanted elements that add too much noise to the output. Insidethe `article` element, we also get:- Social media share buttons: actually, not the buttons themselves because  for elements are removed by default, but the `ul` and `li` elements that  encapsulated them remain -- newslinkrss simple does not know which ones  are useful and which ones are not, so it can not remove them automatically;- Related articles and &quot;Read next&quot; sections: we want only the text of the  current article in a clean and readable format.Usually it is very easy to get rid of these using options `--body-remove-xpath`(shortcut `-X`) and `--body-remove-csss` (shortcut `-C`), but this particularsite has a very non-semantic structure and uses automatically generatedand unstable-looking class names like `article-header__toolbar__3lT1M`,`article-body__toolbar__1_CO8`, and `article__read-next__Kjxdw`. However, wecan notice a pattern here: there are some stable prefixes and random suffixesthat will possibly change faster than the &quot;stable&quot; part.We can handle these with a bit more advanced object selections. For thiscase, let's use CSS Selectors as they are a bit shorter and more readable:- First, we look for [attribute substrings](https://www.w3.org/TR/selectors/#attribute-substrings)  and remove the toolbars with a  `--body-remove-csss 'div[class*=&quot;article-header__toolbar__&quot;], div[class*=&quot;article-body__toolbar__&quot;]'`.  Notice that we are playing really loose here! The first rule does not mean  &quot;any `div` element marked with a class with a name starting with  `article-header__toolbar__`&quot;, it actually means &quot;any `div` element with a  `class` attribute containing the substring `article-header__toolbar__`&quot;. In  a site with shorter class names, we would probably face a lot of false  positives! But extracting feeds is almost always a game of kludges, so let  it be.- Do the same hack for the related articles but, looking a bit closer we can  notice that inside its main div we also have another tagged with class  `read-next-panel`, which is a pretty readable and stable-looking name! So  let's add an extra standard match for it as a fail-safe in the event the  external div stops matching; it will still leave some garbage text, but  will remove the most of the article list. The syntax is  `--body-remove-csss 'div[class*=&quot;article__read-next__&quot;], div.read-next-panel'`.We could have combined these two rules in one, but leaving them separatemakes debugging easier as the rules are printed with the log messages whenrunning with `--log debug`, allowing us to see which ones are matching.It is also important to notice that options `--body-remove-*` work over theDOM that was generated from `--body-xpath` or `--body-csss`. These optionsallows a lot of control over the way the elements are copied from the maindocument DOM, and the result can end up being very different once we startusing some clever selections.So, our updated command line is now:    newslinkrss \        --follow \        -p 'https://www.reuters.com/.+/.+\d{4}-\d{2}-\d{2}.+' \        --max-page-length 512 \        --with-body \        --body-xpath '//article' \        --body-remove-csss 'div[class*=&quot;article-header__toolbar__&quot;], div[class*=&quot;article-body__toolbar__&quot;]' \        --body-remove-csss 'div[class*=&quot;article__read-next__&quot;], div.read-next-panel' \        --require-dates \        --title-regex '(.+)\s+\|' \        https://www.reuters.com/... and it gives us a cleaner feed!### A single feed from multiple start URLsIt is possible to have several start URLs and make newslinkrss generate asingle feed with content gathered from all of them, all other optionsremaining the same. Typically, this can be used to filter only the sectionsof interest from a site while keeping all articles in the same subscriptionin your newsreader.Let's continue with our Reuters example, but imagine we only want articlesfrom sections &quot;World&quot; and &quot;Technology&quot;. We noticed before that the sectionnames appears in the URL, so a first approach may be just hack the linkpattern to require `(world|technology)` in it. However this solution may failto grab, for example, some news articles that appears in the &quot;technology&quot;page but have a different section in their URLs (as they can be related toboth) or skip articles that do not appear in the front page at all (as spacethere is limited).This can be solved with this command:    newslinkrss \        --follow \        -p 'https://www.reuters.com/.+/.+\d{4}-\d{2}-\d{2}.+' \        --max-page-length 512 \        --with-body \        --body-xpath '//article' \        --body-remove-csss 'div[class*=&quot;article-header__toolbar__&quot;], div[class*=&quot;article-body__toolbar__&quot;]' \        --body-remove-csss 'div[class*=&quot;article__read-next__&quot;], div.read-next-panel' \        --require-dates \        --title-regex '(.+)\s+\|' \        --title &quot;Reuters (Technology and World only)&quot; \        https://www.reuters.com/technology/ https://www.reuters.com/world/Notice that it is almost the same command line used in the previous example,except for the multiple URLs and option `--title`, which allow us to give analternate title to the feed. This is welcome because newslinkrss picks thetitle from the first URL that has one, and it may be related to the firstsection only (alternatively, you can rename the feed in your newsreader ifit has an option for it).When using multiple start pages, a special attention is required to theparameter `--max-links`! If the limit is reached in a page, links loadedfrom later pages will be skipped.This feature may be also used to capture more items from sites that splitthem in very short pages but does still have stable pagination URLs, like:`https://news.example.org/page-1.html https://news.example.org/page-2.htmlhttps://news.example.org/page-3.html`.  If you are using newslinkrss in ashell script, you may avoid repeated URLs by using sequence expansions (inbash) or `seq -f` (in everything else).### Gathering information from limited metadataSome sites do not provide standard (not even quasi-standard) metadata thatnewslinkrss can use automatically, so we must gather it from the pageswith site-specific approaches, following links and stitching informationfrom several elements together. Assume we want to generate a feed fromhttps://revistaquestaodeciencia.com.br/ , which provides no much facilitiesfor it. Looking into the site we find that:- URLs for news articles have a date on them (in format `YYYY/MM/DD`), so it  is possible to use this in the URL pattern (option `-p`) to limit which  links the script will look for. Some are prefixed by a section stub and all  are followed by a string generated from the title, so the regex must accept  the date anywhere in it. Anything could be a filter here, but as all  articles have a date on it we don't need to look anywhere else;- There is no standard, not even de-facto standard, representation for the  date an article was published, so the alternative is taking it from the URL  too. This is done with options `--date-from-url` (which requires regular  expression with a group capturing the substring that contains the date)  `--url-date-fmt` (which defines the format of the date);- Inconsistencies in the link formats prevent us from getting all articles  titles from the links in the front page, so the alternative is to  `--follow` every candidate link, downloading the target page and looking  for  the title there.- As we are already following, there is no much extra effort to also  generate a complete feed. The full text of the article is in an &quot;article&quot;  element, so we can use `--body-xpath &quot;//article&quot;` here too.- The target page has no author information in its metadata so newslinkrss  will not be able to find their names automatically. However, it has a link  to the list of articles from the same author in format  `&lt;a href=&quot;/autor/xxxxxxxx&quot;&gt;Author Name&lt;/a&gt;` and we can use the prefix  from attribute `href` to pick it with XPath (using option  `--author-from-xpath`) or CSS Selectors (with `--author-from-csss`). The  CSS approach is simpler for this particular case, so let's use an  `--author-from-csss 'a[href^=&quot;/autor/&quot;]'`. While not required for this  site (name is the only child element of &quot;a&quot;), some sites may prefix the  author name with a &quot;by &quot; or similar; For these cases, we could also use  options `--csss-author-regex` and `--xpath-author-regex` to select only the  name with a regular expression. Also notice that author is an optional  element in both RSS and Atom, but it is nice to have it appearing in the  correct fields in the news reader so we can use this information for  filtering reading lists, for example.The resulting command line is:    newslinkrss \        -p 'https://revistaquestaodeciencia.com.br/.*\d{4}/\d{2}/\d{2}/.+' \        --date-from-url '.*/(\d{4}/\d{2}/\d{2})/.*' \        --url-date-fmt '%Y/%m/%d' \        --follow \        --with-body \        --body-xpath &quot;//article&quot; \        --author-from-csss 'a[href^=&quot;/autor/&quot;]' \        --max-page-length 512 \        --http-timeout 4 \        --title-regex '(.+)\s+\|' \        'https://revistaquestaodeciencia.com.br/'### Using complex XPath expressionsSometimes we need to fight really hard to get the date that a particular itemwas last updated. Taking GitHub issues as an example: while GH provides Atomfeeds for releases and commits (but always to specific branches), there isno equivalent for issues and pull requests. Of course, there is an API forthat but it requires authentication with a GitHub account, enables tracking,and requires writing a specific bridge to get the data as a feed. This makesthe scraping approach easier even with the very convoluted example thatfollows.The URLs for issues and PRs are pretty usable, we can already use them tolimit how many issues will be shown, their status, filter by date, etc. Justlook at the one used in the example.However, we need to get the date of the last comment on the issue and set itas the publishing date of the item, otherwise the reader won't show us thatit was updated. A solution is to follow every issue page while using a XPathexpression to find the last occurrence of a &quot;relative-time&quot; tag that GitHubuses to mark the timestamp of a comment and parse the absolute date fromattribute &quot;datetime&quot;. This is done with options `--date-from-xpath` and`--xpath-date-fmt`.The resulting command line is the following:    newslinkrss \        --follow \        --with-body \        --http-timeout 5 \        --max-links 30 \        --max-page-length 1024 \        -p '^https://github.com/.+/issues/\d+$' \        --date-from-xpath '(//h3/a/relative-time)[last()]/@datetime' \        --xpath-date-fmt '%Y-%m-%dT%H:%M:%SZ' \        'https://github.com/lwindolf/liferea/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc'Debugging XPath expressions is not a very easy task, the simplest way is justopen the target page in Firefox, launch web developer tools and use the $x()helper function to get what the expression will return, for example:`$x('(//h3/a/relative-time)[last()]/@datetime')`.### The first example, revisitedNow that we have a few extra tricks in our sleeves we can check back thatvery first example and fix some of its limitations:- First, we need the correct publish dates; they are neither in the URL nor  in the anchor text, so we need to `--follow` the pages and get the dates  from there. The pages also have no metadata to do this easily, but they  have a publish date intended for human readers in a slice of HTML like this:  `&lt;small class=&quot;text-muted&quot;&gt;&lt;b&gt;23/12/2022&lt;/b&gt; - some unrelated text&lt;/small&gt;`.  We can use a XPath expression to select the date from element &quot;b&quot;, a good  one (but technically incorrect as it does not follow CSS rules for attribute  &quot;class&quot;) is `--date-from-xpath '//small[@class=&quot;text-muted&quot;]/b/text()'` .  It is a bit convoluted so we can replace it with a CSS Selector (that also  fixes the class attribute issue): `--date-from-csss 'small.text-muted &gt; b'`.  It will capture the &quot;23/12/2022&quot; from the text node of inner element &quot;b&quot;,  no need to filter through `--csss-date-regex` to remove unwanted text, but  as the date format is still ambiguous, we need to give an explicit format  with option `--csss-date-fmt '%d/%m/%Y'`;- Let's be extra careful with the URL pattern and never capture (or follow!)  links pointing to other domains. We can replace it with a more restricted  `-p 'https://www.jaraguadosul.sc.gov.br/news/.+'`;- As we are already following the pages, let's also generate a full feed.  The relevant part of the articles are inside a &quot;div&quot; element with  attribute &quot;id&quot; set to &quot;area_impressao&quot; (that's Portuguese for  &quot;printing_area&quot; and one may imagine it is being used to format the page  for printing, however there is neither an alternate style sheet nor a  @media selector for it ... anyway, at least it helps us to select the  correct text). We can isolate this element with a XPath expression  `'//div[@id=&quot;area_impressao&quot;]'` but this is slightly more complex than  the equivalent CSS Selector `div#area_impressao` and, as we already used  XPath in several other examples, let's stick with CSSS;- That site can be a bit slow sometimes, so let's be extra tolerant and  increase the HTTP timeout to 10 s;And then we have our fixed command line:    newslinkrss \        -p 'https://www.jaraguadosul.sc.gov.br/news/.+' \        --http-timeout 10 \        --follow \        --with-body \        --body-csss 'div#area_impressao' \        --date-from-csss 'small.text-muted &gt; b' \        --csss-date-fmt '%d/%m/%Y' \        https://www.jaraguadosul.sc.gov.br/noticias.php... not perfect, but it gives us a very practical and usable feed!## More useful notes### Capturing good titlesThe title of an item is one of the most important attributes you can get froma feed, as you probably will choose between continue reading or not based onit. newslinkrss has some strategies for finding a good title even if thesource site has some very bad design choices.If you are not following the target pages (i.e. not using option `--follow`),the only title available will be the anchor text. If following, newslinkrsswill, by default, use the target document title with a fall back to the anchortext or description for pages that do not have titles.Some sites have titles that are just too polluted but also have an alternateelement with a descriptive text which can be used in its place. For thesecases, newslinkrss has options `--title-from-xpath` and `--title-from-csss`.Both have the same effect but the former selects the element or attribute withthe alternate title using a XPath expression and the later picks an elementusing a CSS Selector (it is not possible to select attributes with CSS). Ifthese options are given but do not return any valid text, newslinkrss willfall back to the usual title selection.After the title is selected, it is also interesting to remove redundant textfrom it. newslinkrss has an option `--title-regex` for this exact purpose:it accepts a regular expression with a single capture group and, if theexpression matches, the captured text will be used as the title. Otherwise itwill keep the original text, so we do not loose titles if something changedat the source, for example.  Most common use case for this option is to removethe name of the site from article titles: it is a good design to have themin the web but, on a news reader, we do not want them using the very limitedspace available for item titles just to repeat information already shown inthe feed title.It is also possible to limit the length of the title, as sometimes errors inthe source site can give us insanely long ones. This is done with option`--max-title-length` (shortcut: `-l`) and the default value, 150, shouldprobably work for the most use cases.### Capturing publish datesThe date when a particular item was published is another very useful pieceof information from a feed, so newslinkrss also has several ways of gettingit from source pages, with availability depending on the way it is used (i.e.with or without `--follow`) and the information available in the HTML.Related options are the following:- If explicitly used, options `--date-from-xpath`, `--xpath-date-regex`, and  `--xpath-date-fmt` will allow reading dates from any element in the target  page which can be found with a XPath expression. Naturally, the target page  which must be downloaded with `--follow`; The XPath expression must return  a string (from the inner text or the attributes of an element), the second  is optional, but if given it must provide a regular expression with a single  capture group with the date, and the third should give the date format. If  no regex is given, all the string will be used and if no date format is  given, the code will try some common date formats;- If explicitly used, options `--date-from-csss`, `--csss-date-regex`, and  `--csss-date-fmt` work in a similar way as the previous ones, but using  CSS Selectors instead. They are not as powerful or flexible as XPath, but  simpler, cleaner, and more suitable for HTML;- If explicitly used, options `--date-from-text` and `--text-date-fmt` allow  reading the date from the anchor text (i.e., the text inside tag `&lt;a&gt;`)  associated to a particular entry in the index page; no `--follow` is  necessary. The first option must provide a regular expression with a single  capture group for the date (which allows removing non-date parts of the  string) and the second option should give the date format. If the format is  not given, the code will try some common date formats;- If explicitly used, options `--date-from-url` and `--url-date-fmt` allow  reading the date from the item link; this is specially interesting for  sites that have URLs in format  &quot;https://example.org/posts/2020/09/22/happy-hobbit-day/&quot; but provide  no better source for a publish date. Again, no `--follow` is necessary. The  first option must provide a regular expression with a single capture group  with the date and the second option should give the date format. If the  format is not given, code will try some common date formats;- If no date was found yet and option `--follow` is used, dates will be read  automatically from standard metadata (Open Graph, etc.) if they exists in  the target page;- If no date was found yet, but the HTTP request returned a &quot;Last-Modified&quot;  header, assumes it as the actual last modification date (even if the  server may be lying to us here).These options are tried in this order with the first valid date being picked.This way, options explicitly included in command line have priority, withhigher precedence being given to the ones which may return a &quot;good&quot; date.If no date options are listed, or if could not grab a date from the document,standard metadata and HTTP headers will be used (in this order).#### Locale-dependent datesSome date formats depend on the system locale, most common cases being monthnames (tokens `%b` and `%B`), and the date parsing options will also dependon it to interpret these dates correctly. By default, newslinkrss will usethe locale set for the current user, read from environment variables, andignore any failure if it is not available or is invalid. Option `--locale`allows setting an explicit locale name, so newslinkrss will use it or abortin the event of a failure. If you want to keep the default best-effortapproach for a non-default locale, set environment variable LC_ALL whencalling newslinkrss (i.e. call with`&quot;LC_ALL=pt_BR.UTF-8 newslinkrss &lt;options&gt;&quot;`).### Ignoring URLsThe link pattern (-p) has a counterpart `--ignore-pattern` (shortcut `-i`)which also accepts a regular expression and makes `newslinkrss` ignore anymatching URL. Depending on the amount of information that the website puts onthe URLs, this can be used for excluding native advertisement, uninterestingsections, or other unwanted content from the feed, without support from thefeed reader and without counting to the total URL limit (`-n`). While it ispossible to add this ignore rule to the link pattern itself, using `-i`prevents that regular expression from becoming excessively complex and makesdebugging easier. This option can be used multiple times, a URL that matchany of them will be ignored.### Cleaning URL query stringsSometimes the source page has URLs with unwanted query string parameters,like the [UTM trackers](https://en.wikipedia.org/wiki/UTM_parameters), ormultiple URLs differing only by irrelevant query string parameters andpointing to the same destination page (and therefore confusing the duplicatelink detection). Option `--qs-remove-param` (shortcut `-Q`) may be used tofix this. If the name of a parameter matches the regular expression given inthis option, that name/value pair will be removed from the URL query string.This option may be repeated many times if necessary. Example: `-Q '^utm.+'`(notice the anchor to only match prefixes).### Excluding body elementsWhen generating a complete feed we may sometimes end up including someunwanted elements from the source page into the item body, usually ads,&quot;related news&quot; boxes, section headers, random images, distracting formatting,unrelated links, etc. Some, but not all, of these noise sources can beremoved by carefully crafting XPath expressions or CSS selectors, but for thecases where this is not possible, demand too much effort, or result in afragile solution, we can just remove the unwanted elements explicitly.newslinkrss has tree command line options for this:- Option `--body-remove-tag` (shortcut `-R`) will remove all occurrences of  the given tag from the feed body and move their child elements to their  parents. This can be used to remove formatting while preserving the inner  text (e.g. `-R strong`) or to remove images from the body (with `-R img`,  as &quot;img&quot; elements have no children);- Option `--body-remove-xpath` (shortcut `-X`) will remove the elements  given by a XPath expression **and** their children. This is a good way  to remove banners, divs, etc. from the generated feed;- Option `--body-remove-csss` (shortcut `-C`) will remove the elements given  by a CSS Selector **and** their children. This is another way to remove  banners, divs, etc. from the generated and more practical when selecting  element by their CSS classes.All three options can be repeated in the command line how many times asnecessary to express all required rules.### Renaming tags and attributesSometimes we need to replace all occurrences of a HTML tag by another onewhile preserving all attributes and structure; this is exceptionally commonwhen facing sites &quot;infected&quot; by Google AMP and exceptionally rare foranything else. Option `--body-rename-tag` (shortcut: `-N`) exists for thisexact reason; assuming that we want to replace all instances of tag `amp-img`by `img`, we can just do a `-N amp-img img`.A similar use case appears when sites use lazy loading strategies for images.It is a bit common to have the image &quot;src&quot; attribute pointing to aplaceholder image, sometimes inlined in a &quot;data:&quot; URL, while the URL for theactual image is in another attribute, with a script loading the image onlyafter the user scrolls the page. Option `--body-rename-attr` can fix some ofthese cases. Assuming that a site has all src attributes for img tags in theattribute data-src, we can do `--body-rename-attr img data-src src`. Elementsthat do not have the old attribute will not be changed. If the elementalready has an attribute with the new name, it will be replaced. Thisoperation always runs **after** `--body-rename-tag` and sees the DOM asalready modified by it, so when renaming both tags and attributes, you mustlook for the new tag name.### Handling request language optionsSome sites change their rendering, response language, **date formats**, etc.according to the preferred language information sent by the user's browser.By default, newslinkrss uses the &quot;LANG&quot; environment variable to set thislanguage information (it is a bit of an optimization for the most commoncase where the system language is the same as the one of the browser used fordebugging the options for a particular feed). To set a different one, you mayoverwrite the LANG variable or use option `--lang`. The later option is moreflexible as it may be repeated to set several languages in order of preference(e.g. `&quot;--lang pt-BR --lang en-US --lang de-DE&quot;`).To not send any preferred language information to the server, independentlyof one currently set for the system, clear the LANG variable for thenewslinkrss process invocation (e.g. `&quot;LANG='' newslinkrss &lt;other options&gt;&quot;`).### Managing cookiesnewslinkrss persists cookies among requests from the same program invocationand forgets them once the program finishes; this is usually the most practicalchoice for typical use cases, but there is an option `--no-cookies` to disableall cookies if it becomes a problem for a particular source.Conversely, sites may expect a particular cookies to be set to a specificvalues, for example, to show news from a particular geographic region.Command line option `--cookie` allows the user to define customized cookiesto handle these cases. This option supports the complete syntax defined by[RFC 2965](https://www.rfc-editor.org/rfc/rfc2965.html), but a simple`name=value` will work for the majority of use cases. Example:`--cookie 'cookie-banner-consent-accepted=false'`. This option can berepeated as many times as necessary.If user-defined cookies are used together with option `--no-cookies`, thetarget site will have access to them, but will not be able to change ordelete them (by overwriting or redefining the expiry date) or set new ones.This results in a read-only cookie jar that can be very useful for sites thatrequire cookies but use them to change behavior in unwanted ways after somenumber of pages are processed.### Setting arbitrary HTTP headersSome sites may require unusual request options to work correctly. newslinkrsshas an option `--header` (shortcut: `-H`) to set any HTTP header for therequests it sends. To use it, just pass the header in format `&quot;NAME: VALUE&quot;`(e.g. `--header 'X-Clacks-Overhead: GNU Terry Pratchett'`). This should notbe a common requirement, however. It is also not recommended to use thisoption for handling language preferences and cookies, as the dedicatedoptions `--lang` and `--cookie` will take care of the specific behavior ofthese headers.### Testing linksnewslinkrss has an option `--test` that will skip the feed generation stepand just print the links and titles that were captured for a particular setof options to stdout. That's a simple way to check if a pattern is workingas intended.### LoggingThings **will** go wrong when experimenting with ugly regexes and confusingXPath expressions or when fighting unexpected changes in some website's DOM.Simplest way to see how newslinkrss is reacting internally is to increaseoutput verbosity with option `--log`; default value is &quot;warning&quot;, but &quot;info&quot;and &quot;debug&quot; will give more information like which element a XPath expressionfound, if a regex matched or how some date was interpreted.This information is always printed to stderr so it will not affect the feedoutput written to stdout; when debugging in the terminal, remember toredirect the output to a file with the shell or command line option `-o`.### Error reportingBy default, newslinkrss writes exceptions and error messages to the feedoutput itself. This is a very practical way to report errors to the user, asthis program is intended to work mostly on the background of the actualuser-facing application. It is possible to disable this behavior with option`--no-exception-feed` (shortcut `-E`).Notice that the program always return a non-zero status code on failures. Somenews readers (e.g. Liferea) won't process the output in these cases and discardthe feed entries with the error reports. You may need to override the statuscode with something like `newslinkrss YOUR_OPTIONS; exit 0`.### Writing output to a fileOption `-o` allows writing the output to an file; it is no much differentthan redirecting stdout, but will ensure that only valid XML with the rightencoding is written.Writing output to files and error reporting on the feed itself allows forsome unusual but interesting use patterns: for example, it is trivial for acompany, group, or development team to have an internal &quot;feed server&quot;, wherefeeds are centrally downloaded by a cron job, saved to a web server publicdirectory and then transparently provided to the end users. A setup withAlpine and nginx running in a LXD container is surprisingly small.## CaveatsBe very careful with escape characters! In the shell, it is recommended touse single quotes for regexes, so &quot;\\&quot; is not escaped. This becomes moreconfusing if your feed reader also use escape sequences but with differentor conflicting rules (e.g. Newsboat uses &quot;\\&quot; as escapes but does not followthe same rules used by bash). When in doubt, run the command in the shell oruse a wrapper script to test for any unusual behavior introduced by theprogram.Some feed readers run commands in a synchronous (blocking) mode and theirinterface will get stuck until the command terminates. Liferea had thisbehavior [until version 1.13.7](https://github.com/lwindolf/liferea/commit/b03af3b0f6a4e42b17dfa49782faa6c044055738),for example. A typical workaround is to create a script with all calls tonewslinkrss that saves the generated feeds to files (see option `-o`),schedule this script to run from cron and configure Liferea to load thefeed from the files. This solves the frozen interface problem, but requiresconfiguring feeds in two different places.# BugsYes :-)# ContributingFor this project I started an experiment: I'm using [sourcehut](https://sr.ht/)as the primary repository for code and other tools, leaving the usual Githuband Gitlab as non-advertised mirrors. Sourcehut does everything by theold-school way and most features just work without an user account. Check the[project page](https://sr.ht/~ittner/newslinkrss/) there to see how it works.If this sounds too strange, just clone the repository with an`git clone https://git.sr.ht/~ittner/newslinkrss`, publish your fork somewhereand send an email with the branches to be merged to `~ittner/newslinkrss@lists.sr.ht`(that's correct: tildes and slashes are valid characters for email addresses).If you change the code, please run in through pyflakes for static analysis and[black](https://pypi.org/project/black/) to ensure a consistent formatting.# LicenseCopyright (C) 2020  Alexandre Erwin Ittner &lt;alexandre@ittner.com.br&gt;This program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.# Contact information- Author: Alexandre Erwin Ittner- Email: &lt;alexandre@ittner.com.br&gt;- Web: &lt;https://www.ittner.com.br&gt;</longdescription>
</pkgmetadata>