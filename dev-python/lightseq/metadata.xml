<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># LightSeq: A High Performance Library for Sequence Processing and Generation![logo](./docs/inference/images/logo.png)**[2021/06/18]** :tada: :tada: :tada: **LightSeq supports fast training for models in the Transformer family now,please check out [here](./lightseq/training/README.md) for details.**---LightSeq is a high performance training and inference library for sequence processing and generation implementedin CUDA.It enables highly efficient computation of modern NLP models such as **BERT**, **GPT**,**Transformer**, etc.It is therefore best useful for *Machine Translation*, *Text Generation*, *Dialog*, *LanguageModelling*, *Sentiment Analysis*, and other related tasks with sequence data.The library is built on top of CUDA officiallibrary([cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html),[Thrust](https://docs.nvidia.com/cuda/thrust/index.html), [CUB](http://nvlabs.github.io/cub/)) andcustom kernel functions which are specially fused and optimized for Transformer model family. Inaddition to model components, the inference library also provide easy-to deploy model management and serving backend based on[TensorRT InferenceServer](https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/tensorrt_inference_server_120/tensorrt-inference-server-guide/docs/quickstart.html).With LightSeq, one can easily develop modified Transformer architecture with little additional code.## Features### [&gt;&gt;&gt; Training](./lightseq/training)The following is a support matrix of LightSeq **training** library compared with[DeepSpeed](https://github.com/microsoft/DeepSpeed).![features](./docs/training/images/features.png)### [&gt;&gt;&gt; Inference](./lightseq/inference)The following is a support matrix of LightSeq **inference** library compared with[TurboTransformers](https://github.com/Tencent/TurboTransformers) and[FasterTransformer](https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer).![support](./docs/inference/images/support.png)## Performance### [&gt;&gt;&gt; Training](./lightseq/training)Here we present the experimental results on WMT14 English to German translation task based on Transformer-big models. We train Transformer models of different sizes on eight NVIDIA Tesla V100/NVIDIA Tesla A100 GPUs with data parallel and fp16 mixed precision.[Fairseq](https://github.com/pytorch/fairseq) with [Apex](https://github.com/NVIDIA/apex) is choosed as our baseline.&lt;img src=&quot;./docs/training/images/single_step.png&quot;  width=&quot;80%&quot; aligned=&quot;middle&quot;&gt;We compute speedup on different batch size using the WPS (real words per second) metric.More results is available [here](./docs/training/performance.md)### [&gt;&gt;&gt; Inference](./lightseq/inference)Here we present the experimental results on neural machine translation based on Transformer-base models using beam search methods.We choose Tensorflow and[FasterTransformer](https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer) as a comparison.The implementation from[tensor2tensor](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)was used as the benchmark of Tensorflow.&lt;img src=&quot;./docs/inference/images/nmt.png&quot;  width=&quot;80%&quot; aligned=&quot;middle&quot;&gt;More results is available [here](./docs/inference/performance.md).## Quick StartComplete user guide is available [here](docs/guide.md).### InstallationYou can install LightSeq from PyPI:```shell$ pip install lightseq```LightSeq installation from PyPI only supports Python 3.6 to 3.8 on Linux for now. Consider compiling from source if you have other environments:```shell$ PATH=/usr/local/hdf5/:$PATH ENABLE_FP32=0 ENABLE_DEBUG=0 pip install -e $PROJECT_DIR```Detailed building introduction is available [here](docs/inference/build.md).### Fast training from FairseqYou can experience lightning fast training by running following commands,Firstly install these requirements.```shell$ pip install lightseq fairseq sacremoses```Then you can train a translation task on wmt14 en2de dataset by running the following script```shell$ sh examples/training/fairseq/ls_fairseq_wmt14en2de.sh```To compare lightseq with fairseq, delete the arguments with `ls_` prefix to using the original fairseq implementationMore usage is available [here](./lightseq/training/README.md).### Fast inference from HuggingFace bartWe provide an end2end bart-base example to see how fast Lightseq is compared to HuggingFace. First you should install these requirements.```shell$ pip install torch tensorflow transformers lightseq$ cd examples/inference/python```then you can check the performance by simply running following commands. `hf_bart_export.py` is used to transform pytorch weights to LightSeq protobuffer.```shell$ python export/huggingface/hf_bart_export.py$ python test/ls_bart.py```More usage is available [here](./lightseq/inference/README.md).### Fast deploy inference serverWe provide a docker image which contains tritonserver and lightseq's dynamic link library, and you can deploy a inference server by simply replace the model file with your own model file.```shell$ sudo docker pull hexisyztem/tritonserver_lightseq:22.01-1```More usage is available [here](https://github.com/bytedance/lightseq/tree/master/examples/triton_backend).## Cite UsIf you use LightSeq in your research, please cite the following paper.```@InProceedings{wang2021lightseq,    title = &quot;{L}ight{S}eq: A High Performance Inference Library for Transformers&quot;,    author = &quot;Wang, Xiaohui and Xiong, Ying and Wei, Yang and Wang, Mingxuan and Li, Lei&quot;,    booktitle = &quot;Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers (NAACL-HLT)&quot;,    month = jun,    year = &quot;2021&quot;,    publisher = &quot;Association for Computational Linguistics&quot;,    pages = &quot;113--120&quot;,}@article{wang2021lightseq2,  title={LightSeq2: Accelerated Training for Transformer-based Models on GPUs},  author={Wang, Xiaohui and Xiong, Ying and Qian, Xian and Wei, Yang and Li, Lei and Wang, Mingxuan},  journal={arXiv preprint arXiv:2110.05722},  year={2021}}```## ContactAny questions or suggestions, please feel free to contact us atwangxiaohui.neo@bytedance.com, xiongying.taka@bytedance.com, qian.xian@bytedance.com, weiyang.god@bytedance.com, wangmingxuan.89@bytedance.com, lilei@cs.ucsb.edu## HiringThe LightSeq team is hiring Interns/FTEs with backgrounds in deep learning system/natural language processing/computer vision/speech.We are based in Beijing and Shanghai. If you are interested, please send your resume to wangxiaohui.neo@bytedance.com.</longdescription>
</pkgmetadata>