<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># timeplus-clientWelcome to the Timeplus HTTP REST API specification.# query```pythonimport osimport tracebackimport jsonfrom pprint import pprintfrom timeplus import Query, Environmentapi_key = os.environ.get(&quot;TIMEPLUS_API_KEY&quot;)api_address = os.environ.get(&quot;TIMEPLUS_HOST&quot;)workspace = os.environ.get(&quot;TIMEPLUS_WORKSPACE&quot;)# Configure API key and addressenv = Environment().address(api_address).workspace(workspace).apikey(api_key)try:    # list all qeuries    query_list = Query(env=env).list()    pprint(f&quot;there are {len(query_list)} queries &quot;)    # create a new query    query = (        Query(env=env).sql(query=&quot;SELECT * FROM car_live_data&quot;)        # .batching_policy(1000, 1000)        .create()    )    pprint(f&quot;query with metadata {json.dumps(query.metadata())}&quot;)    # query header is the colume definitions of query result table    # it is a list of name/value pair    # for example : [{'name': 'in_use', 'type': 'bool'}, {'name': 'speed', 'type': 'float32'}]    query_header = query.header()    pprint(f&quot;query with header {query.header()}&quot;)    # iterate query result    limit = 3    count = 0    # query.result() is an iterator which will pull all the query result in small batches    # the iterator will continously pulling query result    # for streaming query, the iterator will not end until user cancel the query    for event in query.result():        # metric event return result time query metrics        # a sample metrics event:        # {'count': 117, 'eps': 75, 'processing_time': 1560,        # 'last_event_time': 1686237113265, 'response_time': 861,        # 'scanned_rows': 117, 'scanned_bytes': 7605}        if event.event == &quot;metrics&quot;:            pprint(json.loads(event.data))        # message event contains query result which is an array of arrays        # representing multiple query result rows        # a sample message event:        # [[True,-73.857],[False, 84.1]]        if event.event == &quot;message&quot;:            pprint(json.loads(event.data))        count += 1        if count &gt;= limit:            break    query.cancel()    query.delete()except Exception as e:    pprint(e)    traceback.print_exc()```# stream```pythonimport osimport tracebackimport jsonfrom pprint import pprintfrom timeplus import Stream, Environmentapi_key = os.environ.get(&quot;TIMEPLUS_API_KEY&quot;)api_address = os.environ.get(&quot;TIMEPLUS_HOST&quot;)worksapce = os.environ.get(&quot;TIMEPLUS_WORKSAPCE&quot;)# Configure API key and addressenv = Environment().address(api_address).apikey(api_key).workspace(worksapce)try:    # list all streams    stream_list = Stream(env=env).list()    pprint(f&quot;there are {len(stream_list)} streams &quot;)    # create a new stream    stream = (        Stream(env=env)        .name(&quot;test&quot;)        .column(&quot;time&quot;, &quot;datetime64(3)&quot;)        .column(&quot;data&quot;, &quot;string&quot;)        .create()    )    stream_list = Stream(env=env).list()    pprint(f&quot;there are {len(stream_list)} streams after create&quot;)    pprint(f&quot;created stream is {stream.metadata()}; type is {type(stream.metadata())}&quot;)    a_stream = Stream(env=env).name(&quot;test&quot;).get()    pprint(f&quot;get stream is {a_stream.metadata()} ; type is {type(a_stream.metadata())}&quot;)    stream.delete()    stream_list = Stream(env=env).list()    pprint(f&quot;there are {len(stream_list)} streams after delete&quot;)except Exception as e:    pprint(e)    traceback.print_exc()```# ingestdefault ingest```pythonstream = (        Stream(env=env)        .name(&quot;test_ingest&quot;)        .column(&quot;time&quot;, &quot;datetime64(3)&quot;)        .column(&quot;data&quot;, &quot;string&quot;)        .create()    )stream.ingest([&quot;time&quot;, &quot;data&quot;], [[datetime.datetime.now(), &quot;abcd&quot;]])```ingest json streams```pythonstream = (        Stream(env=env)        .name(&quot;test_ingest&quot;)        .column(&quot;a&quot;, &quot;integer&quot;)        .column(&quot;b&quot;, &quot;string&quot;)        .create()    )payload = &quot;&quot;&quot;{&quot;a&quot;:2,&quot;b&quot;:&quot;hello&quot;}{&quot;a&quot;:1,&quot;b&quot;:&quot;world&quot;}&quot;&quot;&quot;stream.ingest(payload=payload, format=&quot;streaming&quot;)```ingest one raw event with multiple lines```pythonstream = Stream(env=env).name(&quot;test_ingest_raw&quot;).column(&quot;raw&quot;, &quot;string&quot;).create()payload = &quot;&quot;&quot;first linesecond line&quot;&quot;&quot;stream.ingest(payload=payload, format=&quot;raw&quot;)```ingest multiple lines```pythonstream = Stream(env=env).name(&quot;test_ingest_lines&quot;).column(&quot;raw&quot;, &quot;string&quot;).create()payload = '{&quot;a&quot;:1,&quot;b&quot;:&quot;world&quot;}\n{&quot;a&quot;:2,&quot;b&quot;:&quot;hello&quot;}'stream.ingest(payload=payload, format=&quot;lines&quot;)```</longdescription>
</pkgmetadata>