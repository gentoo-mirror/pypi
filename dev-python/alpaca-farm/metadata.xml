<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot; width=&quot;100%&quot;&gt;&lt;img src=&quot;assets/AlpacaFarm_big.png&quot; alt=&quot;AlpacaFarm&quot; style=&quot;width: 50%; min-width: 300px; display: block; margin: auto;&quot;&gt;&lt;/p&gt;# AlpacaFarm: A Simulation Framework for Methods that &lt;br/&gt;Learn from Human Feedback[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/alpaca_farm/blob/main/LICENSE)[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/alpaca_farm/blob/main/DATA_LICENSE)[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)Research and development on learning from human feedback is difficult because methodslike [RLHF](https://arxiv.org/abs/2203.02155) are complex and costly to run.AlpacaFarm is a simulator that enables research and development on learning from feedback at a fraction of the usualcost, promoting accessible research on instruction following and alignment.Please read our [paper](https://arxiv.org/abs/2305.14387)and [blog post](https://crfm.stanford.edu/2023/05/22/alpaca-farm.html) for details on our research findings.This repo contains code for- [simulating preference feedback from language models such as GPT-4](#simulating-pairwise-preference)- [automated evaluation for instruction-following models](#running-automatic-evaluation)- [validated reference implementations of baseline methods such as PPO and best-of-n](#running-reference-methods)The data needed to run our code is hosted on HuggingFace: &lt;https://huggingface.co/datasets/tatsu-lab/alpaca_farm&gt;.**Usage and License Notices**: AlpacaFarm is intended and licensed for research use only.The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be usedoutside of research purposes.The weight diff is also CC BY NC 4.0 (allowing only non-commercial use).## The AlpacaFarm&lt;br&gt;&lt;p style=&quot;text-align:center;&quot;&gt;  &lt;img style=&quot;max-width:70%; height:auto;&quot; src=&quot;./assets/fig1.jpg&quot; alt=&quot;Workflow&quot;&gt;&lt;/p&gt;Instruction-following models are typically developed in 3 steps1. Supervised fine-tuning with demonstrations2. Learning from human feedback; usually pairwise preferences3. Human evaluation with interactionThe goal of AlpacaFarm is to provide three key components that tackles steps 2 and 3:Low-cost simulation of pairwise feedback from API models (e.g. GPT-4, ChatGPT), automated evaluations for methodsdevelopment, and reference implementations oflearning algorithms for comparison and modification.## InstallationTo install the stable release, run```bashpip install alpaca-farm```To install from the latest commit on `main` branch, run```bashpip install git+https://github.com/tatsu-lab/alpaca_farm.git```To enable FlashAttention and other optimizations, installthe [`flash-attn`](https://github.com/HazyResearch/flash-attention) and [`apex`](https://github.com/NVIDIA/apex)packages.## Simulating pairwise preference**Notebookexample:** [![Using](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tatsu-lab/alpaca_farm/blob/main/examples/auto_annotations.ipynb)For all the evaluation and annotations we use [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval/tree/main#making-a-new-evaluator) with our pool of automatic annotators and additional noise to simulate the variance of human annotations.To get started, set the environment variable `OPENAI_API_KEY` to your OpenAI API key, and (optionally) `OPENAI_ORG` totheorganization ID.You can do this by running```bashexport OPENAI_API_KEY=&quot;sk...&quot;```To annotate the pairs of outputs of your model use the following code.For more details or functions to use if you have outputs in different formats refer tothe [example notebook](https://github.com/tatsu-lab/alpaca_farm/blob/main/examples/auto_annotations.ipynb).```pythonfrom alpaca_farm.auto_annotations import PairwiseAutoAnnotatorimport json# load some datawith open(&quot;examples/data/outputs_pairs.json&quot;) as f:    outputs_pairs = json.load(f)[:6]print(outputs_pairs[-1:])# [{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',#   'input': '',#   'output_1': &quot;Dear Friends, \r\n\r\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \r\n\r\nBest,\r\n[Name]&quot;,#   'output_2': &quot;Hey everyone! \n\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \n\nLet me know if you can make it - I'd love to see you all there!\n\nCheers,\n[Your Name]&quot;}]annotator = PairwiseAutoAnnotator()annotated = annotator.annotate_pairs(outputs_pairs)print(annotated[-1:])# [{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.', # 'input': '', # 'output_1': &quot;Dear Friends, \r\n\r\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \r\n\r\nBest,\r\n[Name]&quot;, # 'output_2': &quot;Hey everyone! \n\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \n\nLet me know if you can make it - I'd love to see you all there!\n\nCheers,\n[Your Name]&quot;,# 'annotator': 'chatgpt_2', # 'preference': 2}]```If instead of pairs you have a list of sampled outputs, you can use the following.```pythonmultisample_outputs = [dict(instruction=&quot;repeat the following&quot;, input=&quot;yes&quot;, output=[&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;, &quot;repeat&quot;])]print(annotator.annotate_samples(multisample_outputs))# [{'sample_id': 0, #   'instruction': 'repeat the following', #   'input': 'yes', #   'output_1': 'yes', #   'output_2': 'maybe', #   'annotator': 'chatgpt_2', #   'preference': 1}]```## Running automatic evaluationFor all the evaluation we use [**AlpacaEval**](https://github.com/tatsu-lab/alpaca_eval/tree/main#making-a-new-evaluator) with our pool of automatic annotators. To get started, set the environment variable OPENAI_API_KEY to your OpenAI API key, and (optionally) OPENAI_ORG to theorganization ID. You can do this by running```bashexport OPENAI_API_KEY=&quot;sk...&quot;```The easiest to add your model to the Alpaca Leaderboard is to run the following code, which only requires having outputsfor your model on our eval data.```pythonfrom alpaca_farm.auto_annotations import alpaca_leaderboardimport datasets# predict on Alpaca eval dataalpaca_eval_data = datasets.load_dataset(&quot;tatsu-lab/alpaca_farm&quot;, &quot;alpaca_farm_evaluation&quot;)[&quot;eval&quot;]...  # use the data to get outputs for your model and save itpath_to_outputs = &quot;examples/data/eval_gpt-3.5-turbo-0301.json&quot;# outputs should be a list of json as such:# [{'instruction': 'What are the names of some famous actors that started their careers on Broadway?', 'input': '', 'output': 'Some famous actors that started their careers on Broadway are Hugh Jackman, Meryl Streep, Denzel Washington, Audra McDonald, and Lin-Manuel Miranda.', 'generator': 'gpt-3.5-turbo-0301', 'dataset': 'helpful_base', 'datasplit': 'eval'},# ...]alpaca_leaderboard(path_or_all_outputs=path_to_outputs, name=&quot;My fancy model&quot;, is_print_metrics=True)#                      n_draws  n_total  n_wins  n_wins_base  standard_error  win_rate# GPT4                   17.00   804.00  631.00       156.00            1.40     79.54# ChatGPT                 9.00   805.00  503.00       293.00            1.69     63.04# My fancy model          9.00   803.00  497.00       297.00            1.70     62.45# RLHF PPO                9.00   805.00  392.00       404.00            1.75     49.25# SFT 52k (Alpaca 7B)    16.00   805.00  312.00       477.00            1.71     39.75# SFT 10k                19.00   802.00  278.00       505.00            1.67     35.85# Davinci001              0.00   805.00  201.00       604.00            1.53     24.97# LLaMA 7B                0.00   775.00   98.00       677.00            1.19     12.65```If you want to compare against our baseline model (Davinci003 withour [prompt](https://github.com/tatsu-lab/alpaca_farm/blob/main/examples/prompts/v0_inputs_noinputs.json)) on your owndata, you can decode it using [main_oai_baselines](#openai-models).## Running reference methodsWe provide reference implementations of several methods for learning from pairwise feedback.Example code to run these methods can be found in the `examples/` directory.This includes [supervised fine-tuning](examples/supervised.py), [reward modeding](examples/reward_modeling.py), [RLHF with PPO](examples/rlhf_ppo.py), [best-of-n decoding](examples/best_of_n.py) and more.Below we give example commands for reproducing the model artifacts in our paper. Notes:- All training code are tested with FlashAttention enabled on a machine with 8 80GB A100 GPUs.- Best-of-n decoding was tested with a single 80GB GPU.- Supervised fine-tuning and reward modeling can fit on 4 80GB A100 GPUs, while PPO training currently requires at least  8  80GB GPUs.- Before running the code below, make sure to convert your LLaMA checkpoint and tokenizer into HuggingFace format and  store it at `&lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt;`.### Supervised fine-tuning (SFT)To replicate our SFT10k model fine-tuned from LLaMA in the paper, run```bashbash examples/scripts/sft.sh \  &lt;your_output_dir_for_sft10k&gt; \  &lt;your_wandb_run_name&gt; \  &lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt;```The SFT10k model will be saved at `&lt;your_output_dir&gt;`, and the name of the wandb run will be `&lt;your_wandb_run_name&gt;`.### Reward modelingTo replicate our reward models trained in the paper, run```bashbash examples/scripts/reward_modeling.sh \  &lt;your_output_dir_for_reward_model&gt; \  &lt;your_wandb_run_name&gt; \  &lt;your_output_dir_for_sft10k&gt; \  &lt;preference_dataset_name&gt;```Set `&lt;preference_dataset_name&gt;` to `&quot;alpaca_noisy_multi_preference&quot;` for simulated preference reward model, and`&quot;alpaca_human_preference&quot;` for human preference reward model.### RLHF with PPOTo replicate our RLHF PPO model trained with simulated reward model in the paper, run```bashbash examples/scripts/rlhf_ppo.sh \  &lt;your_output_dir_for_ppo&gt; \  &lt;your_wandb_run_name&gt; \  &lt;your_output_dir_for_reward_model&gt; \  &lt;your_output_dir_for_sft10k&gt; \  &lt;kl_coef&gt;````&lt;your_output_dir_for_reward_model&gt;` should point to either simulated reward model or human reward model trainedaccordingto the previous step.Note the KL penalty coefficient for human reward PPO is much larger than for simulated PPO.Set `&lt;kl_coef&gt;` to `0.0067` for simulated PPO, and `0.02` for human PPO to recover our original results.Performance of the PPO model is typically much better than SFT at 20-80 PPO steps (less than 4 passes through the entireset of instructions) and starts to decay with more PPO steps.### Best-of-n decodingTo replicate our best-of-n inference-time decoding results for the AlpacaFarm evaluation suite, run```bashpython examples/best_of_n.py \  --task &quot;run_best_of_n&quot; \  --decoder_name_or_path &lt;your_output_dir_for_decoder&gt; \  # Can be SFT model or even PPO tuned model.  --scorer_name_or_path &lt;your_output_dir_for_reward_model&gt; \  --num_return_sequences 16 \  # This is the n in best-of-n.  --per_device_batch_size 4 \  # Reduce this if you don't have enough memory.  --split &quot;eval&quot; \  --mixed_precision &quot;bf16&quot; \  --tf32 True \  --flash_attn True \  --output_path &lt;your_output_path_to_store_samples&gt;```You can then use the generated samples at `&lt;your_output_path_to_store_samples&gt;` directly with our automated evaluation.### Expert IterationTo replicate our expert iteration results for the AlpacaFarm evaluation suite, first produce best-of-n samples. Run```bashpython examples/best_of_n.py \  --task &quot;run_best_of_n&quot; \  --decoder_name_or_path &lt;your_output_dir_for_decoder&gt; \  # SFT10k model.  --scorer_name_or_path &lt;your_output_dir_for_reward_model&gt; \  --num_return_sequences 16 \  # This is the n in best-of-n.  --per_device_batch_size 4 \  # Reduce this if you don't have enough memory.  --split &quot;unlabeled&quot; \  --mixed_precision &quot;bf16&quot; \  --tf32 True \  --flash_attn True \  --output_path '&lt;your_output_dir_for_expiter_data&gt;/best_of_n_samples.json'```Then perform supervised fine-tuning from the SFT10k checkpoint with the best-of-n samples```bashbash examples/scripts/expiter.sh \  &lt;your_output_dir_for_expiter&gt; \  &lt;your_wandb_run_name&gt; \  &lt;your_output_dir_for_sft10k&gt; \  &lt;your_output_dir_for_expiter_data&gt;```### QuarkTo replicate our Quark results for the AlpacaFarm evaluation suite, run```bashbash examples/scripts/rlhf_quark.sh \  &lt;your_output_dir_for_quark&gt; \  &lt;your_wandb_run_name&gt; \  &lt;your_output_dir_for_reward_model&gt; \  &lt;your_output_dir_for_sft10k&gt; \  &lt;kl_coef&gt;```### OpenAI modelsTo run the OpenAI reference models with our prompts and decoding hyperparameters, run```bashpython examples/oai_baselines.py \  --model_name &lt;oai_model_name&gt; \  --save_path &lt;save_path&gt; ```You can then use the generated samples at `&lt;save_path&gt;` directly with our automated evaluation.## Downloading pre-tuned AlpacaFarm modelsWe provide model checkpoints for reward models and all our reference methods, listed in Table 2 ofour [paper](https://arxiv.org/abs/2305.14387). Concretely, we tune each reference method in AlpacaFarm simulation and onhuman preference data and release both versions. The current list of models(available [here](https://huggingface.co/tatsu-lab)) includes:- `sft10k`, the supervised learning base model that we collect preference data with.- `reward-model-sim`, the reward model trained on AlpacaFarm preference data.- `reward-model-human`, the reward model trained on human preference data.- `ppo-sim`, the best PPO checkpoint trained in simulation.- `ppo-human`, the best PPO checkpoint trained on human data.- `expiter-sim`, the best expert iteration checkpoint trained in simulation.- `expiter-human`, the best expert iteration checkpoint trained on human data.- `feedme-sim`, the FeedME method trained on simulated preferences.- `feedme-human`, the FeedME method trained on human preferences.- `reward-condition-sim`, the reward conditioning method trained on simulated preferences.To download and recover these checkpoints, first make sure to have a LLaMA-7Bcheckpoint [converted into the Hugging Face format](https://huggingface.co/docs/transformers/main/model_doc/llama)**with transformers&gt;=4.29.2**.Then, run the following to download all AlpacaFarm models:```python -m pretrained_models.recover_model_weights \  --llama-7b-hf-dir &lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt; \  --alpaca-farm-model-name all```Or, specify a particular model name to download just that model:```python -m pretrained_models.recover_model_weights \  --llama-7b-hf-dir &lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&gt; \  --alpaca-farm-model-name &lt;one_of_the_model_names_from_above&gt; \  --models-save-dir &lt;dir_to_save_all_models&gt;```To download either of the reward models individually, you'll need to have `sft10k` downloaded firstto `&lt;dir_to_save_all_models&gt;`.## CitationPlease consider citing our work if you use the data or code in this repo.```@misc{dubois2023alpacafarm,      title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback},       author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},      year={2023},      eprint={2305.14387},      archivePrefix={arXiv},      primaryClass={cs.LG}}```</longdescription>
</pkgmetadata>