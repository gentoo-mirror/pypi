<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Rev AI Python SDK[![CI](https://github.com/revdotcom/revai-python-sdk/actions/workflows/build_test.yml/badge.svg)](https://github.com/revdotcom/revai-python-sdk/actions/workflows/build_test.yml)## DocumentationSee the [API docs](https://docs.rev.ai/sdk/python/) for more information about the API andmore python examples.## InstallationYou don't need this source code unless you want to modify the package. If you justwant to use the package, just run:    pip install --upgrade rev_aiInstall from source with:    python setup.py install### Requirements- Python 3.8+## UsageAll you need to get started is your Access Token, which can be generated onyour [AccessToken Settings Page](https://www.rev.ai/access_token). Create a client with thegenerated Access Token:```pythonfrom rev_ai import apiclient# create your clientclient = apiclient.RevAiAPIClient(&quot;ACCESS TOKEN&quot;)```### Sending a fileOnce you've set up your client with your Access Token sending a file is easy!```python# you can send a local filejob = client.submit_job_local_file(&quot;FILE PATH&quot;)# or send a link to the file you want transcribedjob = client.submit_job_url(&quot;https://example.com/file-to-transcribe.mp3&quot;)````job` will contain all the information normally found in a successful response from our[Submit Job](https://docs.rev.ai/api/asynchronous/reference/#operation/SubmitTranscriptionJob) endpoint.If you want to get fancy, both submit job methods take `metadata`,`notification_config`, `skip_diarization`, `skip_punctuation`, `speaker_channels_count`,`custom_vocabularies`,`filter_profanity`, `remove_disfluencies`, `delete_after_seconds`,`language`,and `custom_vocabulary_id` as optional parameters.The url submission option also supports authentication headers by using the `source_config` option.All options are described in the request body of the[Submit Job](https://docs.rev.ai/api/asynchronous/reference/#operation/SubmitTranscriptionJob) endpoint.### Human TranscriptionIf you want transcription to be performed by a human, both methods allow you to submit human transcription jobsusing `transcriber=human` with `verbatim`, `rush`, `segments_to_transcribe` and `test_mode` as optional parameters.Check out our documentation for [Human Transcription](https://docs.rev.ai/api/asynchronous/transcribers/#human-transcription) for more details.```python# submitting a human transcription jobsjob = client.submit_job_url(&quot;https://example.com/file-to-transcribe.mp3&quot;,    transcriber='human',    verbatim=False,    rush=False,    test_mode=True    segments_to_transcribe=[{        start: 2.0,        end: 4.5    }])```### Checking your file's statusYou can check the status of your transcription job using its `id````pythonjob_details = client.get_job_details(job.id)````job_details` will contain all information normally found in a successful response fromour [Get Job](https://docs.rev.ai/api/asynchronous/reference/#operation/GetJobById) endpoint### Checking multiple filesYou can retrieve a list of transcription jobs with optional parameters```pythonjobs = client.get_list_of_jobs()# limit amount of retrieved jobsjobs = client.get_list_of_jobs(limits=3)# get jobs starting after a certain job idjobs = client.get_list_of_jobs(starting_after='Umx5c6F7pH7r')````jobs` will contain a list of job details having all information normally found in a successful responsefrom our [Get List of Jobs](https://docs.rev.ai/api/asynchronous/reference/#operation/GetListOfJobs) endpoint### Deleting a jobYou can delete a transcription job using its `id````pythonclient.delete_job(job.id)``` All data related to the job, such as input media and transcript, will be permanently deleted. A job can only by deleted once it's completed (either with success or failure).### Getting your transcriptOnce your file is transcribed, you can get your transcript in a few different forms:```python# as texttranscript_text = client.get_transcript_text(job.id)# as jsontranscript_json = client.get_transcript_json(job.id)# or as a python objecttranscript_object = client.get_transcript_object(job.id)```Both the json and object forms contain all the formation outlined in the responseof the [Get Transcript](https://docs.rev.ai/api/asynchronous/reference/#operation/GetTranscriptById) endpointwhen using the json response schema. While the text output is a string containingjust the text of your transcript### Getting captions outputYou can also get captions output from the SDK. We offer both SRT and VTT caption formats.If you submitted your job as speaker channel audio then you must also provide a `channel_id` to be captioned:```pythoncaptions = client.get_captions(job.id, content_type=CaptionType.SRT, channel_id=None)```### Streamed outputsAny output format can be retrieved as a stream. In these cases we return the raw http response to you. The output can be retrieved via `response.content`, `response.iter_lines()` or `response.iter_content()`.```pythontext_stream = client.get_transcript_text_as_stream(job.id)json_stream = client.get_transcript_json_as_stream(job.id)captions_stream = client.get_captions_as_stream(job.id)```## Streaming audioIn order to stream audio, you will need to setup a streaming client and a media configuration for the audio you will be sending.```pythonfrom rev_ai.streamingclient import RevAiStreamingClientfrom rev_ai.models import MediaConfig#on_error(error)#on_close(code, reason)#on_connected(id)config = MediaConfig()streaming_client = RevAiStreamingClient(&quot;ACCESS TOKEN&quot;,                                        config,                                        on_error=ERRORFUNC,                                        on_close=CLOSEFUNC,                                        on_connected=CONNECTEDFUNC)````on_error`, `on_close`, and `on_connected` are optional parameters that are functions to be called when the websocket errors, closes, and connects respectively. The default `on_error` raises the error, `on_close` prints out the code and reason for closing, and `on_connected` prints out the job ID.If passing in custom functions, make sure you provide the right parameters. See the sample code for the parameters.Once you have a streaming client setup with a `MediaConfig` and access token, you can obtain a transcription generator of your audio. You can also use a custom vocabulary with your streaming job by supplying the optional `custom_vocabulary_id` when starting a connection!More optional parameters can be supplied when starting a connection, these are `metadata`, `filter_profanity`, `remove_disfluencies`, `delete_after_seconds`, and `detailed_partials`. For a description of these optional parameters look at our [streaming documentation](https://docs.rev.ai/api/streaming/requests/#request-parameters).```pythonresponse_generator = streaming_client.start(AUDIO_GENERATOR, custom_vocabulary_id=&quot;CUSTOM VOCAB ID&quot;)````response_generator` is a generator object that yields the transcription results of the audio including partial and final transcriptions. The `start` method creates a thread sending audio pieces from the `AUDIO_GENERATOR` to our[streaming] endpoint.If you want to end the connection early, you can!```pythonstreaming_client.end()```Otherwise, the connection will end when the server obtains an &quot;EOS&quot; message.### Submitting custom vocabulariesIn addition to passing custom vocabularies as parameters in the async API client, you can create and submit your custom vocabularies independently and directly to the custom vocabularies API, as well as check on their progress.Primarily, the custom vocabularies client allows you to submit and preprocess vocabularies for use with the streaming client, in order to have streaming jobs with custom vocabularies!In this example you see how to construct custom vocabulary objects, submit them to the API, and check on their progress and metadata!```pythonfrom rev_ai import custom_vocabularies_clientfrom rev_ai.models import CustomVocabulary# Create a clientclient = custom_vocabularies_client.RevAiCustomVocabulariesClient(&quot;ACCESS TOKEN&quot;)# Construct a CustomVocabulary object using your desired phrasescustom_vocabulary = CustomVocabulary([&quot;Patrick Henry Winston&quot;, &quot;Robert C Berwick&quot;, &quot;Noam Chomsky&quot;])# Submit the CustomVocabularycustom_vocabularies_job = client.submit_custom_vocabularies([custom_vocabulary])# View the job's progressjob_state = client.get_custom_vocabularies_information(custom_vocabularies_job['id'])# Get list of previously submitted custom vocabulariescustom_vocabularies_jobs = client.get_list_of_custom_vocabularies()# Delete the CustomVocabularyclient.delete_custom_vocabulary(custom_vocabularies_job['id'])```For more details, check out the custom vocabularies example in our [examples](https://github.com/revdotcom/revai-python-sdk/tree/develop/examples).# For Rev AI Python SDK DevelopersRemember in your development to follow the PEP8 style guide. Your code editor likely has Python PEP8 linting packages which can assist you in your development.# Local testing instructionsPrequisites: virtualenv, toxTo test locally use the following commands from the repo root    virtualenv ./sdk-test    . ./sdk-test/bin/activate    toxThis will locally run the test suite, and saves significant dev time overwaiting for the CI tool to pick it up.=======History=======0.0.0 (2018-09-28)------------------* Initial alpha release2.1.0------------------* Revamped official release2.1.1------------------* File upload bug fixes2.2.1------------------* Better Documentation2.2.2------------------* Fix pypi readme formatting2.3.0------------------* Add get_list_of_jobs2.4.0------------------* Add support for custom vocabularies2.5.0------------------* Add examples* Improve error handling* Add streaming client2.6.0------------------* Support skip_punctuation* Support .vtt captions output* Support speaker channel jobs2.6.1------------------* Add metadata to streaming client2.7.0------------------* Add custom vocabularies to streaming client2.7.1------------------* Use v1 of the streaming api* Add custom vocabulary to async example* Add filter_profanity to async and streaming clients, examples, and documentation* Add remove_disfluencies to async client2.11.0------------------* Add language selection option for multi-lingual ASR jobs to async client2.12.0------------------* Add custom_vocabulary_id to async client2.13.0------------------* Add detailed_partials to streaming client* Switch to Github Actions for automated testing2.14.0------------------* Add transcriber to async client* Add verbatim, rush, segments_to_transcribe, test_mode to async client for human transcription* Add start_ts and transcriber to streaming client2.15.0------------------* Add topic extraction client* Add speaker_names to async client for human transcription2.16.0------------------* Add sentiment analysis client* Add source_config and notification_config job options to support customer provided urls with authentication headers* Deprecate media_url option, replace with source_config* Deprecate callback_url option, replace with notification_config2.17.0------------------* Add language to the streaming client2.18.0------------------* Add atmospherics and speaker_count support* Deprecated support for Python versions up to 3.8</longdescription>
</pkgmetadata>