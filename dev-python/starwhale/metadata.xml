<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;    &lt;img src=&quot;https://github.com/star-whale/docs/raw/main/static/img/starwhale.png&quot; width=&quot;600&quot; style=&quot;max-width: 600px;&quot;&gt;    &lt;h1 align=&quot;center&quot; style=&quot;margin-top: 10px&quot;&gt;An MLOps/LLMOps Platform&lt;/h1&gt;üöÄ Ô∏è‚òÅÔ∏è [Starwhale Cloud](https://cloud.starwhale.cn) is now open to the public, try it! üéâüçª&lt;/div&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://pypi.org/project/starwhale/&quot;&gt;    &lt;img src=&quot;https://img.shields.io/pypi/v/starwhale?style=flat&quot;&gt;&lt;/a&gt;&lt;a href='https://artifacthub.io/packages/helm/starwhale/starwhale'&gt;    &lt;img src='https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/starwhale' alt='Artifact Hub'/&gt;&lt;/a&gt;&lt;a href=&quot;https://pypi.org/project/starwhale/&quot;&gt;    &lt;img alt=&quot;PyPI - Python Version&quot; src=&quot;https://img.shields.io/pypi/pyversions/starwhale&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/star-whale/starwhale/actions/workflows/client.yaml&quot;&gt;    &lt;img src=&quot;https://github.com/star-whale/starwhale/actions/workflows/client.yaml/badge.svg&quot;  alt=&quot;Client/SDK UT&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/star-whale/starwhale/actions/workflows/server-ut-report.yml&quot;&gt;    &lt;img src=&quot;https://github.com/star-whale/starwhale/actions/workflows/server-ut-report.yml/badge.svg&quot; alt=&quot;Server UT&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/star-whale/starwhale/actions/workflows/console.yml&quot;&gt;    &lt;img src=&quot;https://github.com/star-whale/starwhale/actions/workflows/console.yml/badge.svg&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/star-whale/starwhale/actions/workflows/e2e-test.yml&quot;&gt;    &lt;img src='https://github.com/star-whale/starwhale/actions/workflows/e2e-test.yml/badge.svg' alt='Starwhale E2E Test'&gt;&lt;/a&gt;&lt;a href='https://app.codecov.io/gh/star-whale/starwhale'&gt;    &lt;img alt=&quot;Codecov&quot; src=&quot;https://img.shields.io/codecov/c/github/star-whale/starwhale?flag=controller&amp;label=Java%20Cov&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://app.codecov.io/gh/star-whale/starwhale&quot;&gt;    &lt;img alt=&quot;Codecov&quot; src=&quot;https://img.shields.io/codecov/c/github/star-whale/starwhale?flag=standalone&amp;label=Python%20cov&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h4 align=&quot;center&quot;&gt;    &lt;p&gt;        &lt;b&gt;English&lt;/b&gt; |        &lt;a href=&quot;https://github.com/star-whale/starwhale/blob/main/README_ZH.md&quot;&gt;‰∏≠Êñá&lt;/a&gt;    &lt;p&gt;&lt;/h4&gt;## What is StarwhaleStarwhale is an MLOps/LLMOps platform that make your model creation, evaluation and publication much easier. It aims to create a handy tool for data scientists and machine learning engineers. Starwhale helps you:- üèóÔ∏è Keep track of your training/testing dataset history including data items and their labels, so that you can easily access them.- üß≥ Manage your model packages that you can share across your team.- üåä Run your models in different environments, either on a Nvidia GPU server or on an embedded device like Cherry Pi.- üî• Create a online service with interactive Web UI for your models.## Key Concepts### ü¶ç Starwhale InstanceEach deployment of Starwhale is called an instance. All instances can be managed by the Starwhale Client (swcli). You can start using Starwhale with one of the following instance types:- üëª **Starwhale Standalone**:  Rather than a running service, Starwhale Standalone is actually a repository that resides in your local file system. It is created and managed by the Starwhale Client (SWCLI). You only need to install SWCLI to use it. Currently, each user on a single machine can have only ONE Starwhale Standalone instance. We recommend you use the Starwhale Standalone to build and test your datasets, runtime, and models before pushing them to Starwhale Server/Cloud instances.- üéç **Starwhale Server**:  Starwhale Server is a service deployed on your local server. Besides text-only results from the Starwhale Client (SWCLI), Starwhale Server provides Web UI for you to manage your datasets and models, evaluate your models in your local Kubernetes cluster, and review the evaluation results.- ‚òÅÔ∏è **Starwhale Cloud**: Starwhale Cloud is a managed service hosted on public clouds. By registering an account on https://cloud.starwhale.cn , you are ready to use Starwhale without needing to install, operate, and maintain your own instances. Starwhale Cloud also provides public resources for you to download, like datasets, runtimes, and models. Check the &quot;starwhale/public&quot; project on Starwhale Cloud for more details.**Starwhale tries to keep concepts consistent across different types of instances. In this way, people can easily exchange data and migrate between them.**### üêò Starwhale DatasetStarwhale Dataset offers efficient data storage, loading, and visualization capabilities, making it a dedicated data management tool tailored for the field of machine learning and deep learning![dataset overview](https://starwhale-examples.oss-cn-beijing.aliyuncs.com/docs/dataset-overview.svg)```pythonimport torchfrom starwhale import dataset, Image# build dataset for starwhale cloud instancewith dataset(&quot;https://cloud.starwhale.cn/project/starwhale:public/dataset/test-image&quot;, create=&quot;empty&quot;) as ds:    for i in range(100):        ds.append({&quot;image&quot;: Image(f&quot;{i}.png&quot;), &quot;label&quot;: i})    ds.commit()# load datasetds = dataset(&quot;https://cloud.starwhale.cn/project/starwhale:public/dataset/test-image&quot;)print(len(ds))print(ds[0].features.image.to_pil())print(ds[0].features.label)torch_ds = ds.to_pytorch()torch_loader = torch.utils.data.DataLoader(torch_ds, batch_size=5)print(next(iter(torch_loader)))```### üêá Starwhale ModelStarwhale Model is a standard format for packaging machine learning models that can be used for various purposes, like model fine-tuning, model evaluation, and online serving. A Starwhale Model contains the model file, inference codes, configuration files, and any other files required to run the model.![overview](https://starwhale-examples.oss-cn-beijing.aliyuncs.com/docs/model-overview.svg)```bash# model buildswcli model build . --module mnist.evaluate --runtime pytorch/version/v1 --name mnist# model copy from standalone to cloudswcli model cp mnist https://cloud.starwhale.cn/project/starwhale:public# model runswcli model run --uri mnist --runtime pytorch --dataset mnistswcli model run --workdir . --module mnist.evaluator --handler mnist.evaluator:MNISTInference.cmp```### üêå Starwhale RuntimeStarwhale Runtime aims to provide a reproducible and sharable running environment for python programs. You can easily share your working environment with your teammates or outsiders, and vice versa. Furthermore, you can run your programs on Starwhale Server or Starwhale Cloud without bothering with the dependencies.![overview](https://starwhale-examples.oss-cn-beijing.aliyuncs.com/docs/runtime-overview.svg)```bash# build from runtime.yaml, conda env, docker image or shellswcli runtime build --yaml runtime.yamlswcli runtime build --conda pytorch --name pytorch-runtime --cuda 11.4swcli runtime build --docker pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtimeswcli runtime build --shell --name pytorch-runtime# runtime activateswcli runtime activate pytorch# integrated with model and datasetswcli model run --uri test --runtime pytorchswcli model build . --runtime pytorchswcli dataset build --runtime pytorch```### üêÑ Starwhale EvaluationStarwhale Evaluation enables users to evaluate sophisticated, production-ready distributed models by writing just a few lines of code with Starwhale Python SDK.```pythonimport typing as timport gradiofrom starwhale import evaluationfrom starwhale.api.service import apidef model_generate(image):    ...    return predict_value, probability_matrix@evaluation.predict(    resources={&quot;nvidia.com/gpu&quot;: 1},    replicas=4,)def predict_image(data: dict, external: dict) -&gt; None:    return model_generate(data[&quot;image&quot;])@evaluation.evaluate(use_predict_auto_log=True, needs=[predict_image])def evaluate_results(predict_result_iter: t.Iterator):    for _data in predict_result_iter:        ...    evaluation.log_summary({&quot;accuracy&quot;: 0.95, &quot;benchmark&quot;: &quot;test&quot;})@api(gradio.File(), gradio.Label())def predict_view(file: t.Any) -&gt; t.Any:    with open(file.name, &quot;rb&quot;) as f:        data = Image(f.read(), shape=(28, 28, 1))    _, prob = predict_image({&quot;image&quot;: data})    return {i: p for i, p in enumerate(prob)}```## Installation### üçâ Starwhale StandaloneRequirements: Python 3.7~3.11 in the Linux or macOS os.```bashpython3 -m pip install starwhale```### ü•≠ Starwhale ServerStarwhale Server is delivered as a Docker image, which can be run with Docker directly or deployed to a Kubernetes cluster. For the laptop environment, using [Minikube](https://minikube.sigs.k8s.io/docs/start/) is a appropriate choice.```bashminikube start --addons ingresshelm repo add starwhale https://star-whale.github.io/chartshelm repo updatehelm pull starwhale/starwhale --untar --untardir ./chartshelm upgrade --install starwhale ./charts/starwhale -n starwhale --create-namespace -f ./charts/starwhale/values.minikube.global.yaml```## Quick TourWe use [MNIST](https://paperswithcode.com/dataset/mnist) as the hello world example to show the basic Starwhale Model workflow.### ü™Ö MNIST Evaluation in Starwhale Standalone- Use your own Python environment, follow the [Standalone quickstart doc](https://starwhale.cn/docs/en/next/getting-started/standalone/).- Use Google Colab environment, follow the [Jupyter notebook example](https://colab.research.google.com/github/star-whale/starwhale/blob/main/example/notebooks/quickstart-standalone.ipynb).### ü™Ü MNIST Evaluation in Starwhale Server- Run it in the your private Starwhale Server instance, please read [Server installation(minikube)](https://starwhale.cn/docs/en/next/server/installation/minikube) and [Server quickstart](https://starwhale.cn/docs/en/next/getting-started/server) docs.- Run it in the [Starwhale Cloud](https://cloud.starwhale.cn), please read [Cloud quickstart doc](https://starwhale.cn/docs/en/next/getting-started/cloud).## Examples- üöÄ LLM:  - üêä OpenSource LLMs Leaderboard: [Evaluation](https://cloud.starwhale.cn/projects/349/evaluations), [Code](https://github.com/star-whale/starwhale/tree/main/example/llm-leaderboard)  - üê¢ Llama2: [Run llama2 chat in five minutes](https://starwhale.cn/docs/en/blog/run-llama2-chat-in-five-minutes/), [Code](https://github.com/star-whale/starwhale/tree/main/example/LLM/llama2)  - ü¶é Stable Diffusion: [Cloud Demo](https://cloud.starwhale.cn/projects/374/models), [Code](https://github.com/star-whale/stable-diffusion-webui)  - ü¶ô LLAMA [evaluation and fine-tune](https://github.com/star-whale/starwhale/tree/main/example/LLM/llama)  - üéπ [MusicGen](https://github.com/star-whale/starwhale/tree/main/example/LLM/musicgen)- ü¶¶ Image Classification:  - üêª‚ùÑÔ∏è MNIST: [Cloud Demo](https://cloud.starwhale.cn/projects/392/evaluations), [Code](https://github.com/star-whale/starwhale/tree/main/example/mnist).  - ü¶´ [CIFAR10](https://github.com/star-whale/starwhale/tree/main/example/cifar10)- üéôÔ∏è Speech Recognition: [Speech Command](https://github.com/star-whale/starwhale/tree/main/example/speech_command)- üê¶ Object Detection: [Pedestrian Detection](https://github.com/star-whale/starwhale/tree/main/example/PennFudanPed)- üìΩÔ∏è Video Recognition: [UCF101](https://github.com/star-whale/starwhale/tree/main/example/ucf101)- ü¶ã Machine Translation: [Neural machine translation](https://github.com/star-whale/starwhale/tree/main/example/nmt)- üêú Text Classification: [AG News](https://github.com/star-whale/starwhale/tree/main/example/text_cls_AG_NEWS)## Documentation, Community, and Support- Visit [Starwhale HomePage](https://starwhale.ai).- More information in the [official documentation](https://doc.starwhale.ai).- For general questions and support, join the [Slack](https://starwhale.slack.com/).- For bug reports and feature requests, please use [Github Issue](https://github.com/star-whale/starwhale/issues).- To get community updates, follow [@starwhaleai](https://twitter.com/starwhaleai) on Twitter.- For Starwhale artifacts, please visit:  - Python Package on [Pypi](https://pypi.org/project/starwhale/).  - Helm Charts on [Artifacthub](https://artifacthub.io/packages/helm/starwhale/starwhale).  - Docker Images on [Docker Hub](https://hub.docker.com/u/starwhaleai), [Github Packages](https://github.com/orgs/star-whale/packages) and [Starwhale Registry](https://docker-registry.starwhale.cn/).- Additionally, you can always find us at *developer@starwhale.ai*.## Contributingüåºüëè**PRs are always welcomed** üëçüç∫. See [Contribution to Starwhale](https://doc.starwhale.ai/community/contribute) for more details.## LicenseStarwhale is licensed under the [Apache License 2.0](https://github.com/star-whale/starwhale/blob/main/LICENSE).</longdescription>
</pkgmetadata>