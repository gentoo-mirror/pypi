<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>### Tetun TokenizerTetun tokenizer is a Python package for tokenizing an input text into tokens. There are several tokenization techniques we built along with this package as follows:1. `TetunStandardTokenizer()`: tokenize the input text by `word`, `punctuations`, and `special characters`.2. `TetunWhiteSpaceTokenizer()`: tokenize the input text by `whitespace` delimiter.3. `TetunSentenceTokenizer()`: tokenize the input text by `.?!` delimiters.4. `TetunBlankLineTokenizer()`: tokenize the input text by `blank lines` delimiter.5. `TetunSimpleTokenizer()`: tokenize the input text by extracting `only string and number` and ignore punctuations and special characters.6. `TetunWordTokenizer()`: tokenize the input text by extracting `only string` and ignore numbers, punctuations, and special characters.### InstallationTo install Tetun tokenizer, run the following command in your console:```python3 -m pip install tetun-tokenizer```or simply run:```pip install tetun-tokenizer```It also supports `conda` and `pipenv` or similar commands.### UsageTo use Tetun tokenizer, `from` the `tokenizer` module on the `tetuntokenizer` package, `import` a tokenizer class. Instantiate the imported class and then call a `tokenize` function as follows:1. Using  `TetunStandardTokenizer()` to tokenize a given text.```pythonfrom tetuntokenizer.tokenizer import TetunStandardTokenizertetun_tokenizer = TetunStandardTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```The output will be:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak&quot;, '.', &quot;Ha'u&quot;, 'iha', '$', '0.25', 'atu', 'fó', 'ba', 'ita', '.']```2. Using `TetunWhiteSpaceTokenizer()` to tokenize a given text.```pythonfrom tetuntokenizer.tokenizer import TetunWhiteSpaceTokenizertetun_tokenizer = TetunWhiteSpaceTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```The output will be:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak.&quot;, &quot;Ha'u&quot;, 'iha', '$0.25', 'atu', 'fó', 'ba', 'ita.']```3. Using `TetunSentenceTokenizer()` to tokenize a given text.```pythonfrom tetuntokenizer.tokenizer import TetunSentenceTokenizertetun_tokenizer = TetunSentenceTokenizer()text = &quot;Ha'u ema-ida ne'ebé baibain de'it. Tebes ga? Ita-nia maluk Dr. ka Ph.D sira hosi U.S.A mós dehan!&quot;output = tetun_tokenizer.tokenize(text)print(output)```The output will be:```[&quot;Ha'u ema-ida ne'ebé baibain de'it.&quot;, 'Tebes ga?', 'Ita-nia maluk Dr. ka Ph.D sira hosi U.S.A mós dehan!']```4. Using `TetunBlankLineTokenizer()` to tokenize a given text.```pythonfrom tetuntokenizer.tokenizer import TetunBlankLineTokenizertetun_tokenizer = TetunBlankLineTokenizer()text = &quot;&quot;&quot;        Ha'u mak ita-nia maluk di'ak.        Ha'u iha $0.25 atu fó ba ita.        &quot;&quot;&quot;output = tetun_tokenizer.tokenize(text)print(output)```The output will be:```[&quot;\n            Ha'u mak ita-nia maluk di'ak.\n            Ha'u iha $0.25 atu fó ba ita.\n            &quot;]```5. Using `TetunSimpleTokenizer()` to tokenize a given text.```pythonfrom tetuntokenizer.tokenizer import TetunSimpleTokenizertetun_tokenizer = TetunSimpleTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```The output will be:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak&quot;, &quot;Ha'u&quot;, 'iha', '0.25', 'atu', 'fó', 'ba', 'ita']```6. Using `TetunWordTokenizer()` to tokenize a given text.```pythonfrom tetuntokenizer.tokenizer import TetunWordTokenizertetun_tokenizer = TetunWordTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```The output will be:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak&quot;, &quot;Ha'u&quot;, 'iha', 'atu', 'fó', 'ba', 'ita']```To print the resulting output to the console, with each element on a new line, you can use `for` loop or simply use `join()` as follows:```print('\n'.join(output))```The output will be:```Ha'umakita-niamalukdi'akHa'uihaatufóbaita```You can also use the tokenizer to tokenize a text from a file. Here is an example:```python# Assume that we use Path instead of a string for the file pathfrom pathlib import Pathfrom tetuntokenizer.tokenizer import TetunSimpleTokenizerfile_path = Path(&quot;myfile/example.txt&quot;)try:    with file_path.open('r', encoding='utf-8') as f:    contents = [line.strip() for line in f]except FileNotFoundError:    print(f&quot;File not found at: {file_path}&quot;)# You can also lowercase the contents before tokenizing them.lowercase_contents = contents.lower()tetun_tokenizer = TetunSimpleTokenizer()output = '\n'.join(tetun_tokenizer.tokenize(str(lowercase_contents)))print(output)```The output will be:```ha'uorgulludezenvolveha'u-nialiantetun ...```There are a few more ways to read file contents that you can use to achieve the same output.</longdescription>
</pkgmetadata>