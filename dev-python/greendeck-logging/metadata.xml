<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Greendeck-Logging:---![Greendeck](https://greendeck-cdn.s3.ap-south-1.amazonaws.com/dumps/gd_transparent_blue_bg.png)  ![logging]### Install from piphttps://pypi.org/project/greendeck-logging/```pip install greendeck-logging```This library can be used for save the critical log, message, info data for your internal services in elasticsearch and create a nice visualization through kibana.For creating  visualization in kibana you can refer this  [documentation](https://www.elastic.co/guide/en/kibana/current/createvis.html)if you don't have elasticsearch and kibana you can refer this  [blog](https://www.elastic.co/guide/en/elastic-stack/current/installing-elastic-stack.html)### How to use?##### import the library```from greendeck_logging import GdLoggingfrom greendeck_logging import GdLoggingStack```### There are Two ways to save the log data, by bulk (*GdLoggingStack*) and one by one ( *GdLogging*)If you want to save the log in memory as stack and send data after process completes, use **GdLoggingStack**. It saves the log messages in stack and send to elastic search on calling push method.### Variables to connect elasticsearch to store the log data    LOG_ECS_HOST =  &lt; YOUR_LOG_ECS_HOST &gt;    LOG_ECS_INDEX = &lt; LOG_ECS_INDEX &gt;    LOG_ECS_TYPE = &lt; LOG_ECS_TYPE &gt;    service_name = &lt; service_name &gt;&gt;  if elasticsearch requires username and password to connect, provide in LOG_ECS_HOST in a standard format like  [https://username:password@elasticsearch_host:port_no]() Eg : [https://admin:admin123@127.0.0.0:9200/]()Some Default Values are :&gt; LOG_ECS_INDEX = gdlogging,  LOG_ECS_TYPE  = '_doc'if you want save data in other index name instead of gdlogging , assign  **LOG_ECS_INDEX = &quot;your index name&quot;****GdLoggingStack :**  Send the bulk log data to elasticsearch```from greendeck_logging import GdLoggingStackgdl_stack = GdLoggingStack(LOG_ECS_HOST = LOG_ECS_HOST,                          service_name = service_name                            LOG_ECS_INDEX = &quot;gdlogging&quot;,                          LOG_ECS_TYPE = &quot;_doc&quot;,                          )```&gt; **default value:      LOG_ECS_INDEX = 'gdlogging',      LOG_ECS_TYPE = '_doc',&gt; ##### **LOG_ECS_TYPE and SERVICE_NAME are required field#### Methods and their functionalityThere are following function for capturing the events- For capturing Error message and info of the error due to which occur ,  ` gdl_stack.error(&quot;your message&quot;, info = {}, value = 1) `  default, value = 1, info = {} # info can be dictionary- To save Debug message and send the object as info    ` gdl_stack.debug(&quot;your debug message&quot;, info = {}, value = 1) `  - To increament the response status of the services like 200, 404 etc   ` gdl_stack.counter_message(message, info = {}, value = 1)`  default, value = 1  - To increment the  message  for a particular ```website ```  ( It can be your unique identifier depending on requirement )   `gdl_stack.counter_website(website, message, info = {}, value = 1)`  default, value = 1- clear the stack  `gdl_stack.clear()`  - total element in stack  `gdl_stack.count()`  - show the first element in the stack  `gdl_stack.show_one()`  - show all elements in the stack  `gdl_stack.show()`  - push all elements to elastic search  `gdl_stack.push()`  &gt; ** Don't forget to push the element to elastic search at end of your stack, use `gdl_stack.push()` for update the data in elastic search**GdLogging** : It updates one element at a time```from greendeck_logging import GdLogginggdl = GdLogging(LOG_ECS_HOST = LOG_ECS_HOST,                          service_name = service_name                            LOG_ECS_INDEX = &quot;gdlogging&quot;,                          LOG_ECS_TYPE = &quot;_doc&quot;,                          )```&gt; **default value:      LOG_ECS_INDEX = 'gdlogging',      LOG_ECS_TYPE = '_doc',### Usage and functionality  There are following function for capturing the events  - For capturing Error message and info due to which occur  `gdl.error(&quot;your message&quot;, info = {}, value = 1) `  default, value = 1 , info = {}  - For capturign Debug message  ` gdl.debug(&quot;your message&quot;, info = {}, value = 1) `default, value = 1  - To increament the response status of the services like 200, 404 etc  ` gdl.counter_message(message, info = {}, value = 1)`  default, value = 1  - To increment the  message  for a particular ```website ```  ( It can be your unique identifier depending on requirement )   `gdl.counter_website(website, message, info = {}, value = 1)`  default, value = 1   Example of the object which will save into the elastic search         &quot;_index&quot; : &quot;gd_logging&quot;,            &quot;_type&quot; : &quot;_doc&quot;,            &quot;_id&quot; : &quot;0TASCGsBFBI3I7B8680B&quot;,            &quot;_score&quot; : 2.3312538,            &quot;_source&quot; : {              &quot;service_name&quot; : &quot;class_service&quot;,              &quot;log_type&quot; : &quot;info&quot;,              &quot;created_at&quot; : &quot;2019-05-30T14:59:45.452035&quot;,              &quot;meta&quot; : {                &quot;file_name&quot; : &quot;service.py&quot;,                &quot;user_name&quot; : &quot;user&quot;,                &quot;system_name&quot; : &quot;user-300-15ISK&quot;,                &quot;os_name&quot; : &quot;Linux&quot;              },              &quot;counter_status&quot; : {                  &quot;message&quot; : &quot;response status&quot;,                &quot;counter_type&quot; : &quot;status&quot;,                &quot;counter_name&quot; : &quot;201&quot;,                &quot;counter_value&quot; : 1              }            }          }</longdescription>
</pkgmetadata>