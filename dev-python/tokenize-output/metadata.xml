<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;Get identifiers, names, paths, URLs and words from the command output.&lt;br&gt; The &lt;a href=&quot;https://github.com/anki-code/xontrib-output-search&quot;&gt;xontrib-output-search&lt;/a&gt; for &lt;a href=&quot;https://xon.sh/&quot;&gt;xonsh shell&lt;/a&gt; is using this library.&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;  If you like the idea click ‚≠ê on the repo and stay tuned by watching releases.&lt;/p&gt;## Install```shell scriptpip install -U tokenize-output```## UsageYou can use `tokenize-output` command as well as export the tokenizers in Python:```pythonfrom tokenize_output.tokenize_output import *tokenizer_split(&quot;Hello world!&quot;)# {'final': set(), 'new': {'Hello', 'world!'}}```#### Words tokenizing```shell scriptecho &quot;Try https://github.com/xxh/xxh&quot; | tokenize-output -p# Try# https://github.com/xxh/xxh```#### JSON, Python dict and JavaScript object tokenizing```shell scriptecho '{&quot;Try&quot;: &quot;xonsh shell&quot;}' | tokenize-output -p# Try# shell# xonsh# xonsh shell```    #### env tokenizing```shell scriptecho 'PATH=/one/two:/three/four' | tokenize-output -p# /one/two# /one/two:/three/four# /three/four# PATH```    ## Development### TokenizersTokenizer is a functions which extract tokens from the text.| Priority | Tokenizer  | Text example  | Tokens || ---------| ---------- | ----- | ------ || 1        | **dict**   | `{&quot;key&quot;: &quot;val as str&quot;}` | `key`, `val as str` || 2        | **env**    | `PATH=/bin:/etc` | `PATH`, `/bin:/etc`, `/bin`, `/etc` |   | 3        | **split**  | `Split  me \n now!` | `Split`, `me`, `now!` |   | 4        | **strip**  | `{Hello}!.` | `Hello` |   You can create your tokenizer and add it to `tokenizers_all` in `tokenize_output.py`.Tokenizing is a recursive process where every tokenizer returns `final` and `new` tokens. The `final` tokens directly go to the result list of tokens. The `new` tokens go to all tokenizers again to find new tokens. As result if there is a mix of json and env data in the output it will be found and tokenized in appropriate way.  ### How to add tokenizerYou can start from `env` tokenizer:1. [Prepare regexp](https://github.com/tokenizer/tokenize-output/blob/25b930cfadf8291e72a72144962e411e47d28139/tokenize_output/tokenize_output.py#L10)2. [Prepare tokenizer function](https://github.com/tokenizer/tokenize-output/blob/25b930cfadf8291e72a72144962e411e47d28139/tokenize_output/tokenize_output.py#L57-L70)3. [Add the function to the list](https://github.com/tokenizer/tokenize-output/blob/25b930cfadf8291e72a72144962e411e47d28139/tokenize_output/tokenize_output.py#L139-L144) and [to the preset](https://github.com/tokenizer/tokenize-output/blob/25b930cfadf8291e72a72144962e411e47d28139/tokenize_output/tokenize_output.py#L147).4. [Add test](https://github.com/tokenizer/tokenize-output/blob/25b930cfadf8291e72a72144962e411e47d28139/tests/test_tokenize.py#L34-L35).5. Now you can test and debug (see below).### Test and debugRun tests:```shell scriptcd ~git clone https://github.com/anki-code/tokenize-outputcd tokenize-outputpython -m pytest tests/```To debug the tokenizer:```shell scriptecho &quot;Hello world&quot; | ./tokenize-output -p```## Related projects* [xontrib-output-search][XONTRIB_OUTPUT_SEARCH] for [xonsh shell][XONSH][XONTRIB_OUTPUT_SEARCH]: https://github.com/anki-code/xontrib-output-search[XONSH]: https://xon.sh/</longdescription>
</pkgmetadata>