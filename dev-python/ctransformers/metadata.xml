<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># [CTransformers](https://github.com/marella/ctransformers) [![PyPI](https://img.shields.io/pypi/v/ctransformers)](https://pypi.org/project/ctransformers/) [![tests](https://github.com/marella/ctransformers/actions/workflows/tests.yml/badge.svg)](https://github.com/marella/ctransformers/actions/workflows/tests.yml) [![build](https://github.com/marella/ctransformers/actions/workflows/build.yml/badge.svg)](https://github.com/marella/ctransformers/actions/workflows/build.yml)Python bindings for the Transformer models implemented in C/C++ using [GGML](https://github.com/ggerganov/ggml) library.&gt; Also see [ChatDocs](https://github.com/marella/chatdocs)- [Supported Models](#supported-models)- [Installation](#installation)- [Usage](#usage)  - [ðŸ¤— Transformers](#transformers)  - [LangChain](#langchain)  - [GPU](#gpu)  - [GPTQ](#gptq)- [Documentation](#documentation)- [License](#license)## Supported Models| Models              | Model Type    | CUDA | Metal || :------------------ | ------------- | :--: | :---: || GPT-2               | `gpt2`        |      |       || GPT-J, GPT4All-J    | `gptj`        |      |       || GPT-NeoX, StableLM  | `gpt_neox`    |      |       || Falcon              | `falcon`      |  âœ…  |       || LLaMA, LLaMA 2      | `llama`       |  âœ…  |  âœ…   || MPT                 | `mpt`         |  âœ…  |       || StarCoder, StarChat | `gpt_bigcode` |  âœ…  |       || Dolly V2            | `dolly-v2`    |      |       || Replit              | `replit`      |      |       |## Installation```shpip install ctransformers```## UsageIt provides a unified interface for all models:```pyfrom ctransformers import AutoModelForCausalLMllm = AutoModelForCausalLM.from_pretrained(&quot;/path/to/ggml-model.bin&quot;, model_type=&quot;gpt2&quot;)print(llm(&quot;AI is going to&quot;))```[Run in Google Colab](https://colab.research.google.com/drive/1GMhYMUAv_TyZkpfvUI1NirM8-9mCXQyL)To stream the output, set `stream=True`:```pyfor text in llm(&quot;AI is going to&quot;, stream=True):    print(text, end=&quot;&quot;, flush=True)```You can load models from Hugging Face Hub directly:```pyllm = AutoModelForCausalLM.from_pretrained(&quot;marella/gpt-2-ggml&quot;)```If a model repo has multiple model files (`.bin` or `.gguf` files), specify a model file using:```pyllm = AutoModelForCausalLM.from_pretrained(&quot;marella/gpt-2-ggml&quot;, model_file=&quot;ggml-model.bin&quot;)```&lt;a id=&quot;transformers&quot;&gt;&lt;/a&gt;### ðŸ¤— Transformers&gt; **Note:** This is an experimental feature and may change in the future.To use it with ðŸ¤— Transformers, create model and tokenizer using:```pyfrom ctransformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained(&quot;marella/gpt-2-ggml&quot;, hf=True)tokenizer = AutoTokenizer.from_pretrained(model)```[Run in Google Colab](https://colab.research.google.com/drive/1FVSLfTJ2iBbQ1oU2Rqz0MkpJbaB_5Got)You can use ðŸ¤— Transformers text generation pipeline:```pyfrom transformers import pipelinepipe = pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer)print(pipe(&quot;AI is going to&quot;, max_new_tokens=256))```You can use ðŸ¤— Transformers generation [parameters](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig):```pypipe(&quot;AI is going to&quot;, max_new_tokens=256, do_sample=True, temperature=0.8, repetition_penalty=1.1)```You can use ðŸ¤— Transformers tokenizers:```pyfrom ctransformers import AutoModelForCausalLMfrom transformers import AutoTokenizermodel = AutoModelForCausalLM.from_pretrained(&quot;marella/gpt-2-ggml&quot;, hf=True)  # Load model from GGML model repo.tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)  # Load tokenizer from original model repo.```### LangChainIt is integrated into LangChain. See [LangChain docs](https://python.langchain.com/docs/ecosystem/integrations/ctransformers).### GPUTo run some of the model layers on GPU, set the `gpu_layers` parameter:```pyllm = AutoModelForCausalLM.from_pretrained(&quot;TheBloke/Llama-2-7B-GGML&quot;, gpu_layers=50)```[Run in Google Colab](https://colab.research.google.com/drive/1Ihn7iPCYiqlTotpkqa1tOhUIpJBrJ1Tp)#### CUDAInstall CUDA libraries using:```shpip install ctransformers[cuda]```#### ROCmTo enable ROCm support, install the `ctransformers` package using:```shCT_HIPBLAS=1 pip install ctransformers --no-binary ctransformers```#### MetalTo enable Metal support, install the `ctransformers` package using:```shCT_METAL=1 pip install ctransformers --no-binary ctransformers```### GPTQ&gt; **Note:** This is an experimental feature and only LLaMA models are supported using [ExLlama](https://github.com/turboderp/exllama).Install additional dependencies using:```shpip install ctransformers[gptq]```Load a GPTQ model using:```pyllm = AutoModelForCausalLM.from_pretrained(&quot;TheBloke/Llama-2-7B-GPTQ&quot;)```[Run in Google Colab](https://colab.research.google.com/drive/1SzHslJ4CiycMOgrppqecj4VYCWFnyrN0)&gt; If model name or path doesn't contain the word `gptq` then specify `model_type=&quot;gptq&quot;`.It can also be used with LangChain. Low-level APIs are not fully supported.## Documentation&lt;!-- API_DOCS --&gt;### Config| Parameter            | Type        | Description                                                     | Default || :------------------- | :---------- | :-------------------------------------------------------------- | :------ || `top_k`              | `int`       | The top-k value to use for sampling.                            | `40`    || `top_p`              | `float`     | The top-p value to use for sampling.                            | `0.95`  || `temperature`        | `float`     | The temperature to use for sampling.                            | `0.8`   || `repetition_penalty` | `float`     | The repetition penalty to use for sampling.                     | `1.1`   || `last_n_tokens`      | `int`       | The number of last tokens to use for repetition penalty.        | `64`    || `seed`               | `int`       | The seed value to use for sampling tokens.                      | `-1`    || `max_new_tokens`     | `int`       | The maximum number of new tokens to generate.                   | `256`   || `stop`               | `List[str]` | A list of sequences to stop generation when encountered.        | `None`  || `stream`             | `bool`      | Whether to stream the generated text.                           | `False` || `reset`              | `bool`      | Whether to reset the model state before generating text.        | `True`  || `batch_size`         | `int`       | The batch size to use for evaluating tokens in a single prompt. | `8`     || `threads`            | `int`       | The number of threads to use for evaluating tokens.             | `-1`    || `context_length`     | `int`       | The maximum context length to use.                              | `-1`    || `gpu_layers`         | `int`       | The number of layers to run on GPU.                             | `0`     |&gt; **Note:** Currently only LLaMA, MPT and Falcon models support the `context_length` parameter.### &lt;kbd&gt;class&lt;/kbd&gt; `AutoModelForCausalLM`---#### &lt;kbd&gt;classmethod&lt;/kbd&gt; `AutoModelForCausalLM.from_pretrained````pythonfrom_pretrained(    model_path_or_repo_id: str,    model_type: Optional[str] = None,    model_file: Optional[str] = None,    config: Optional[ctransformers.hub.AutoConfig] = None,    lib: Optional[str] = None,    local_files_only: bool = False,    revision: Optional[str] = None,    hf: bool = False,    **kwargs) â†’ LLM```Loads the language model from a local file or remote repo.**Args:**- &lt;b&gt;`model_path_or_repo_id`&lt;/b&gt;: The path to a model file or directory or the name of a Hugging Face Hub model repo.- &lt;b&gt;`model_type`&lt;/b&gt;: The model type.- &lt;b&gt;`model_file`&lt;/b&gt;: The name of the model file in repo or directory.- &lt;b&gt;`config`&lt;/b&gt;: `AutoConfig` object.- &lt;b&gt;`lib`&lt;/b&gt;: The path to a shared library or one of `avx2`, `avx`, `basic`.- &lt;b&gt;`local_files_only`&lt;/b&gt;: Whether or not to only look at local files (i.e., do not try to download the model).- &lt;b&gt;`revision`&lt;/b&gt;: The specific model version to use. It can be a branch name, a tag name, or a commit id.- &lt;b&gt;`hf`&lt;/b&gt;: Whether to create a Hugging Face Transformers model.**Returns:**`LLM` object.### &lt;kbd&gt;class&lt;/kbd&gt; `LLM`### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.__init__````python__init__(    model_path: str,    model_type: Optional[str] = None,    config: Optional[ctransformers.llm.Config] = None,    lib: Optional[str] = None)```Loads the language model from a local file.**Args:**- &lt;b&gt;`model_path`&lt;/b&gt;: The path to a model file.- &lt;b&gt;`model_type`&lt;/b&gt;: The model type.- &lt;b&gt;`config`&lt;/b&gt;: `Config` object.- &lt;b&gt;`lib`&lt;/b&gt;: The path to a shared library or one of `avx2`, `avx`, `basic`.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.bos_token_idThe beginning-of-sequence token.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.configThe config object.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.context_lengthThe context length of model.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.embeddingsThe input embeddings.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.eos_token_idThe end-of-sequence token.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.logitsThe unnormalized log probabilities.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.model_pathThe path to the model file.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.model_typeThe model type.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.pad_token_idThe padding token.---##### &lt;kbd&gt;property&lt;/kbd&gt; LLM.vocab_sizeThe number of tokens in vocabulary.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.detokenize````pythondetokenize(tokens: Sequence[int], decode: bool = True) â†’ Union[str, bytes]```Converts a list of tokens to text.**Args:**- &lt;b&gt;`tokens`&lt;/b&gt;: The list of tokens.- &lt;b&gt;`decode`&lt;/b&gt;: Whether to decode the text as UTF-8 string.**Returns:**The combined text of all tokens.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.embed````pythonembed(    input: Union[str, Sequence[int]],    batch_size: Optional[int] = None,    threads: Optional[int] = None) â†’ List[float]```Computes embeddings for a text or list of tokens.&gt; **Note:** Currently only LLaMA and Falcon models support embeddings.**Args:**- &lt;b&gt;`input`&lt;/b&gt;: The input text or list of tokens to get embeddings for.- &lt;b&gt;`batch_size`&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: `8`- &lt;b&gt;`threads`&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: `-1`**Returns:**The input embeddings.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.eval````pythoneval(    tokens: Sequence[int],    batch_size: Optional[int] = None,    threads: Optional[int] = None) â†’ None```Evaluates a list of tokens.**Args:**- &lt;b&gt;`tokens`&lt;/b&gt;: The list of tokens to evaluate.- &lt;b&gt;`batch_size`&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: `8`- &lt;b&gt;`threads`&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: `-1`---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.generate````pythongenerate(    tokens: Sequence[int],    top_k: Optional[int] = None,    top_p: Optional[float] = None,    temperature: Optional[float] = None,    repetition_penalty: Optional[float] = None,    last_n_tokens: Optional[int] = None,    seed: Optional[int] = None,    batch_size: Optional[int] = None,    threads: Optional[int] = None,    reset: Optional[bool] = None) â†’ Generator[int, NoneType, NoneType]```Generates new tokens from a list of tokens.**Args:**- &lt;b&gt;`tokens`&lt;/b&gt;: The list of tokens to generate tokens from.- &lt;b&gt;`top_k`&lt;/b&gt;: The top-k value to use for sampling. Default: `40`- &lt;b&gt;`top_p`&lt;/b&gt;: The top-p value to use for sampling. Default: `0.95`- &lt;b&gt;`temperature`&lt;/b&gt;: The temperature to use for sampling. Default: `0.8`- &lt;b&gt;`repetition_penalty`&lt;/b&gt;: The repetition penalty to use for sampling. Default: `1.1`- &lt;b&gt;`last_n_tokens`&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: `64`- &lt;b&gt;`seed`&lt;/b&gt;: The seed value to use for sampling tokens. Default: `-1`- &lt;b&gt;`batch_size`&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: `8`- &lt;b&gt;`threads`&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: `-1`- &lt;b&gt;`reset`&lt;/b&gt;: Whether to reset the model state before generating text. Default: `True`**Returns:**The generated tokens.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.is_eos_token````pythonis_eos_token(token: int) â†’ bool```Checks if a token is an end-of-sequence token.**Args:**- &lt;b&gt;`token`&lt;/b&gt;: The token to check.**Returns:**`True` if the token is an end-of-sequence token else `False`.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.reset````pythonreset() â†’ None```Resets the model state.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.sample````pythonsample(    top_k: Optional[int] = None,    top_p: Optional[float] = None,    temperature: Optional[float] = None,    repetition_penalty: Optional[float] = None,    last_n_tokens: Optional[int] = None,    seed: Optional[int] = None) â†’ int```Samples a token from the model.**Args:**- &lt;b&gt;`top_k`&lt;/b&gt;: The top-k value to use for sampling. Default: `40`- &lt;b&gt;`top_p`&lt;/b&gt;: The top-p value to use for sampling. Default: `0.95`- &lt;b&gt;`temperature`&lt;/b&gt;: The temperature to use for sampling. Default: `0.8`- &lt;b&gt;`repetition_penalty`&lt;/b&gt;: The repetition penalty to use for sampling. Default: `1.1`- &lt;b&gt;`last_n_tokens`&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: `64`- &lt;b&gt;`seed`&lt;/b&gt;: The seed value to use for sampling tokens. Default: `-1`**Returns:**The sampled token.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.tokenize````pythontokenize(text: str, add_bos_token: Optional[bool] = None) â†’ List[int]```Converts a text into list of tokens.**Args:**- &lt;b&gt;`text`&lt;/b&gt;: The text to tokenize.- &lt;b&gt;`add_bos_token`&lt;/b&gt;: Whether to add the beginning-of-sequence token.**Returns:**The list of tokens.---#### &lt;kbd&gt;method&lt;/kbd&gt; `LLM.__call__````python__call__(    prompt: str,    max_new_tokens: Optional[int] = None,    top_k: Optional[int] = None,    top_p: Optional[float] = None,    temperature: Optional[float] = None,    repetition_penalty: Optional[float] = None,    last_n_tokens: Optional[int] = None,    seed: Optional[int] = None,    batch_size: Optional[int] = None,    threads: Optional[int] = None,    stop: Optional[Sequence[str]] = None,    stream: Optional[bool] = None,    reset: Optional[bool] = None) â†’ Union[str, Generator[str, NoneType, NoneType]]```Generates text from a prompt.**Args:**- &lt;b&gt;`prompt`&lt;/b&gt;: The prompt to generate text from.- &lt;b&gt;`max_new_tokens`&lt;/b&gt;: The maximum number of new tokens to generate. Default: `256`- &lt;b&gt;`top_k`&lt;/b&gt;: The top-k value to use for sampling. Default: `40`- &lt;b&gt;`top_p`&lt;/b&gt;: The top-p value to use for sampling. Default: `0.95`- &lt;b&gt;`temperature`&lt;/b&gt;: The temperature to use for sampling. Default: `0.8`- &lt;b&gt;`repetition_penalty`&lt;/b&gt;: The repetition penalty to use for sampling. Default: `1.1`- &lt;b&gt;`last_n_tokens`&lt;/b&gt;: The number of last tokens to use for repetition penalty. Default: `64`- &lt;b&gt;`seed`&lt;/b&gt;: The seed value to use for sampling tokens. Default: `-1`- &lt;b&gt;`batch_size`&lt;/b&gt;: The batch size to use for evaluating tokens in a single prompt. Default: `8`- &lt;b&gt;`threads`&lt;/b&gt;: The number of threads to use for evaluating tokens. Default: `-1`- &lt;b&gt;`stop`&lt;/b&gt;: A list of sequences to stop generation when encountered. Default: `None`- &lt;b&gt;`stream`&lt;/b&gt;: Whether to stream the generated text. Default: `False`- &lt;b&gt;`reset`&lt;/b&gt;: Whether to reset the model state before generating text. Default: `True`**Returns:**The generated text.&lt;!-- API_DOCS --&gt;## License[MIT](https://github.com/marella/ctransformers/blob/main/LICENSE)</longdescription>
</pkgmetadata>