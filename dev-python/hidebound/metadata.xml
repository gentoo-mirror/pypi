<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># IntroductionA local database service for converting directories of arbitrary files intovalidated assets and derived metadata for export to databases like AWS S3 andMongoDB.See [documentation](https://thenewflesh.github.io/hidebound/) for details.# Installation### Python`pip install hidebound`### Docker1. Install [docker-desktop](https://docs.docker.com/desktop/)2. `docker pull thenewflesh/hidebound:latest`### Docker For Developers1. Install [docker-desktop](https://docs.docker.com/desktop/)2. Ensure docker-desktop has at least 4 GB of memory allocated to it.4. `git clone git@github.com:theNewFlesh/hidebound.git`5. `cd hidebound`6. `chmod +x bin/hidebound`7. `bin/hidebound start`The service should take a few minutes to start up.Run `bin/hidebound --help` for more help on the command line tool.# OverviewHidebound is an ephemeral database and asset framework used for generating,validating and exporting assets to various data stores. Hidebound enablesdevelopers to ingest arbitrary sets of files and output them as content andgenerated metadata, which has validated according to specifications they define.Assets are placed into an ingress directory, typically reserved for Hideboundprojects, and then processed by Hidebound. Hidebound extracts metadata from thefiles and directories that make each asset according to their name, location andfile properties. This data comprises the entirety of Hidebound's database at anyone time.# Dataflow![](resources/screenshots/data_flow.png)Data begins as files on disk. Hidebound creates a JSON-compatible dict fromtheir name traits and file traits and then constructs an internal database tablefrom them, one dict per row. All the rows are then aggregated by asset, andconverted into JSON blobs. Those blobs are then validated according to theirrespective specifications. Files from valid assets are then copied or moved intoHidebound's content directory, according to their same directory structure andnaming. Metadata is written to JSON files inside Hidebound's metadata directory.Each file's metadata is written as a JSON file in /hidebound/metadata/file, andeach asset's metadata (the aggregate of its file metadata) is written to/hidebound/metadata/asset. From their exporters, can export the validasset data and its accompanying metadata to various locations, like an AWS S3bucket.# WorkflowThe acronynm to remember for workflows is **CRUDES**: create, read, update,delete, export and search. Those operations constitue the main functionalitythat Hidebound supports.### *Create Asset*For example, an asset could be an image sequence, such as a directory full ofPNG files, all of which have a frame number, have 3 (RGB) channels, and are 1024pixels wide by 1024 pixels tall. Let's call the specification for this type ofasset &quot;spec001&quot;. We create an image sequence of a cat running, and we move itinto the Hidebound projects directory.### *Update*![](resources/screenshots/update.png)We call the update function via Hidebound's web app. Hidebound createsa new database based upon the recursive listing of all the files within saiddirectory. This database is displayed to us as a table, with one file per row.If we choose to group by asset in the app, the table will display one asset perrow. Hidebound extracts metadata from each filename (not any directory name) aswell as from the file itself. That metadata is called file_traits. Using onlyinformation derived from filename and file traits, Hidebound determines whichfiles are grouped together as a single asset and the specification of thatasset. Asset traits are then derived from this set of files (one or more).Finally, Hidebound validates each asset according to its determinedspecification. All of this data is displayed as a table within the web app.Importantly, all of the errors in filenames, file traits and asset traits areincluded.### *Review Graph*![](resources/screenshots/graph.png)If we click on the graph tab, we are greeted by a hierarchical graph of all ourassets in our project directory. Our asset is red, meaning it's invalid. Validasset's are green, and all other files and directories, including parentdirectories, are cyan.### *Diagnose and Repair*We flip back to the data tab. Using table within it, we search (via SQL) for ourasset within Hidebound's freshly created database. We see an error in one of thefilenames, conveniently displayed in red text. The descriptor in one orf ourfilenames has capital letters in it. This violates Hidebound's namingconvention, and so we get an error. We go and rename the file appropriately andcall update again.  Our asset is now valid. The filenames are correct and we cansee in the height and width columns, that it's 1024 by 1024 and the channelscolumn says it has three.### *Create*Next we click the create button. For each valid asset, Hidebound generates fileand asset metadata as JSON files within the hidebound/metadata directory.Hidebound also copies or moves, depending on the config write mode, valid filesand directories into the hidebound/content directory. Hidebound/content andhidebound/metadata are both staging directories used for generating a validephemeral database. We now have a hidebound directory that looks like this(unmentioned assets are collapsed behind the ellipses):```shell/tmp/hidebound├── hidebound_config.yaml│├── specifications│   └── specifications.py│├── data│   ...│   └── p-cat001│       └── spec001│           └── p-cat001_s-spec001_d-running-cat_v001│               ├── p-cat001_s-spec001_d-running-cat_v001_c0000-0005_f0001.png│               ├── p-cat001_s-spec001_d-running-cat_v001_c0000-0005_f0002.png│               └── p-cat001_s-spec001_d-running-cat_v001_c0000-0005_f0003.png│├── metadata    ├── asset    │   ...    │   └── a9f3727c-cb9b-4eb1-bc84-a6bc3b756cc5.json    │    └── file        ...        ├── 279873a2-bfd0-4757-abf2-7dc4f771f992.json        ├── e50160ae-8678-40b3-b766-ee8311b1f0c9.json        └── ea95bd79-cb8f-4262-8489-efe734c5f65c.json```### *Export*The hidebound directories contain only valid assets. Thus, we are now free toexport this data to various data stores, such as AWS S3, MongoDB, and Girder.Exporters are are defined within the exporters subpackage. They expect apopulated hidebound directory and use the files and metadata therein to exporthidebound data. Exporter configurations are stored in the hidebound config,under the &quot;exporters&quot; key. Currently supported exporters include, disk, s3 andgirder. Below we can see the results of an export to Girder in the Girder webapp.![](resources/screenshots/girder.png)### *Delete*Once this export process is complete, we may click the delete button. Hidebounddeletes the hidebound/content and hidebound/metdata directories and all theircontents. If write_mode in the Hidebound configuration is set to &quot;copy&quot;, thenthis step will merely delete data created by Hidebound. If it is set to &quot;move&quot;,then Hidebound will presumably delete, the only existing copy of out asset dataon the host machine. The delete stage in combination with the removal of assetsfrom the ingress directory is what makes Hidebound's database ephemeral.### *Workflow*`/api/workflow` is a API endpoint that initializes a database a with a givenconfig, and then calls each method from a given list. For instance, if you sendthis data to `/api/workflow`:```{config={...}, workflow=['update', 'create', 'export', 'delete']}```A database instance will be created with the given config, and then thatinstance will call its update, create, export and delete methods, in that order.# Naming ConventionHidebound is a highly opinionated framework that relies upon a strict butcomposable naming convention in order to extract metadata from filenames. Allfiles and directories that are part of assets must conform to a namingconvention defined within that asset's specification.In an over-simplified sense; sentences are constructions of words. Syntaxconcerns how each word is formed, grammar concerns how to form words into asentence, and semantics concerns what each word means. Similarly, filenames canbe thought of as crude sentences. They are made of several words (ie fields).These words have distinct semantics (as determines by field indicators). Eachword is constructed according to a syntax (ie indicator + token). All words arejoined together by spaces (ie underscores) in a particular order as determinedby grammar (as defined in each specification).### *Syntax*- Names consist of a series of fields, each separated by a single underscore  “_”, also called a field separator.- Periods, &quot;.&quot;, are the exception to this, as it indicates file extension.- Legal characters include and only include:| Name             | Characters |  Use                      || ---------------- | ---------- | ------------------------- || Underscore       | _          | only for field separation || Period           | .          | only for file extensions  || Lowercase letter | a to z     | everything                || Number           | 0 to 9     | everything                || Hyphen           | -          | token separator           |Fields are comprised of two main parts:| Name             | Use                                                 || ---------------- | --------------------------------------------------- || Field indicator  | determines metadata key                             || Field token      | a set of 1+ characters that define the field's data |---### **Example Diagrams**In our example filename:`p-cat001_s-spec001_d-running-cat_v001_c0000-0005_f0003.png` the metadata will be:```json{    &quot;project&quot;: &quot;cat001&quot;,    &quot;specification&quot;: &quot;spec001&quot;,    &quot;descriptor&quot;: &quot;running-cat&quot;,    &quot;version&quot;: 1,    &quot;coordinate&quot;: [0, 5],    &quot;frame&quot;: 3,    &quot;extension&quot;: &quot;png&quot;,}```The spec001 specification is derived from the second field of this filename:```shell      field   field  indicator   token          | __|__         | |     |p-cat001_s-spec001_d-running-cat_v001_c0000-0005_f0003.png         |_______|             |           field```| Part             | Value                    || ---------------- | ------------------------ || Field            | s-spec001                || Field indicator  | s-                       || Field token      | spec001                  || Derived metadata | {specification: spec001} |### *Special Field Syntax*- Projects begin with 3 or 4 letters followed by 1 to 4 numbers- Specifications begin with 3 or 4 letters followed by 3 numbers- Descriptors begin with a letter or number and may also contain hyphens- Descriptors may not begin with the words master, final or last- Versions are triple-padded with zeros and must be greater than 0- Coordinates may contain up to 3 quadruple-padded numbers, separated by hyphens- Coordinates are always evaluated in XYZ order. For example: `c0001-0002-0003`  produces `{x: 1, y: 2, z: 3}`.- Each element of a coordinate may be equal to or greater than zero- Frames are quadruple-padded and are greater than or equal to 0- Extensions may only contain upper and lower case letters a to z and numbers 0  to 9### *Semantics*Hidebound is highly opionated, especially with regards to its semantics. Itcontains exactly seven field types, as indicated by their field indicators.They are:| Field         | Indicator || ------------- | --------- || project       | p-        || specification | s-        || descriptor    | d-        || version       | v         || coordinate    | c         || frame         | f         || extension     | .         |### *Grammar*The grammar is fairly simple:  - Names are comprised of an ordered set of fields drawn from the seven above  - All names must contain the specification field  - All specification must define a field order  - All fields of a name under that specification must occcur in its defined    field orderIts is highly encouraged that fields be defined in the following order:`project specification descriptor version coordinate frame extension`The grammatical concept of field order here is one of rough encapsulation:- Projects contain assets- Assets are grouped by specification- A set of assets of the same content is grouped by a descriptor- That set of assets consists of multiple versions of the same content- A single asset may broken into chunks, identified by 1, 2 or 3 coordinates- Each chunk may consist of a series of files seperated by frame number- Each file has an extension### *Encouraged Lexical Conventions*- Specifications end with a triple padded number so that they may be explicitely  versioned. You redefine an asset specification to something slightly  different, by copying its specification class, adding one to its name and  change the class attributes in some way. That way you always maintain  backwards compatibility with legacy assets.- Descriptors are not a dumping ground for useless terms like wtf, junk, stuff,  wip and test.- Descriptors should not specify information known at the asset specification  level, such as the project name, the generic content of the asset (ie image,  mask, png, etc).- Descriptors should not include information that can be known from the  preceding tokens, such as version, frame or extension.- A descriptor should be applicable to every version of the asset it designates.- Use of hyphens in descriptors is encouraged.- When in doubt, hyphenate and put into the descriptor.---# Project StructureHidebound does not formally define a project structure. It merely stipulatesthat assets must exist under some particular root directory. Each assetspecification does define a directory structure for the files that make up thatasset. Assets are divided into 3 types: file, sequence and complex. File definesan asset that consists of a single file. Sequence is defined to be a singledirectory containing one or more files. Complex is for assets that consist of anarbitrarily complex layout of directories and files.The following project structure is recommended:```shellproject    |-- specification        |-- descriptor            |-- asset      # either a file or directory of files and directories                |- file```#### For Example```shell/tmp/projects└── p-cat001    ├── s-spec002    │   ├── d-calico-jumping    │   │   └── p-cat001_s-spec002_d-calico-jumping_v001    │   │       ├── p-cat001_s-spec002_d-calico-jumping_v001_f0001.png    │   │       ├── p-cat001_s-spec002_d-calico-jumping_v001_f0002.png    │   │       └── p-cat001_s-spec002_d-calico-jumping_v001_f0003.png    │   │    │   └── d-tabby-playing    │       ├── p-cat001_s-spec002_d-tabby-playing_v001    │       │   ├── p-cat001_s-spec002_d-tabby-playing_v001_f0001.png    │       │   ├── p-cat001_s-spec002_d-tabby-playing_v001_f0002.png    │       │   └── p-cat001_s-spec002_d-tabby-playing_v001_f0003.png    │       │    │       └── p-cat001_s-spec002_d-tabby-playing_v002    │           ├── p-cat001_s-spec002_d-tabby-playing_v002_f0001.png    │           ├── p-cat001_s-spec002_d-tabby-playing_v002_f0002.png    │           └── p-cat001_s-spec002_d-tabby-playing_v002_f0003.png    │    └── spec001        └── p-cat001_s-spec001_d-running-cat_v001            ├── p-cat001_s-spec001_d-Running-Cat_v001_c0000-0005_f0002.png            ├── p-cat001_s-spec001_d-running-cat_v001_c0000-0005_f0001.png            └── p-cat001_s-spec001_d-running-cat_v001_c0000-0005_f0003.png```# ApplicationThe Hidebound web application has five sections: data, graph, config, api anddocs.### DataThe data tab is the workhorse of the Hidebound app.![](resources/screenshots/data.png)Its functions are as follows:* Search - Search the updated database's data via SQL* Dropdown - Groups search results by file or asset* Init - Initialized the database with the current config* Update - Initializes and updates the database with the current config* Create - Copies or moves valid assets to hidebound/content directory and           creates JSON files in hidebound/metadata directory* Delete - Deletes hidebound/content and hidebound/metadata directoriesPrior to calling update, the application will look like this:![](resources/screenshots/pre_update.png)### GraphThe graph tab is used for visualizing the state of all the assets within a rootdirectory.![](resources/screenshots/graph.png)It's color code is as follows:| Color | Meaning                     || ----- | --------------------------- || Cyan  | Non-asset file or directory || Green | Valid asset                 || Red   | Invalid asset               |### ConfigThe config tab is used for uploading and writing Hidebound's configuration file.![](resources/screenshots/config.png)### APIThe API tab is really a link to Hidebound's REST API documentation.![](resources/screenshots/api.png)### DocsThe API tab is really a link to Hidebound's github documentation.![](resources/screenshots/docs.png)### ErrorsHidebound is oriented towards developers and technically proficient users. Itdisplays errors in their entirety within the application.![](resources/screenshots/error.png)# ConfigurationHidebound is configured via a configuration file or environment variables.Hidebound configs consist of four main sections:### Base* ingress_directory - the directory hidebound parses for assets that comprise its database* staging_directory - the staging directory valid assets are created in* specification_files - a list of python specification files* include_regex - filepaths in the root that match this are included in the database* exclude_regex - filepaths in the root that match this are excluded from the database* write_mode - whether to copy or move files from root to staging* redact_regex - regular expression which matches config keys whose valuse are to be redacted* redact_hash - whether to redact config values with &quot;REDACTED&quot; or a hash of the value* workflow - order list of steps to be followed in workflow### DaskDefault configuration of Dask distributed framework.cluster_type - dask cluster typenum_partitions - number of partions for each dataframelocal_num_workers - number of workers on local clusterlocal_threads_per_worker - number of threads per worker on local clusterlocal_multiprocessing - use multiprocessing for local clustergateway_address - gateway server addressgateway_proxy_address - scheduler proxy server addressgateway_public_address - gateway server address, as accessible from a web browsergateway_auth_type - authentication typegateway_api_token - api tokengateway_cluster_options - list of dask gateway cluster optionsgateway_shutdown_on_close - whether to shudown cluster upon close### ExportersWhich exporters to us in the workflow.Options include:* s3* disk* girder### WebhooksWebhooks to call after the export phase has completed.---### Environment VariablesIf `HIDEBOUND_CONFIG_FILEPATH` is set, Hidebound will ignore all otherenvironment variables and read the given filepath in as a yaml or json configfile.| Variable                                 | Format | Portion                                                  || ---------------------------------------- | ------ | -------------------------------------------------------- || HIDEBOUND_CONFIG_FILEPATH                | str    | Entire Hidebound config file                             || HIDEBOUND_INGRESS_DIRECTORY              | str    | ingress_directory parameter of config                    || HIDEBOUND_STAGING_DIRECTORY              | str    | staging_directory parameter of config                    || HIDEBOUND_INCLUDE_REGEX                  | str    | include_regex parameter of config                        || HIDEBOUND_EXCLUDE_REGEX                  | str    | exclude_regex parameter of config                        || HIDEBOUND_WRITE_MODE                     | str    | write_mode parameter of config                           || HIDEBOUND_REDACT_REGEX                   | str    | redact_regex parameter of config                         || HIDEBOUND_REDACT_HASH                    | str    | redact_hash parameter of config                          || HIDEBOUND_WORKFLOW                       | yaml   | workflow paramater of config                             || HIDEBOUND_SPECIFICATION_FILES            | yaml   | specification_files section of config                    || HIDEBOUND_DASK_CLUSTER_TYPE              | str    | dask cluster type                                        || HIDEBOUND_DASK_NUM_PARTITIONS            | int    | number of partions for each dataframe                    || HIDEBOUND_DASK_LOCAL_NUM_WORKERS         | int    | number of workers on local cluster                       || HIDEBOUND_DASK_LOCAL_THREADS_PER_WORKER  | int    | number of threads per worker on local cluster            || HIDEBOUND_DASK_LOCAL_MULTIPROCESSING     | str    | use multiprocessing for local cluster                    || HIDEBOUND_DASK_GATEWAY_ADDRESS           | str    | gateway server address                                   || HIDEBOUND_DASK_GATEWAY_PROXY_ADDRESS     | str    | scheduler proxy server address                           || HIDEBOUND_DASK_GATEWAY_PUBLIC_ADDRESS    | str    | gateway server address, as accessible from a web browser || HIDEBOUND_DASK_GATEWAY_AUTH_TYPE         | str    | authentication type                                      || HIDEBOUND_DASK_GATEWAY_API_TOKEN         | str    | api token                                                || HIDEBOUND_DASK_GATEWAY_CLUSTER_OPTIONS   | yaml   | list of dask gateway cluster options                     || HIDEBOUND_DASK_GATEWAY_SHUTDOWN_ON_CLOSE | str    | whether to shudown cluster upon close                    || HIDEBOUND_EXPORTERS                      | yaml   | exporters section of config                              || HIDEBOUND_WEBHOOKS                       | yaml   | webhooks section of config                               || HIDEBOUND_TESTING                        | str    | run in test mode                                         |---### Config FileHere is a full example config with comments:```yamlingress_directory: /mnt/storage/projects                                 # where hb looks for assetsstaging_directory: /mnt/storage/hidebound                                # hb staging directoryinclude_regex: &quot;&quot;                                                        # include files that matchexclude_regex: &quot;\\.DS_Store&quot;                                             # exclude files that matchwrite_mode: copy                                                         # copy files from root to staging                                                                         # options: copy, moveredact_regex: &quot;(_key|_id|_token|url)$&quot;                                   # regex matched config keys to redactredact_hash: true                                                        # hash redacted valuesworkflow:                                                                # workflow steps  - delete                                                               # clear staging directory  - update                                                               # create database from ingress files  - create                                                               # stage valid assets  - export                                                               # export assets in stagingspecification_files:                                                     # list of spec files  - /mnt/storage/specs/image_specs.py  - /mnt/storage/specs/video_specs.pydask:  cluster_type: local                                                    # Dask cluster type                                                                         # options: local, gateway  num_partitions: 16                                                     # number of partions for each dataframe  local_num_workers: 16                                                  # number of workers on local cluster  local_threads_per_worker: 1                                            # number of threads per worker on local cluster  local_multiprocessing: true                                            # use multiprocessing for local cluster  gateway_address: http://proxy-public/services/dask-gateway             # gateway server address  gateway_proxy_address: gateway://traefik-daskhub-dask-gateway.core:80  # scheduler proxy server address  gateway_public_address: https://dask-gateway/services/dask-gateway/    # gateway server address, as accessible from a web browser  gateway_auth_type: jupyterhub                                          # authentication type  gateway_api_token: token123                                            # api token  gateway_cluster_options:                                               # list of dask gateway options    - field: image                                                       # option field      label: image                                                       # option label      option_type: select                                                # options: bool, float, int, mapping, select, string      default: &quot;some-image:latest&quot;                                       # option default value      options:                                                           # list of choices if option_type is select        - &quot;some-image:latest&quot;                                            # choice 1        - &quot;some-image:0.1.2&quot;                                             # choice 2  gateway_shutdown_on_close: true                                        # whether to shudown cluster upon closeexporters:                                                               # dict of exporter configs  - name: disk                                                           # export to disk    target_directory: /mnt/storage/archive                               # target location    metadata_types:                                                      # options: asset, file, asset-chunk, file-chunk      - asset                                                            # only asset and file metadata      - file    dask:                                                                # dask settings override      num_workers: 8      local_threads_per_worker: 2  - name: s3                                                             # export to s3    access_key: ABCDEFGHIJKLMNOPQRST                                     # aws access key    secret_key: abcdefghijklmnopqrstuvwxyz1234567890abcd                 # aws secret key    bucket: prod-data                                                    # s3 bucket    region: us-west-2                                                    # bucket region    metadata_types:                                                      # options: asset, file, asset-chunk, file-chunk      - asset                                                            # drop file metadata      - asset-chunk      - file-chunk    dask:                                                                # dask settings override      cluster_type: gateway      num_workers: 64  - name: girder                                                         # export to girder    api_key: eyS0nj9qPC5E7yK5l7nhGVPqDOBKPdA3EC60Rs9h                    # girder api key    root_id: 5ed735c8d8dd6242642406e5                                    # root resource id    root_type: collection                                                # root resource type    host: http://prod.girder.com                                         # girder server url    port: 8180                                                           # girder server port    metadata_types:                                                      # options: asset, file      - asset                                                            # only asset metadata    dask:                                                                # dask settings override      num_workers: 10    dask:                                                                # dask settings override      num_workers: 10webhooks:                                                                # call these after export  - url: https://hooks.slack.com/services/ABCDEFGHI/JKLMNO               # slack URL    method: post                                                         # post this to slack    timeout: 60                                                          # timeout after 60 seconds    # params: {}                                                         # params to post (NA here)    # json: {}                                                           # json to post (NA here)    data:                                                                # data to post      channel: &quot;#hidebound&quot;                                              # slack data      text: export complete                                              # slack data      username: hidebound                                                # slack data    headers:                                                             # request headers      Content-type: application/json```# SpecificationAsset specifications are defined in python using the base classes found inspecification_base.py. The base classes are defined using the schematicsframework. Hidebound generates a single JSON blob of metadata for each file ofan asset, and then combines blob into a single blob with a list values per key.Thus every class member defined with schematics is encapsulated with ListType.### Example assetSuppose we have an image sequence asset that we wish to define a specificqtionfor. Our image sequences consist of a directory containing 1 or 3 channel pngwith frame numbers in the filename.```shellprojects    └── cat001        └── raw001            └── p-cat001_s-raw001_d-calico-jumping_v001                ├── p-cat001_s-raw001_d-calico-jumping_v001_f0001.png                ├── p-cat001_s-raw001_d-calico-jumping_v001_f0002.png                └── p-cat001_s-raw001_d-calico-jumping_v001_f0003.png```### Example specificationWe would write the following specification for such an asset.```pythonfrom schematics.types import IntType, ListType, StringTypeimport hidebound.core.validators as vd  # validates traitsimport hidebound.core.traits as tr      # gets properties of files and file namesfrom hidebound.core.specification_base import SequenceSpecificationBaseclass Raw001(SequenceSpecificationBase):    asset_name_fields = [  # naming convention for asset directory        'project', 'specification', 'descriptor', 'version'    ]    filename_fields = [    # naming convention for asset files        'project', 'specification', 'descriptor', 'version', 'frame',        'extension'    ]    height = ListType(IntType(), required=True)  # heights of png images    width = ListType(IntType(), required=True)   # widths of png images    frame = ListType(        IntType(),        required=True,        validators=[vd.is_frame]  # validates that frame is between 0 and 9999    )    channels = ListType(        IntType(),        required=True,        validators=[lambda x: vd.is_in(x, [1, 3])]  # validates that png is 1 or 3 channel    )    extension = ListType(        StringType(),        required=True,        validators=[            vd.is_extension,            lambda x: vd.is_eq(x, 'png')  # validates that image is png        ]    )    file_traits = dict(        width=tr.get_image_width,            # retrieves image width from file        height=tr.get_image_height,          # retrieves image height from file        channels=tr.get_num_image_channels,  # retrieves image channel number from file    )```</longdescription>
</pkgmetadata>