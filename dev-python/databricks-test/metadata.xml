<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># databricks_test## AboutAn experimental unit test framework for Databricks notebooks._This open-source project is not developed by nor affiliated with Databricks._## Installing```pip install databricks_test```## UsageAdd a cell at the beginning of your Databricks notebook:```python# Instrument for unit tests. This is only executed in local unit tests, not in Databricks.if 'dbutils' not in locals():    import databricks_test    databricks_test.inject_variables()```The `if` clause causes the inner code to be skipped when run in Databricks.Therefore there is no need to install the `databricks_test` module on your Databricks environment.Add your notebook into a code project, for example using [GitHub version control in Azure Databricks](https://docs.microsoft.com/en-us/azure/databricks/notebooks/azure-devops-services-version-control).Set up pytest in your code project (outside of Databricks).Create a test case with the following structure:```pythonimport databricks_testdef test_method():    with databricks_test.session() as dbrickstest:        # Set up mocks on dbrickstest        # ...        # Run notebook        dbrickstest.run_notebook(&quot;notebook_dir&quot;, &quot;notebook_name_without_py_suffix&quot;)        # Test assertions        # ...```You can set up [mocks](https://docs.python.org/dev/library/unittest.mock.html) on `dbrickstest`, for example:```pythondbrickstest.dbutils.widgets.get.return_value = &quot;myvalue&quot;```See samples below for more examples.## Supported features* Spark context injected into Databricks notebooks: `spark`, `table`, `sql` etc.* PySpark with all Spark features including reading and writing to disk, UDFs and Pandas UDFs* Databricks Utilities (`dbutils`, `display`) with user-configurable mocks* Mocking connectors such as Azure Storage, S3 and SQL Data Warehouse## Unsupported features* Notebook formats other than `.py` (`.ipynb`, `.dbc`) are not supported* Non-python cells such as `%scala` and `%sql` (those cells are skipped, as they are stored in `.py` notebooks as comments)* Writing directly to `/dbfs` mount on local filesystem:  write to a local temporary file instead and use dbutils.fs.cp() to copy to DBFS, which you can intercept with a mock* Databricks extensions to Spark such as `spark.read.format(&quot;binaryFile&quot;)`## Sample testSample test case for an ETL notebook reading CSV and writing Parquet.```pythonimport pandas as pdimport databricks_testfrom tempfile import TemporaryDirectoryfrom pandas.testing import assert_frame_equaldef test_etl():    with databricks_test.session() as dbrickstest:        with TemporaryDirectory() as tmp_dir:            out_dir = f&quot;{tmp_dir}/out&quot;            # Provide input and output location as widgets to notebook            switch = {                &quot;input&quot;: &quot;tests/etl_input.csv&quot;,                &quot;output&quot;: out_dir,            }            dbrickstest.dbutils.widgets.get.side_effect = lambda x: switch.get(                x, &quot;&quot;)            # Run notebook            dbrickstest.run_notebook(&quot;.&quot;, &quot;etl_notebook&quot;)            # Notebook produces a Parquet file (directory)            resultDF = pd.read_parquet(out_dir)        # Compare produced Parquet file and expected CSV file        expectedDF = pd.read_csv(&quot;tests/etl_expected.csv&quot;)        assert_frame_equal(expectedDF, resultDF, check_dtype=False)```In the notebook, we pass parameters using widgets.This makes it easy to passa local file location in tests, and a remote URL (such as Azure Storage or S3)in production.```python# Databricks notebook source# This notebook processed the training dataset (imported by Data Factory)# and computes a cleaned dataset with additional features such as city.from pyspark.sql.types import StructType, StructFieldfrom pyspark.sql.types import DoubleType, IntegerTypefrom pyspark.sql.functions import col, pandas_udf, PandasUDFType# COMMAND ----------# Instrument for unit tests. This is only executed in local unit tests, not in Databricks.if 'dbutils' not in locals():    import databricks_test    databricks_test.inject_variables()# COMMAND ----------# Widgets for interactive development.dbutils.widgets.text(&quot;input&quot;, &quot;&quot;)dbutils.widgets.text(&quot;output&quot;, &quot;&quot;)dbutils.widgets.text(&quot;secretscope&quot;, &quot;&quot;)dbutils.widgets.text(&quot;secretname&quot;, &quot;&quot;)dbutils.widgets.text(&quot;keyname&quot;, &quot;&quot;)# COMMAND ----------# Set up storage credentialsspark.conf.set(    dbutils.widgets.get(&quot;keyname&quot;),    dbutils.secrets.get(        scope=dbutils.widgets.get(&quot;secretscope&quot;),        key=dbutils.widgets.get(&quot;secretname&quot;)    ),)# COMMAND ----------# Import CSV filesschema = StructType(    [        StructField(&quot;aDouble&quot;, DoubleType(), nullable=False),        StructField(&quot;anInteger&quot;, IntegerType(), nullable=False),    ])df = (    spark.read.format(&quot;csv&quot;)    .options(header=&quot;true&quot;, mode=&quot;FAILFAST&quot;)    .schema(schema)    .load(dbutils.widgets.get('input')))display(df)# COMMAND ----------df.count()# COMMAND ----------# Inputs and output are pandas.Series of doubles@pandas_udf('integer', PandasUDFType.SCALAR)def square(x):    return x * x# COMMAND ----------# Write out Parquet data(df    .withColumn(&quot;aSquaredInteger&quot;, square(col(&quot;anInteger&quot;)))    .write    .parquet(dbutils.widgets.get('output')) )```## Advanced mockingSample test case mocking PySpark classes for a notebook connecting to Azure SQL Data Warehouse.```pythonimport databricks_testimport pysparkimport pyspark.sql.functions as Ffrom tempfile import TemporaryDirectoryfrom pandas.testing import assert_frame_equalimport pandas as pddef test_sqldw(monkeypatch):    with databricks_test.session() as dbrickstest, TemporaryDirectory() as tmp:        out_dir = f&quot;{tmp}/out&quot;        # Mock SQL DW loader, creating a Spark DataFrame instead        def mock_load(reader):            return (                dbrickstest.spark                .range(10)                .withColumn(&quot;age&quot;, F.col(&quot;id&quot;) * 6)                .withColumn(&quot;salary&quot;, F.col(&quot;id&quot;) * 10000)            )        monkeypatch.setattr(            pyspark.sql.readwriter.DataFrameReader, &quot;load&quot;, mock_load)        # Mock SQL DW writer, writing to a local Parquet file instead        def mock_save(writer):            monkeypatch.undo()            writer.format(&quot;parquet&quot;)            writer.save(out_dir)        monkeypatch.setattr(            pyspark.sql.readwriter.DataFrameWriter, &quot;save&quot;, mock_save)        # Run notebook        dbrickstest.run_notebook(&quot;.&quot;, &quot;sqldw_notebook&quot;)        # Notebook produces a Parquet file (directory)        resultDF = pd.read_parquet(out_dir)        # Compare produced Parquet file and expected CSV file        expectedDF = pd.read_csv(&quot;tests/sqldw_expected.csv&quot;)        assert_frame_equal(expectedDF, resultDF, check_dtype=False)```## IssuesPlease report issues at [https://github.com/microsoft/DataOps/issues](https://github.com/microsoft/DataOps/issues).</longdescription>
</pkgmetadata>