<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Documentation Status](https://readthedocs.org/projects/njet/badge/?version=latest)](https://njet.readthedocs.io/en/latest/?badge=latest)# njet: Lightweight automatic differentiationA lightweight AD package, using forward-mode automatic differentiation, in order to determine thehigher-order derivatives of a given function in several variables.## Features- Higher-order (forward-mode) automatic differentiation in several variables.- Support for NumPy, SymPy and mpmath.- Differentiation of expressions containing nested higher-order derivatives.- Complex differentiation (Wirtinger calculus) possible.- Faa di Bruno's formula for vector-valued functions implemented.- Lightweight and easy to use.## InstallationInstall this module with pip```shpip install njet```## QuickstartAn example function we want to differentiate```pythonfrom njet.functions import expf = lambda x, y, z: exp(-0.23*x**2 - 0.33*x*y - 0.11*z**2)```Generate a class to handle the derivatives of the given function (in this example up to order 3)```pythonfrom njet import derivedf = derive(f, order=3)```Evaluate the derivatives at a specific point```pythondf(0.4, 2.1, 1.73){(0, 0, 0): 0.5255977986928584, (0, 0, 1): -0.2000425221825019, (1, 0, 0): -0.46094926945363685, (0, 1, 0): -0.06937890942745731, (0, 0, 2): -0.03949533176976862, (0, 2, 0): 0.009158016044424365, (1, 0, 1): 0.1754372919540542, (0, 1, 1): 0.026405612928090252, (2, 0, 0): 0.1624775219121247, (1, 1, 0): -0.11260197000076322, (2, 1, 0): 0.2827794849469999, (1, 1, 1): 0.04285630978229049, (0, 1, 2): 0.005213383793609458, (0, 2, 1): -0.0034855409065079135, (0, 3, 0): -0.0012088581178640162, (3, 0, 0): 0.2815805411804125, (2, 0, 1): -0.061838944839754675, (0, 0, 3): 0.10305063303187477, (1, 2, 0): 0.03775850015116166, (1, 0, 2): 0.034637405962087094}```The indices here correspond to the powers of the variables x, y, zin the multivariate Taylor expansion. They can be translated tothe tensor indices of the corresponding multilinear map using abuilt-in routine. Example:Obtain the gradient and the Hessian```pythondf.grad(){(2,): -0.2000425221825019, (0,): -0.46094926945363685, (1,): -0.06937890942745731}``````pythondf.hess(){(2, 2): -0.03949533176976862, (1, 1): 0.009158016044424365, (0, 2): 0.1754372919540542, (1, 2): 0.026405612928090252, (0, 0): 0.1624775219121247, (0, 1): -0.11260197000076322}```## Further readinghttps://njet.readthedocs.io/en/latest/index.html## Licensenjet: Automatic Differentiation LibraryCopyright (C) 2021, 2022, 2023 by Malte Titzenjet is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.njet is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with njet. If not, see &lt;https://www.gnu.org/licenses/&gt;.</longdescription>
</pkgmetadata>