<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># LarynxðŸŽ¥ [DEMO VIDEO](https://www.youtube.com/watch?v=hBmhDf8cl0k)Offline end-to-end text to speech system using [gruut](https://github.com/rhasspy/gruut) and [onnx](https://onnx.ai/) ([architecture](#architecture)). There are [50 voices available across 9 languages](#samples).```shcurl https://raw.githubusercontent.com/rhasspy/larynx/master/docker/larynx-server \    &gt; ~/bin/larynx-server &amp;&amp; chmod +755 ~/bin/larynx-serverlarynx-server```Visit http://localhost:5002 for the test page. See http://localhost:5002/openapi/ for HTTP endpoint documentation.![Larynx screenshot](img/web_screenshot.png)Supports a [subset of SSML](#ssml) that can use multiple voices and languages!``` xml&lt;speak&gt;  The 1st thing to remember is that 9 languages are supported in Larynx TTS as of 10/19/2021 at 10:39am.  &lt;voice name=&quot;harvard&quot;&gt;    &lt;s&gt;      The current voice can be changed!    &lt;/s&gt;  &lt;/voice&gt;  &lt;voice name=&quot;northern_english_male&quot;&gt;    &lt;s&gt;Breaks are possible&lt;/s&gt;    &lt;break time=&quot;0.5s&quot; /&gt;    &lt;s&gt;between sentences.&lt;/s&gt;  &lt;/voice&gt;  &lt;s lang=&quot;en&quot;&gt;    One language is never enough  &lt;/s&gt;  &lt;s lang=&quot;de&quot;&gt;   Eine Sprache ist niemals genug  &lt;/s&gt;  &lt;s lang=&quot;sw&quot;&gt;    Lugha moja haitoshi  &lt;/s&gt;&lt;/speak&gt;```Larynx's goals are:* &quot;Good enough&quot; synthesis to avoid using a cloud service* Faster than realtime performance on a Raspberry Pi 4 (with low quality vocoder)* Broad language support (9 languages)* Voices trained purely from public datasetsYou can use Larynx to:* Host a [text to speech HTTP endpoint](#docker-installation)* Synthesize text [on the command-line](#basic-synthesis)* [Read a book](#long-texts) to you## Samples[Listen to voice samples](https://rhasspy.github.io/larynx/) from all of the [pre-trained voices](https://github.com/rhasspy/larynx/releases).---## Docker InstallationPre-built Docker images are available for the following platforms:* `linux/amd64` - desktop/laptop/server* `linux/arm64` - Raspberry Pi 64-bit* `linux/arm/v7` - Raspberry Pi 32-bitThese images include a single English voice, but [many more can be downloaded](https://github.com/rhasspy/larynx/releases/tag/2021-03-28) from within the web interface.The [larynx](https://raw.githubusercontent.com/rhasspy/larynx/master/docker/larynx) and [larynx-server](https://raw.githubusercontent.com/rhasspy/larynx/master/docker/larynx-server) shell scripts wrap the Docker images, allowing you to use Larynx as a command-line tool.To manually run the Larynx web server in Docker:```shdocker run \    -it \    -p 5002:5002 \    -e &quot;HOME=${HOME}&quot; \    -v &quot;$HOME:${HOME}&quot; \    -v /usr/share/ca-certificates:/usr/share/ca-certificates \    -v /etc/ssl/certs:/etc/ssl/certs \    -w &quot;${PWD}&quot; \    --user &quot;$(id -u):$(id -g)&quot; \    rhasspy/larynx```Downloaded voices will be stored in `${HOME}/.local/share/larynx`.Visit http://localhost:5002 for the test page. See http://localhost:5002/openapi/ for HTTP endpoint documentation.## Debian InstallationPre-built Debian packages for [bullseye](https://www.debian.org/releases/bullseye/) are [available for download](https://github.com/rhasspy/larynx/releases/latest) with the name `larynx-tts_&lt;VERSION&gt;_&lt;ARCH&gt;.deb` where `ARCH` is one of `amd64` (most desktops, laptops), `armhf` (32-bit Raspberry Pi), and `arm64` (64-bit Raspberry Pi)    Example installation on a typical desktop:```shsudo apt install ./larynx-tts_&lt;VERSION&gt;_amd64.deb```From there, you may run the `larynx` command or `larynx-server` to start the web server (http://localhost:5002).## Python InstallationYou may need to install the following dependencies (besides Python 3.7+):```shsudo apt-get install libopenblas-base libgomp1 libatomic1```On 32-bit ARM systems (Raspberry Pi), you will also need:```shsudo apt-get install libatlas3-base libgfortran5```Next, create a Python virtual environment:```shpython3 -m venv larynx_venvsource larynx_venv/bin/activatepip3 install --upgrade pippip3 install --upgrade wheel setuptools```Next, install larynx:```shpip3 install -f 'https://synesthesiam.github.io/prebuilt-apps/' -f 'https://download.pytorch.org/whl/cpu/torch_stable.html' larynx```Then run `larynx` or `larynx.server` for the web server. You may also execute the Python module directly with `python3 -m larynx` and `python3 -m larynx.server`.### Voice/Vocoder DownloadVoices and vocoders are automatically downloaded when used on the command-line or in the web server. You can also [manually download each voice](https://github.com/rhasspy/larynx/releases/tag/2021-03-28). Extract them to `${HOME}/.local/share/larynx/voices` so that the directory structure follows the pattern `${HOME}/.local/share/larynx/voices/&lt;language&gt;,&lt;voice&gt;`.---## Command-Line InterfaceLarynx has a flexible command-line interface, available with:* The [larynx script](https://raw.githubusercontent.com/rhasspy/larynx/master/docker/larynx) for Docker * The `larynx` command from the Debian package* `larynx` or `python3 -m larynx` for Python installations### Basic Synthesis```shlarynx -v &lt;VOICE&gt; &quot;&lt;TEXT&gt;&quot; &gt; output.wav```where `&lt;VOICE&gt;` is a language name (`en`, `de`, etc) or a voice name (`ljspeech`, `thorsten`, etc). `&lt;TEXT&gt;` may contain multiple sentences, which will be combined in the final output WAV file. These can also be [split into separate WAV files](#multiple-wav-output).To adjust the quality of the output, use `-q &lt;QUALITY&gt;` where `&lt;QUALITY&gt;` is &quot;high&quot; (slowest), &quot;medium&quot;, or &quot;low&quot; (fastest).### SSML Synthesis```shlarynx --ssml -v &lt;VOICE&gt; &quot;&lt;SSML&gt;&quot; &gt; output.wav```where `&lt;SSML&gt;` is valid [SSML](https://www.w3.org/TR/speech-synthesis11/). Not all features are supported; for example:* Breaks (pauses) can only occur between sentences and can only be specified in seconds or milliseconds* Voices can only be referenced by name* Custom lexicons are not yet supported (you can use `&lt;phoneme ph=&quot;...&quot;&gt;`, however)If your SSML contains `&lt;mark&gt;` tags, add `--mark-file &lt;FILE&gt;` to the command-line. As the marks are encountered (between sentences), their names will be written on separate lines to the file.### CUDA Accelerated SynthesisThe `--cuda` flag will make use of a GPU if its available to PyTorch:``` shlarynx --cuda 'This is spoken on the GPU.' &gt; output.wav```Adding the `--half` flag will enable half-precision inference, which is often faster:``` shlarynx --cuda --half 'This is spoken on the GPU even faster.' &gt; output.wav```For CUDA acceleration to work, your voice must contain a PyTorch checkpoint file (`generator.pth`). Older Larynx voices did not have these, so you may need to [re-download your voices](https://github.com/rhasspy/larynx/releases/latest/).### Long TextsIf your text is very long, and you would like to listen to it as its being synthesized, use the `--raw-stream` option:```shlarynx -v en --raw-stream &lt; long.txt | aplay -r 22050 -c 1 -f S16_LE```Each input line will be synthesized and written the standard out as raw 16-bit 22050Hz mono PCM. By default, 5 sentences will be kept in an output queue, only blocking synthesis when the queue is full. You can adjust this value with `--raw-stream-queue-size`. Additionally, you can adjust `--max-thread-workers` to change how many threads are available for synthesis.If your long text is fixed-width with blank lines separating paragraphs like those from [Project Gutenberg](https://www.gutenberg.org/), use the `--process-on-blank-line` option so that sentences will not be broken at line boundaries. For example, you can listen to &quot;Alice in Wonderland&quot; like this:```shcurl --output - 'https://www.gutenberg.org/files/11/11-0.txt' | \    larynx -v ek --raw-stream --process-on-blank-line | aplay -r 22050 -c 1 -f S16_LE```### Multiple WAV OutputWith `--output-dir` set to a directory, Larynx will output a separate WAV file for each sentence:```shlarynx -v en 'Test 1. Test 2.' --output-dir /path/to/wavs```By default, each WAV file will be named using the (slightly modified) text of the sentence. You can have WAV files named using a timestamp instead with `--output-naming time`. For full control of the output naming, the `--csv` command-line flag indicates that each sentence is of the form `id|text` where `id` will be the name of the WAV file.```shcat &lt;&lt; EOF |s01|The birch canoe slid on the smooth planks.s02|Glue the sheet to the dark blue background.s03|It's easy to tell the depth of a well.s04|These days a chicken leg is a rare dish.s05|Rice is often served in round bowls.s06|The juice of lemons makes fine punch.s07|The box was thrown beside the parked truck.s08|The hogs were fed chopped corn and garbage.s09|Four hours of steady work faced us.s10|Large size in stockings is hard to sell.EOF  larynx --csv --voice en --output-dir /path/to/wavs```### Interactive ModeWith no text input and no output directory, Larynx will switch into interactive mode. After entering a sentence, it will be played with `--play-command` (default is `play` from SoX).```shlarynx -v enReading text from stdin...Hello world!&lt;ENTER&gt;```Use `CTRL+D` or `CTRL+C` to exit.### GlowTTS SettingsThe GlowTTS voices support two additional parameters:* `--noise-scale` - determines the speaker volatility during synthesis (0-1, default is  0.667)* `--length-scale` - makes the voice speaker slower (&gt; 1) or faster (&lt; 1)### Vocoder Settings* `--denoiser-strength` - runs the denoiser if &gt; 0; a small value like 0.005 is a good place to start.### List Voices and Vocoders```shlarynx --list```---## MaryTTS Compatible APITo use Larynx as a drop-in replacement for a [MaryTTS](http://mary.dfki.de/) server (e.g., for use with [Home Assistant](https://www.home-assistant.io/integrations/marytts/)), run:```shdocker run \    -it \    -p 59125:5002 \    -e &quot;HOME=${HOME}&quot; \    -v &quot;$HOME:${HOME}&quot; \    -v /usr/share/ca-certificates:/usr/share/ca-certificates \    -v /etc/ssl/certs:/etc/ssl/certs \    -w &quot;${PWD}&quot; \    --user &quot;$(id -u):$(id -g)&quot; \    rhasspy/larynx```The `/process` HTTP endpoint should now work for voices formatted as `&lt;LANG&gt;` or `&lt;VOICE&gt;`, e.g. `en` or `harvard`.You can specify the vocoder quality by adding `;&lt;QUALITY&gt;` to the MaryTTS voice where `QUALITY` is &quot;high&quot;, &quot;medium&quot;, or &quot;low&quot;.For example: `en;low` will use the lowest quality (but fastest) vocoder. This is usually necessary to get decent performance on a Raspberry Pi.---## SSMLA subset of [SSML](https://www.w3.org/TR/speech-synthesis11/) is supported (use `--ssml`):* `&lt;speak&gt;` - wrap around SSML text    * `lang` - set language for document* `&lt;s&gt;` - sentence (disables automatic sentence breaking)    * `lang` - set language for sentence* `&lt;w&gt;` / `&lt;token&gt;` - word (disables automatic tokenization)* `&lt;voice name=&quot;...&quot;&gt;` - set voice of inner text    * `voice` - name or language of voice* `&lt;say-as interpret-as=&quot;&quot;&gt;` - force interpretation of inner text    * `interpret-as` one of &quot;spell-out&quot;, &quot;date&quot;, &quot;number&quot;, &quot;time&quot;, or &quot;currency&quot;    * `format` - way to format text depending on `interpret-as`        * number - one of &quot;cardinal&quot;, &quot;ordinal&quot;, &quot;digits&quot;, &quot;year&quot;        * date - string with &quot;d&quot; (cardinal day), &quot;o&quot; (ordinal day), &quot;m&quot; (month), or &quot;y&quot; (year)* `&lt;break time=&quot;&quot;&gt;` - Pause for given amount of time    * time - seconds (&quot;123s&quot;) or milliseconds (&quot;123ms&quot;)* `&lt;mark name=&quot;&quot;&gt;` - User-defined mark (written to `--mark-file` or part of `TextToSpeechResult`)    * name - name of mark* `&lt;sub alias=&quot;&quot;&gt;` - substitute `alias` for inner text* `&lt;phoneme ph=&quot;...&quot;&gt;` - supply phonemes for inner text    * `ph` - phonemes for each word of inner text, separated by whitespace* `&lt;lexicon id=&quot;...&quot;&gt;` - inline pronunciation lexicon    * `id` - unique id of lexicon (used in `&lt;lookup ref=&quot;...&quot;&gt;`)    * One or more `&lt;lexeme&gt;` child elements with:        * `&lt;grapheme role=&quot;...&quot;&gt;WORD&lt;/grapheme&gt;` - word text (optional [role][#word-roles])        * `&lt;phoneme&gt;P H O N E M E S&lt;/phoneme&gt;` - word pronunciation (phonemes separated by whitespace)* `&lt;lookup ref=&quot;...&quot;&gt;` - use inline pronunciation lexicon for child elements    * `ref` - id from a `&lt;lexicon id=&quot;...&quot;&gt;`### Word RolesDuring phonemization, word roles are used to disambiguate pronunciations. Unless manually specified, a word's role is derived from its part of speech tag as `gruut:&lt;TAG&gt;`. For initialisms and `spell-out`, the role `gruut:letter` is used to indicate that e.g., &quot;a&quot; should be spoken as `/eÉª/` instead of `/É™/`.For `en-us`, the following additional roles are available from the part-of-speech tagger:* `gruut:CD` - number* `gruut:DT` - determiner* `gruut:IN` - preposition or subordinating conjunction * `gruut:JJ` - adjective* `gruut:NN` - noun* `gruut:PRP` - personal pronoun* `gruut:RB` - adverb* `gruut:VB` - verb* `gruut:VB` - verb (past tense)### Inline LexiconsInline [pronunciation lexicons](https://www.w3.org/TR/2008/REC-pronunciation-lexicon-20081014/) are supported via the `&lt;lexicon&gt;` and `&lt;lookup&gt;` tags. gruut diverges slightly from the [SSML standard](https://www.w3.org/TR/speech-synthesis11/) here by only allowing lexicons to be defined within the SSML document itself. Additionally, the `id` attribute of the `&lt;lexicon&gt;` element can be left off to indicate a &quot;default&quot; inline lexicon that does not require a corresponding `&lt;lookup&gt;` tag.For example, the following document will yield three different pronunciations for the word &quot;tomato&quot;:``` xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;speak version=&quot;1.1&quot;       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis                 http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;       xml:lang=&quot;en-US&quot;&gt;  &lt;lexicon xml:id=&quot;test&quot; alphabet=&quot;ipa&quot;&gt;    &lt;lexeme&gt;      &lt;grapheme&gt;        tomato      &lt;/grapheme&gt;      &lt;phoneme&gt;        &lt;!-- Individual phonemes are separated by whitespace --&gt;        t É™ m ËˆÉ‘ t oÊŠ      &lt;/phoneme&gt;    &lt;/lexeme&gt;    &lt;lexeme&gt;      &lt;grapheme role=&quot;fake-role&quot;&gt;        tomato      &lt;/grapheme&gt;      &lt;phoneme&gt;        &lt;!-- Made up pronunciation for fake word role --&gt;        t É™ m Ëˆi t oÊŠ      &lt;/phoneme&gt;    &lt;/lexeme&gt;  &lt;/lexicon&gt;  &lt;w&gt;tomato&lt;/w&gt;  &lt;lookup ref=&quot;test&quot;&gt;    &lt;w&gt;tomato&lt;/w&gt;    &lt;w role=&quot;fake-role&quot;&gt;tomato&lt;/w&gt;  &lt;/lookup&gt;&lt;/speak&gt;```The first &quot;tomato&quot; will be looked up in the U.S. English lexicon (`/t É™ m ËˆeÉª t oÊŠ/`). Within the `&lt;lookup&gt;` tag's scope, the second and third &quot;tomato&quot; words will be looked up in the inline lexicon. The third &quot;tomato&quot; word has a [role](#word-roles) attached  (selecting a made up pronunciation in this case).Even further from the SSML standard, gruut allows you to leave off the `&lt;lexicon&gt;` id entirely. With no `id`, a `&lt;lookup&gt;` tag is no longer needed, allowing you to override the pronunciation of any word in the document: ``` xml&lt;?xml version=&quot;1.0&quot;?&gt;&lt;speak version=&quot;1.1&quot;       xmlns=&quot;http://www.w3.org/2001/10/synthesis&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xsi:schemaLocation=&quot;http://www.w3.org/2001/10/synthesis                 http://www.w3.org/TR/speech-synthesis11/synthesis.xsd&quot;       xml:lang=&quot;en-US&quot;&gt;  &lt;!-- No id means change all words without a lookup --&gt;  &lt;lexicon&gt;    &lt;lexeme&gt;      &lt;grapheme&gt;        tomato      &lt;/grapheme&gt;      &lt;phoneme&gt;        t É™ m ËˆÉ‘ t oÊŠ      &lt;/phoneme&gt;    &lt;/lexeme&gt;  &lt;/lexicon&gt;  &lt;w&gt;tomato&lt;/w&gt;&lt;/speak&gt;```This will yield a pronunciation of `/t É™ m ËˆÉ‘ t oÊŠ/` for all instances of &quot;tomato&quot; in the document (unless they have a `&lt;lookup&gt;`).---## Text to Speech Models* [GlowTTS](https://github.com/rhasspy/glow-tts-train) (50 voices)    * English (`en-us`, 27 voices)        * blizzard\_fls (F, accent, [Blizzard](https://www.cstr.ed.ac.uk/projects/blizzard/2017/usborne_blizzard2017/license.html))        * blizzard\_lessac (F, [Blizzard](https://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011/license.html))        * cmu\_aew (M, [Arctic](licenses/cmuarctic.txt))        * cmu\_ahw (M, [Arctic](licenses/cmuarctic.txt))        * cmu\_aup (M, accent, [Arctic](licenses/cmuarctic.txt))        * cmu\_bdl (M, [Arctic](licenses/cmuarctic.txt))        * cmu\_clb (F, [Arctic](licenses/cmuarctic.txt))        * cmu\_eey (F, [Arctic](licenses/cmuarctic.txt))        * cmu\_fem (M, [Arctic](licenses/cmuarctic.txt))        * cmu\_jmk (M, [Arctic](licenses/cmuarctic.txt))        * cmu\_ksp (M, accent, [Arctic](licenses/cmuarctic.txt))        * cmu\_ljm (F, [Arctic](licenses/cmuarctic.txt))        * cmu\_lnh (F, [Arctic](licenses/cmuarctic.txt))        * cmu\_rms (M, [Arctic](licenses/cmuarctic.txt))        * cmu\_rxr (M, [Arctic](licenses/cmuarctic.txt))        * cmu\_slp (F, accent, [Arctic](licenses/cmuarctic.txt))        * cmu\_slt (F, [Arctic](licenses/cmuarctic.txt))        * ek (F, accent, [M-AILabs](licenses/m-ailabs.txt))        * harvard (F, accent, [CC/Attr/NC](https://creativecommons.org/licenses/by-nc/4.0/legalcode))        * kathleen (F, [CC0](licenses/cc0.txt))        * ljspeech (F, [Public Domain](https://librivox.org/pages/public-domain/))        * mary\_ann (F, [M-AILabs](licenses/m-ailabs.txt))        * northern\_english\_male (M, [CC/Attr/SA](licenses/cc4asa.txt))        * scottish\_english\_male (M, [CC/Attr/SA](licenses/cc4asa.txt))        * southern\_english\_female (F, [CC/Attr/SA](licenses/cc4asa.txt))        * southern\_english\_male (M, [CC/Attr/SA](licenses/cc4asa.txt))        * judy\_bieber (F, [M-AILabs](licenses/m-ailabs.txt))    * German (`de-de`, 7 voices)        * eva\_k (F, [M-AILabs](licenses/m-ailabs.txt))        * hokuspokus (F, [CC0](licenses/cc0.txt))        * karlsson (M, [M-AILabs](licenses/m-ailabs.txt))        * kerstin (F, [CC0](licenses/cc0.txt))        * pavoque (M, [CC4/BY/NC/SA](https://github.com/marytts/pavoque-data))        * rebecca\_braunert\_plunkett (F, [M-AILabs](licenses/m-ailabs.txt))        * thorsten (M, [CC0](licenses/cc0.txt))    * French (`fr-fr`, 3 voices)        * gilles\_le\_blanc (M, [M-AILabs](licenses/m-ailabs.txt))        * siwis (F, [CC/Attr](licenses/cc4a.txt))        * tom (M, [ODbL](licenses/odbl.txt))    * Spanish (`es-es`, 2 voices)        * carlfm (M, public domain)        * karen\_savage (F, [M-AILabs](licenses/m-ailabs.txt))    * Dutch (`nl`, 4 voices)        * bart\_de\_leeuw (M, [Apache2](licenses/apache2.txt))        * flemishguy (M, [CC0](licenses/cc0.txt))        * rdh (M, [CC0](licenses/cc0.txt))        * nathalie (F, [CC0](licenses/cc0.txt))    * Italian (`it-it`, 2 voices)        * lisa (F, [M-AILabs](licenses/m-ailabs.txt))        * riccardo\_fasol (M, [Apache2](licenses/apache2.txt))    * Swedish (`sv-se`, 1 voice)        * talesyntese (M, [CC0](licenses/cc0.txt))    * Swahili (`sw`, 1 voice)        * blblia\_takatifu (M, [Sermon Online](https://www.sermon-online.com))    * Russian (`ru-ru`, 3 voices)        * hajdurova (F, [M-AILabs](licenses/m-ailabs.txt))        * nikolaev (M, [M-AILabs](licenses/m-ailabs.txt))        * minaev (M, [M-AILabs](licenses/m-ailabs.txt))## Vocoders* [Hi-Fi GAN](https://github.com/rhasspy/hifi-gan-train)    * Universal large (slowest)    * VCTK &quot;small&quot;    * VCTK &quot;medium&quot; (fastest)    ---## BenchmarksThe following benchmarks were run on:* Core i7-8750H (`amd64`)* Raspberry Pi 4 (`aarch64`)* Raspberry Pi 3 (`armv7l`)Multiple runs were done at each quality level, with the first run being discarded so that cache for the model files was hot.The RTF (real-time factor) is computed as the time taken to synthesize audio divided by the duration of the synthesized audio. An RTF less than 1 indicates that audio was able to be synthesized faster than real-time.| Platform | Quality | RTF   || -------- | ------- | ---   || amd64    | high    | 0.25  || amd64    | medium  | 0.06  || amd64    | low     | 0.05  || -------- | ------- | ---   || aarch64  | high    | 4.28  || aarch64  | medium  | 1.82  || aarch64  | low     | 0.56  || -------- | ------- | ---   || armv7l   | high    | 16.83 || armv7l   | medium  | 7.16  || armv7l   | low     | 2.22  |See the benchmarking scripts in `scripts/` for more details.---## ArchitectureLarynx breaks text to speech into 4 distinct steps:1. Text to [IPA](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet) phonemes ([gruut](https://github.com/rhasspy/gruut))2. Phonemes to ids (`phonemes.txt` file from voice)3. Phoneme ids to mel spectrograms ([glow-tts](https://github.com/rhasspy/glow-tts-train))4. Mel spectrograms to waveforms ([hifi-gan](https://github.com/rhasspy/hifi-gan-train))![Larynx architecture](img/architecture.png)Voices are trained on phoneme ids and mel spectrograms. For each language, the voice with the most data available was used as a base model and fine-tuned.</longdescription>
</pkgmetadata>