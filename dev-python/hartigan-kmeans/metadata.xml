<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;!-- This should be the location of the title of the repository, normally the short name --&gt;# Hartigan's K-Means&lt;!-- Build Status, is a great thing to have at the top of your repository, it shows that you take your CI/CD as first class citizens --&gt;&lt;!-- [![Build Status](https://travis-ci.org/jjasghar/ibm-cloud-cli.svg?branch=master)](https://travis-ci.org/jjasghar/ibm-cloud-cli) --&gt;[![Build Status](https://github.com/IBM/hartigan-kmeans/actions/workflows/build.yml/badge.svg)](https://github.com/IBM/hartigan-kmeans/actions/workflows/build.yml)&lt;!-- Not always needed, but a scope helps the user understand in a short sentence like below, why this repo exists --&gt;## ScopeThis project provides an efficient implementation of Hartigan’s method for k-means clustering ([Hartigan 1975](#references)). It builds on the work of [Slonim, Aharoni and Crammer (2013)](#references), which introduced a significant improvement to the algorithm computational complexity, and adds an additional optimization for inputs in sparse vector representation. The project is packaged as a python library with a cython-wrapped C++ extension for the partition optimization code. A pure python implementation is included as well.## Installation```pip install hartigan-kmeans```&lt;!-- A more detailed Usage or detailed explanation of the repository here --&gt;## UsageThe main class in this library is `HKmeans`, which implements the clustering interface of [SciKit Learn][sklearn], providing methods such as `fit()`, `fit_transform()`, `fit_predict()`, etc. The sample code below clusters the 18.8K documents of the 20-News-Groups dataset into 20 clusters:```pythonimport numpy as npfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.datasets import fetch_20newsgroupsfrom sklearn import metricsfrom hkmeans import HKMeans# read the datasetdataset = fetch_20newsgroups(subset='all', categories=None,                             shuffle=True, random_state=256)gold_labels = dataset.targetn_clusters = np.unique(gold_labels).shape[0]# create count vectors using the 10K most frequent wordsvectorizer = TfidfVectorizer(max_features=10000)X = vectorizer.fit_transform(dataset.data)# HKMeans initialization and clustering; parameters:# perform 10 random initializations (n_init=10); the best one is returned.# up to 15 optimization iterations in each initialization (max_iter=15)# use all cores in the running machine for parallel execution (n_jobs=-1)hkmeans = HKMeans(n_clusters=n_clusters, random_state=128, n_init=10,                  n_jobs=-1, max_iter=15, verbose=True)hkmeans.fit(X)# report standard clustering metricsprint(&quot;Homogeneity: %0.3f&quot; % metrics.homogeneity_score(gold_labels, hkmeans.labels_))print(&quot;Completeness: %0.3f&quot; % metrics.completeness_score(gold_labels, hkmeans.labels_))print(&quot;V-measure: %0.3f&quot; % metrics.v_measure_score(gold_labels, hkmeans.labels_))print(&quot;Adjusted Rand-Index: %.3f&quot; % metrics.adjusted_rand_score(gold_labels, hkmeans.labels_))print(&quot;Adjusted Mutual-Info: %.3f&quot; % metrics.adjusted_mutual_info_score(gold_labels, hkmeans.labels_))```Expected result:```Homogeneity: 0.245Completeness: 0.290V-measure: 0.266Adjusted Rand-Index: 0.099Adjusted Mutual-Info: 0.263```See the [Examples](examples) directory for more illustrations and a comparison against Lloyd's K-Means.&lt;!-- License and Authors is optional here, but gives you the ability to highlight who is involed in the project --&gt;## License```textCopyright IBM Corporation 2021Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.```If you would like to see the detailed LICENSE click [here](LICENSE).## Authors - Algorithm: [Hartigan 1975](#references)- Pseudo-code and optimization: [Slonim, Aharoni and Crammer (2013)](#references)- Programming, optimization and maintenance: [Assaf Toledo](https://github.com/assaftibm)&lt;!-- Questions can be useful but optional, this gives you a place to say, &quot;This is how to contact this project maintainers or create PRs --&gt;If you have any questions or issues you can create a new [issue here][issues].## References- Hartigan, John A. Clustering algorithms. Wiley series in probability and mathematical statistics: Applied probability and statistics. John Wiley &amp; Sons, Inc., 1975.- Slonim, Noam, Ehud Aharoni, and Koby Crammer. &quot;Hartigan's K-Means Versus Lloyd's K-Means—Is It Time for a Change?.&quot; Twenty-Third International Joint Conference on Artificial Intelligence. 2013.[issues]: https://github.com/IBM/hartigan-kmeans/issues/new[sklearn]: https://scikit-learn.org</longdescription>
</pkgmetadata>