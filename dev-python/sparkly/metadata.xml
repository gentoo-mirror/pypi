<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Sparkly=======|Sparkly PyPi Version| |Sparkly Build Status| |Documentation Status|Helpers &amp; syntax sugar for PySpark. There are several features to make your life easier:- Definition of spark packages, external jars, UDFs and spark options within your code;- Simplified reader/writer api for Cassandra, Elastic, MySQL, Kafka;- Testing framework for spark applications.More details could be found in `the officialdocumentation &lt;https://sparkly.readthedocs.org&gt;`__.Installation------------Sparkly itself is easy to install::    pip install sparklyThe tricky part is ``pyspark``. There is no official distribution onPyPI. As a workaround we can suggest:1) Use env variable ``PYTHONPATH`` to point to your Spark installation,   something like::       export PYTHONPATH=&quot;/usr/local/spark/python/lib/pyspark.zip:/usr/local/spark/python/lib/py4j-0.10.4-src.zip&quot;2) Use our ``setup.py`` file for ``pyspark``. Just add this to your   ``requirements.txt``::       -e git+https://github.com/Tubular/spark@branch-2.1.0#egg=pyspark&amp;subdirectory=pythonHere in Tubular, we published ``pyspark`` to our internal PyPirepository.Getting Started---------------Here is a small code snippet to show how to easily read Cassandra tableand write its content to ElasticSearch index::    from sparkly import SparklySession    class MySession(SparklySession):        packages = [            'datastax:spark-cassandra-connector:2.0.0-M2-s_2.11',            'org.elasticsearch:elasticsearch-spark-20_2.11:6.5.4',        ]            if __name__ == '__main__':        spark = MySession()        df = spark.read_ext.cassandra('localhost', 'my_keyspace', 'my_table')        df.write_ext.elastic('localhost', 'my_index', 'my_type')See `the online documentation &lt;https://sparkly.readthedocs.org&gt;`__ formore details.Testing-------To run tests you have to have `docker &lt;https://www.docker.com/&gt;`__ and`docker-compose &lt;https://docs.docker.com/compose/&gt;`__ installed on yoursystem. If you are working on MacOS we highly recommend you to use`docker-machine &lt;https://docs.docker.com/machine/&gt;`__. As soon as thetools mentioned above have been installed, all you need is to run::    make testSupported Spark Versions------------------------At the moment we support:+---------------------------------------------------------------------------+| sparkly &gt;= 2.7 | Spark 2.4.x                                              |+---------------------------------------------------------------------------+| sparkly 2.x    | Spark 2.0.x and Spark 2.1.x and Spark 2.2.x              |+---------------------------------------------------------------------------+| sparkly 1.x    | Spark 1.6.x                                              |+---------------------------------------------------------------------------+.. |Sparkly PyPi Version| image:: http://img.shields.io/pypi/v/sparkly.svg   :target: https://pypi.python.org/pypi/sparkly.. |Sparkly Build Status| image:: https://travis-ci.org/tubular/sparkly.svg?branch=master   :target: https://travis-ci.org/tubular/sparkly.. |Documentation Status| image:: https://readthedocs.org/projects/sparkly/badge/?version=latest   :target: http://sparkly.readthedocs.io/en/latest/?badge=latest</longdescription>
</pkgmetadata>