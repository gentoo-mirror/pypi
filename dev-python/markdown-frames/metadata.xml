<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Markdown FramesHelper package for testing Apache Spark and Pandas DataFrames.It makes your data-related unit tests more readable.## HistoryWhile working at [Exacaster](https://exacaster.com/) [Vaidas Armonas](https://github.com/Va1da2) came up with the idea to make testing data more representable. And with the help of his team, he implemented the initial version of this package.Before that, we had to define our testing data as follows:```pythonschema = [&quot;user_id&quot;, &quot;even_type&quot;, &quot;item_id&quot;, &quot;event_time&quot;, &quot;country&quot;, &quot;dt&quot;]input_df = spark.createDataFrame([    (123456, 'page_view', None, datetime(2017,12,31,23,50,50), &quot;uk&quot;, &quot;2017-12-31&quot;),    (123456, 'item_view', 68471513, datetime(2017,12,31,23,50,55), &quot;uk&quot;, &quot;2017-12-31&quot;)],     schema)```And with this library you can define same data like this:```pythoninput_data = &quot;&quot;&quot;     |  user_id   |  even_type  | item_id  |    event_time       | country  |     dt      |    |   bigint   |   string    |  bigint  |    timestamp        |  string  |   string    |    | ---------- | ----------- | -------- | ------------------- | -------- | ----------- |    |   123456   |  page_view  |   None   | 2017-12-31 23:50:50 |   uk     | 2017-12-31  |    |   123456   |  item_view  | 68471513 | 2017-12-31 23:50:55 |   uk     | 2017-12-31  |&quot;&quot;&quot;input_df = spark_df(input_data, spark)```## InstallationTo install this package, run this command on your python environment:```bashpip install markdown_frames[pyspark]```## UsageWhen you have this package installed, you can use it in your unit tests as follows (assuming you are using `pytest-spark` ang have Spark Session available):```pythonfrom pyspark.sql import SparkSessionfrom markdown_frames.spark_dataframe import spark_dfdef test_your_use_case(spark: SpakSession): -&gt; None    expected_data = &quot;&quot;&quot;        | column1 | column2 | column3 | column4 |        |   int   |  string |  float  |  bigint |        | ------- | ------- | ------- | ------- |        |   1     |   user1 |   3.14  |  111111 |        |   2     |   None  |   1.618 |  222222 |        |   3     |   ''    |   2.718 |  333333 |        &quot;&quot;&quot;    expected_df = spark_df(expected_data, spark)    actaual_df = your_use_case(spark)    assert expected_df.collect()) == actaual_df.collect())```## Supported data typesThis package supports all major datatypes, use these type names in your table definitions:- `int`- `bigint`- `float`- `double`- `string`- `boolean`- `date`- `timestamp`- `decimal(precision,scale)` (scale and precision must be integers)- `array&lt;int&gt;` (int can be replaced by  any of mentioned types)- `map&lt;string,int&gt;` (string and int can be replaced by any of mentioned types)For `null` values use `None` keyword.## LicenseThis project is [MIT](./LICENSE) licensed.</longdescription>
</pkgmetadata>