<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>.. image:: https://travis-ci.org/Parsely/pykafka.svg?branch=master    :target: https://travis-ci.org/Parsely/pykafka.. image:: https://codecov.io/github/Parsely/pykafka/coverage.svg?branch=master    :target: https://codecov.io/github/Parsely/pykafka?branch=masterPyKafka=======.. image:: http://i.imgur.com/ztYl4lG.jpgPyKafka is a programmer-friendly Kafka client for Python. It includes Pythonimplementations of Kafka producers and consumers, which are optionally backedby a C extension built on `librdkafka`_. It runs under Python 2.7+, Python 3.4+,and PyPy, and supports versions of Kafka 0.8.2 and newer... _librdkafka: https://github.com/edenhill/librdkafkaPyKafka's primary goal is to provide a similar level of abstraction to the`JVM Kafka client`_ using idioms familiar to Python programmers and exposingthe most Pythonic API possible.You can install PyKafka from PyPI with::    $ pip install pykafkaor from conda-forge with::    $ conda install -c conda-forge pykafkaFull documentation and usage examples for PyKafka can be found on `readthedocs`_.You can install PyKafka for local development and testing by cloning this repository andrunning::    $ python setup.py develop.. _JVM Kafka client: https://github.com/apache/kafka/tree/0.8.2/clients/src/main/java/org/apache/kafka.. _readthedocs: http://pykafka.readthedocs.org/en/latest/Getting Started---------------Assuming you have at least one Kafka instance running on localhost, you can use PyKafkato connect to it... sourcecode:: python    &gt;&gt;&gt; from pykafka import KafkaClient    &gt;&gt;&gt; client = KafkaClient(hosts=&quot;127.0.0.1:9092,127.0.0.1:9093,...&quot;)Or, for a TLS connection, you might write (and also see ``SslConfig`` docsfor further details):.. sourcecode:: python    &gt;&gt;&gt; from pykafka import KafkaClient, SslConfig    &gt;&gt;&gt; config = SslConfig(cafile='/your/ca.cert',    ...                    certfile='/your/client.cert',  # optional    ...                    keyfile='/your/client.key',  # optional    ...                    password='unlock my client key please')  # optional    &gt;&gt;&gt; client = KafkaClient(hosts=&quot;127.0.0.1:&lt;ssl-port&gt;,...&quot;,    ...                      ssl_config=config)If the cluster you've connected to has any topics defined on it, you can listthem with:.. sourcecode:: python    &gt;&gt;&gt; client.topics    &gt;&gt;&gt; topic = client.topics['my.test']Once you've got a `Topic`, you can create a `Producer` for it and startproducing messages... sourcecode:: python    &gt;&gt;&gt; with topic.get_sync_producer() as producer:    ...     for i in range(4):    ...         producer.produce('test message ' + str(i ** 2))The example above would produce to kafka synchronously - the call onlyreturns after we have confirmation that the message made it to the cluster.To achieve higher throughput, we recommend using the ``Producer`` inasynchronous mode, so that ``produce()`` calls will return immediately and theproducer may opt to send messages in larger batches. The ``Producer`` collectsproduced messages in an internal queue for ``linger_ms`` before sending each batch.This delay can be removed or changed at the expense of efficiency with ``linger_ms``,``min_queued_messages``, and other keyword arguments (see `readthedocs`_). You can still obtaindelivery confirmation for messages, through a queue interface which can beenabled by setting ``delivery_reports=True``.  Here's a rough usage example:.. sourcecode:: python    &gt;&gt;&gt; with topic.get_producer(delivery_reports=True) as producer:    ...     count = 0    ...     while True:    ...         count += 1    ...         producer.produce('test msg', partition_key='{}'.format(count))    ...         if count % 10 ** 5 == 0:  # adjust this or bring lots of RAM ;)    ...             while True:    ...                 try:    ...                     msg, exc = producer.get_delivery_report(block=False)    ...                     if exc is not None:    ...                         print 'Failed to deliver msg {}: {}'.format(    ...                             msg.partition_key, repr(exc))    ...                     else:    ...                         print 'Successfully delivered msg {}'.format(    ...                         msg.partition_key)    ...                 except Queue.Empty:    ...                     breakNote that the delivery report queue is thread-local: it will only serve reportsfor messages which were produced from the current thread. Also, if you're using`delivery_reports=True`, failing to consume the delivery report queue will causePyKafka's memory usage to grow unbounded.You can also consume messages from this topic using a `Consumer` instance... sourcecode:: python    &gt;&gt;&gt; consumer = topic.get_simple_consumer()    &gt;&gt;&gt; for message in consumer:    ...     if message is not None:    ...         print message.offset, message.value    0 test message 0    1 test message 1    2 test message 4    3 test message 9This `SimpleConsumer` doesn't scale - if you have two `SimpleConsumers`consuming the same topic, they will receive duplicate messages. To get aroundthis, you can use the `BalancedConsumer`... sourcecode:: python    &gt;&gt;&gt; balanced_consumer = topic.get_balanced_consumer(    ...     consumer_group='testgroup',    ...     auto_commit_enable=True,    ...     zookeeper_connect='myZkClusterNode1.com:2181,myZkClusterNode2.com:2181/myZkChroot'    ... )You can have as many `BalancedConsumer` instances consuming a topic as thattopic has partitions. If they are all connected to the same zookeeper instance,they will communicate with it to automatically balance the partitions betweenthemselves. The partition assignment strategy used by the `BalancedConsumer` isthe &quot;range&quot; strategy by default. The strategy is switchable via the `membership_protocol`keyword argument, and can be either an object exposed by `pykafka.membershipprotocol` ora custom instance of `pykafka.membershipprotocol.GroupMembershipProtocol`.You can also use the Kafka 0.9 Group Membership API with the ``managed``keyword argument on ``get_balanced_consumer``.Using the librdkafka extension------------------------------PyKafka includes a C extension that makes use of librdkafka to speed up producerand consumer operation. To use the librdkafka extension, you need to make sure the headerfiles and shared library are somewhere where python can find them, both when you buildthe extension (which is taken care of by ``setup.py develop``) and at run time.Typically, this means that you need to either install librdkafka in a placeconventional for your system, or declare ``C_INCLUDE_PATH``, ``LIBRARY_PATH``,and ``LD_LIBRARY_PATH`` in your shell environment to point to the installation locationof the librdkafka shared objects. You can find this location with `locate librdkafka.so`.After that, all that's needed is that you pass an extra parameter``use_rdkafka=True`` to ``topic.get_producer()``,``topic.get_simple_consumer()``, or ``topic.get_balanced_consumer()``.  Notethat some configuration options may have different optimal values; it may beworthwhile to consult librdkafka's `configuration notes`_ for this... _0.9.1: https://github.com/edenhill/librdkafka/releases/tag/0.9.1.. _configuration notes: https://github.com/edenhill/librdkafka/blob/0.9.1/CONFIGURATION.mdOperational Tools-----------------PyKafka includes a small collection of `CLI tools`_ that can help with common tasksrelated to the administration of a Kafka cluster, including offset and lag monitoring andtopic inspection. The full, up-to-date interface for these tools can be fould by running.. sourcecode:: sh    $ python cli/kafka_tools.py --helpor after installing PyKafka via setuptools or pip:.. sourcecode:: sh    $ kafka-tools --help.. _CLI tools: https://github.com/Parsely/pykafka/blob/master/pykafka/cli/kafka_tools.pyPyKafka or kafka-python?------------------------These are two different projects.See `the discussion here &lt;https://github.com/Parsely/pykafka/issues/334&gt;`_ for comparisonsbetween the two projects.Contributing------------If you're interested in contributing code to PyKafka, a good place to start is the`&quot;help wanted&quot;`_ issue tag. We also recommend taking a look at the `contribution guide`_... _&quot;help wanted&quot;: https://github.com/Parsely/pykafka/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22Support-------If you need help using PyKafka, there are a bunch of resources available.For usage questions or common recipes, check out the `StackOverflow tag`_.The `Google Group`_ can be useful for more in-depth questions or inquriesyou'd like to send directly to the PyKafka maintainers. If you believe you'vefound a bug in PyKafka, please open a `github issue`_ after reading the`contribution guide`_... _StackOverflow tag: https://stackoverflow.com/questions/tagged/pykafka.. _github issue: https://github.com/Parsely/pykafka/issues.. _Google Group: https://groups.google.com/forum/#!forum/pykafka-user.. _contribution guide: https://github.com/Parsely/pykafka/blob/master/CONTRIBUTING.rst</longdescription>
</pkgmetadata>