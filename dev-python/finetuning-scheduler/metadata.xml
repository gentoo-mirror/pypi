<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/speediedan/finetuning-scheduler/raw/v2.1.0/docs/source/_static/images/logos/logo_fts.png&quot; width=&quot;401px&quot;&gt;**A PyTorch Lightning extension that enhances model experimentation with flexible fine-tuning schedules.**______________________________________________________________________&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://finetuning-scheduler.readthedocs.io/en/stable/&quot;&gt;Docs&lt;/a&gt; •  &lt;a href=&quot;#Setup&quot;&gt;Setup&lt;/a&gt; •  &lt;a href=&quot;#examples&quot;&gt;Examples&lt;/a&gt; •  &lt;a href=&quot;#community&quot;&gt;Community&lt;/a&gt;&lt;/p&gt;[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/finetuning-scheduler)](https://pypi.org/project/finetuning-scheduler/)[![PyPI Status](https://badge.fury.io/py/finetuning-scheduler.svg)](https://badge.fury.io/py/finetuning-scheduler)\[![codecov](https://codecov.io/gh/speediedan/finetuning-scheduler/release/2.1.0/graph/badge.svg?flag=gpu)](https://codecov.io/gh/speediedan/finetuning-scheduler)[![ReadTheDocs](https://readthedocs.org/projects/finetuning-scheduler/badge/?version=latest)](https://finetuning-scheduler.readthedocs.io/en/stable/)[![DOI](https://zenodo.org/badge/455666112.svg)](https://zenodo.org/badge/latestdoi/455666112)[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/speediedan/finetuning-scheduler/blob/master/LICENSE)&lt;/div&gt;______________________________________________________________________&lt;img width=&quot;300px&quot; src=&quot;https://github.com/speediedan/finetuning-scheduler/raw/v2.1.0/docs/source/_static/images/fts/fts_explicit_loss_anim.gif&quot; alt=&quot;FinetuningScheduler explicit loss animation&quot; align=&quot;right&quot;/&gt;[FinetuningScheduler](https://finetuning-scheduler.readthedocs.io/en/stable/api/finetuning_scheduler.fts.html#finetuning_scheduler.fts.FinetuningScheduler) is simple to use yet powerful, offering a number of features that facilitate model research and exploration:- easy specification of flexible fine-tuning schedules with explicit or regex-based parameter selection  - implicit schedules for initial/naive model exploration  - explicit schedules for performance tuning, fine-grained behavioral experimentation and computational efficiency- automatic restoration of best per-phase checkpoints driven by iterative application of early-stopping criteria to each fine-tuning phase- composition of early-stopping and manually-set epoch-driven fine-tuning phase transitions______________________________________________________________________## Setup### Step 0: Install from PyPI```bashpip install finetuning-scheduler```&lt;!--  --&gt;### Step 1: Import the FinetuningScheduler callback and start fine-tuning!```pythonimport lightning as Lfrom finetuning_scheduler import FinetuningSchedulertrainer = L.Trainer(callbacks=[FinetuningScheduler()])```Get started by following [the Fine-Tuning Scheduler introduction](https://finetuning-scheduler.readthedocs.io/en/stable/index.html) which includes a [CLI-based example](https://finetuning-scheduler.readthedocs.io/en/stable/index.html#example-scheduled-fine-tuning-for-superglue) or by following the [notebook-based](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/finetuning-scheduler.html) Fine-Tuning Scheduler tutorial.______________________________________________________________________### Installation Using the Standalone `pytorch-lightning` Package*applicable to versions &gt;= `2.0.0`*Now that the core Lightning package is `lightning` rather than `pytorch-lightning`, Fine-Tuning Scheduler (FTS) by default depends upon the `lightning` package rather than the standalone `pytorch-lightning`. If you would like to continue to use FTS with the standalone `pytorch-lightning` package instead, you can still do so as follows:Install a given FTS release (for example v2.0.0) using standalone `pytorch-lightning`:```bashexport FTS_VERSION=2.0.0export PACKAGE_NAME=pytorchwget https://github.com/speediedan/finetuning-scheduler/releases/download/v${FTS_VERSION}/finetuning-scheduler-${FTS_VERSION}.tar.gzpip install finetuning-scheduler-${FTS_VERSION}.tar.gz```______________________________________________________________________## Examples### Scheduled Fine-Tuning For SuperGLUE- [Notebook-based Tutorial](https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/finetuning-scheduler.html)- [CLI-based Tutorial](https://finetuning-scheduler.readthedocs.io/en/stable/#example-scheduled-fine-tuning-for-superglue)- [FSDP Scheduled Fine-Tuning](https://finetuning-scheduler.readthedocs.io/en/stable/advanced/fsdp_scheduled_fine_tuning.html)- [LR Scheduler Reinitialization](https://finetuning-scheduler.readthedocs.io/en/stable/advanced/lr_scheduler_reinitialization.html) (advanced)- [Optimizer Reinitialization](https://finetuning-scheduler.readthedocs.io/en/stable/advanced/optimizer_reinitialization.html) (advanced)______________________________________________________________________## Continuous IntegrationFine-Tuning Scheduler is rigorously tested across multiple CPUs, GPUs and against major Python and PyTorch versions. Each Fine-Tuning Scheduler minor release (major.minor.patch) is paired with a Lightning minor release (e.g. Fine-Tuning Scheduler 2.0 depends upon Lightning 2.0).To ensure maximum stability, the latest Lightning patch release fully tested with Fine-Tuning Scheduler is set as a maximum dependency in Fine-Tuning Scheduler's requirements.txt (e.g. \&lt;= 1.7.1). If you'd like to test a specific Lightning patch version greater than that currently in Fine-Tuning Scheduler's requirements.txt, it will likely work but you should install Fine-Tuning Scheduler from source and update the requirements.txt as desired.&lt;details&gt;  &lt;summary&gt;Current build statuses for Fine-Tuning Scheduler &lt;/summary&gt;| System / (PyTorch/Python ver) |                                                                                                         1.12/3.8                                                                                                         |                                                                                                              2.1.0/3.8, 2.1.0/3.11                                                                                                               || :---------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: ||      Linux \[GPUs\*\*\]       |                                                                                                            -                                                                                                             | [![Build Status](https://dev.azure.com//speediedan/finetuning-scheduler/_apis/build/status/Multi-GPU%20&amp;%20Example%20Tests?branchName=refs%2Ftags%2F2.1.0)](https://dev.azure.com/speediedan/finetuning-scheduler/_build/latest?definitionId=2&amp;branchName=refs%2Ftags%2F2.1.0) ||     Linux (Ubuntu 20.04)      | [![Test](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml/badge.svg?tag=2.1.0)](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml) |             [![Test](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml/badge.svg?tag=2.1.0)](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml)             ||           OSX (11)            | [![Test](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml/badge.svg?tag=2.1.0)](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml) |             [![Test](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml/badge.svg?tag=2.1.0)](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml)             ||        Windows (2022)         | [![Test](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml/badge.svg?tag=2.1.0)](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml) |             [![Test](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml/badge.svg?tag=2.1.0)](https://github.com/speediedan/finetuning-scheduler/actions/workflows/ci_test-full.yml)             |- \*\* tests run on one RTX 4090 and one RTX 2070&lt;/details&gt;## CommunityFine-Tuning Scheduler is developed and maintained by the community in close communication with the [Lightning team](https://pytorch-lightning.readthedocs.io/en/stable/governance.html). Thanks to everyone in the community for their tireless effort building and improving the immensely useful core Lightning project.PR's welcome! Please see the [contributing guidelines](https://finetuning-scheduler.readthedocs.io/en/stable/generated/CONTRIBUTING.html) (which are essentially the same as Lightning's).______________________________________________________________________## Citing Fine-Tuning SchedulerPlease cite:```tex@misc{Dan_Dale_2022_6463952,    author       = {Dan Dale},    title        = {{Fine-Tuning Scheduler}},    month        = Feb,    year         = 2022,    doi          = {10.5281/zenodo.6463952},    publisher    = {Zenodo},    url          = {https://zenodo.org/record/6463952}    }```Feel free to star the repo as well if you find it useful or interesting. Thanks 😊!</longdescription>
</pkgmetadata>