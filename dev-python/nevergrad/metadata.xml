<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;labelColor=005BBB)](https://opensource.fb.com/support-ukraine) [![CircleCI](https://circleci.com/gh/facebookresearch/nevergrad/tree/main.svg?style=svg)](https://circleci.com/gh/facebookresearch/nevergrad/tree/main)# Nevergrad - A gradient-free optimization platform![Nevergrad](https://raw.githubusercontent.com/facebookresearch/nevergrad/0.7.0/docs/resources/Nevergrad-LogoMark.png)`nevergrad` is a Python 3.6+ library. It can be installed with:```pip install nevergrad```More installation options, including windows installation, and complete instructions are available in the &quot;Getting started&quot; section of the [**documentation**](https://facebookresearch.github.io/nevergrad/).You can join Nevergrad users Facebook group [here](https://www.facebook.com/groups/nevergradusers/).Minimizing a function using an optimizer (here `NGOpt`) is straightforward:```pythonimport nevergrad as ngdef square(x):    return sum((x - .5)**2)optimizer = ng.optimizers.NGOpt(parametrization=2, budget=100)recommendation = optimizer.minimize(square)print(recommendation.value)  # recommended value&gt;&gt;&gt; [0.49971112 0.5002944]````nevergrad` can also support bounded continuous variables as well as discrete variables, and mixture of those.To do this, one can specify the input space:```pythonimport nevergrad as ngdef fake_training(learning_rate: float, batch_size: int, architecture: str) -&gt; float:    # optimal for learning_rate=0.2, batch_size=4, architecture=&quot;conv&quot;    return (learning_rate - 0.2)**2 + (batch_size - 4)**2 + (0 if architecture == &quot;conv&quot; else 10)# Instrumentation class is used for functions with multiple inputs# (positional and/or keywords)parametrization = ng.p.Instrumentation(    # a log-distributed scalar between 0.001 and 1.0    learning_rate=ng.p.Log(lower=0.001, upper=1.0),    # an integer from 1 to 12    batch_size=ng.p.Scalar(lower=1, upper=12).set_integer_casting(),    # either &quot;conv&quot; or &quot;fc&quot;    architecture=ng.p.Choice([&quot;conv&quot;, &quot;fc&quot;]))optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=100)recommendation = optimizer.minimize(fake_training)# show the recommended keyword arguments of the functionprint(recommendation.kwargs)&gt;&gt;&gt; {'learning_rate': 0.1998, 'batch_size': 4, 'architecture': 'conv'}```Learn more on parametrization in the [**documentation**](https://facebookresearch.github.io/nevergrad/)!![Example of optimization](https://raw.githubusercontent.com/facebookresearch/nevergrad/0.7.0/docs/resources/TwoPointsDE.gif)*Convergence of a population of points to the minima with two-points DE.*## DocumentationCheck out our [**documentation**](https://facebookresearch.github.io/nevergrad/)! It's still a work in progress, don't hesitate to submit issues and/or PR to update it and make it clearer!## Citing```bibtex@misc{nevergrad,    author = {J. Rapin and O. Teytaud},    title = {{Nevergrad - A gradient-free optimization platform}},    year = {2018},    publisher = {GitHub},    journal = {GitHub repository},    howpublished = {\url{https://GitHub.com/FacebookResearch/Nevergrad}},}```## License`nevergrad` is released under the MIT license. See [LICENSE](https://github.com/facebookresearch/nevergrad/blob/0.7.0/LICENSE) for additional details about it.See also our [Terms of Use](https://opensource.facebook.com/legal/terms) and [Privacy Policy](https://opensource.facebook.com/legal/privacy).</longdescription>
</pkgmetadata>