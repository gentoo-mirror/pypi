<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Damo-Embedding[![Deploy to GitHub Pages](https://github.com/uopensail/damo-embedding/actions/workflows/gh-pages.yml/badge.svg)](https://uopensail.github.io/damo-embedding/docs/Intro) [![Build and upload to PyPI](https://github.com/uopensail/damo-embedding/actions/workflows/main.yml/badge.svg?event=release)](https://pypi.org/project/damo-embedding/)# Quick Install```shellpip install damo-embedding```# Example## DeepFM```pythonimport torchimport torch.nn as nnfrom damo_embedding import Embeddingclass DeepFM(torch.nn.Module):    def __init__(        self,        emb_size: int,        fea_size: int,        hid_dims=[256, 128],        num_classes=1,        dropout=[0.2, 0.2],        **kwargs,    ):        super(DeepFM, self).__init__()        self.emb_size = emb_size        self.fea_size = fea_size        initializer = {            &quot;name&quot;: &quot;truncate_normal&quot;,            &quot;mean&quot;: float(kwargs.get(&quot;mean&quot;, 0.0)),            &quot;stddev&quot;: float(kwargs.get(&quot;stddev&quot;, 0.0001)),        }        optimizer = {            &quot;name&quot;: &quot;adam&quot;,            &quot;gamma&quot;: float(kwargs.get(&quot;gamma&quot;, 0.001)),            &quot;beta1&quot;: float(kwargs.get(&quot;beta1&quot;, 0.9)),            &quot;beta2&quot;: float(kwargs.get(&quot;beta2&quot;, 0.999)),            &quot;lambda&quot;: float(kwargs.get(&quot;lambda&quot;, 0.0)),            &quot;epsilon&quot;: float(kwargs.get(&quot;epsilon&quot;, 1e-8)),        }        self.w = Embedding(            1,            initializer=initializer,            optimizer=optimizer,            **kwargs,        )        self.v = Embedding(            self.emb_size,            initializer=initializer,            optimizer=optimizer,            **kwargs,        )        self.w0 = torch.zeros(1, dtype=torch.float32, requires_grad=True)        self.dims = [fea_size * emb_size] + hid_dims        self.layers = nn.ModuleList()        for i in range(1, len(self.dims)):            self.layers.append(nn.Linear(self.dims[i - 1], self.dims[i]))            self.layers.append(nn.BatchNorm1d(self.dims[i]))            self.layers.append(nn.BatchNorm1d(self.dims[i]))            self.layers.append(nn.ReLU())            self.layers.append(nn.Dropout(dropout[i - 1]))        self.layers.append(nn.Linear(self.dims[-1], num_classes))        self.sigmoid = nn.Sigmoid()    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:        &quot;&quot;&quot;forward        Args:            input (torch.Tensor): input tensor        Returns:            tensor.Tensor: deepfm forward values        &quot;&quot;&quot;        assert input.shape[1] == self.fea_size        w = self.w.forward(input)        v = self.v.forward(input)        square_of_sum = torch.pow(torch.sum(v, dim=1), 2)        sum_of_square = torch.sum(v * v, dim=1)        fm_out = (            torch.sum((square_of_sum - sum_of_square)                      * 0.5, dim=1, keepdim=True)            + torch.sum(w, dim=1)            + self.w0        )        dnn_out = torch.flatten(v, 1)        for layer in self.layers:            dnn_out = layer(dnn_out)        out = fm_out + dnn_out        out = self.sigmoid(out)        return out```## Save Model```pythonfrom damo_embedding import save_modelmodel = DeepFM(8, 39)save_model(model, &quot;./&quot;)```# Document[Doc Website](https://uopensail.github.io/damo-embedding/docs/Intro)</longdescription>
</pkgmetadata>