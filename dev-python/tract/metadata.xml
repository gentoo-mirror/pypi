<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># `tract` python bindings`tract` is a library for neural network inference. While PyTorch and TensorFlowdeal with the much harder training problem, `tract` focuses on what happens oncethe model in trained.`tract` ultimate goal is to use the model on end-user data (aka &quot;running themodel&quot;) as efficiently as possible, in a variety of possible deployments,including some which are no completely mainstream : a lot of energy have beeninvested in making `tract` an efficient engine to run models on ARM single boardcomputers.## Getting started### Install tract library`pip install tract`. Prebuilt wheels are provided for x86-64 Linux andWindows, x86-64 and arm64 for MacOS.### Downloading the modelFirst we need to obtain the model. We will download an ONNX-converted MobileNET2.7 from the ONNX model zoo.`wget https://github.com/onnx/models/raw/main/vision/classification/mobilenet/model/mobilenetv2-7.onnx`.### Preprocessing an imageThen we need a sample image. You can use pretty much anything. If you lackinspiration, you can this picture of Grace Hopper.`wget https://s3.amazonaws.com/tract-ci-builds/tests/grace_hopper.jpg`We will be needing `pillow` to load the image and crop it.`pip install pillow`Now let's start our python script. We will want to use tract, obviously, but wewill also need PIL's Image and numpy to put the data in the form MobileNet expects it.```python#!/usr/bin/env pythonimport tractimport numpyfrom PIL import Image```We want to load the image, crop it into its central square, then scale thissquare to be 224x224.```pythonim = Image.open(&quot;grace_hopper.jpg&quot;)if im.height &gt; im.width:    top_crop = int((im.height - im.width) / 2)    im = im.crop((0, top_crop, im.width, top_crop + im.width))else:    left_crop = int((im.width - im.height) / 2)    im = im.crop((left_crop, 0, left_crop + im_height, im.height))im = im.resize((224, 224))im = numpy.array(im)```At this stage, we obtain a 224x224x3 tensor of 8-bit positive integers. We need to transformthese integers to floats and normalize them for MobileNet.At some point during this normalization, numpy decides to promote our tensor todouble precision, but our model is single precison, so we are converting itagain after the normalization.```pythonim = (im.astype(float) / 255. - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]im = im.astype(numpy.single)```Finally, ONNX variant of Mobilenet expects its input in NCHW convention, andour data is in HWC. We need to move the C axis before H and W, then insert theN at the left.```pythonim = numpy.moveaxis(im, 2, 0)im = numpy.expand_dims(im, 0)```### Loading the modelLoading a model is relatively simple. We need to instantiate the ONNX loaderfirst, the we use it to load the model. Then we ask tract to optimize the modeland get it ready to run.```pythonmodel = tract.onnx().model_for_path(&quot;./mobilenetv2-7.onnx&quot;).into_optimized().into_runnable()```If we wanted to process several images, this would only have to be done onceout of our image loop.### Running the modeltract run methods take a list of inputs and returns a list of outputs. Each inputcan be a numpy array. The outputs are tract's own Value data type, which should be converted to numpy array.```pythonoutputs = model.run([im])output = outputs[0].to_numpy()```### Interpreting the resultIf we print the output, what we get is a array of 1000 values. Each value isthe score of our image on one of the 1000 categoris of ImageNet. What we wantis to find the category with the highest score.```pythonprint(numpy.argmax(output))```If all goes according to plan, this should output the number 652. There is a copyof ImageNet categories at the following URL, with helpful line numbering.```https://github.com/sonos/tract/blob/main/examples/nnef-mobilenet-v2/imagenet_slim_labels.txt```And... 652 is &quot;microphone&quot;. Which is wrong. The trick is, the lines arenumbered from 1, while our results starts at 0, plus the label list includes a&quot;dummy&quot; label first that should be ignored. So the right value is at the line654: &quot;military uniform&quot;. If you looked at the picture before you noticed thatGrace Hopper is in uniform on the picture, so it does make sense.## Model cooking with `tract`Over the years of `tract` development, it became clear that beside &quot;training&quot;and &quot;running&quot;, there was a third time in the life-cycle of a model. One ofour contributors nicknamed it &quot;model cooking&quot; and the term stuck. This extra stageis about all what happens after the training and before running.If training and Runtime are relatively easy to define, the model cooking gets abit less obvious. It comes from the realisation that the training form (an ONNXor TensorFlow file or ste of files) of a model may is usually not the mostconvenient form for running it. Every time a device loads a model in ONNX formand transform it into a suitable form for runtime, it goes through the sameseries or more or less complicated operations, that can amount to severalseconds of high-CPU usage for current models. When running the model on adevice, this can have several negative impact on experience: the device willtake time to start-up, consume a lot of battery energy to get ready, maybe fightover CPU availability with other processes trying to get ready at the sameinstant on the device.As this sequence of operations is generally the same, it becomes relevant topersist the model resulting of the transformation. It could be persisted at thefirst application start-up for instance. But it could also be &quot;prepared&quot;, or&quot;cooked&quot; before distribution to the devices.## Cooking to NNEF`tract` supports NNEF. It can read a NNEF neural network and run it. But it canalso dump its preferred representation of a model in NNEF.At this stage, a possible path to production for a neural model becomes can be drawn:* model is trained, typically on big servers on the cloud, and exported to ONNX.* model is cooked, simplified, using `tract` command line or python bindings.* model is shipped to devices or servers in charge of running it.## Testing and benching models earlyAs soon as the model is in ONNX form, `tract` can load and run it. It givesopportunities to validate and test on the training system, asserting early on that`tract` will compute at runtime the same result than what the training modelpredicts, limiting the risk of late-minute surprise.But tract command line can also be used to bench and profile an ONNX model onthe target system answering very early the &quot;will the device be fast enough&quot;question. The nature of neural network is such that in many cases anuntrained model, or a poorly trained one will perform the same computations thanthe final model, so it may be possible to bench the model for on-deviceefficiency before going through a costly and long model training.## tract-oplNNEF is a pretty little standard. But we needed to go beyond it and we extendedit in several ways. For instance, NNEF does not provide syntax for recurringneural network (LSTM and friends), which are an absolute must in signal and voiceprocessing. `tract` also supports symbolic dimensions, which are useful torepresent a late bound batch dimension (if you don't know in advance how manyinputs will have to be computed concurrently).## PulsingFor interactive applications where time plays a role (voice, signal, ...),`tract` can automatically transform batch models, to equivalent streaming modelssuitable for runtime. While batch models are presented at training time thewhole signal in one go, a streaming model received the signal by &quot;pulse&quot; andproduces step by step the same output that the batching model.It does not work for every model, `tract` can obviously not generate a modelwhere the output at a time depends on input not received yet. Of course, modelshave to be *causal* to be pulsable. For instance, a bi-directional LSTM is notpulsable. Most convolution nets can be made causal at designe time by padding,or at cooking time by adding fixed delays.This cooking step is a recurring annoyance in the real-time voice and signalfield : it can be done manually, but is very easy to get wrong. `tract` makesit automactic.</longdescription>
</pkgmetadata>