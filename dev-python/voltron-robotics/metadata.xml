<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;    &lt;img src=&quot;https://raw.githubusercontent.com/siddk/voltron-robotics/main/docs/assets/voltron-banner.png&quot; alt=&quot;Voltron Logo&quot;/&gt;&lt;/div&gt;&lt;div align=&quot;center&quot;&gt;[![arXiv](https://img.shields.io/badge/arXiv-2302.12766-df2a2a.svg?style=for-the-badge)](https://arxiv.org/abs/2302.12766)[![PyTorch](https://img.shields.io/badge/PyTorch-1.12.0-EE4C2C.svg?style=for-the-badge&amp;logo=pytorch)](https://pytorch.org/get-started/previous-versions/#v1120)[![Code Style: Black](https://img.shields.io/badge/Code%20Style-Black-000000?style=for-the-badge)](https://github.com/psf/black)[![Ruff](https://img.shields.io/badge/%E2%9A%A1%EF%B8%8F-Ruff-orange?style=for-the-badge)](https://github.com/charliermarsh/ruff)![License](https://img.shields.io/github/license/siddk/lila?color=blueviolet&amp;style=for-the-badge)&lt;/div&gt;---# Language-Driven Representation Learning for RoboticsPackage repository for Voltron: Language-Driven Representation Learning for Robotics. Provides code for loadingpretrained Voltron, R3M, and MVP representations for adaptation to downstream tasks, as well as code for pretrainingsuch representations on arbitrary datasets.---## QuickstartThis repository is built with PyTorch; while specified as a dependency for the package, we highly recommend thatyou install the desired version (e.g., with accelerator support) for your given hardware and environmentmanager (e.g., `conda`).PyTorch installation instructions [can be found here](https://pytorch.org/get-started/locally/). This repositoryshould work with PyTorch &gt;= 1.12, but has only been thoroughly tested with PyTorch 1.12.0, Torchvision 0.13.0,Torchaudio 0.12.0.Once PyTorch has been properly installed, you can install this package via PyPI, and you're off!```bashpip install voltron-robotics```You can also install this package locally via an editable installation in case you want to run examples/extend thecurrent functionality:```bashgit clone https://github.com/siddk/voltron-roboticscd voltron-roboticspip install -e .```## UsageVoltron Robotics (package: `voltron`) is structured to provide easy access to pretrained Voltron models (andreproductions), to facilitate use for various downstream tasks. Using a pretrained Voltron model is easy:```pythonfrom torchvision.io import read_imagefrom voltron import instantiate_extractor, load# Load a frozen Voltron (V-Cond) model &amp; configure a vector extractorvcond, preprocess = load(&quot;v-cond&quot;, device=&quot;cuda&quot;, freeze=True)vector_extractor = instantiate_extractor(vcond)()# Obtain &amp; Preprocess an image =&gt;&gt; can be from a dataset, or camera on a robot, etc.#   =&gt; Feel free to add any language if you have it (Voltron models work either way!)img = preprocess(read_image(&quot;examples/img/peel-carrot-initial.png&quot;))[None, ...].to(&quot;cuda&quot;)lang = [&quot;peeling a carrot&quot;]# Extract both multimodal AND vision-only embeddings!multimodal_embeddings = vcond(img, lang, mode=&quot;multimodal&quot;)visual_embeddings = vcond(img, mode=&quot;visual&quot;)# Use the `vector_extractor` to output dense vector representations for downstream applications!#   =&gt; Pass this representation to model of your choice (object detector, control policy, etc.)representation = vector_extractor(multimodal_embeddings)```Voltron representations can be used for a variety of different applications; in the[`voltron-evaluation`](https://github.com/siddk/voltron-evaluation) repository, you can find code for adapting Voltronrepresentations to various downstream tasks (segmentation, object detection, control, etc.); all the applications fromour paper.---## API![Voltron Framework](https://raw.githubusercontent.com/siddk/voltron-robotics/main/docs/assets/voltron-framework.png)The package `voltron` provides the following functionality for using and adapting existing representations:#### `voltron.available_models()`Returns the name of available Voltron models; right now, the following models (all models trained in the paper) areavailable:- `v-cond` – V-Cond (ViT-Small) trained on Sth-Sth; single-frame w/ language-conditioning.- `v-dual` – V-Dual (ViT-Small) trained on Sth-Sth; dual-frame w/ language-conditioning.- `v-gen` – V-Gen (ViT-Small) trained on Sth-Sth; dual-frame w/ language conditioning AND generation.- `r-mvp` – R-MVP (ViT-Small); reproduction of [MVP](https://github.com/ir413/mvp) trained on Sth-Sth.- `r-r3m-vit` – R-R3M (ViT-Small); reproduction of [R3M](https://github.com/facebookresearch/r3m) trained on Sth-Sth.- `r-r3m-rn50` – R-R3M (ResNet-50); reproduction of [R3M](https://github.com/facebookresearch/r3m) trained on Sth-Sth.- `v-cond-base` – V-Cond (ViT-Base) trained on Sth-Sth; larger (86M parameter) variant of V-Cond.#### `voltron.load(name: str, device: str, freeze: bool, cache: str = cache/)`Returns the model and the Torchvision Transform needed by the model, where `name` is one of the strings returnedby `voltron.available_models()`; this in general follows the same API as[OpenAI's CLIP](https://github.com/openai/CLIP).---Voltron models (`v-{cond, dual, gen, ...}`) returned by `voltron.load()` support the following:#### `model(img: Tensor, lang: Optional[List[str]], mode: str = &quot;multimodal&quot;)`Returns a sequence of embeddings corresponding to the output of the multimodal encoder; note that `lang` can be None,which is totally fine for Voltron models! However, if you have any language (even a coarse task description), it'llprobably be helpful!The parameter `mode` in `[&quot;multimodal&quot;, &quot;visual&quot;]` controls whether the output will contain the fused image patch andlanguage embeddings, or only the image patch embeddings.**Note:** For the API for the non-Voltron models (e.g., R-MVP, R-R3M), take a look at[`examples/verify.py`](examples/verify.py); this file shows how representations from *every* model can be extracted.### AdaptationSee [`examples/adapt.py`](examples/adapt.py) and the [`voltron-evaluation`](https://github.com/siddk/voltron-evaluation)repository for more examples on the various ways to adapt/use Voltron representations.---## ContributingBefore committing to the repository, make sure to set up your dev environment!Here are the basic development environment setup guidelines:+ Fork/clone the repository, performing an editable installation. Make sure to install with the development dependencies  (e.g., `pip install -e &quot;.[dev]&quot;`); this will install `black`, `ruff`, and `pre-commit`.+ Install `pre-commit` hooks (`pre-commit install`).+ Branch for the specific feature/issue, issuing PR against the upstream repository for review.Additional Contribution Notes:- This project has migrated to the recommended  [`pyproject.toml` based configuration for setuptools](https://setuptools.pypa.io/en/latest/userguide/quickstart.html).  However, as some tools haven't yet adopted [PEP 660](https://peps.python.org/pep-0660/), we provide a  [`setup.py` file](https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html).- This package follows the [`flat-layout` structure](https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#flat-layout)  described in `setuptools`.- Make sure to add any new dependencies to the `project.toml` file!---## Repository StructureHigh-level overview of repository/project file-tree:+ `docs/` - Package documentation &amp; assets - including project roadmap.+ `voltron` - Package source code; has all core utilities for model specification, loading, feature extraction,              preprocessing, etc.+ `examples/` - Standalone examples scripts for demonstrating various functionality (e.g., extracting different types                of representations, adapting representations in various contexts, pretraining, amongst others).+ `.pre-commit-config.yaml` - Pre-commit configuration file (sane defaults + `black` + `ruff`).+ `LICENSE` - Code is made available under the MIT License.+ `Makefile` - Top-level Makefile (by default, supports linting - checking &amp; auto-fix); extend as needed.+ `pyproject.toml` - Following PEP 621, this file has all project configuration details (including dependencies), as                     well as tool configurations (for `black` and `ruff`).+ `README.md` - You are here!</longdescription>
</pkgmetadata>