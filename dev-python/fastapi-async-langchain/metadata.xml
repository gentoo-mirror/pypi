<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># fastapi-async-langchain[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/ajndkr/fastapi-async-langchain/blob/main/LICENSE)[![PyPI version](https://badge.fury.io/py/fastapi-async-langchain.svg)](https://pypi.org/project/fastapi-async-langchain/)&gt; ‚ö†Ô∏è DISCLAIMER: `fastapi-async-langchain` has been deprecated in favor of `lanarky`. You can find the PyPI project [here](https://pypi.org/project/lanarky/).Ship production-ready [LangChain](https://github.com/hwchase17/langchain) projects with[FastAPI](https://github.com/tiangolo/fastapi).## üöÄ Features- supports token streaming over HTTP and Websocket- supports multiple langchain `Chain` types- simple gradio chatbot UI for fast prototyping- follows FastAPI responses naming convention## ‚ùì Why?There are great low-code/no-code solutions in the open source to deploy your Langchain projects. However,most of them are opinionated in terms of cloud or deployment code. This project aims to provide FastAPI userswith a cloud-agnostic and deployment-agnostic solution which can be easily integrated into existingbackend infrastructures.## üíæ InstallationThe library is available on PyPI and can be installed via `pip`.```bashpip install fastapi-async-langchain```## üî• Deploy in under 20 lines of code```pythonfrom dotenv import load_dotenvfrom fastapi import FastAPIfrom langchain import ConversationChainfrom langchain.chat_models import ChatOpenAIfrom pydantic import BaseModelfrom fastapi_async_langchain.responses import StreamingResponseload_dotenv()app = FastAPI()class Request(BaseModel):    query: str@app.post(&quot;/chat&quot;)async def chat(request: Request) -&gt; StreamingResponse:    chain = ConversationChain(llm=ChatOpenAI(temperature=0, streaming=True), verbose=True)    return StreamingResponse.from_chain(chain, request.query, media_type=&quot;text/event-stream&quot;)```See [`examples/`](https://github.com/ajndkr/fastapi-async-langchain/blob/main/examples/README.md) for list of available demo examples.Create a `.env` file using `.env.sample` and add your OpenAI API key to itbefore running the examples.![demo](https://raw.githubusercontent.com/ajndkr/fastapi-async-langchain/main/assets/demo.gif)## ü§ù Contributing[![Code check](https://github.com/ajndkr/fastapi-async-langchain/actions/workflows/code-check.yaml/badge.svg)](https://github.com/ajndkr/fastapi-async-langchain/actions/workflows/code-check.yaml)[![Publish](https://github.com/ajndkr/fastapi-async-langchain/actions/workflows/publish.yaml/badge.svg)](https://github.com/ajndkr/fastapi-async-langchain/actions/workflows/publish.yaml)Contributions are more than welcome! If you have an idea for a new feature or want to help improve fastapi-async-langchain, please create an issue or submit a pull requeston [GitHub](https://github.com/ajndkr/fastapi-async-langchain).See [CONTRIBUTING.md](./CONTRIBUTING.md) for more information.## ‚öñÔ∏è LicenseThe library is released under the [MIT License](https://github.com/ajndkr/fastapi-async-langchain/blob/main/LICENSE).</longdescription>
</pkgmetadata>