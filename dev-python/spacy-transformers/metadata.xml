<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;a href=&quot;https://explosion.ai&quot;&gt;&lt;img src=&quot;https://explosion.ai/assets/img/logo.svg&quot; width=&quot;125&quot; height=&quot;125&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;# spacy-transformers: Use pretrained transformers like BERT, XLNet and GPT-2 in spaCyThis package provides [spaCy](https://github.com/explosion/spaCy) components andarchitectures to use transformer models via[Hugging Face's `transformers`](https://github.com/huggingface/transformers) inspaCy. The result is convenient access to state-of-the-art transformerarchitectures, such as BERT, GPT-2, XLNet, etc.&gt; **This release requires [spaCy v3](https://spacy.io/usage/v3).** For the&gt; previous version of this library, see the&gt; [`v0.6.x` branch](https://github.com/explosion/spacy-transformers/tree/v0.6.x).[![tests](https://github.com/explosion/spacy-transformers/actions/workflows/tests.yml/badge.svg)](https://github.com/explosion/spacy-transformers/actions/workflows/tests.yml)[![PyPi](https://img.shields.io/pypi/v/spacy-transformers.svg?style=flat-square&amp;logo=pypi&amp;logoColor=white)](https://pypi.python.org/pypi/spacy-transformers)[![GitHub](https://img.shields.io/github/release/explosion/spacy-transformers/all.svg?style=flat-square&amp;logo=github)](https://github.com/explosion/spacy-transformers/releases)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)## Features- Use pretrained transformer models like **BERT**, **RoBERTa** and **XLNet** to  power your spaCy pipeline.- Easy **multi-task learning**: backprop to one transformer model from several  pipeline components.- Train using spaCy v3's powerful and extensible config system.- Automatic alignment of transformer output to spaCy's tokenization.- Easily customize what transformer data is saved in the `Doc` object.- Easily customize how long documents are processed.- Out-of-the-box serialization and model packaging.## üöÄ InstallationInstalling the package from pip will automatically install all dependencies,including PyTorch and spaCy. Make sure you install this package **before** youinstall the models. Also note that this package requires **Python 3.6+**,**PyTorch v1.5+** and **spaCy v3.0+**.```bashpip install 'spacy[transformers]'```For GPU installation, find your CUDA version using `nvcc --version` and add the[version in brackets](https://spacy.io/usage/#gpu), e.g.`spacy[transformers,cuda92]` for CUDA9.2 or `spacy[transformers,cuda100]` forCUDA10.0.If you are having trouble installing PyTorch, follow the[instructions](https://pytorch.org/get-started/locally/) on the official websitefor your specific operating system and requirements.## üìñ Documentation&gt; ‚ö†Ô∏è **Important note:** This package has been extensively refactored to take&gt; advantage of [spaCy v3.0](https://spacy.io). Previous versions that were built&gt; for [spaCy v2.x](https://v2.spacy.io) worked considerably differently. Please&gt; see previous tagged versions of this README for documentation on prior&gt; versions.- üìò  [Embeddings, Transformers and Transfer Learning](https://spacy.io/usage/embeddings-transformers):  How to use transformers in spaCy- üìò [Training Pipelines and Models](https://spacy.io/usage/training): Train and  update components on your own data and integrate custom models- üìò  [Layers and Model Architectures](https://spacy.io/usage/layers-architectures):  Power spaCy components with custom neural networks- üìó [`Transformer`](https://spacy.io/api/transformer): Pipeline component API  reference- üìó  [Transformer architectures](https://spacy.io/api/architectures#transformers):  Architectures and registered functions## Applying pretrained text and token classification modelsNote that the `transformer` component from `spacy-transformers` does not supporttask-specific heads like token or text classification. A task-specifictransformer model can be used as a source of features to train spaCy componentslike `ner` or `textcat`, but the `transformer` component does not provide accessto task-specific heads for training or inference.Alternatively, if you only want use to the **predictions** from an existingHugging Face text or token classification model, you can use the wrappers from[`spacy-huggingface-pipelines`](https://github.com/explosion/spacy-huggingface-pipelines)to incorporate task-specific transformer models into your spaCy pipelines.## Bug reports and other issuesPlease use [spaCy's issue tracker](https://github.com/explosion/spaCy/issues) toreport a bug, or open a new thread on the[discussion board](https://github.com/explosion/spaCy/discussions) for any otherissue.</longdescription>
</pkgmetadata>