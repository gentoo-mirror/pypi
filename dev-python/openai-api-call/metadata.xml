<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&gt; **中文文档移步[这里](README_zh_CN.md)。**# Openai API call[![PyPI version](https://img.shields.io/pypi/v/openai_api_call.svg)](https://pypi.python.org/pypi/openai_api_call)[![Tests](https://github.com/cubenlp/openai_api_call/actions/workflows/test.yml/badge.svg)](https://github.com/cubenlp/openai_api_call/actions/workflows/test.yml/)[![Documentation Status](https://img.shields.io/badge/docs-github_pages-blue.svg)](https://apicall.wzhecnu.cn)[![Coverage](https://codecov.io/gh/cubenlp/openai_api_call/branch/master/graph/badge.svg)](https://codecov.io/gh/cubenlp/openai_api_call.jl)&lt;!-- [![Updates](https://pyup.io/repos/github/cubenlp/openai_api_call/shield.svg)](https://pyup.io/repos/github/cubenlp/openai_api_call/) --&gt;A simple wrapper for OpenAI API, which can be used to send requests and get responses.## Installation```bashpip install openai-api-call --upgrade```## Usage### Set API Key```pyimport openai_api_call as apicallapicall.api_key = &quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;```Or set `OPENAI_API_KEY` in `~/.bashrc` to avoid setting the API key every time:```bash# Add the following code to ~/.bashrcexport OPENAI_API_KEY=&quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;```Also, you might set different `api_key` for each `Chat` object:```pyfrom openai_api_call import Chatchat = Chat(&quot;hello&quot;)chat.api_key = &quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;```### Set Proxy (Optional)```pyfrom openai_api_call import proxy_on, proxy_off, proxy_status# Check the current proxyproxy_status()# Set proxy(example)proxy_on(http=&quot;127.0.0.1:7890&quot;, https=&quot;127.0.0.1:7890&quot;)# Check the updated proxyproxy_status()# Turn off proxyproxy_off() ```Alternatively, you can use a proxy URL to send requests from restricted network, as shown below:```pyfrom openai_api_call import request# set request urlrequest.base_url = &quot;https://api.example.com&quot;```You can set `OPENAI_BASE_URL` in `~/.bashrc` as well.### Basic UsageExample 1, send prompt and return response:```pythonfrom openai_api_call import Chat, show_apikey# Check if API key is setshow_apikey()# Check if proxy is enabledproxy_status()# Send prompt and return responsechat = Chat(&quot;Hello, GPT-3.5!&quot;)resp = chat.getresponse(update=False) # Not update the chat history, default to True```Example 2, customize the message template and return the information and the number of consumed tokens:```pythonimport openai_api_call as apicall# Customize the sending templateapicall.default_prompt = lambda msg: [    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;帮我翻译这段文字&quot;},    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: msg}]chat = Chat(&quot;Hello!&quot;)# Set the number of retries to Inf# The timeout for each request is 10 secondsresponse = chat.getresponse(temperature=0.5, max_requests=-1, timeout=10)print(&quot;Number of consumed tokens: &quot;, response.total_tokens)print(&quot;Returned content: &quot;, response.content)# Reset the default templateapicall.default_prompt = None```Example 3, continue chatting based on the last response:```python# first callchat = Chat(&quot;Hello, GPT-3.5!&quot;)resp = chat.getresponse() # update chat history, default is Trueprint(resp.content)# continue chattingchat.user(&quot;How are you?&quot;)next_resp = chat.getresponse()print(next_resp.content)# fake responsechat.user(&quot;What's your name?&quot;)chat.assistant(&quot;My name is GPT-3.5.&quot;)# get the last resultprint(chat[-1])# save chat historychat.save(&quot;chat_history.log&quot;, mode=&quot;w&quot;) # default to &quot;a&quot;# print chat historychat.print_log()```Moreover, you can check the usage status of the API key:```py# show usage status of the default API keychat = Chat()chat.show_usage_status()# show usage status of the specified API keychat.api_key = &quot;sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;chat.show_usage_status()```### Advance usageSave the chat history to a file:```pythoncheckpoint = &quot;tmp.log&quot;# chat 1chat = Chat()chat.save(checkpoint, mode=&quot;w&quot;) # default to &quot;a&quot;# chat 2chat = Chat(&quot;hello!&quot;)chat.save(checkpoint)# chat 3chat.assistant(&quot;你好, how can I assist you today?&quot;)chat.save(checkpoint)```Load the chat history from a file:```python# load chats(default)chats = load_chats(checkpoint)assert chats == [Chat(log) for log in chat_logs]# load chat log onlychat_logs = load_chats(checkpoint, chat_log_only=True)assert chat_logs == [[], [{'role': 'user', 'content': 'hello!'}],                      [{'role': 'user', 'content': 'hello!'},                        {'role': 'assistant', 'content': '你好, how can I assist you today?'}]]# load the last message onlychat_msgs = load_chats(checkpoint, last_message_only=True)assert chat_msgs == [&quot;&quot;, &quot;hello!&quot;, &quot;你好, how can I assist you today?&quot;]```In general, one can create a function `msg2chat` and use `process_chats` to process the data:```pythondef msg2chat(msg):    chat = Chat(api_key=api_key)    chat.system(&quot;You are a helpful translator for numbers.&quot;)    chat.user(f&quot;Please translate the digit to Roman numerals: {msg}&quot;)    chat.getresponse()checkpath = &quot;tmp.log&quot;# first part of the datamsgs = [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]chats = process_chats(msgs, msg2chat, checkpath, clearfile=True)assert len(chats) == 3assert all([len(chat) == 3 for chat in chats])# continue the processmsgs = msgs + [&quot;4&quot;, &quot;5&quot;, &quot;6&quot;]continue_chats = process_chats(msgs, msg2chat, checkpath)```## LicenseThis package is licensed under the MIT license. See the LICENSE file for more details.## update log- Since version `0.2.0`, `Chat` type is used to handle data- Since version `0.3.0`, you can use different API Key to send requests.- Since version `0.4.0`, this package is mantained by [cubenlp](https://github.com/cubenlp).- Since version `0.5.0`, one can use `process_chats` to process the data, with a customized `msg2chat` function and a checkpoint file.</longdescription>
</pkgmetadata>