<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>databricks-dbapi================|pypi| |pyversions|.. |pypi| image:: https://img.shields.io/pypi/v/databricks-dbapi.svg    :target: https://pypi.python.org/pypi/databricks-dbapi.. |pyversions| image:: https://img.shields.io/pypi/pyversions/databricks-dbapi.svg    :target: https://pypi.python.org/pypi/databricks-dbapiA thin wrapper around `pyhive &lt;https://github.com/dropbox/PyHive&gt;`__ and `pyodbc &lt;https://github.com/mkleehammer/pyodbc&gt;`__ for creating a `DBAPI &lt;https://www.python.org/dev/peps/pep-0249/&gt;`__ connection to Databricks Workspace and SQL Analytics clusters. SQL Analytics clusters require the `Simba ODBC driver &lt;https://databricks.com/spark/odbc-driver-download&gt;`__.Also provides SQLAlchemy Dialects using ``pyhive`` and ``pyodbc`` for Databricks clusters. Databricks SQL Analytics clusters only support the ``pyodbc``-driven dialect.Installation------------Install using pip. You *must* specify at least one of the extras {``hive`` or ``odbc``}. For ``odbc`` the `Simba driver &lt;https://databricks.com/spark/odbc-driver-download&gt;`__ is required:.. code-block:: bash    pip install databricks-dbapi[hive,odbc]For SQLAlchemy support install with:.. code-block:: bash    pip install databricks-dbapi[hive,odbc,sqlalchemy]Usage-----PyHive~~~~~~The ``connect()`` function returns a ``pyhive`` Hive connection object, which internally wraps a ``thrift`` connection.Connecting with ``http_path``, ``host``, and a ``token``:.. code-block:: python    import os    from databricks_dbapi import hive    token = os.environ[&quot;DATABRICKS_TOKEN&quot;]    host = os.environ[&quot;DATABRICKS_HOST&quot;]    http_path = os.environ[&quot;DATABRICKS_HTTP_PATH&quot;]    connection = hive.connect(        host=host,        http_path=http_path,        token=token,    )    cursor = connection.cursor()    cursor.execute(&quot;SELECT * FROM some_table LIMIT 100&quot;)    print(cursor.fetchone())    print(cursor.fetchall())The ``pyhive`` connection also provides async functionality:.. code-block:: python    import os    from databricks_dbapi import hive    from TCLIService.ttypes import TOperationState    token = os.environ[&quot;DATABRICKS_TOKEN&quot;]    host = os.environ[&quot;DATABRICKS_HOST&quot;]    cluster = os.environ[&quot;DATABRICKS_CLUSTER&quot;]    connection = hive.connect(        host=host,        cluster=cluster,        token=token,    )    cursor = connection.cursor()    cursor.execute(&quot;SELECT * FROM some_table LIMIT 100&quot;, async_=True)    status = cursor.poll().operationState    while status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):        logs = cursor.fetch_logs()        for message in logs:            print(message)        # If needed, an asynchronous query can be cancelled at any time with:        # cursor.cancel()        status = cursor.poll().operationState    print(cursor.fetchall())ODBC~~~~The ODBC DBAPI requires the Simba ODBC driver.Connecting with ``http_path``, ``host``, and a ``token``:.. code-block:: python    import os    from databricks_dbapi import odbc    token = os.environ[&quot;DATABRICKS_TOKEN&quot;]    host = os.environ[&quot;DATABRICKS_HOST&quot;]    http_path = os.environ[&quot;DATABRICKS_HTTP_PATH&quot;]    connection = odbc.connect(        host=host,        http_path=http_path,        token=token,        driver_path=&quot;/path/to/simba/driver&quot;,    )    cursor = connection.cursor()    cursor.execute(&quot;SELECT * FROM some_table LIMIT 100&quot;)    print(cursor.fetchone())    print(cursor.fetchall())SQLAlchemy Dialects-------------------databricks+pyhive~~~~~~~~~~~~~~~~~Installing registers the ``databricks+pyhive`` dialect/driver with SQLAlchemy. Fill in the required information when passing the engine URL... code-block:: python    from sqlalchemy import *    from sqlalchemy.engine import create_engine    from sqlalchemy.schema import *    engine = create_engine(        &quot;databricks+pyhive://token:&lt;databricks_token&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;&quot;,        connect_args={&quot;http_path&quot;: &quot;&lt;cluster_http_path&gt;&quot;}    )    logs = Table(&quot;my_table&quot;, MetaData(bind=engine), autoload=True)    print(select([func.count(&quot;*&quot;)], from_obj=logs).scalar())databricks+pyodbc~~~~~~~~~~~~~~~~~Installing registers the ``databricks+pyodbc`` dialect/driver with SQLAlchemy. Fill in the required information when passing the engine URL... code-block:: python    from sqlalchemy import *    from sqlalchemy.engine import create_engine    from sqlalchemy.schema import *    engine = create_engine(        &quot;databricks+pyodbc://token:&lt;databricks_token&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;&quot;,        connect_args={&quot;http_path&quot;: &quot;&lt;cluster_http_path&gt;&quot;, &quot;driver_path&quot;: &quot;/path/to/simba/driver&quot;}    )    logs = Table(&quot;my_table&quot;, MetaData(bind=engine), autoload=True)    print(select([func.count(&quot;*&quot;)], from_obj=logs).scalar())Refer to the following documentation for more details on hostname, cluster name, and http path:* `Databricks &lt;https://docs.databricks.com/user-guide/bi/jdbc-odbc-bi.html&gt;`__* `Azure Databricks &lt;https://docs.azuredatabricks.net/user-guide/bi/jdbc-odbc-bi.html&gt;`__Related-------* `pyhive &lt;https://github.com/dropbox/PyHive&gt;`__* `thrift &lt;https://github.com/apache/thrift/tree/master/lib/py&gt;`__* `pyodbc &lt;https://github.com/mkleehammer/pyodbc&gt;`__</longdescription>
</pkgmetadata>