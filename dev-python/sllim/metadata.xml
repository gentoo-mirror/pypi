<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Simple Large Language Inference Model`sllim` serves as a quality of life wrapper around the `openai-python` library.I found myself writing and rewriting the same helper functions with each new project I began, so now I am working to put these functions together into a easy to use library.Nothing here is ground-breaking; everything here is opinionated.## UsageUse the `chat` function to connect with the `ChatCompletion.create` models. By default, it uses the `gpt-3.5-turbo` model, but you can pass a `model` param to use `gpt-4````pythonfrom sllim import chatchat(    [        {            &quot;role&quot;: &quot;system&quot;,            &quot;content&quot;: &quot;Example system message&quot;,        },        {            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;Example user message&quot;,        }    ])````complete` works just like `Completion.create`, and `embed` is `Embedding.create`.## Map Reduce```pythonfrom sllim import map_reducetemplate = [ {    &quot;role&quot;: &quot;system&quot;,    &quot;content&quot;: &quot;You are an excellent copy writer and you will rewrite my work into {adjective} words.&quot; }, {    &quot;role&quot;: &quot;user&quot;,    &quot;content&quot;: &quot;Below is my writing, please improve it.\n\n{writing}&quot; },]writings = [...] # long list of copywritingfor adjective in [&quot;clearer&quot;, &quot;more expressive&quot;, &quot;fewer&quot;]:    # Since `writings` is a list, this is what we will reduce over.    # The other variables are treated as constants through the reduction.    gen = map_reduce(template, adjective=adjective, writing=writings, model=&quot;gpt-4&quot;)    for idx, result in enumerate(gen):        # This is a multithreaded generator to optimize latency to networked services        original = writings[idx]        print(&quot;Was:&quot;, original, &quot;\nNow:&quot;, result)```## Benefits* Local file caching. Each of the functions is locally cached in request-response key-pairs to prevent excessive network activity.* Auto-retry. Timeouts for rate limits, retry for internal errors (&gt;=500 status code).* Parameter names are in the functions so that you don't have to go looking at the docs constantly.* Map reduce prompts onto data* TODO: Cost estimates before running long tasks* TODO: Describe task -&gt; run task* TODO: Allow easy estimate* TODO: Allow easy logging</longdescription>
</pkgmetadata>