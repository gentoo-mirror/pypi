<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># `nbautoeval``nbautoeval` is a very lightweight python framework for creating **auto-evaluated**exercises inside a jupyter (python) notebook.two flavours of exercises are supported at this point :* code-oriented : given a text that describes the expectations, students are invited to  write their own code, and can then see the outcome on teacher-defined data samples,  compared with the results obtained through a teacher-provided solution, with a visual  (green/red) feedback* quizzes : a separate module allows to create quizzesAt this point, due to lack of knowledge/documentation about open/edx (read: theversion running at FUN), there is no available code for exporting the results asgrades or anything similar (hence the `autoeval` name).There indeed are provisions in the code to accumulate statistics on allattempted corrections, as an attempt to provide feedback to teachers.# Try it on `mybinder`Click the badge below to see a few sample demos under `mybinder.org` - it's allin the `demo-notebooks` subdir.**NOTE** the demo notebooks ship under a `.py` format and require `jupytext` to beinstalled before you can open them in Jupyter.[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/parmentelat/nbautoeval/master?filepath=demo-notebooks)# HistoryThis was initially embedded into a [MOOC onpython2](https://github.com/parmentelat/flotpython) that ran for the first time on [theFrench FUN platform](https://www.france-universite-numerique-mooc.fr/) in Fall 2014. Itwas then duplicated into a [MOOC onbioinformatics](https://github.com/parmentelat/flotbioinfo) in Spring 2016 where it wasnamed `nbautoeval` for the first time, but still embedded in a greater git module.The current git repo is created in June 2016 from that basis, with the intentionto be used as a git subtree from these 2 repos, and possibly others since a fewpeople have proved interested.# Installation```pip install nbautoeval```# Overview## code-orientedCurrently supports the following types of exercises  * `ExerciseFunction` : the student is asked to write a function  * `ExerciseRegexp` : the student is asked to write a regular expression  * `ExerciseGenerator` : the student is asked to write a generator function   * `ExerciseClass` : tests will happen on a class implementationA teacher who wishes to implement an exercise needs to write 2 parts :* One python file that defines an instance of an exercise class; this in a nutshell  typically involves  * providing one solution (let's say a function) written in Python  * providing a set of input data  * plus optionnally various tweaks for rendering results* One notebook that imports this exercise object, and can then take advantage of it to  write jupyter cells that typically  * invoke `example()` on  the  exercise  object to show examples of the expected output  * invite the student to write their own code  * invoke `correction()` on  the  exercise  object to display the outcome.## quizzesHere again there will be 2 parts at work :* The recommended way is to define quizzes in YAML format :  * one YAML file can contain several quizzes - see examples in the `yaml/` subdir  * and each quiz contain a set of questions  * grouping questions into quizzes essentially makes sense wrt the maximal number of    attempts  * mostly all the pieces can be written in markdown (currently we use `myst_parser`)* then one invokes `run_yaml_quiz()` from a notebook to display the test  * this function takes 2 arguments, one to help locate the YAML file  * one to spot the quiz inside the YAML file  * run with `debug=True` to pinpoint errors in the source## results and storageRegardless of their type all tests have an `exoname` that is used to store informationabout that specific test; for quizzes it is recommended to use a different name than the quiz name used in `run_yaml_quiz()` so that students cant guess it too easily.stuff is stored in 2 separate locations :* `~/.nbautoeval.trace` contain one JSON line per attempt (correction or submit)* `~/.nbautoeval.storage` for quizzes only, preserves previous choices, number of attempts# Known issuessee https://github.com/parmentelat/nbautoeval/issues</longdescription>
</pkgmetadata>