<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;# RL4CO&lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;&lt;img alt=&quot;PyTorch&quot; src=&quot;https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&amp;logoColor=white&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://pytorchlightning.ai/&quot;&gt;&lt;img alt=&quot;Lightning&quot; src=&quot;https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&amp;logoColor=white&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/pytorch/rl&quot;&gt;&lt;img alt=&quot;base: TorchRL&quot; src=&quot;https://img.shields.io/badge/base-TorchRL-red&quot;&gt;&lt;a href=&quot;https://hydra.cc/&quot;&gt;&lt;img alt=&quot;config: Hydra&quot; src=&quot;https://img.shields.io/badge/config-Hydra-89b8cd&quot;&gt;&lt;/a&gt; [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![Slack](https://img.shields.io/badge/slack-chat-611f69.svg?logo=slack)](https://join.slack.com/t/rl4co/shared_invite/zt-1ytz2c1v4-0IkQ8NQH4TRXIX8PrRmDhQ)![license](https://img.shields.io/badge/license-Apache%202.0-green.svg?)&lt;a href=&quot;https://colab.research.google.com/github/kaist-silab/rl4co/blob/main/notebooks/1-quickstart.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;[![PyPI](https://img.shields.io/pypi/v/rl4co?logo=pypi)](https://pypi.org/project/rl4co)[![Test](https://github.com/kaist-silab/rl4co/actions/workflows/tests.yml/badge.svg)](https://github.com/kaist-silab/rl4co/actions/workflows/tests.yml)&lt;!-- ![testing](https://github.com/kaist-silab/ncobench/actions/workflows/tests.yml/badge.svg) --&gt;[**Documentation**](https://rl4co.readthedocs.io/) |  [**Getting Started**](#getting-started) | [**Usage**](#usage) | [**Contributing**](#contributing) | [**Paper**](https://arxiv.org/abs/2306.17100) | [**Citation**](#cite-us)&lt;/div&gt;---An extensive Reinforcement Learning (RL) for Combinatorial Optimization (CO) benchmark. Our goal is to provide a unified framework for RL-based CO algorithms, and to facilitate reproducible research in this field, decoupling the science from the engineering.RL4CO is built upon:- [TorchRL](https://github.com/pytorch/rl): official PyTorch framework for RL algorithms and vectorized environments on GPUs- [TensorDict](https://github.com/pytorch-labs/tensordict): a library to easily handle heterogeneous data such as states, actions and rewards- [PyTorch Lightning](https://github.com/Lightning-AI/lightning): a lightweight PyTorch wrapper for high-performance AI research- [Hydra](https://github.com/facebookresearch/hydra): a framework for elegantly configuring complex applications![image](https://github.com/kaist-silab/rl4co/assets/48984123/0db4efdd-1c93-4991-8f09-f3c6c1f35d60)## Getting started&lt;a href=&quot;https://colab.research.google.com/github/kaist-silab/rl4co/blob/main/notebooks/1-quickstart.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;RL4CO is now available for installation on `pip`!```bashpip install rl4co```### Local install and developmentIf you want to develop RL4CO or access the latest builds, we recommend you to install it locally with `pip` in editable mode:```bashgit clone https://github.com/kaist-silab/rl4co &amp;&amp; cd rl4copip install -e .```&lt;details&gt;    &lt;summary&gt;[Optional] Automatically install PyTorch with correct CUDA version&lt;/summary&gt;These commands will [automatically install](https://github.com/pmeier/light-the-torch) PyTorch with the right GPU version for your system:```bashpip install light-the-torchpython3 -m light_the_torch install -r  --upgrade torch```&gt; Note: `conda` is also a good candidate for hassle-free installation of PyTorch: check out the [PyTorch website](https://pytorch.org/get-started/locally/) for more details.&lt;/details&gt;To get started, we recommend checking out our [quickstart notebook](notebooks/1-quickstart.ipynb) or the [minimalistic example](#minimalistic-example) below.## UsageTrain model with default configuration (AM on TSP environment):```bashpython run.py```&lt;details&gt;    &lt;summary&gt;Change experiment&lt;/summary&gt;Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/) (e.g. tsp/am, and environment with 42 cities)```bashpython run.py experiment=tsp/am env.num_loc=42```&lt;/details&gt;&lt;details&gt;    &lt;summary&gt;Disable logging&lt;/summary&gt;```bashpython run.py experiment=test/am logger=none '~callbacks.learning_rate_monitor'```Note that `~` is used to disable a callback that would need a logger.&lt;/details&gt;&lt;details&gt;    &lt;summary&gt;Create a sweep over hyperparameters (-m for multirun)&lt;/summary&gt;```bashpython run.py -m experiment=tsp/am  train.optimizer.lr=1e-3,1e-4,1e-5```&lt;/details&gt;### Minimalistic ExampleHere is a minimalistic example training the Attention Model with greedy rollout baseline on TSP in less than 50 lines of code:```pythonfrom omegaconf import DictConfigimport lightning as Lfrom rl4co.envs import TSPEnvfrom rl4co.models.zoo.am import AttentionModelfrom rl4co.tasks.rl4co import RL4COLitModuleconfig = DictConfig(    {&quot;data&quot;: {            &quot;train_size&quot;: 100000,            &quot;val_size&quot;: 10000,            &quot;batch_size&quot;: 512,        },    &quot;optimizer&quot;: {&quot;lr&quot;: 1e-4}})# Environment, Model, and Lightning Moduleenv = TSPEnv(num_loc=20)model = AttentionModel(env)lit_module = RL4COLitModule(config, env, model)# Trainertrainer = L.Trainer(    max_epochs=3, # only few epochs    accelerator=&quot;gpu&quot;, # use GPU if available, else you can use others as &quot;cpu&quot;    logger=None, # can replace with WandbLogger, TensorBoardLogger, etc.    precision=&quot;16-mixed&quot;, # Lightning will handle faster training with mixed precision    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients    reload_dataloaders_every_n_epochs=1, # necessary for sampling new data)# Fit the modeltrainer.fit(lit_module)```### TestingRun tests with `pytest` from the root directory:```bashpytest tests```## Contributing[![Slack](https://img.shields.io/badge/slack-chat-611f69.svg?logo=slack)](https://join.slack.com/t/rl4co/shared_invite/zt-1ytz2c1v4-0IkQ8NQH4TRXIX8PrRmDhQ)Have a suggestion, request, or found a bug? Feel free to [open an issue](https://github.com/kaist-silab/rl4co/issues) or [submit a pull request](https://github.com/kaist-silab/rl4co/pulls).If you would like to contribute, please check out our contribution guidelines   [here](.github/CONTRIBUTING.md). We welcome and look forward to all contributions to RL4CO!We are also on [Slack](https://join.slack.com/t/rl4co/shared_invite/zt-1ytz2c1v4-0IkQ8NQH4TRXIX8PrRmDhQ) if you have any questions or would like to discuss RL4CO with us. We are open to collaborations and would love to hear from you ðŸš€### Contributors&lt;a href=&quot;https://github.com/kaist-silab/rl4co/graphs/contributors&quot;&gt;  &lt;img src=&quot;https://contrib.rocks/image?repo=kaist-silab/rl4co&quot; /&gt;&lt;/a&gt;## Cite usIf you find RL4CO valuable for your research or applied projects:```bibtex@article{berto2023rl4co,    title = {{RL4CO}: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark},    author={Federico Berto and Chuanbo Hua and Junyoung Park and Minsu Kim and Hyeonah Kim and Jiwoo Son and Haeyeon Kim and Joungho Kim and Jinkyoo Park},    journal={arXiv preprint arXiv:2306.17100},    year={2023},    url = {https://github.com/kaist-silab/rl4co}}```</longdescription>
</pkgmetadata>