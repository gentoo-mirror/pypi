<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>aiohttp-s3-client================[![PyPI - License](https://img.shields.io/pypi/l/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![Wheel](https://img.shields.io/pypi/wheel/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![Mypy](http://www.mypy-lang.org/static/mypy_badge.svg)]() [![PyPI](https://img.shields.io/pypi/v/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![PyPI](https://img.shields.io/pypi/pyversions/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![Coverage Status](https://coveralls.io/repos/github/mosquito/aiohttp-s3-client/badge.svg?branch=master)](https://coveralls.io/github/mosquito/aiohttp-s3-client?branch=master) ![tox](https://github.com/mosquito/aiohttp-s3-client/workflows/tox/badge.svg?branch=master)The simple module for putting and getting object from Amazon S3 compatible endpoints## Installation```bashpip install aiohttp-s3-client```## Usage```pythonfrom http import HTTPStatusfrom aiohttp import ClientSessionfrom aiohttp_s3_client import S3Clientasync with ClientSession(raise_for_status=True) as session:    client = S3Client(        url=&quot;http://s3-url&quot;,        session=session,        access_key_id=&quot;key-id&quot;,        secret_access_key=&quot;hackme&quot;,        region=&quot;us-east-1&quot;    )    # Upload str object to bucket &quot;bucket&quot; and key &quot;str&quot;    async with client.put(&quot;bucket/str&quot;, &quot;hello, world&quot;) as resp:        assert resp.status == HTTPStatus.OK    # Upload bytes object to bucket &quot;bucket&quot; and key &quot;bytes&quot;    resp = await client.put(&quot;bucket/bytes&quot;, b&quot;hello, world&quot;)    assert resp.status == HTTPStatus.OK    # Upload AsyncIterable to bucket &quot;bucket&quot; and key &quot;iterable&quot;    async def gen():        yield b'some bytes'    resp = await client.put(&quot;bucket/file&quot;, gen())    assert resp.status == HTTPStatus.OK    # Upload file to bucket &quot;bucket&quot; and key &quot;file&quot;    resp = await client.put_file(&quot;bucket/file&quot;, &quot;/path_to_file&quot;)    assert resp.status == HTTPStatus.OK    # Check object exists using bucket+key    resp = await client.head(&quot;bucket/key&quot;)    assert resp == HTTPStatus.OK    # Get object by bucket+key    resp = await client.get(&quot;bucket/key&quot;)    data = await resp.read()    # Delete object using bucket+key    resp = await client.delete(&quot;bucket/key&quot;)    assert resp == HTTPStatus.NO_CONTENT    # List objects by prefix    async for result in client.list_objects_v2(&quot;bucket/&quot;, prefix=&quot;prefix&quot;):        # Each result is a list of metadata objects representing an object        # stored in the bucket.        do_work(result)```Bucket may be specified as subdomain or in object name:```pythonclient = S3Client(url=&quot;http://bucket.your-s3-host&quot;, ...)resp = await client.put(&quot;key&quot;, gen())client = S3Client(url=&quot;http://your-s3-host&quot;, ...)resp = await client.put(&quot;bucket/key&quot;, gen())client = S3Client(url=&quot;http://your-s3-host/bucket&quot;, ...)resp = await client.put(&quot;key&quot;, gen())```Auth may be specified with keywords or in URL:```pythonclient = S3Client(url=&quot;http://your-s3-host&quot;, access_key_id=&quot;key_id&quot;,                  secret_access_key=&quot;access_key&quot;, ...)client = S3Client(url=&quot;http://key_id:access_key@your-s3-host&quot;, ...)```## Multipart uploadFor uploading large files [multipart uploading](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)can be used. It allows you to asynchronously upload multiple parts of a fileto S3.S3Client handles retries of part uploads and calculates part hash for integrity checks.```pythonclient = S3Client()await client.put_file_multipart(    &quot;test/bigfile.csv&quot;,    headers={    &quot;Content-Type&quot;: &quot;text/csv&quot;,    },    workers_count=8,)```## Parallel download to fileS3 supports `GET` requests with `Range` header. It's possible to downloadobjects in parallel with multiple connections for speedup.S3Client handles retries of partial requests and makes sure that file won'tchanged during download with `ETag` header.If your system supports `pwrite` syscall (linux, macos, etc) it will be used towrite simultaneously to a single file. Otherwise, each worker will have own filewhich will be concatenated after downloading.```pythonclient = S3Client()await client.get_file_parallel(    &quot;dump/bigfile.csv&quot;,    &quot;/home/user/bigfile.csv&quot;,    workers_count=8,)```</longdescription>
</pkgmetadata>