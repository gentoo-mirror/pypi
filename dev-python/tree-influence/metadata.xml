<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>TreeInfluence: Influence Estimation for Gradient-Boosted Decision Trees---[![PyPi version](https://img.shields.io/pypi/v/tree_influence)](https://pypi.org/project/tree_influence/)[![Python version](https://img.shields.io/badge/python-3.9%20%7C%203.10-blue)](https://pypi.org/project/tree_influence/)[![Github License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/jjbrophy47/tree_influence/blob/master/LICENSE)[![Build](https://github.com/jjbrophy47/tree_influence/actions/workflows/wheels.yml/badge.svg?branch=v0.0.3)](https://github.com/jjbrophy47/tree_influence/actions/workflows/wheels.yml)**tree-influence** is a python library that implements influence estimation for gradient-boosted decision trees (GBDTs), adapting popular techniques such as TracIn and Influence Functions to GBDTs. This library is compatible with all major GBDT frameworks including LightGBM, XGBoost, CatBoost, and SKLearn.&lt;p align=&quot;center&quot;&gt;&lt;img align=&quot;center&quot; src=&quot;images/illustration.png&quot; alt=&quot;illustration&quot;&gt;&lt;/p&gt;Installation---```shellpip install tree-influence```Usage---Simple example using *BoostIn* to identify the most influential training instances to a given test instance:```pythonimport numpy as npfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom lightgbm import LGBMClassifierfrom tree_influence.explainers import BoostIn# load iris datadata = load_iris()X, y = data['data'], data['target']# use two classes, then split into train and testidxs = np.where(y != 2)[0]X, y = X[idxs], y[idxs]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)# train GBDT modelmodel = LGBMClassifier().fit(X_train, y_train)# fit influence estimatorexplainer = BoostIn().fit(model, X_train, y_train)# estimate training influences on each test instanceinfluence = explainer.get_local_influence(X_test, y_test)  # shape=(no. train, no. test)# extract influence values for the first test instancevalues = influence[:, 0]  # shape=(no. train,)# sort training examples from:# - most positively influential (decreases loss of the test instance the most), to# - most negatively influential (increases loss of the test instance the most)training_idxs = np.argsort(values)[::-1]```Supported Estimators---**tree-influence** supports the following influence-estimation techniques in GBDTs:| Method | Description || -------| ----------- || BoostIn | Traces the influence of a training instance throughout the training process (adaptation of TracIn). || TREX | Trains a surrogate kernel model that approximates the original model and decomposes any prediction into a weighted sum of the training examples (adaptation of representer-point methods). || LeafInfluence | Estimates the impact of a training example on the *final* GBDT model (adaptation of influence functions). || TreeSim | Computes influence via similarity in tree-kernel space. || LOO | Leave-one-out retraining, measures the influence of a training instance by removing and retraining without that instance.License---[Apache License 2.0](https://github.com/jjbrophy47/tree_influence/blob/master/LICENSE).Reference---Brophy, Hammoudeh, and Lowd. [Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees](https://arxiv.org/abs/2205.00359). arXiv 2022.```@article{brophy2022treeinfluence,  title={Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees},  author={Brophy, Jonathan, and Hammoudeh, Zayd, and Lowd, Daniel},  journal={arXiv preprint arXiv:2205.00359},  year={2022},}```</longdescription>
</pkgmetadata>