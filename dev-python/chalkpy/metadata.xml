<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># ChalkChalk enables innovative machine learning teams to focus on buildingthe unique products and models which make their business stand out.Behind the scenes Chalk seamlessly handles data infrastructure witha best-in-class developer experience. Here’s how it works –---## DevelopChalk makes it simple to develop feature pipelines for machinelearning. Define Python functions using the libraries and tools you'refamiliar with instead of specialized DSLs. Chalk then orchestratesyour functions into pipelines that execute in parallel on a Rust-basedengine and coordinates the infrastructure required to computefeatures.### FeaturesTo get started, [define your features](/docs/features) with[Pydantic](https://pydantic-docs.helpmanual.io/)-inspired Python classes.You can define schemas, specify relationships, and add metadatato help your team share and re-use work.```py@featuresclass User:    id: int    full_name: str    nickname: Optional[str]    email: Optional[str]    birthday: date    credit_score: float    datawarehouse_feature: float    transactions: DataFrame[Transaction] = has_many(lambda: Transaction.user_id == User.id)```### ResolversNext, tell Chalk how to compute your features.Chalk ingests data from your existing data stores,and lets you use Python to compute features with[feature resolvers](/docs/resolver-overview).Feature resolvers are declared with the decorators `@online` and`@offline`, and can depend on the outputs of other feature resolvers.Resolvers make it easy to rapidly integrate a wide variety of datasources, join them together, and use them in your model.#### SQL```pythonpg = PostgreSQLSource()@onlinedef get_user(uid: User.id) -&gt; Features[User.full_name, User.email]:    return pg.query_string(        &quot;select email, full_name from users where id=:id&quot;,        args=dict(id=uid)    ).one()```#### REST```pythonimport requests@onlinedef get_socure_score(uid: User.id) -&gt; Features[User.socure_score]:    return (        requests.get(&quot;https://api.socure.com&quot;, json={            id: uid        }).json()['socure_score']    )```---## ExecuteOnce you've defined your features and resolvers, Chalk orchestratesthem into flexible pipelines that make training and executing models easy.Chalk has built-in support for feature engineering workflows --no need to manage Airflow or orchestrate complicated streaming flows.You can execute resolver pipelines with declarative caching,ingest batch data on a schedule, and easily make slow sourcesavailable online for low-latency serving.### CachingMany data sources (like vendor APIs) are too slow for online use casesand/or charge a high dollar cost-per-call. Chalk lets you optimize latencyand cost by defining declarative caching policies which are well-integratedthroughout the system. You no longer have to manage Redis, Memcached, DynamodDB,or spend time tuning cache-warming pipelines.Add a caching policy with one line of code in your feature definition:```python@featuresclass ExternalBankAccount:-   balance: int+   balance: int = feature(max_staleness=&quot;**1d**&quot;)```Optionally warm feature caches by executing resolvers on a schedule:```py@online(cron=&quot;**1d**&quot;)def fn(id: User.id) -&gt; User.credit_score:  return redshift.query(...).all()```Or override staleness tolerances at query time when you need fresherdata for your models:```pychalk.query(    ...,    outputs: [User.fraud_score],    max_staleness: { User.fraud_score: &quot;1m&quot; })```### Batch ETL ingestionChalk also makes it simple to generate training sets from data warehousesources -- join data from services like S3, Redshift, BQ, Snowflake(or other custom sources) with historical features computed online.Specify a cron schedule on an `@offline` resolver and Chalk automatically ingestsdata with support for incremental reads:```py@offline(cron=&quot;**1h**&quot;)def fn() -&gt; Feature[User.id, User.datawarehouse_feature]:  return redshift.query(...).incremental()```Chalk makes this data available for point-in-time-correct datasetgeneration for data science use-cases. Every pipeline has built-inmonitoring and alerting to ensure data quality and timeliness.### Reverse ETLWhen your model needs to use features that are canonically stored ina high-latency data source (like a data warehouse), Chalk's ReverseETL support makes it simple to bring those features online and servethem quickly.Add a single line of code to an `offline` resolver, and Chalk constructsa managed reverse ETL pipeline for that data source:```py@offline(offline_to_online_etl=&quot;5m&quot;)```Now data from slow offline data sources is automatically available forlow-latency online serving.---## Deploy + queryOnce you've defined your pipelines, you can rapidly deploy them toproduction with Chalk's CLI:```bashchalk apply```This creates a deployment of your project, which is served at a uniquepreview URL. You can promote this deployment to production, orperform QA workflows on your preview environment to make sure thatyour Chalk deployment performs as expected.Once you promote your deployment to production, Chalk makes featuresavailable for low-latency [online inference](/docs/query-basics) and[offline training](/docs/training-client). Significantly, Chalk usesthe exact same source code to serve temporally-consistent trainingsets to data scientists and live feature values to models. This re-useensures that feature values from online and offline contexts match anddramatically cuts development time.### Online inferenceChalk's online store &amp; feature computation engine make it easy to queryfeatures with ultra low-latency, so you can use your feature pipelinesto serve online inference use-cases.Integrating Chalk with your production application takes minutes viaChalk's simple REST API:&lt;RequestingFeaturesOnline style={{ height: 328 }} /&gt;Features computed to serve online requests are also replicated to Chalk'soffline store for historical performance tracking and training set generation.### Offline trainingData scientists can use Chalk's Jupyter integration to create datasetsand train models. Datasets are stored and tracked so that they can bere-used by other modelers, and so that model provenance is tracked foraudit and reproducibility.```pythonX = ChalkClient.offline_query(    input=labels[[User.uid, timestamp]],    output=[        User.returned_transactions_last_60,        User.user_account_name_match_score,        User.socure_score,        User.identity.has_verified_phone,        User.identity.is_voip_phone,        User.identity.account_age_days,        User.identity.email_age,    ],)```Chalk datasets are always &quot;temporally consistent.&quot;This means that you can provide labels with different past timestamps andget historical features that represent what your application would haveretrieved online at those past times. Temporal consistency ensures thatyour model training doesn't mix &quot;future&quot; and &quot;past&quot; data.</longdescription>
</pkgmetadata>