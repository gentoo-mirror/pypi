<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;h1 align=&quot;center&quot;&gt;  &lt;br&gt;  &lt;a href=&quot;https://github.com/jschnurr/scrapyscript&quot;&gt;&lt;img src=&quot;https://i.ibb.co/ww3bNZ3/scrapyscript.png&quot; alt=&quot;Scrapyscript&quot;&gt;&lt;/a&gt;  &lt;br&gt;&lt;/h1&gt;&lt;h4 align=&quot;center&quot;&gt;Embed Scrapy jobs directly in your code&lt;/h4&gt;&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://github.com/jschnurr/scrapyscript/releases&quot;&gt;    &lt;img src=&quot;https://img.shields.io/github/release/jschnurr/scrapyscript.svg&quot;&gt;  &lt;/a&gt;  &lt;a href=&quot;https://pypi.org/project/scrapyscript/&quot;&gt;    &lt;img src=&quot;https://img.shields.io/pypi/v/scrapyscript.svg&quot;&gt;  &lt;/a&gt;  &lt;img src=&quot;https://github.com/jschnurr/scrapyscript/workflows/Tests/badge.svg&quot;&gt;    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/scrapyscript.svg&quot;&gt;&lt;/p&gt;### What is Scrapyscript?Scrapyscript is a Python library you can use to run [Scrapy](https://github.com/scrapy/scrapy) spiders directly from your code. Scrapy is a great framework to use for scraping projects, but sometimes you don't need the whole framework, and just want to run a small spider from a script or a [Celery](https://github.com/celery/celery) job. That's where Scrapyscript comes in.With Scrapyscript, you can:- wrap regular Scrapy [Spiders](https://docs.scrapy.org/en/latest/topics/spiders.html) in a `Job`- load the `Job(s)` in a `Processor`- call `processor.run()` to execute them... returning all results when the last job completes.Let's see an example.```pythonimport scrapyfrom scrapyscript import Job, Processorprocessor = Processor(settings=None)class PythonSpider(scrapy.spiders.Spider):    name = &quot;myspider&quot;    def start_requests(self):        yield scrapy.Request(self.url)    def parse(self, response):        data = response.xpath(&quot;//title/text()&quot;).extract_first()        return {'title': data}job = Job(PythonSpider, url=&quot;http://www.python.org&quot;)results = processor.run(job)print(results)``````json[{ &quot;title&quot;: &quot;Welcome to Python.org&quot; }]```See the [examples](examples/) directory for more, including a complete `Celery` example.### Install```pythonpip install scrapyscript```### Requirements- Linux or MacOS- Python 3.8+- Scrapy 2.5+### API#### Job (spider, \*args, \*\*kwargs)A single request to call a spider, optionally passing in \*args or \*\*kwargs, which will be passed through to the spider constructor at runtime.```python# url will be available as self.url inside MySpider at runtimemyjob = Job(MySpider, url='http://www.github.com')```#### Processor (settings=None)Create a multiprocessing reactor for running spiders. Optionally provide a `scrapy.settings.Settings` object to configure the Scrapy runtime.```pythonsettings = scrapy.settings.Settings(values={'LOG_LEVEL': 'WARNING'})processor = Processor(settings=settings)```#### Processor.run(jobs)Start the Scrapy engine, and execute one or more jobs. Blocks and returns consolidated results in a single list.`jobs` can be a single instance of `Job`, or a list.```pythonresults = processor.run(myjob)```or```pythonresults = processor.run([myjob1, myjob2, ...])```#### A word about Spider outputsAs per the [scrapy docs](https://doc.scrapy.org/en/latest/topics/spiders.html), a `Spider`must return an iterable of `Request` and/or `dict` or `Item` objects.Requests will be consumed by Scrapy inside the `Job`. `dict` or `scrapy.Item` objects will be queuedand output together when all spiders are finished.Due to the way billiard handles communication between processes, each `dict` or `Item` must bepickle-able using pickle protocol 0. **It's generally best to output `dict` objects from your Spider.**### ContributingUpdates, additional features or bug fixes are always welcome.#### Setup- Install [Poetry](https://python-poetry.org/docs/#installation)- `git clone git@github.com:jschnurr/scrapyscript.git`- `poetry install`#### Tests- `make test` or `make tox`### Version HistorySee [CHANGELOG.md](CHANGELOG.md)### LicenseThe MIT License (MIT). See LICENCE file for details.</longdescription>
</pkgmetadata>