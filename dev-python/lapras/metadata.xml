<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># LAPRAS[![PyPi version][pypi-image]][pypi-url][![Python version][python-image]][docs-url]Lapras is designed to make the model developing job easily and conveniently.It contains these functions below in one key operation: data exploratory analysis, feature selection, feature binning,data visualization, scorecard modeling(a logistic regression model with excellent interpretability), performance measure.Let's get started.## Usage1.Exploratory Data Analysislapras.detect()lapras.quality()lapras.IV()lapras.VIF()lapras.PSI()2.Feature Selectionlapras.select()lapras.stepwise()3.Binninglapras.Combiner()lapras.WOETransformer()lapras.bin_stats()lapras.bin_plot()4.Modelinglapras.ScoreCard()5.Performance Measurelapras.perform()lapras.LIFT()lapras.score_plot()lapras.KS_bucket()lapras.PPSI()lapras.KS()lapras.AUC()6.One Key Auto ModelingLapras also provides a function which runs all the steps above automatically:lapras.auto_model()## Installvia pip```bashpip install lapras --upgrade -i https://pypi.org/simple```via source code```bashpython setup.py install```install_requires = ['numpy &gt;= 1.18.4','pandas &gt;= 0.25.1','scipy &gt;= 1.3.2','scikit-learn =0.22.2','seaborn &gt;= 0.10.1','statsmodels &gt;= 0.13.1','tensorflow &gt;= 2.2.0','hyperopt &gt;= 0.2.7','pickle &gt;= 4.0',]## Documents```pythonimport laprasimport pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitimport matplotlib as mplimport matplotlib.pyplot as pltpd.options.display.max_colwidth = 100import math%matplotlib inline``````python# Read in data filedf = pd.read_csv('data/demo.csv',encoding=&quot;utf-8&quot;)``````pythonto_drop = ['id'] # exclude the features which not being used， eg:idtarget = 'bad' # Y label nametrain_df, test_df, _, _ = train_test_split(df, df[[target]], test_size=0.3, random_state=42) # to divide the training set and testing set, strongly recommended``````python# EDA(Exploratory Data Analysis)# Parameter details：# dataframe=None lapras.detect(train_df).sort_values(&quot;missing&quot;)```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;type&lt;/th&gt;      &lt;th&gt;size&lt;/th&gt;      &lt;th&gt;missing&lt;/th&gt;      &lt;th&gt;unique&lt;/th&gt;      &lt;th&gt;mean_or_top1&lt;/th&gt;      &lt;th&gt;std_or_top2&lt;/th&gt;      &lt;th&gt;min_or_top3&lt;/th&gt;      &lt;th&gt;1%_or_top4&lt;/th&gt;      &lt;th&gt;10%_or_top5&lt;/th&gt;      &lt;th&gt;50%_or_bottom5&lt;/th&gt;      &lt;th&gt;75%_or_bottom4&lt;/th&gt;      &lt;th&gt;90%_or_bottom3&lt;/th&gt;      &lt;th&gt;99%_or_bottom2&lt;/th&gt;      &lt;th&gt;max_or_bottom1&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;id&lt;/td&gt;      &lt;td&gt;int64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.0000&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;3947.266630&lt;/td&gt;      &lt;td&gt;2252.395671&lt;/td&gt;      &lt;td&gt;2.0&lt;/td&gt;      &lt;td&gt;87.03&lt;/td&gt;      &lt;td&gt;820.1&lt;/td&gt;      &lt;td&gt;3931.5&lt;/td&gt;      &lt;td&gt;5889.25&lt;/td&gt;      &lt;td&gt;7077.8&lt;/td&gt;      &lt;td&gt;7782.99&lt;/td&gt;      &lt;td&gt;7861.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;bad&lt;/td&gt;      &lt;td&gt;int64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.0000&lt;/td&gt;      &lt;td&gt;2&lt;/td&gt;      &lt;td&gt;0.073246&lt;/td&gt;      &lt;td&gt;0.260564&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.00&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.00&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;1.00&lt;/td&gt;      &lt;td&gt;1.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;score&lt;/td&gt;      &lt;td&gt;int64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.0000&lt;/td&gt;      &lt;td&gt;265&lt;/td&gt;      &lt;td&gt;295.280625&lt;/td&gt;      &lt;td&gt;66.243181&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.00&lt;/td&gt;      &lt;td&gt;223.0&lt;/td&gt;      &lt;td&gt;303.0&lt;/td&gt;      &lt;td&gt;336.00&lt;/td&gt;      &lt;td&gt;366.0&lt;/td&gt;      &lt;td&gt;416.00&lt;/td&gt;      &lt;td&gt;461.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;age&lt;/td&gt;      &lt;td&gt;float64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.0002&lt;/td&gt;      &lt;td&gt;34&lt;/td&gt;      &lt;td&gt;27.659880&lt;/td&gt;      &lt;td&gt;4.770299&lt;/td&gt;      &lt;td&gt;19.0&lt;/td&gt;      &lt;td&gt;21.00&lt;/td&gt;      &lt;td&gt;23.0&lt;/td&gt;      &lt;td&gt;27.0&lt;/td&gt;      &lt;td&gt;30.00&lt;/td&gt;      &lt;td&gt;34.0&lt;/td&gt;      &lt;td&gt;43.00&lt;/td&gt;      &lt;td&gt;53.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;wealth&lt;/td&gt;      &lt;td&gt;float64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.0244&lt;/td&gt;      &lt;td&gt;18&lt;/td&gt;      &lt;td&gt;4.529806&lt;/td&gt;      &lt;td&gt;1.823149&lt;/td&gt;      &lt;td&gt;1.0&lt;/td&gt;      &lt;td&gt;1.00&lt;/td&gt;      &lt;td&gt;3.0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;5.00&lt;/td&gt;      &lt;td&gt;7.0&lt;/td&gt;      &lt;td&gt;10.00&lt;/td&gt;      &lt;td&gt;22.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;education&lt;/td&gt;      &lt;td&gt;float64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.1427&lt;/td&gt;      &lt;td&gt;5&lt;/td&gt;      &lt;td&gt;3.319483&lt;/td&gt;      &lt;td&gt;1.005660&lt;/td&gt;      &lt;td&gt;1.0&lt;/td&gt;      &lt;td&gt;1.00&lt;/td&gt;      &lt;td&gt;2.0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;4.00&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;5.00&lt;/td&gt;      &lt;td&gt;5.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;period&lt;/td&gt;      &lt;td&gt;float64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.1714&lt;/td&gt;      &lt;td&gt;5&lt;/td&gt;      &lt;td&gt;7.246326&lt;/td&gt;      &lt;td&gt;1.982060&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;4.00&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;      &lt;td&gt;10.00&lt;/td&gt;      &lt;td&gt;10.0&lt;/td&gt;      &lt;td&gt;10.00&lt;/td&gt;      &lt;td&gt;14.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;max_unpay_day&lt;/td&gt;      &lt;td&gt;float64&lt;/td&gt;      &lt;td&gt;5502&lt;/td&gt;      &lt;td&gt;0.9253&lt;/td&gt;      &lt;td&gt;11&lt;/td&gt;      &lt;td&gt;185.476886&lt;/td&gt;      &lt;td&gt;22.339647&lt;/td&gt;      &lt;td&gt;28.0&lt;/td&gt;      &lt;td&gt;86.00&lt;/td&gt;      &lt;td&gt;171.0&lt;/td&gt;      &lt;td&gt;188.0&lt;/td&gt;      &lt;td&gt;201.00&lt;/td&gt;      &lt;td&gt;208.0&lt;/td&gt;      &lt;td&gt;208.00&lt;/td&gt;      &lt;td&gt;208.0&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```python# Calculate IV value of features（Calculate by default decision tree binning）# Parameter details：# dataframe=None original data# target = 'target' Y label namelapras.quality(train_df.drop(to_drop,axis=1),target = target)```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;iv&lt;/th&gt;      &lt;th&gt;unique&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;score&lt;/td&gt;      &lt;td&gt;0.758342&lt;/td&gt;      &lt;td&gt;265.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;age&lt;/td&gt;      &lt;td&gt;0.504588&lt;/td&gt;      &lt;td&gt;35.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;wealth&lt;/td&gt;      &lt;td&gt;0.275775&lt;/td&gt;      &lt;td&gt;19.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;education&lt;/td&gt;      &lt;td&gt;0.230553&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;max_unpay_day&lt;/td&gt;      &lt;td&gt;0.170061&lt;/td&gt;      &lt;td&gt;12.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;period&lt;/td&gt;      &lt;td&gt;0.073716&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```python# Calculate PSI betweet features# Parameter details：# actual=None actual feature# predict=None prediction feature# bins=10 count of binning# return_frame=False return the dataframe of binning if set to truecols = list(lapras.quality(train_df,target = target).reset_index()['index'])for col in cols:    if col not in [target]:        print(&quot;%s: %.4f&quot; % (col,lapras.PSI(train_df[col], test_df[col])))``````pythonscore: 0.1500age: 0.0147wealth: 0.0070education: 0.0010max_unpay_day: 0.0042id: 0.0000period: 0.0030``````python# Calculate VIF# Parameter details：# dataframe=None lapras.VIF(train_df.drop(['id','bad'],axis=1))``````pythonwealth            1.124927max_unpay_day     2.205619score            18.266471age              17.724547period            1.193605education         1.090158dtype: float64``````python# Calculate IV value# Parameter details：# feature=None feature data# target=None Y label datalapras.IV(train_df['age'],train_df[target])``````python0.5045879202656338``````python# Features filtering# Parameter details：# frame=None original data# target=None Y label name# empty=0.9 empty feature filtering， feature will be removed if data missing ratio greater than the threshold# iv=0.02 IV value filtering， feature will be removed if IV value lesser than the threshold# corr=0.7 correlation filtering， feature will be removed if correlation value greater than the threshold# vif=False multicollinearity filtering， feature will be removed if multicollinearity value greater than the threshold, default False due to a large number of calculations # return_drop=False reture the removed features if set to true# exclude=None features will be remained if set into this parametertrain_selected, dropped = lapras.select(train_df.drop(to_drop,axis=1),target = target, empty = 0.95, \                                                iv = 0.05, corr = 0.9, vif = False, return_drop=True, exclude=[])print(dropped)print(train_selected.shape)train_selected``````python{'empty': array([], dtype=float64), 'iv': array([], dtype=object), 'corr': array([], dtype=object)}(5502, 7)```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;bad&lt;/th&gt;      &lt;th&gt;wealth&lt;/th&gt;      &lt;th&gt;max_unpay_day&lt;/th&gt;      &lt;th&gt;score&lt;/th&gt;      &lt;th&gt;age&lt;/th&gt;      &lt;th&gt;period&lt;/th&gt;      &lt;th&gt;education&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;4168&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;288&lt;/td&gt;      &lt;td&gt;23.0&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;605&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;216&lt;/td&gt;      &lt;td&gt;32.0&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3018&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;5.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;250&lt;/td&gt;      &lt;td&gt;23.0&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;      &lt;td&gt;2.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;4586&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;7.0&lt;/td&gt;      &lt;td&gt;171.0&lt;/td&gt;      &lt;td&gt;413&lt;/td&gt;      &lt;td&gt;31.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;2.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;1468&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;5.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;204&lt;/td&gt;      &lt;td&gt;29.0&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;      &lt;td&gt;2.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;5226&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;171.0&lt;/td&gt;      &lt;td&gt;346&lt;/td&gt;      &lt;td&gt;23.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;3.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;5390&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;5.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;207&lt;/td&gt;      &lt;td&gt;32.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;3.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;860&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;6.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;356&lt;/td&gt;      &lt;td&gt;42.0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;3.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;7603&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;3.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;323&lt;/td&gt;      &lt;td&gt;34.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;3.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;7270&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;      &lt;td&gt;NaN&lt;/td&gt;      &lt;td&gt;378&lt;/td&gt;      &lt;td&gt;24.0&lt;/td&gt;      &lt;td&gt;10.0&lt;/td&gt;      &lt;td&gt;4.0&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;5502 rows × 7 columns&lt;/p&gt;&lt;/div&gt;```python# Feature Binning， following methods are supported: monotonous binning， decision tree binning， Kmeans binning， equal frequency binning， equal step size binning# Parameter details：# X=None original data# y=None Y label name# method='dt' Binning method：'dt':decision tree binning(default),'mono':monotonous binning,'kmeans':Kmeans binning,'quantile':equal frequency binning,'step':equal step size binning# min_samples=1 the least sample numbers in each binning， represent the count of numbers when greater than 1， represent the ratio of total count when between 0 and 1# n_bins=10 maximun binning count# c.load(dict) adjust the binning by loading a customized dict# c.export() export current binning information by dict formatc = lapras.Combiner()c.fit(train_selected, y = target,method = 'mono', min_samples = 0.05,n_bins=8) #empty_separate = False# # c.load({'age': [22.5, 23.5, 24.5, 25.5, 28.5,36.5],# #  'education': [ 3.5],# #  'max_unpay_day': [59.5],# #  'period': [5.0, 9.0],# #  'score': [205.5, 236.5, 265.5, 275.5, 294.5, 329.5, 381.5],# #  'wealth': [2.5, 3.5, 6.5]})c.export()``````python{'age': [23.0, 24.0, 25.0, 26.0, 28.0, 29.0, 37.0],'education': [3.0, 4.0],'max_unpay_day': [171.0],'period': [6.0, 10.0],'score': [237.0, 272.0, 288.0, 296.0, 330.0, 354.0, 384.0],'wealth': [3.0, 4.0, 5.0, 7.0]}``````python# To transform the original data into binning data# Parameter details：# X=None original data# labels=False binning label will be shown when set to truec.transform(train_selected, labels=True).iloc[0:10,:]```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;bad&lt;/th&gt;      &lt;th&gt;wealth&lt;/th&gt;      &lt;th&gt;max_unpay_day&lt;/th&gt;      &lt;th&gt;score&lt;/th&gt;      &lt;th&gt;age&lt;/th&gt;      &lt;th&gt;period&lt;/th&gt;      &lt;th&gt;education&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;4168&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;02.[4.0,5.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;03.[288.0,296.0)&lt;/td&gt;      &lt;td&gt;01.[23.0,24.0)&lt;/td&gt;      &lt;td&gt;01.[6.0,10.0)&lt;/td&gt;      &lt;td&gt;02.[4.0,inf)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;605&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;02.[4.0,5.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,237.0)&lt;/td&gt;      &lt;td&gt;06.[29.0,37.0)&lt;/td&gt;      &lt;td&gt;01.[6.0,10.0)&lt;/td&gt;      &lt;td&gt;02.[4.0,inf)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3018&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;03.[5.0,7.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;01.[237.0,272.0)&lt;/td&gt;      &lt;td&gt;01.[23.0,24.0)&lt;/td&gt;      &lt;td&gt;01.[6.0,10.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;4586&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;04.[7.0,inf)&lt;/td&gt;      &lt;td&gt;01.[171.0,inf)&lt;/td&gt;      &lt;td&gt;07.[384.0,inf)&lt;/td&gt;      &lt;td&gt;06.[29.0,37.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,6.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;1468&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;03.[5.0,7.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,237.0)&lt;/td&gt;      &lt;td&gt;06.[29.0,37.0)&lt;/td&gt;      &lt;td&gt;01.[6.0,10.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;6251&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;03.[5.0,7.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;01.[237.0,272.0)&lt;/td&gt;      &lt;td&gt;01.[23.0,24.0)&lt;/td&gt;      &lt;td&gt;02.[10.0,inf)&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3686&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,237.0)&lt;/td&gt;      &lt;td&gt;01.[23.0,24.0)&lt;/td&gt;      &lt;td&gt;01.[6.0,10.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3615&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;02.[4.0,5.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;03.[288.0,296.0)&lt;/td&gt;      &lt;td&gt;06.[29.0,37.0)&lt;/td&gt;      &lt;td&gt;02.[10.0,inf)&lt;/td&gt;      &lt;td&gt;02.[4.0,inf)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;5338&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;04.[296.0,330.0)&lt;/td&gt;      &lt;td&gt;03.[25.0,26.0)&lt;/td&gt;      &lt;td&gt;02.[10.0,inf)&lt;/td&gt;      &lt;td&gt;00.[-inf,3.0)&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3985&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;03.[5.0,7.0)&lt;/td&gt;      &lt;td&gt;00.[-inf,171.0)&lt;/td&gt;      &lt;td&gt;01.[237.0,272.0)&lt;/td&gt;      &lt;td&gt;01.[23.0,24.0)&lt;/td&gt;      &lt;td&gt;01.[6.0,10.0)&lt;/td&gt;      &lt;td&gt;02.[4.0,inf)&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```python# To output bin_stats and bin_plot# Parameter details：# frame=None data transformed by Combiner, keeping binning labels# col=None features to be outputed# target='target' Y label name# Note：The binning details may be different between traning set and testing set due to Population Stability. cols = list(lapras.quality(train_selected,target = target).reset_index()['index'])for col in cols:    if col != target:        print(lapras.bin_stats(c.transform(train_selected[[col, target]], labels=True), col=col, target=target))        lapras.bin_plot(c.transform(train_selected[[col,target]], labels=True), col=col, target=target)``````python          score  bad_count  total_count  bad_rate     ratio       woe  \0   00.[-inf,237.0)        136          805  0.168944  0.146310  0.9447341  01.[237.0,272.0)        101          832  0.121394  0.151218  0.5585702  02.[272.0,288.0)         46          533  0.086304  0.096874  0.1782403  03.[288.0,296.0)         20          295  0.067797  0.053617 -0.0831764  04.[296.0,330.0)         73         1385  0.052708  0.251727 -0.3509855  05.[330.0,354.0)         18          812  0.022167  0.147583 -1.2488496  06.[354.0,384.0)          8          561  0.014260  0.101963 -1.6980537    07.[384.0,inf)          1          279  0.003584  0.050709 -3.089758     iv  total_iv  0  0.194867  0.7351161  0.059912  0.7351162  0.003322  0.7351163  0.000358  0.7351164  0.026732  0.7351165  0.138687  0.7351166  0.150450  0.7351167  0.160788  0.735116```![png](http://img.badtom.cn/output_13_1.png)```python          age  bad_count  total_count  bad_rate     ratio       woe  \0  00.[-inf,23.0)         90          497  0.181087  0.090331  1.0288601  01.[23.0,24.0)         77          521  0.147793  0.094693  0.7858442  02.[24.0,25.0)         57          602  0.094684  0.109415  0.2801293  03.[25.0,26.0)         38          539  0.070501  0.097964 -0.0411574  04.[26.0,28.0)         58          997  0.058175  0.181207 -0.2465095  05.[28.0,29.0)         20          379  0.052770  0.068884 -0.3497276  06.[29.0,37.0)         57         1657  0.034400  0.301163 -0.7968447   07.[37.0,inf)          6          310  0.019355  0.056343 -1.387405     iv  total_iv  0  0.147647   0.455791  0.081721   0.455792  0.009680   0.455793  0.000163   0.455794  0.009918   0.455795  0.007267   0.455796  0.137334   0.455797  0.062060   0.45579```![png](http://img.badtom.cn/output_13_3.png)```python      wealth  bad_count  total_count  bad_rate     ratio       woe  \0  00.[-inf,3.0)        106          593  0.178752  0.107779  1.0130381   01.[3.0,4.0)         84         1067  0.078725  0.193929  0.0780712   02.[4.0,5.0)         88         1475  0.059661  0.268084 -0.2196983   03.[5.0,7.0)         99         1733  0.057126  0.314976 -0.2658034   04.[7.0,inf)         26          634  0.041009  0.115231 -0.614215     iv  total_iv  0  0.169702  0.2362051  0.001222  0.2362052  0.011787  0.2362053  0.019881  0.2362054  0.033612  0.236205```![png](http://img.badtom.cn/output_13_5.png)```python   education  bad_count  total_count  bad_rate     ratio       woe  \0  00.[-inf,3.0)        225         2123  0.105982  0.385860  0.4054081   01.[3.0,4.0)         61          648  0.094136  0.117775  0.2737122   02.[4.0,inf)        117         2731  0.042841  0.496365 -0.568600     iv  total_iv  0  0.075439  0.2117751  0.009920  0.2117752  0.126415  0.211775```![png](http://img.badtom.cn/output_13_7.png)```python max_unpay_day  bad_count  total_count  bad_rate     ratio       woe  \0  00.[-inf,171.0)        330         5098  0.064731  0.926572 -0.1327261   01.[171.0,inf)         73          404  0.180693  0.073428  1.026204     iv  total_iv  0  0.015426  0.1346991  0.119272  0.134699```![png](http://img.badtom.cn/output_13_9.png)```python      period  bad_count  total_count  bad_rate     ratio       woe  \0  00.[-inf,6.0)         52         1158  0.044905  0.210469 -0.5193981  01.[6.0,10.0)        218         2871  0.075932  0.521810  0.0389122  02.[10.0,inf)        133         1473  0.090292  0.267721  0.227787     iv  total_iv  0  0.045641  0.0617581  0.000803  0.0617582  0.015314  0.061758```![png](http://img.badtom.cn/output_13_11.png)```python# WOE value transformation# transer.fit()：# X=None data transformed by Combiner# y=None Y label# exclude=None features exclude from transformation# transer.transform()：# X=None # transer.export()：# Note： Only training set need to be fittransfer = lapras.WOETransformer()transfer.fit(c.transform(train_selected), train_selected[target], exclude=[target])train_woe = transfer.transform(c.transform(train_selected))transfer.export()``````python{'age': {0: 1.0288596439961428,1: 0.7858440185299318,2: 0.2801286322797789,3: -0.041156782250006324,4: -0.24650930955337075,5: -0.34972695582581514,6: -0.7968444812848496,7: -1.387405073069694},'education': {0: 0.4054075821430197,1: 0.27371220345368763,2: -0.5685998002779383},'max_unpay_day': {0: -0.13272639517618706, 1: 1.026204224879801},'period': {0: -0.51939830439238,1: 0.0389118677598222,2: 0.22778739438526965},'score': {0: 0.9447339847162963,1: 0.5585702161999536,2: 0.17824043251497793,3: -0.08317566500410743,4: -0.3509853692471706,5: -1.2488485442424984,6: -1.6980533007340262,7: -3.089757954582164},'wealth': {0: 1.01303813013795,1: 0.0780708378046198,2: -0.21969844672815222,3: -0.2658032661768855,4: -0.6142151848362123}}``````python# Features filtering could be done once more after transformed into WOE value. This is optional.train_woe, dropped = lapras.select(train_woe,target = target, empty = 0.9, \                                                iv = 0.02, corr = 0.9, vif = False, return_drop=True, exclude=[])print(dropped)print(train_woe.shape)train_woe.head(10)``````python{'empty': array([], dtype=float64), 'iv': array([], dtype=object), 'corr': array([], dtype=object)}(5502, 7)```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;bad&lt;/th&gt;      &lt;th&gt;wealth&lt;/th&gt;      &lt;th&gt;max_unpay_day&lt;/th&gt;      &lt;th&gt;score&lt;/th&gt;      &lt;th&gt;age&lt;/th&gt;      &lt;th&gt;period&lt;/th&gt;      &lt;th&gt;education&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;4168&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.219698&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;-0.083176&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;      &lt;td&gt;0.038912&lt;/td&gt;      &lt;td&gt;-0.568600&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;605&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.219698&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.944734&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;      &lt;td&gt;0.038912&lt;/td&gt;      &lt;td&gt;-0.568600&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3018&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.558570&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;      &lt;td&gt;0.038912&lt;/td&gt;      &lt;td&gt;0.405408&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;4586&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.614215&lt;/td&gt;      &lt;td&gt;1.026204&lt;/td&gt;      &lt;td&gt;-3.089758&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;      &lt;td&gt;-0.519398&lt;/td&gt;      &lt;td&gt;0.405408&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;1468&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.944734&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;      &lt;td&gt;0.038912&lt;/td&gt;      &lt;td&gt;0.405408&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;6251&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.558570&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;      &lt;td&gt;0.227787&lt;/td&gt;      &lt;td&gt;0.405408&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3686&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;1.013038&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.944734&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;      &lt;td&gt;0.038912&lt;/td&gt;      &lt;td&gt;0.405408&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3615&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.219698&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;-0.083176&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;      &lt;td&gt;0.227787&lt;/td&gt;      &lt;td&gt;-0.568600&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;5338&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;1.013038&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;-0.350985&lt;/td&gt;      &lt;td&gt;-0.041157&lt;/td&gt;      &lt;td&gt;0.227787&lt;/td&gt;      &lt;td&gt;0.405408&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3985&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.558570&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;      &lt;td&gt;0.038912&lt;/td&gt;      &lt;td&gt;-0.568600&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```python# stepwise regression, to select best features, this is optional# Parameter details：# frame=None original data# target='target' Y label name# estimator='ols' model for regression, supporting 'ols', 'lr', 'lasso', 'ridge'# direction='both' direction for stepwise, supporting 'forward', 'backward', 'both' # criterion='aic' metric, supporting 'aic', 'bic', 'ks', 'auc'# max_iter=None max iteration times# return_drop=False return cols being removed if set to true# exclude=None exclude featuresfinal_data = lapras.stepwise(train_woe,target = target, estimator='ols', direction = 'both', criterion = 'aic', exclude = [])final_data```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;bad&lt;/th&gt;      &lt;th&gt;wealth&lt;/th&gt;      &lt;th&gt;max_unpay_day&lt;/th&gt;      &lt;th&gt;score&lt;/th&gt;      &lt;th&gt;age&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;4168&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.219698&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;-0.083176&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;605&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.219698&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.944734&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3018&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.558570&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;4586&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.614215&lt;/td&gt;      &lt;td&gt;1.026204&lt;/td&gt;      &lt;td&gt;-3.089758&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;1468&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.944734&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;5226&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.219698&lt;/td&gt;      &lt;td&gt;1.026204&lt;/td&gt;      &lt;td&gt;-1.248849&lt;/td&gt;      &lt;td&gt;0.785844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;5390&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;0.944734&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;860&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.265803&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;-1.698053&lt;/td&gt;      &lt;td&gt;-1.387405&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;7603&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0.078071&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;-0.350985&lt;/td&gt;      &lt;td&gt;-0.796844&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;7270&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;-0.219698&lt;/td&gt;      &lt;td&gt;-0.132726&lt;/td&gt;      &lt;td&gt;-1.698053&lt;/td&gt;      &lt;td&gt;0.280129&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;5502 rows × 5 columns&lt;/p&gt;&lt;/div&gt;```python# Scorecard modeling# Parameter details：# base_odds=1/60,base_score=600 When base_odds is 1/60, the corresponding base_score will be 600# pdo=40,rate=2 If the base_odds decrease by half, the corresponding pdo will increase by 40, these are the default parameters# combiner=None Combiner, input the fitted object# transfer=None WOETransformer, input the fitted object# ScoreCard.fit()：# X=None WOE value# y=None Y labelcard = lapras.ScoreCard(    combiner = c,    transfer = transfer)col = list(final_data.drop([target],axis=1).columns)card.fit(final_data[col], final_data[target])``````pythonScoreCard(base_odds=0.016666666666666666, base_score=600, card=None,combiner=&lt;lapras.transform.Combiner object at 0x000001EC0FB72438&gt;,pdo=40, rate=2,transfer=&lt;lapras.transform.WOETransformer object at 0x000001EC0FDAEF98&gt;)``````python# ScoreCard class method expaination# ScoreCard.predict() predict score for each sample：# X=None # ScoreCard.predict_prob() predict prob for each sample：# X=None # ScoreCard.export() output the details of scorecard by dict format# ScoreCard.get_params() to get the parameters of scorecard by dict format, usually used in deployment# card.intercept_  intercept of logical regression# card.coef_  coefficient of logical regressionfinal_result = final_data[[target]].copy()score = card.predict(final_data[col])prob = card.predict_prob(final_data[col])final_result['score'] = scorefinal_result['prob'] = probprint(&quot;card.intercept_:%s&quot; % (card.intercept_))print(&quot;card.coef_:%s&quot; % (card.coef_))card.get_params()['combiner']card.get_params()['transfer']card.export()``````pythoncard.intercept_:-2.5207582925622476card.coef_:[0.32080944 0.3452988  0.68294643 0.66842902]{'age': {'[-inf,23.0)': -39.69,'[23.0,24.0)': -30.31,'[24.0,25.0)': -10.81,'[25.0,26.0)': 1.59,'[26.0,28.0)': 9.51,'[28.0,29.0)': 13.49,'[29.0,37.0)': 30.74,'[37.0,inf)': 53.52},'intercept': {'[-inf,inf)': 509.19},'max_unpay_day': {'[-inf,171.0)': 2.64, '[171.0,inf)': -20.45},'score': {'[-inf,237.0)': -37.23,'[237.0,272.0)': -22.01,'[272.0,288.0)': -7.02,'[288.0,296.0)': 3.28,'[296.0,330.0)': 13.83,'[330.0,354.0)': 49.22,'[354.0,384.0)': 66.92,'[384.0,inf)': 121.77},'wealth': {'[-inf,3.0)': -18.75,'[3.0,4.0)': -1.45,'[4.0,5.0)': 4.07,'[5.0,7.0)': 4.92,'[7.0,inf)': 11.37}}``````python# model performance metrics, including KS, AUC, ROC curve, KS curve, PR curve# Parameter details# feature=None predicted value# target=None actual labellapras.perform(prob,final_result[target])``````pythonKS: 0.4160AUC: 0.7602```![png](http://img.badtom.cn/output_19_1.png)![png](http://img.badtom.cn/output_19_2.png)![png](http://img.badtom.cn/output_19_3.png)```python# Parameter details# frame=None original dataframe# score='score' score label name# target='target' Y label name# score_bond=None score boundary, default by 30, customized by list, e.g. [100,200,300]lapras.score_plot(final_result,score='score', target=target)``````pythonbad: [42, 78, 70, 104, 61, 28, 18, 1, 1, 0]good: [129, 249, 494, 795, 1075, 972, 825, 282, 164, 114]all: [171, 327, 564, 899, 1136, 1000, 843, 283, 165, 114]all_rate: ['3.11%', '5.94%', '10.25%', '16.34%', '20.65%', '18.18%', '15.32%', '5.14%', '3.00%', '2.07%']bad_rate: ['24.56%', '23.85%', '12.41%', '11.57%', '5.37%', '2.80%', '2.14%', '0.35%', '0.61%', '0.00%']```![png](http://img.badtom.cn/output_20_1.png)```python# LIFT show# feature=None predicted value# target=None actual label# recall_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] defaultlapras.LIFT(prob,final_data[target])```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;recall&lt;/th&gt;      &lt;th&gt;precision&lt;/th&gt;      &lt;th&gt;improve&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0.1&lt;/td&gt;      &lt;td&gt;0.240000&lt;/td&gt;      &lt;td&gt;3.202779&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;1&lt;/td&gt;      &lt;td&gt;0.2&lt;/td&gt;      &lt;td&gt;0.261290&lt;/td&gt;      &lt;td&gt;3.486897&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;2&lt;/td&gt;      &lt;td&gt;0.3&lt;/td&gt;      &lt;td&gt;0.240964&lt;/td&gt;      &lt;td&gt;3.215642&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;3&lt;/td&gt;      &lt;td&gt;0.4&lt;/td&gt;      &lt;td&gt;0.189535&lt;/td&gt;      &lt;td&gt;2.529327&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;4&lt;/td&gt;      &lt;td&gt;0.5&lt;/td&gt;      &lt;td&gt;0.179170&lt;/td&gt;      &lt;td&gt;2.391013&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;5&lt;/td&gt;      &lt;td&gt;0.6&lt;/td&gt;      &lt;td&gt;0.174352&lt;/td&gt;      &lt;td&gt;2.326707&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;6&lt;/td&gt;      &lt;td&gt;0.7&lt;/td&gt;      &lt;td&gt;0.161622&lt;/td&gt;      &lt;td&gt;2.156831&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;7&lt;/td&gt;      &lt;td&gt;0.8&lt;/td&gt;      &lt;td&gt;0.126972&lt;/td&gt;      &lt;td&gt;1.694425&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;8&lt;/td&gt;      &lt;td&gt;0.9&lt;/td&gt;      &lt;td&gt;0.113936&lt;/td&gt;      &lt;td&gt;1.520466&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td&gt;9&lt;/td&gt;      &lt;td&gt;1.0&lt;/td&gt;      &lt;td&gt;0.074935&lt;/td&gt;      &lt;td&gt;1.000000&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;# Automatical modeling```python# auto_model parameters  df,target,to_drop are necessary, others are optional# bins_show=False showing the binning graphs when set to true# iv_rank=False feature IV values will be ranked when set to true# perform_show=False showing performance(training set)# coef_negative=True coefficient can be negative if set to true# return: ScoreCard objectauto_card = lapras.auto_model(df=train_df,target=target,to_drop=to_drop,bins_show=False,iv_rank=False,perform_show=False,                              coef_negative = False, empty = 0.95, iv = 0.02, corr = 0.9, vif = False, method = 'mono',                              n_bins=8, min_samples=0.05, pdo=40, rate=2, base_odds=1 / 60, base_score=600)``````python——data filtering——original feature：6  filtered features：6——feature binning————WOE value transformation————feature filtering once more——original feature：6  filtered features：6——scorecard modeling——intercept: -2.520670026708529coef: [0.66928671 0.59743968 0.31723278 0.22972838 0.28750881 0.26435224]——model performance metrics——KS: 0.4208AUC: 0.7626   recall  precision   improve0     0.1   0.238095  3.1885861     0.2   0.254777  3.4119902     0.3   0.239521  3.2076793     0.4   0.193742  2.5946114     0.5   0.182805  2.4481415     0.6   0.171510  2.2968666     0.7   0.160501  2.1494377     0.8   0.130259  1.7444358     0.9   0.110603  1.4812069     1.0   0.074671  1.000000Automatic modeling finished, time costing： 0 second```[pypi-image]: https://img.shields.io/badge/pypi-V0.0.22-%3Cgreen%3E[pypi-url]: https://github.com/yhangang/lapras[python-image]: https://img.shields.io/pypi/pyversions/toad.svg?style=flat-square[docs-url]: https://github.com/yhangang/lapras</longdescription>
</pkgmetadata>