<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Build Status](https://travis-ci.org/bmwcarit/zubbi.svg?branch=master)](https://travis-ci.org/bmwcarit/zubbi)![Welcome to the Zuul Building Blocks Index](https://github.com/bmwcarit/zubbi/raw/master/.github/zubbi-index.png)&lt;br/&gt;The Zuul Building Blocks Index (aka Zubbi) makes it easy to search for availablejobs and roles (&quot;Building Blocks&quot;) within a [Zuul](https://zuul-ci.org/docs/zuul/)based CI system - even if they are spread over multiple tenants or repositories.---*Contents:***[Architecture](#architecture)** |**[Quickstart](#quickstart)** |**[Development](#development)** |**[Scraper usage](#scraper-usage)** |**[Configuration Examples](#configuration-examples)** |**[Available Connections](#available-connections)** |---## Architecture![zubbi-architecture](https://github.com/bmwcarit/zubbi/raw/master/.github/zubbi-architecture.png)Zubbi consists of two parts, **zubbi web** and **zubbi scraper**. It uses**Elasticsearch** as storage backend and needs **Git repositories** assource for job and role definitions.### Zubbi webA web frontend based on Flask that reads the data from Elasticsearch. It allowssearching for roles and jobs used within the CI system and shows the resultsincluding their documentation, last updates, changelog and some additional metadata.### Zubbi scraperA Python application that scrapes Git repositories, searches for job androle definitions in specific files and stores them in Elasticsearch.## QuickstartPrerequisites: [Docker Compose](https://docs.docker.com/compose/)Zubbi can simply be started by using the provided `docker-compose.yaml` file.---**NOTE**The provided `Dockerfile` should only be used for demonstration purposes and notin a production system. Flask is running in development mode and listens on allpublic IPs to make it reachable from outside the docker container.---To get the whole stack up and running, do the following:```shell$ cd docker$ docker-compose build$ docker-compose up```This will build the docker container with the newest Zubbi version, start allnecessary services (Elasticsearch, zubbi-scraper, zubbi-web) and does a fullscrape of the `openstack-infra/zuul-jobs` repository to get an initial set ofdata.When everything is up, you can visit `http://localhost:5000` and explore the jobsand roles from the `openstack-infra/zuul-jobs` repo.## DevelopmentPrerequisites: Python 3.6, [Tox](https://tox.readthedocs.io/en/latest/) and[Pipenv](https://docs.pipenv.org/) installed.To install necessary dependencies for development, run:```shell$ pipenv shell$ pipenv install --dev```We are using [black](https://black.readthedocs.io/en/stable/) to ensurewell-formatted Python code. To automatically ensure this on each commit, you canuse the included pre-commit hook. To install the hook, simply run:```shell$ pre-commit install```Before submitting pull requests, run tests and static code checks using tox:```shell$ tox```### Installing &amp; updating dependenciesNew dependencies should be added to the `requires` list in the `setup.py` file:```pythonrequires = [    &quot;arrow&quot;,    &quot;click&quot;,    ...,    &quot;&lt;new dependency&gt;&quot;,]```Afterwards, run the following command to update the `Pipfile.lock` and install thenew dependencies in your local pipenv environment:```shell$ pipenv update```Test dependencies should be installed as development dependencies:```shell$ pipenv install --dev my-test-dependency```To update the dependencies to the latest version or after a new dependency wasinstalled you have to run `tox -e update-requirements` and commit the changedPipenv and requirements files.### Configuring and starting ZubbiIf you followed the [Development](#development) guide so far, you should alreadyhave a virtual environment with all required packages to run Zubbi. What's left,are a few configuration files and a local Elasticsearch instance for testing.#### ElasticsearchZubbi is currently depending on Elasticsearch as data backend. If you have[Docker Compose](https://docs.docker.com/compose/) installed, you can usethe provided `docker-compose.yaml` file to start Elasticsearch locally.```shell$ cd docker$ docker-compose up elasticsearch```If not, we recommend to use the latest available Elasticsearch Docker image, toget a local instance up and running for development.#### ConfigurationBoth - Zubbi scraper and Zubbi web - read their configuration from the file pathgiven via the `ZUBBI_SETTINGS` environment variable:```shell$ export ZUBBI_SETTINGS=$(pwd)/settings.cfg```In order to show jobs and roles in Zubbi, we need to provide a minimal [tenant configuration](https://zuul-ci.org/docs/zuul/admin/tenants.html)containing at least a single repository (which is used as source).Therefore, put the following in a `tenant-config.yaml` file:```yaml- tenant:    name: openstack    source:      openstack-gerrit:        untrusted-projects:          - openstack-infra/zuul-jobs```Put the following in your `settings.cfg` to allow scraping based on the tenantconfiguration above and store the results in the local Elasticsearch instance.Please note, that the key in the `CONNECTIONS` dictionary must go in hand withthe `source` names in the tenant configuration.```iniELASTICSEARCH = {    'host': 'localhost',    'port': 9200,}TENANT_SOURCES_FILE = 'tenant-config.yaml'CONNECTIONS = {    'openstack-gerrit': {        'provider': 'git',        'git_host_url': 'https://git.openstack.org',    },}```#### Running ZubbiNow we can scrape the `openstack-infra/zuul-jobs` repository to get a first setof jobs and roles into Elasticsearch and show them in Zubbi:```shell$ zubbi-scraper scrape --full```When the scraper run was successful, we can start Zubbi web to take a look atour data:```shell$ export FLASK_APP=zubbi$ export FLASK_DEBUG=true$ flask run```### Building the syntax highlighting stylesheet with pygmentsWe are using a pre-build pygments stylesheet to highlight the code examples injob and roles documentations. In case you want to rebuild this syntax highlightingstylesheet (e.g. to try out another highlighting style) you can run the followingcommand:```shell$ pygmentize -S default -f html -a .highlight &gt; zubbi/static/pygments.css```## Scraper usageThe Zubbi scraper supports two different modes: `periodic` (default) and `immediate`.To start the scraper in periodic mode, simply run:```shell$ zubbi-scraper scrape```This should also scrape all repositories specified in the tenant configurationfor the first time.To immediately scrape one or more repositories, you can use the following command:```shell# Scrape one or more repositories$ zubbi-scraper scrape --repo 'orga1/repo1' --repo 'orga1/repo2'# Scrape all repositories$ zubbi-scraper scrape --full```Additionally, the scraper provides a `list-repos` command to list allavailable repositories together with some additional information like thelast scraping timestamp and the git provider (connection type):```shell$ zubbi-scraper list-repos```## Configuration examplesExamples for all available settings can be found in `settings.cfg.example`.### Tenant ConfigurationZubbi needs to know which projects contain the job and role definitions thatare used inside the CI system. To achieve this, it uses Zuul's[tenant configuration](https://zuul-ci.org/docs/zuul/admin/tenants.html).Usually, this tenant configuration is stored in a file that must be specifiedin the `settings.cfg`, but it could also come from a repository.```ini# Use only one of the following, not bothTENANT_SOURCES_FILE = '&lt;path_to_the_yaml_file&gt;'TENANT_SOURCES_REPO = '&lt;orga&gt;/&lt;repo&gt;'```### Elasticsearch ConnectionThe Elasticsearch connection can be configured in the `settings.cfg` likethe following:```iniELASTICSEARCH = {    'host': '&lt;elasticsearch_host&gt;',    'port': 9200,  # default    'user': '&lt;user&gt;',    'password': '&lt;password&gt;',    # Optional, to avoid name clashes with existing ES indices from other applications    # E.g. 'zubbi' will result in indices like 'zubbi-zuul-jobs', 'zubbi-ansible-roles', ...    index_prefix: '&lt;prefix&gt;',    # Optional, to enable SSL for the Elasticsearch connection.    # You must at least set 'enabled' to True and provide other parameters if the default    # values are not sufficient.    'tls': {        'enabled': False,  # default        'check_hostname': True,  # default        'verify_mode': 'CERT_REQUIRED',  # default    },}```## Available ConnectionsCurrently, Zubbi supports the following connection types: **GitHub**, **Gerrit**and **Git**. The latter one can be used for repositories that are not hosted oneither GitHub or Gerrit.### GitHubThe GitHub connection uses GitHub's REST API to scrape the repositories. To beable to use this connection, you need to create a GitHub App with the followingpermissions:```yamlRepository contents: Read-onlyRepository metadata: Read-only```If you are unsure about how to set up a GitHub App, take a look at the[official guide](https://developer.github.com/apps/building-github-apps/creating-a-github-app/).Once you have successfully created your GitHub App, you can define the connectionwith the following parameters in your `settings.cfg` accordingly:```iniCONNECTIONS = {    '&lt;name&gt;': {        'provider': 'github',        'url': '&lt;github_url&gt;',        'app_id': &lt;your_github_app_id&gt;,        'app_key': '&lt;path_to_keyfile&gt;',    },    ...}```#### Using GitHub WebhooksGitHub webhooks can be used to keep your Zubbi data up to date.To activate GitHub webhooks, you have to provide a weebhook URL pointing tothe `/api/webhook` endpoint of your Zubbi web installation. The generated webhooksecret must be specified in the `GITHUB_WEBHOOK_SECRET` setting in your `settings.cfg`:**NOTE:** As of now, GitHub webhooks are not supported on a per-connection base.You can only have one webhook active in zubbi.```iniGITHUB_WEBHOOK_SECRET = '&lt;secret&gt;'```Zubbi web receives webhook events from GitHub, validates the secret and publishesrelevant events to the scraper via [ZMQ](https://pyzmq.readthedocs.io/en/latest/).The Zubbi scraper on the other hand subscribes to the ZMQ socket and scrapesnecessary repositories whenever a event is received. In order to make thiscommunication work, you need to specify the following parameters in your `settings.cfg`:```ini# Zubbi web (publish)ZMQ_PUB_SOCKET_ADDRESS = 'tcp://*:5556'# Zubbi scraper (subscribe)ZMQ_SUB_SOCKET_ADDRESS = 'tcp://localhost:5556'```### GerritIn contrary to GitHub, the Gerrit connection is based on[GitPython](https://gitpython.readthedocs.io/en/stable/) as the Gerrit REST APIdoes not support all use cases. To use this connection, you haveto provide the following parameters in your `settings.cfg`:```iniCONNECTIONS = {    '&lt;name&gt;': {        'provider': 'gerrit',        'url': '&lt;git_remote_url&gt;',        # Only necessary if different from the git_remote_url        'web_url': '&lt;gerrit_url&gt;',        # The web_type is necessary to build the correct URLs for Gerrit.        # Currently supported types are 'cgit' (default) and 'gitweb'.        'web_type': 'cgit|gitweb',        # Optional, if authentication is required        'user': '&lt;username&gt;',        'password': '&lt;password&gt;',    },    ...}```### GitThe Git connection is also based on[GitPython](https://gitpython.readthedocs.io/en/stable/) and can be used for Gitrepositories that are not hosted on either GitHub or Gerrit. To use this connection,put the following in your `settings.cfg`:```iniCONNECTIONS = {    '&lt;name&gt;': {        'provider': 'git',        'url': '&lt;git_host_url&gt;',        # Optional, if authentication is required        'user': '&lt;username&gt;',        'password': '&lt;password',    },    ...}```*Happy coding!*</longdescription>
</pkgmetadata>