<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;h1 align=&quot;center&quot;&gt;  &lt;a target=&quot;_blank&quot; href=&quot;https://dstack.ai&quot;&gt;    &lt;img alt=&quot;dstack&quot; src=&quot;https://raw.githubusercontent.com/dstackai/dstack/master/docs/assets/images/dstack-logo.svg&quot; width=&quot;350px&quot;/&gt;  &lt;/a&gt;&lt;/h1&gt;&lt;h3 align=&quot;center&quot;&gt;Orchestrate GPU workloads across clouds&lt;/h3&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://dstack.ai/docs&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Docs&lt;/b&gt;&lt;/a&gt; •&lt;a href=&quot;https://dstack.ai/examples&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Examples&lt;/b&gt;&lt;/a&gt; •&lt;a href=&quot;https://dstack.ai/blog&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; •&lt;a href=&quot;https://discord.gg/u8SmfwPpMd&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;[![Last commit](https://img.shields.io/github/last-commit/dstackai/dstack?style=flat-square)](https://github.com/dstackai/dstack/commits/)[![PyPI - License](https://img.shields.io/pypi/l/dstack?style=flat-square&amp;color=blue)](https://github.com/dstackai/dstack/blob/master/LICENSE.md)&lt;/div&gt;`dstack` is an open-source framework for orchestrating GPU workloadsacross multiple cloud GPU providers. It provides a simple cloud-agnostic interface for development and deployment of generative AI models.## Latest news ✨- [2023/09] [Deploying LLMs using Python API](https://dstack.ai/examples/python-api) (Example)- [2023/09] [Managed gateways](https://dstack.ai/blog/2023/09/01/managed-gateways) (Release)- [2023/08] [Fine-tuning Llama 2 using QLoRA](https://dstack.ai/examples/finetuning-llama-2) (Example)- [2023/08] [Deploying Stable Diffusion using FastAPI](https://dstack.ai/examples/stable-diffusion-xl) (Example)- [2023/07] [Deploying LLMS using TGI](https://dstack.ai/examples/text-generation-inference) (Example)- [2023/07] [Deploying LLMS using vLLM](https://dstack.ai/examples/vllm) (Example)## InstallationTo use `dstack`, install it with `pip`, and start the server.```shellpip install &quot;dstack[all]&quot; -Udstack start```## Configure cloudsUpon startup, the server sets up the default project called `main`.Prior to using `dstack`, make sure to [configure clouds](https://dstack.ai/docs/guides/clouds#configure-backends).Once the server is up, you can orchestrate GPU workloads usingeither the CLI or Python API.## Using CLI### Define a configurationThe CLI allows you to define what you want to run as a YAML file andrun it via the `dstack run` CLI command.Configurations can be of three types: `dev-environment`, `task`, and `service`.#### Dev environmentsA dev environment is a virtual machine with a pre-configured IDE.```yamltype: dev-environmentpython: &quot;3.11&quot; # (Optional) If not specified, your local version is usedsetup: # (Optional) Executed once at the first startup  - pip install -r requirements.txtide: vscode```#### TasksA task can be either a batch job, such as training or fine-tuning a model, or a web application.```yamltype: taskpython: &quot;3.11&quot; # (Optional) If not specified, your local version is usedports:  - 7860commands:  - pip install -r requirements.txt  - python app.py```While the task is running in the cloud, the CLI forwards its ports traffic to `localhost`for convenient access.#### ServicesA service is an application that is accessible through a public endpoint.```yamltype: serviceport: 7860commands:  - pip install -r requirements.txt  - python app.py```Once the service is up, `dstack` makes it accessible from the Internet throughthe [gateway](https://dstack.ai/docs/guides/clouds#configure-gateways).### Run a configurationTo run a configuration, use the [`dstack run`](https://dstack.ai/docs/reference/cli/run.md) command followed by working directory and the path to the configuration file.```shelldstack run . -f text-generation-inference/serve.dstack.yml --gpu 80GB -y RUN           BACKEND  INSTANCE              SPOT  PRICE STATUS    SUBMITTED tasty-zebra-1 lambda   200GB, 1xA100 (80GB)  no    $1.1  Submitted now Privisioning...Serving on https://tasty-zebra-1.mydomain.com```## Using APIAs an alternative to the CLI, you can run tasks and services programmatically via [Python API](https://dstack.ai/docs/reference/api/python/).```pythonimport sysimport dstacktask = dstack.Task(    image=&quot;ghcr.io/huggingface/text-generation-inference:latest&quot;,    env={&quot;MODEL_ID&quot;: &quot;TheBloke/Llama-2-13B-chat-GPTQ&quot;},    commands=[        &quot;text-generation-launcher --trust-remote-code --quantize gptq&quot;,    ],    ports=[&quot;8080:80&quot;],)resources = dstack.Resources(gpu=dstack.GPU(memory=&quot;20GB&quot;))if __name__ == &quot;__main__&quot;:    print(&quot;Initializing the client...&quot;)    client = dstack.Client.from_config(repo_dir=&quot;~/dstack-examples&quot;)    print(&quot;Submitting the run...&quot;)    run = client.runs.submit(configuration=task, resources=resources)    print(f&quot;Run {run.name}: &quot; + run.status())    print(&quot;Attaching to the run...&quot;)    run.attach()    # After the endpoint is up, http://127.0.0.1:8080/health will return 200 (OK).    try:        for log in run.logs():            sys.stdout.buffer.write(log)            sys.stdout.buffer.flush()    except KeyboardInterrupt:        print(&quot;Aborting the run...&quot;)        run.stop(abort=True)    finally:        run.detach()```## More informationFor additional information and examples, see the following links:- [Docs](https://dstack.ai/docs)- [Examples](https://dstack.ai/examples)- [Blog](https://dstack.ai/blog)- [Discord](https://discord.gg/u8SmfwPpMd)## Licence[Mozilla Public License 2.0](LICENSE.md)</longdescription>
</pkgmetadata>