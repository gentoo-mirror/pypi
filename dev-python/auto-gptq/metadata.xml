<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;h1 align=&quot;center&quot;&gt;AutoGPTQ&lt;/h1&gt;&lt;p align=&quot;center&quot;&gt;An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://github.com/PanQiWei/AutoGPTQ/releases&quot;&gt;        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/PanQiWei/AutoGPTQ.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://pypi.org/project/auto-gptq/&quot;&gt;        &lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/dd/auto-gptq&quot;&gt;    &lt;/a&gt;&lt;/p&gt;&lt;h4 align=&quot;center&quot;&gt;    &lt;p&gt;        &lt;b&gt;English&lt;/b&gt; |        &lt;a href=&quot;https://github.com/PanQiWei/AutoGPTQ/blob/main/README_zh.md&quot;&gt;ä¸­æ–‡&lt;/a&gt;    &lt;p&gt;&lt;/h4&gt;## News or Update**To experience adapter training using `auto_gptq` quantized model in advance, you can try [this branch](https://github.com/PanQiWei/AutoGPTQ/tree/peft_integration) and discuss [in here](https://github.com/PanQiWei/AutoGPTQ/issues/103), examples are [in here](https://github.com/PanQiWei/AutoGPTQ/tree/peft_integration/examples/peft).**- 2023-05-25 - (In Progress) - Integrate with ðŸ¤— peft to use gptq quantized model to train adapters, support LoRA, AdaLoRA, AdaptionPrompt, etc.- 2023-05-30 - (Update) - Support download/upload quantized model from/to ðŸ¤— Hub.- 2023-05-27 - (Update) - Support quantization and inference for `gpt_bigcode`, `codegen` and `RefineWeb/RefineWebModel`(falcon) model types.- 2023-05-04 - (Update) - Support using faster cuda kernel when `not desc_act or group_size == -1`.*For more histories please turn to [here](docs/NEWS_OR_UPDATE.md)*## Performance Comparison### Inference Speed&gt; The result is generated using [this script](examples/benchmark/generation_speed.py), batch size of input is 1, decode strategy is beam search and enforce the model to generate 512 tokens, speed metric is tokens/s (the larger, the better).&gt; &gt; The quantized model is loaded using the setup that can gain the fastest inference speed.| model         | GPU           | num_beams | fp16  | gptq-int4 ||---------------|---------------|-----------|-------|-----------|| llama-7b      | 1xA100-40G    | 1         | 18.87 | 25.53     || llama-7b      | 1xA100-40G    | 4         | 68.79 | 91.30     || moss-moon 16b | 1xA100-40G    | 1         | 12.48 | 15.25     || moss-moon 16b | 1xA100-40G    | 4         | OOM   | 42.67     || moss-moon 16b | 2xA100-40G    | 1         | 06.83 | 06.78     || moss-moon 16b | 2xA100-40G    | 4         | 13.10 | 10.80     || gpt-j 6b      | 1xRTX3060-12G | 1         | OOM   | 29.55     || gpt-j 6b      | 1xRTX3060-12G | 4         | OOM   | 47.36     |### PerplexityFor perplexity comparison, you can turn to [here](https://github.com/qwopqwop200/GPTQ-for-LLaMa#result) and [here](https://github.com/qwopqwop200/GPTQ-for-LLaMa#gptq-vs-bitsandbytes)## Installation### Quick InstallationYou can install the latest stable release of AutoGPTQ from pip:```shellpip install auto-gptq```Start from v0.2.0, you can download pre-build wheel that satisfied your environment setup from each version's release assets and install it to skip building stage for the fastest installation speed. For example:```shell# firstly, cd the directory where the wheel saved, then execute command belowpip install auto_gptq-0.2.0+cu118-cp310-cp310-linux_x86_64.whl # install v0.2.0 auto_gptq pre-build wheel for linux in an environment whose python=3.10 and cuda=11.8```#### disable cuda extensionsBy default, cuda extensions will be installed when `torch` and `cuda` is already installed in your machine, if you don't want to use them, using:```shellBUILD_CUDA_EXT=0 pip install auto-gptq```And to make sure `autogptq_cuda` is not ever in your virtual environment, run:```shellpip uninstall autogptq_cuda -y```#### to support LLaMa modelFor some people want to try LLaMa and whose `transformers` version not meet the newest one that supports it, using:```shellpip install auto-gptq[llama]```#### to support triton speedupTo integrate with `triton`, using:&gt; warning: currently triton only supports linux; 3-bit quantization is not supported when using triton```shellpip install auto-gptq[triton]```### Install from source&lt;details&gt;&lt;summary&gt;click to see details&lt;/summary&gt;Clone the source code:```shellgit clone https://github.com/PanQiWei/AutoGPTQ.git &amp;&amp; cd AutoGPTQ```Then, install from source:```shellpip install .```Like quick installation, you can also set `BUILD_CUDA_EXT=0` to disable pytorch extension building.Use `.[llama]` if you want to try LLaMa model.Use `.[triton]` if you want to integrate with triton and it's available on your operating system.&lt;/details&gt;## Quick Tour### Quantization and Inference&gt; warning: this is just a showcase of the usage of basic apis in AutoGPTQ, which uses only one sample to quantize a much small model, quality of quantized model using such little samples may not good.Below is an example for the simplest use of `auto_gptq` to quantize a model and inference after quantization: ```pythonfrom transformers import AutoTokenizer, TextGenerationPipelinefrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfigimport logginglogging.basicConfig(    format=&quot;%(asctime)s %(levelname)s [%(name)s] %(message)s&quot;, level=logging.INFO, datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;)pretrained_model_dir = &quot;facebook/opt-125m&quot;quantized_model_dir = &quot;opt-125m-4bit&quot;tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)examples = [    tokenizer(        &quot;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&quot;    )]quantize_config = BaseQuantizeConfig(    bits=4,  # quantize model to 4-bit    group_size=128,  # it is recommended to set the value to 128    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad )# load un-quantized model, by default, the model will always be loaded into CPU memorymodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)# quantize model, the examples should be list of dict whose keys can only be &quot;input_ids&quot; and &quot;attention_mask&quot;model.quantize(examples)# save quantized modelmodel.save_quantized(quantized_model_dir)# save quantized model using safetensorsmodel.save_quantized(quantized_model_dir, use_safetensors=True)# push quantized model to Hugging Face Hub. # to use use_auth_token=True, Login first via huggingface-cli login.# or pass explcit token with: use_auth_token=&quot;hf_xxxxxxx&quot;# (uncomment the following three lines to enable this feature)# repo_id = f&quot;YourUserName/{quantized_model_dir}&quot;# commit_message = f&quot;AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}&quot;# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)# alternatively you can save and push at the same time# (uncomment the following three lines to enable this feature)# repo_id = f&quot;YourUserName/{quantized_model_dir}&quot;# commit_message = f&quot;AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}&quot;# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)# load quantized model to the first GPUmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=&quot;cuda:0&quot;)# download quantized model from Hugging Face Hub and load to the first GPU# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=&quot;cuda:0&quot;, use_safetensors=True, use_triton=False)# inference with model.generateprint(tokenizer.decode(model.generate(**tokenizer(&quot;auto_gptq is&quot;, return_tensors=&quot;pt&quot;).to(model.device))[0]))# or you can also use pipelinepipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)print(pipeline(&quot;auto-gptq is&quot;)[0][&quot;generated_text&quot;])```For more advanced features of model quantization, please reference to [this script](examples/quantization/quant_with_alpaca.py)### Customize Model&lt;details&gt;&lt;summary&gt;Below is an example to extend `auto_gptq` to support `OPT` model, as you will see, it's very easy:&lt;/summary&gt;```pythonfrom auto_gptq.modeling import BaseGPTQForCausalLMclass OPTGPTQForCausalLM(BaseGPTQForCausalLM):    # chained attribute name of transformer layer block    layers_block_name = &quot;model.decoder.layers&quot;    # chained attribute names of other nn modules that in the same level as the transformer layer block    outside_layer_modules = [        &quot;model.decoder.embed_tokens&quot;, &quot;model.decoder.embed_positions&quot;, &quot;model.decoder.project_out&quot;,        &quot;model.decoder.project_in&quot;, &quot;model.decoder.final_layer_norm&quot;    ]    # chained attribute names of linear layers in transformer layer module    # normally, there are four sub lists, for each one the modules in it can be seen as one operation,     # and the order should be the order when they are truly executed, in this case (and usually in most cases),     # they are: attention q_k_v projection, attention output projection, MLP project input, MLP project output    inside_layer_modules = [        [&quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;, &quot;self_attn.q_proj&quot;],        [&quot;self_attn.out_proj&quot;],        [&quot;fc1&quot;],        [&quot;fc2&quot;]    ]```After this, you can use `OPTGPTQForCausalLM.from_pretrained` and other methods as shown in Basic.&lt;/details&gt;### Evaluation on Downstream TasksYou can use tasks defined in `auto_gptq.eval_tasks` to evaluate model's performance on specific down-stream task before and after quantization.The predefined tasks support all causal-language-models implemented in [ðŸ¤— transformers](https://github.com/huggingface/transformers) and in this project.&lt;details&gt;&lt;summary&gt;Below is an example to evaluate `EleutherAI/gpt-j-6b` on sequence-classification task using `cardiffnlp/tweet_sentiment_multilingual` dataset:&lt;/summary&gt;```pythonfrom functools import partialimport datasetsfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfigfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfigfrom auto_gptq.eval_tasks import SequenceClassificationTaskMODEL = &quot;EleutherAI/gpt-j-6b&quot;DATASET = &quot;cardiffnlp/tweet_sentiment_multilingual&quot;TEMPLATE = &quot;Question:What's the sentiment of the given text? Choices are {labels}.\nText: {text}\nAnswer:&quot;ID2LABEL = {    0: &quot;negative&quot;,    1: &quot;neutral&quot;,    2: &quot;positive&quot;}LABELS = list(ID2LABEL.values())def ds_refactor_fn(samples):    text_data = samples[&quot;text&quot;]    label_data = samples[&quot;label&quot;]    new_samples = {&quot;prompt&quot;: [], &quot;label&quot;: []}    for text, label in zip(text_data, label_data):        prompt = TEMPLATE.format(labels=LABELS, text=text)        new_samples[&quot;prompt&quot;].append(prompt)        new_samples[&quot;label&quot;].append(ID2LABEL[label])    return new_samples#  model = AutoModelForCausalLM.from_pretrained(MODEL).eval().half().to(&quot;cuda:0&quot;)model = AutoGPTQForCausalLM.from_pretrained(MODEL, BaseQuantizeConfig())tokenizer = AutoTokenizer.from_pretrained(MODEL)task = SequenceClassificationTask(        model=model,        tokenizer=tokenizer,        classes=LABELS,        data_name_or_path=DATASET,        prompt_col_name=&quot;prompt&quot;,        label_col_name=&quot;label&quot;,        **{            &quot;num_samples&quot;: 1000,  # how many samples will be sampled to evaluation            &quot;sample_max_len&quot;: 1024,  # max tokens for each sample            &quot;block_max_len&quot;: 2048,  # max tokens for each data block            # function to load dataset, one must only accept data_name_or_path as input             # and return datasets.Dataset            &quot;load_fn&quot;: partial(datasets.load_dataset, name=&quot;english&quot;),              # function to preprocess dataset, which is used for datasets.Dataset.map,             # must return Dict[str, list] with only two keys: [prompt_col_name, label_col_name]            &quot;preprocess_fn&quot;: ds_refactor_fn,              # truncate label when sample's length exceed sample_max_len            &quot;truncate_prompt&quot;: False          }    )# note that max_new_tokens will be automatically specified internally based on given classesprint(task.run())# self-consistencyprint(    task.run(        generation_config=GenerationConfig(            num_beams=3,            num_return_sequences=3,            do_sample=True        )    ))```&lt;/details&gt;## Learn More[tutorials](docs/tutorial) provide step-by-step guidance to integrate `auto_gptq` with your own project and some best practice principles.[examples](examples/README.md) provide plenty of example scripts to use `auto_gptq` in different ways.## Supported Models&gt; you can use `model.config.model_type` to compare with the table below to check whether the model you use is supported by `auto_gptq`.&gt; &gt; for example, model_type of `WizardLM`, `vicuna` and `gpt4all` are all `llama`, hence they are all supported by `auto_gptq`.| model type                         | quantization | inference | peft-lora | peft-adaption_prompt ||------------------------------------|--------------|-----------|-----------|----------------------|| bloom                              | âœ…            | âœ…         |           |                      || gpt2                               | âœ…            | âœ…         |           |                      || gpt_neox                           | âœ…            | âœ…         |           |                      || gptj                               | âœ…            | âœ…         |           |                      || llama                              | âœ…            | âœ…         |           | âœ…                    || moss                               | âœ…            | âœ…         |           |                      || opt                                | âœ…            | âœ…         |           |                      || gpt_bigcode                        | âœ…            | âœ…         |           |                      || codegen                            | âœ…            | âœ…         |           |                      || falcon(RefinedWebModel/RefinedWeb) | âœ…            | âœ…         |           |                      |## Supported Evaluation TasksCurrently, `auto_gptq` supports: `LanguageModelingTask`, `SequenceClassificationTask` and `TextSummarizationTask`; more Tasks will come soon!## Acknowledgement- Specially thanks **Elias Frantar**, **Saleh Ashkboos**, **Torsten Hoefler** and **Dan Alistarh** for proposing **GPTQ** algorithm and open source the [code](https://github.com/IST-DASLab/gptq).- Specially thanks **qwopqwop200**, for code in this project that relevant to quantization are mainly referenced from [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda).[![Star History Chart](https://api.star-history.com/svg?repos=PanQiwei/AutoGPTQ&amp;type=Date)](https://star-history.com/#PanQiWei/AutoGPTQ&amp;Date)</longdescription>
</pkgmetadata>