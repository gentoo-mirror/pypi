<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>properscoring=============.. image:: https://travis-ci.org/TheClimateCorporation/properscoring.svg?branch=master    :target: https://travis-ci.org/TheClimateCorporation/properscoring`Proper scoring rules`_ for evaluating probabilistic forecasts in Python.Evaluation methods that are &quot;strictly proper&quot; cannot be artificially improvedthrough hedging, which makes them fair methods for accessing the accuracy ofprobabilistic forecasts. In particular, these rules are often used forevaluating weather forecasts... _Proper scoring rules: https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdfproperscoring runs on both Python 2 and 3. It requires NumPy (1.8 orlater) and SciPy (any recent version should be fine). Numba is optional,but highly encouraged: it enables significant speedups (e.g., 20x faster)for ``crps_ensemble`` and ``threshold_brier_score``.To install, use pip: ``pip install properscoring``.Example: five ways to calculate CRPS------------------------------------This library focuses on the closely related`Continuous Ranked Probability Score`_ (CRPS) and `Brier Score`_. We likethese scores because they are both interpretable (e.g., CRPS is ageneralization of mean absolute error) and easily calculated from a finitenumber of samples of a probability distribution... _Continuous Ranked Probability Score: http://www.eumetcal.org/resources/ukmeteocal/verification/www/english/msg/ver_prob_forec/uos3b/uos3b_ko1.htm.. _Brier score: https://en.wikipedia.org/wiki/Brier_scoreWe will illustrate how to calculate CRPS against a forecast given by aGaussian random variable. To begin, import properscoring::    import numpy as np    import properscoring as ps    from scipy.stats import normExact calculation using ``crps_gaussian`` (this is the fastest method)::    &gt;&gt;&gt;&gt; ps.crps_gaussian(0, mu=0, sig=1)    0.23369497725510913Numerical integration with ``crps_quadrature``::    &gt;&gt;&gt; ps.crps_quadrature(0, norm)    array(0.23369497725510724)From a finite sample with ``crps_ensemble``::    &gt;&gt;&gt; ensemble = np.random.RandomState(0).randn(1000)    &gt;&gt;&gt; ps.crps_ensemble(0, ensemble)    0.2297109370729622Weighted by PDF values with ``crps_ensemble``::    &gt;&gt;&gt; x = np.linspace(-5, 5, num=1000)    &gt;&gt;&gt; ps.crps_ensemble(0, x, weights=norm.pdf(x))    0.23370047937569616Based on the `threshold decomposition`_ of CRPS with``threshold_brier_score``::    &gt;&gt;&gt; threshold_scores = ps.threshold_brier_score(0, ensemble, threshold=x)    &gt;&gt;&gt; (x[1] - x[0]) * threshold_scores.sum(axis=-1)    0.22973090090090081.. _threshold decomposition: https://www.stat.washington.edu/research/reports/2008/tr533.pdfIn this example, we only scored a single observation/forecast pair. Butto reliably evaluate a forecast model, you need to average these scores acrossmany observations. Fortunately, all scoring rules in properscoring happilyaccept and return observations as multi-dimensional arrays::    &gt;&gt;&gt; ps.crps_gaussian([-2, -1, 0, 1, 2], mu=0, sig=1)    array([ 1.45279182,  0.60244136,  0.23369498,  0.60244136,  1.45279182])Once you calculate an average score, is often useful to normalize themrelative to a baseline forecast to calculate a so-called &quot;skill score&quot;,defined such that 0 indicates no improvement over the baseline and 1indicates a perfect forecast. For example, suppose that our baselineforecast is to always predict 0::    &gt;&gt;&gt; obs = [-2, -1, 0, 1, 2]    &gt;&gt;&gt; baseline_score = ps.crps_ensemble(obs, [0, 0, 0, 0, 0]).mean()    &gt;&gt;&gt; forecast_score = ps.crps_gaussian(obs, mu=0, sig=1).mean()    &gt;&gt;&gt; skill = (baseline_score - forecast_score) / baseline_score    &gt;&gt;&gt; skill    0.27597311068630859A standard normal distribution was 28% better at predicting these fiveobservations.API---properscoring contains optimized and extensively tested routines forscoring probability forecasts. These functions currently fall into twocategories:* Continuous Ranked Probability Score (CRPS):  - for an ensemble forecast: ``crps_ensemble``  - for a Gaussian distribution: ``crps_gaussian``  - for an arbitrary cumulative distribution function: ``crps_quadrature``* Brier score:  - for binary probability forecasts: ``brier_score``  - for threshold exceedances with an ensemble forecast: ``threshold_brier_score``All functions are robust to missing values represented by the floatingpoint value ``NaN``.History-------This library was written by researchers at The Climate Corporation. Theoriginal authors include Leon Barrett, Stephan Hoyer, Alex Kleeman andDrew O'Kane.License-------Copyright 2015 The Climate CorporationLicensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.Contributions-------------Outside contributions (bug fixes or new features related to proper scoringrules) would be very welcome! Please open a GitHub issue to discuss yourplans.</longdescription>
</pkgmetadata>