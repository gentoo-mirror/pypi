<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Hugging Face ü§ó x Stable-baselines3 v3.0A library to load and upload Stable-baselines3 models from the Hub with Gymnasium and Gymnasium compatible environments.‚ö†Ô∏è If you use Gym, you need to install `huggingface_sb3==2.3.1`## Installation### With pip```pip install huggingface-sb3```## ExamplesWe wrote a tutorial on how to use ü§ó Hub and Stable-Baselines3 [here](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)If you use **Colab or a Virtual/Screenless Machine**, you can check Case 3 and Case 4.### Case 1: I want to download a model from the Hub```pythonimport gymnasium as gymfrom huggingface_sb3 import load_from_hubfrom stable_baselines3 import PPOfrom stable_baselines3.common.evaluation import evaluate_policy# Retrieve the model from the hub## repo_id = id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})## filename = name of the model zip file from the repositorycheckpoint = load_from_hub(    repo_id=&quot;sb3/demo-hf-CartPole-v1&quot;,    filename=&quot;ppo-CartPole-v1.zip&quot;,)model = PPO.load(checkpoint)# Evaluate the agent and watch iteval_env = gym.make(&quot;CartPole-v1&quot;)mean_reward, std_reward = evaluate_policy(    model, eval_env, render=False, n_eval_episodes=5, deterministic=True, warn=False)print(f&quot;mean_reward={mean_reward:.2f} +/- {std_reward}&quot;)```### Case 2: I trained an agent and want to upload it to the HubWith `package_to_hub()` **we'll save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub**.It currently **works for Gym and Atari environments**. If you use another environment, you should use `push_to_hub()` instead.First you need to be logged in to Hugging Face:- If you're using Colab/Jupyter Notebooks:```pythonfrom huggingface_hub import notebook_loginnotebook_login()```- Else:```huggingface-cli login```Then**With `package_to_hub()`**:```pythonimport gymnasium as gymfrom stable_baselines3 import PPOfrom stable_baselines3.common.env_util import make_vec_envfrom huggingface_sb3 import package_to_hub# Create the environmentenv_id = &quot;LunarLander-v2&quot;env = make_vec_env(env_id, n_envs=1)# Create the evaluation enveval_env = make_vec_env(env_id, n_envs=1)# Instantiate the agentmodel = PPO(&quot;MlpPolicy&quot;, env, verbose=1)# Train the agentmodel.learn(total_timesteps=int(5000))# This method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hubpackage_to_hub(model=model,                model_name=&quot;ppo-LunarLander-v2&quot;,               model_architecture=&quot;PPO&quot;,               env_id=env_id,               eval_env=eval_env,               repo_id=&quot;ThomasSimonini/ppo-LunarLander-v2&quot;,               commit_message=&quot;Test commit&quot;)```**With `push_to_hub()`**:Push to hub only **push a file to the Hub**, if you want to save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub, use `package_to_hub()````pythonimport gymnasium as gymfrom stable_baselines3 import PPOfrom stable_baselines3.common.env_util import make_vec_envfrom huggingface_sb3 import push_to_hub# Create the environmentenv_id = &quot;LunarLander-v2&quot;env = make_vec_env(env_id, n_envs=1)# Instantiate the agentmodel = PPO(&quot;MlpPolicy&quot;, env, verbose=1)# Train it for 10000 timestepsmodel.learn(total_timesteps=10_000)# Save the modelmodel.save(&quot;ppo-LunarLander-v2&quot;)# Push this saved model .zip file to the hf repo# If this repo does not exists it will be created## repo_id = id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name})## filename: the name of the file == &quot;name&quot; inside model.save(&quot;ppo-LunarLander-v2&quot;)push_to_hub(    repo_id=&quot;ThomasSimonini/ppo-LunarLander-v2&quot;,    filename=&quot;ppo-LunarLander-v2.zip&quot;,    commit_message=&quot;Added LunarLander-v2 model trained with PPO&quot;,)```### Case 3: I use Google Colab with Classic Control/Box2D Gym Environments- You can use xvbf (virtual screen)```!apt-get install -y xvfb python-opengl &gt; /dev/null 2&gt;&amp;1```- Just put your code inside a python file and run```!xvfb-run -s &quot;-screen 0 1400x900x24&quot; &lt;your_python_file&gt;```### Case 4: I use a Virtual/Remote Machine- You can use xvbf (virtual screen)```xvfb-run -s &quot;-screen 0 1400x900x24&quot; &lt;your_python_file&gt;```### Case 5: I want to automate upload/download from the HubIf you want to upload or download models for many environments, you might want to automate this process. It makes sense to adhere to a fixed naming scheme for models and repositories.You will run into trouble when your environment names contain slashes.Therefore, we provide some helper classes:```pythonimport gymnasium as gymfrom huggingface_sb3.naming_schemes import EnvironmentName, ModelName, ModelRepoIdenv_name = EnvironmentName(&quot;seals/Walker2d-v0&quot;)model_name = ModelName(&quot;ppo&quot;, env_name)repo_id = ModelRepoId(&quot;YourOrganization&quot;, model_name)# prints 'seals-Walker2d-v0'. Notice how the slash is removed so you can use it to # construct file paths if you like.print(env_name)# you can still access the original gym id if neededenv = gym.make(env_name.gym_id)  # prints `ppo-seals-Walker2d-v0`print(model_name)  # prints: `ppo-seals-Walker2d-v0.zip`. # This is where `model.save(model_name)` will place the model fileprint(model_name.filename)  # prints: `YourOrganization/ppo-seals-Walker2d-v0`print(repo_id)```</longdescription>
</pkgmetadata>