<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># üóÇÔ∏è LlamaIndex ü¶ô[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)LlamaIndex (GPT Index) is a data framework for your LLM application.PyPI:- LlamaIndex: https://pypi.org/project/llama-index/.- GPT Index (duplicate): https://pypi.org/project/gpt-index/.LlamaIndex.TS (Typescript/Javascript): https://github.com/run-llama/LlamaIndexTS.Documentation: https://gpt-index.readthedocs.io/.Twitter: https://twitter.com/llama_index.Discord: https://discord.gg/dGcwcsnxhU.### Ecosystem- LlamaHub (community library of data loaders): https://llamahub.ai- LlamaLab (cutting-edge AGI projects using LlamaIndex): https://github.com/run-llama/llama-lab## üöÄ Overview**NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!### Context- LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.- How do we best augment LLMs with our own private data?We need a comprehensive toolkit to help perform this data augmentation for LLMs.### Proposed SolutionThat's where **LlamaIndex** comes in. LlamaIndex is a &quot;data framework&quot; to help you build LLM apps. It provides the following tools:- Offers **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)- Provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.- Provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.- Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, anything else).LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),to fit their needs.## üí° ContributingInterested in contributing? See our [Contribution Guide](CONTRIBUTING.md) for more details.## üìÑ DocumentationFull documentation can be found here: https://gpt-index.readthedocs.io/en/latest/.Please check it out for the most up-to-date tutorials, how-to guides, references, and other resources!## üíª Example Usage```pip install llama-index```Examples are in the `examples` folder. Indices are in the `indices` folder (see list of indices below).To build a simple vector store index using OpenAI:```pythonimport osos.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR_OPENAI_API_KEY&quot;from llama_index import VectorStoreIndex, SimpleDirectoryReaderdocuments = SimpleDirectoryReader(&quot;YOUR_DATA_DIRECTORY&quot;).load_data()index = VectorStoreIndex.from_documents(documents)```To build a simple vector store index using non-OpenAI LLMs, e.g. Llama 2 hosted on [Replicate](https://replicate.com/), where you can easily create a free trial API token:```pythonimport osos.environ[&quot;REPLICATE_API_TOKEN&quot;] = &quot;YOUR_REPLICATE_API_TOKEN&quot;from llama_index.llms import Replicatellama2_7b_chat = &quot;meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e&quot;llm = Replicate(    model=llama2_7b_chat,    temperature=0.01,    additional_kwargs={&quot;top_p&quot;: 1, &quot;max_new_tokens&quot;:300})from llama_index.embeddings import HuggingFaceEmbeddingfrom llama_index import ServiceContextembed_model = HuggingFaceEmbedding(model_name=&quot;BAAI/bge-small-en-v1.5&quot;)service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)from llama_index import VectorStoreIndex, SimpleDirectoryReaderdocuments = SimpleDirectoryReader(&quot;YOUR_DATA_DIRECTORY&quot;).load_data()index = VectorStoreIndex.from_documents(documents, service_context=service_context)```To query:```pythonquery_engine = index.as_query_engine()query_engine.query(&quot;YOUR_QUESTION&quot;)```By default, data is stored in-memory.To persist to disk (under `./storage`):```pythonindex.storage_context.persist()```To reload from disk:```pythonfrom llama_index import StorageContext, load_index_from_storage# rebuild storage contextstorage_context = StorageContext.from_defaults(persist_dir='./storage')# load indexindex = load_index_from_storage(storage_context)```## üîß DependenciesThe main third-party package requirements are `tiktoken`, `openai`, and `langchain`.All requirements should be contained within the `setup.py` file.To run the package locally without building the wheel, simply run:```bashpip install poetrypoetry install --with dev```## üìñ CitationReference to cite if you use LlamaIndex in a paper:```@software{Liu_LlamaIndex_2022,author = {Liu, Jerry},doi = {10.5281/zenodo.1234},month = {11},title = {{LlamaIndex}},url = {https://github.com/jerryjliu/llama_index},year = {2022}}```</longdescription>
</pkgmetadata>