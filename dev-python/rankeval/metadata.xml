<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Build Status](https://img.shields.io/travis/hpclab/rankeval/master.svg?logo=travis)](https://travis-ci.org/hpclab/rankeval)[![Python version](https://img.shields.io/pypi/pyversions/rankeval.svg)](https://badge.fury.io/py/rankeval)[![PyPI version](https://img.shields.io/pypi/v/rankeval.svg)](https://badge.fury.io/py/rankeval)[![Wheel](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&amp;logo=python&amp;logoColor=white)](https://badge.fury.io/py/rankeval)[![CPython Implementation](https://img.shields.io/pypi/implementation/rankeval.svg)](https://badge.fury.io/py/rankeval)[![License](https://img.shields.io/badge/license-MPL%202.0-blue.svg)](https://badge.fury.io/py/rankeval)[![DOI](https://img.shields.io/badge/DOI-10.1145%2F3077136.3084140-blue)](https://doi.org/10.1145/3077136.3084140)&lt;img src=&quot;https://github.com/hpclab/rankeval/blob/master/doc/banner.png?raw=true&quot;&gt;# RankEval: An Evaluation and Analysis Framework for Learning-to-Rank SolutionsRankEval is an open-source tool for the analysis and evaluation ofLearning-to-Rank models based on ensembles of regression trees. Thesuccess of ensembles of regression trees fostered the development ofseveral open-source libraries targeting efficiency of the learning phaseand effectiveness of the resulting models. However, these libraries offeronly very limited help for the tuning and evaluation of the trained models.RankEval aims at providing a common ground for several Learning to Rank libraries by providing useful and interoperable tools for a comprehensivecomparison and in-depth analysis of ranking models. Target audience is the *machine learning* (ML) and *information retrieval* (IR) communities.RankEval is available under Mozilla Public License 2.0.The official GitHub repository is: [here](https://github.com/hpclab/rankeval).For questions/suggestions on how to improve RankEval, send us an email: rankeval@isti.cnr.it## FeaturesRankeval provides a common ground between several pre-existing tools and offers services which support the interpretation of differently generated models in a unified environment, allowing an easy, comprehensive comparison and in-depth analysis.The main functionalities of RankEval can be summarized along five dimensions:- effectiveness analysis- feature analysis- structural analysis- topological analysis- interoperability among GBRT libraries    - support the model format of the most popular learning tools such as     QuickRank, RankLib, XGBoost, LightGBM, Scikit-Learn, etcThese functionalities can be applied to several models at the same time, so to have a direct comparison of the analysis performed. The tool has been written to ensure **flexibility**, **extensibility**, and **efficiency**. ## DocumentationThe official API documentation is available at: [here](http://rankeval.isti.cnr.it/docs/).Soon on ReadTheDocs!## InstallationThe library works with OpenMP so you need a compiler supporting it. If your machine uses a default compiler different from GNU GCC, change it appropriately before proceeding with the installation:```export CC=gcc-5export CXX=g++-5```Moreover, RankEval needs the following libraries to be installed before the installation process begin:  - numpy &gt;= 1.13  - scipy &gt;= 0.14  - cython &gt;= 0.25  - matplotlib &gt;= 2.0.2RankEval can be easily installed from Python Package Index (PyPI). You may download and install it by running:```pip install rankeval```Alternatively, you can build the library from source.Below an example of installation.```python setup.py install```or```pip install -e .```## DevelopmentInstallation of libraries required for development (documentation generation and unittests):```pip install -e .[develop]```Local installation of compiled libraries: ```python setup.py build_ext -i```Execution of unit tests:```python setup.py test```or (if you have nose already installed):```nosetests -v```## Cite RankEvalIf you use RankEval, please cite us!```@inproceedings{rankeval-sigir17,  author = {Claudio Lucchese and Cristina Ioana Muntean and Franco Maria Nardini and            Raffaele Perego and Salvatore Trani},  title = {RankEval: An Evaluation and Analysis Framework for Learning-to-Rank Solutions},  booktitle = {SIGIR 2017: Proceedings of the 40th International {ACM} {SIGIR}               Conference on Research and Development in Information Retrieval},  year = {2017},  location = {Tokyo, Japan}}```## Credits    - Dataset loader: https://github.com/deronnek/svmlight-loader    - Query id implementation: https://github.com/mblondel/svmlight-loader/pull/6</longdescription>
</pkgmetadata>