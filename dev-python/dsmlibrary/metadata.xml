<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># DSM Library## DataNode0. init DataNode```pythonfrom dsmlibrary.datanode import DataNode data = DataNode(token)```1. upload file```pythondata.upload_file(directory_id=&lt;directory_id&gt;, file_path='&lt;file_path&gt;', description=&quot;&lt;description(optional)&gt;&quot;)```2. download file```pythondata.download_file(file_id=&lt;file_id&gt;, download_path=&quot;&lt;place download file save&gt; (default ./dsm.tmp)&quot;)```3. get file```pythonmeta, file = data.get_file(file_id=&quot;&lt;file_id&gt;&quot;)# meta -&gt; dict# file -&gt; io bytes``````python# example read csv pandas meta, file = data.get_file(file_id=&quot;&lt;file_id&gt;&quot;)df = pd.read_csv(file)...``` 4. read df```pythondf = data.read_df(file_id=&quot;&lt;file_id&gt;&quot;)# df return as pandas dataframe```6. read ddf* ```.parquet must use this function``````pythonddf = data.read_ddf(file_id=&quot;&lt;file_id&gt;&quot;)# ddf return as dask dataframe```7. write parquet file```pythondf = ... # pandas dataframe or dask dataframedata.write(df=df, directory=&lt;directory_id&gt;, name=&quot;&lt;save_file_name&gt;&quot;, description=&quot;&lt;description&gt;&quot;, replace=&lt;replace if file exists. default False&gt;, profiling=&lt;True or False default False&gt;, lineage=&lt;list of file id. eg [1,2,3]&gt;)```8. writeListDataNode```pythondf = ... # pandas dataframe or dask dataframedata.writeListDataNode(df=df, directory_id=&lt;directory_id&gt;, name=&quot;&lt;save_file_name&gt;&quot;, description=&quot;&lt;description&gt;&quot;, replace=&lt;replace if file exists. default False&gt;, profiling=&lt;True or False default False&gt;, lineage=&lt;list of file id. eg [1,2,3]&gt;)```9. get file id```pythonfile_id = data.get_file_id(name=&lt;file name&gt;, directory_id=&lt;directory id&gt;)# file_id return int fileID```10. get directory id```directory_id = data.get_directory_id(parent_dir_id=&lt;directory id&gt;, name=&lt;file name&gt;)# directory_id return int directoryID```11. get get_file_version```use for listDataNode``````pythonfileVersion = data.get_file_version(file_id=&lt;file id&gt;)# return dict `file_id` and `timestamp````## Clickhouse1. imoprt data to clickhouse```pythonfrom dsmlibrary.clickhouse import ClickHouseddf = ... # pandas dataframe or dask dataframe## to warehousetable_name = &lt;your_table_name&gt;partition_by = &lt;your_partition_by&gt;connection = {   'host': '',   'port': ,   'database': '',   'user': '',   'password': '',   'settings':{      'use_numpy': True   },   'secure': False }warehouse = ClickHouse(connection=connection)tableName = warehouse.get_or_createTable(ddf=ddf, tableName=table_name, partition_by=partition_by)warehouse.write(ddf=ddf, tableName=tableName)```2. query data from clickhouse```pythonquery = f&quot;&quot;&quot;     SELECT * FROM {tableName} LIMIT 10 &quot;&quot;&quot; warehouse.read(sqlQuery=query)```3. drop table```pythonwarehouse.dropTable(tableName=table_name)```- optional```use for custom config insert data to clickhouse``````pythonconfig = {  'n_partition_per_block': 10,  'n_row_per_loop': 1000}warehouse = ClickHouse(connection=connection, config=config)```4. truncate table```warehouse.truncateTable(tableName=table_name)```# API## dsmlibrary### dsmlibrary.datanode.DataNode- upload_file- download_file- read_df- read_ddf- write- get_file_id### dsmlibrary.clickhouse.ClickHouse- get_or_createTable- write- read- dropTable# Use for pipeline ```data = DataNode(apikey=&quot;&lt;APIKEY&gt;&quot;)```use api key for authenticate</longdescription>
</pkgmetadata>