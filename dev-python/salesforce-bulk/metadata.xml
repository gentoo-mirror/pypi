<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>.. figure:: https://travis-ci.org/heroku/salesforce-bulk.svg?branch=main   :alt: travis-badgeSalesforce Bulk===============Python client library for accessing the asynchronous Salesforce.com BulkAPI.Installation------------.. code-block:: bash    pip install salesforce-bulkAuthentication--------------To access the Bulk API you need to authenticate a user into Salesforce.The easiest way to do this is just to supply ``username``, ``password``and ``security_token``. This library will use the ``simple-salesforce``package to handle password based authentication... code-block:: python    from salesforce_bulk import SalesforceBulk    bulk = SalesforceBulk(username=username, password=password, security_token=security_token)    ...Alternatively if you run have access to a session ID and instance\_urlyou can use those directly:.. code-block:: python    from urlparse import urlparse    from salesforce_bulk import SalesforceBulk    bulk = SalesforceBulk(sessionId=sessionId, host=urlparse(instance_url).hostname)    ...Operations----------The basic sequence for driving the Bulk API is:1. Create a new job2. Add one or more batches to the job3. Close the job4. Wait for each batch to finishBulk Query----------``bulk.create_query_job(object_name, contentType='JSON')``Using API v39.0 or higher, you can also use the queryAll operation:``bulk.create_queryall_job(object_name, contentType='JSON')``Example.. code-block:: python    import json    from salesforce_bulk.util import IteratorBytesIO    job = bulk.create_query_job(&quot;Contact&quot;, contentType='JSON')    batch = bulk.query(job, &quot;select Id,LastName from Contact&quot;)    bulk.close_job(job)    while not bulk.is_batch_done(batch):        sleep(10)    for result in bulk.get_all_results_for_query_batch(batch):        result = json.load(IteratorBytesIO(result))        for row in result:            print row # dictionary rowsSame example but for CSV:.. code-block:: python    import unicodecsv        job = bulk.create_query_job(&quot;Contact&quot;, contentType='CSV')    batch = bulk.query(job, &quot;select Id,LastName from Contact&quot;)    bulk.close_job(job)    while not bulk.is_batch_done(batch):        sleep(10)    for result in bulk.get_all_results_for_query_batch(batch):        reader = unicodecsv.DictReader(result, encoding='utf-8')        for row in reader:            print(row) # dictionary rowsNote that while CSV is the default for historical reasons, JSON shouldbe prefered since CSV has some drawbacks including its handling of NULLvs empty string.PK Chunk Header^^^^^^^^^^^^^^^If you are querying a large number of records you probably want to turn on `PK Chunking&lt;https://developer.salesforce.com/docs/atlas.en-us.api_asynch.meta/api_asynch/async_api_headers_enable_pk_chunking.htm&gt;`_:``bulk.create_query_job(object_name, contentType='CSV', pk_chunking=True)``That will use the default setting for chunk size. You can use a different chunk size by providing anumber of records per chunk:``bulk.create_query_job(object_name, contentType='CSV', pk_chunking=100000)``Additionally if you want to do something more sophisticated you can provide a header value:``bulk.create_query_job(object_name, contentType='CSV', pk_chunking='chunkSize=50000; startRow=00130000000xEftMGH')``Bulk Insert, Update, Delete---------------------------All Bulk upload operations work the same. You set the operation when youcreate the job. Then you submit one or more documents that specifyrecords with columns to insert/update/delete. When deleting you shouldonly submit the Id for each record.For efficiency you should use the ``post_batch`` method to post eachbatch of data. (Note that a batch can have a maximum 10,000 records andbe 1GB in size.) You pass a generator or iterator into this function andit will stream data via POST to Salesforce. For help sending CSVformatted data you can use the salesforce\_bulk.CsvDictsAdapter class.It takes an iterator returning dictionaries and returns an iteratorwhich produces CSV data.Full example:.. code-block:: python    from salesforce_bulk import CsvDictsAdapter    job = bulk.create_insert_job(&quot;Account&quot;, contentType='CSV')    accounts = [dict(Name=&quot;Account%d&quot; % idx) for idx in xrange(5)]    csv_iter = CsvDictsAdapter(iter(accounts))    batch = bulk.post_batch(job, csv_iter)    bulk.wait_for_batch(job, batch)    bulk.close_job(job)    print(&quot;Done. Accounts uploaded.&quot;)Concurrency mode^^^^^^^^^^^^^^^^When creating the job, pass ``concurrency='Serial'`` or``concurrency='Parallel'`` to set the concurrency mode for the job.</longdescription>
</pkgmetadata>