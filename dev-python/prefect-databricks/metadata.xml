<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Integrate Databricks jobs into your dataflow with prefect-databricks &lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://user-images.githubusercontent.com/15331990/219822439-70ec82fb-e93a-4983-bec7-8c8250fbb7ac.png&quot;&gt;    &lt;br&gt;    &lt;a href=&quot;https://pypi.python.org/pypi/prefect-databricks/&quot; alt=&quot;PyPI version&quot;&gt;        &lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/prefect-databricks?color=0052FF&amp;labelColor=090422&quot;&gt;&lt;/a&gt;    &lt;a href=&quot;https://github.com/PrefectHQ/prefect-databricks/&quot; alt=&quot;Stars&quot;&gt;        &lt;img src=&quot;https://img.shields.io/github/stars/PrefectHQ/prefect-databricks?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;    &lt;a href=&quot;https://pepy.tech/badge/prefect-databricks/&quot; alt=&quot;Downloads&quot;&gt;        &lt;img src=&quot;https://img.shields.io/pypi/dm/prefect-databricks?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;    &lt;a href=&quot;https://github.com/PrefectHQ/prefect-databricks/pulse&quot; alt=&quot;Activity&quot;&gt;        &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/PrefectHQ/prefect-databricks?color=0052FF&amp;labelColor=090422&quot; /&gt;&lt;/a&gt;    &lt;br&gt;    &lt;a href=&quot;https://prefect-community.slack.com&quot; alt=&quot;Slack&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/slack-join_community-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=slack&quot; /&gt;&lt;/a&gt;    &lt;a href=&quot;https://discourse.prefect.io/&quot; alt=&quot;Discourse&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/discourse-browse_forum-red.svg?color=0052FF&amp;labelColor=090422&amp;logo=discourse&quot; /&gt;&lt;/a&gt;&lt;/p&gt;Visit the full docs [here](https://PrefectHQ.github.io/prefect-databricks) to see additional examples and the API reference.The prefect-databricks collection makes it easy to coordiante Databricks jobs with other tools in your data stack using Prefect. Check out the examples below to get started!## Getting Started### Integrate with Prefect flowsUsing Prefect with Databricks allows you to define and orchestrate complex data workflows that take advantage of the scalability and performance of Databricks.This can be especially useful for data-intensive tasks such as ETL (extract, transform, load) pipelines, machine learning training and inference, and real-time data processing.Below is an example of how you can incorporate Databricks notebooks within your Prefect flows.Be sure to install [prefect-databricks](#installation) and [save a credentials block](#saving-credentials-to-block) to run the examples below!If you don't have an existing notebook ready on Databricks, you can copy the following, and name it `example.ipynb`. This notebook, accepts a name parameter from the flow and simply prints a message.```pythonname = dbutils.widgets.get(&quot;name&quot;)message = f&quot;Don't worry {name}, I got your request! Welcome to prefect-databricks!&quot;print(message)```Here, the flow launches a new cluster to run `example.ipynb` and waits for the completion of the notebook run. Replace the placeholders and run.```pythonfrom prefect import flowfrom prefect_databricks import DatabricksCredentialsfrom prefect_databricks.flows import jobs_runs_submit_and_wait_for_completionfrom prefect_databricks.models.jobs import (    AutoScale,    AwsAttributes,    JobTaskSettings,    NotebookTask,    NewCluster,)@flowdef jobs_runs_submit_flow(block_name: str, notebook_path: str, **base_parameters):    databricks_credentials = DatabricksCredentials.load(block_name)    # specify new cluster settings    aws_attributes = AwsAttributes(        availability=&quot;SPOT&quot;,        zone_id=&quot;us-west-2a&quot;,        ebs_volume_type=&quot;GENERAL_PURPOSE_SSD&quot;,        ebs_volume_count=3,        ebs_volume_size=100,    )    auto_scale = AutoScale(min_workers=1, max_workers=2)    new_cluster = NewCluster(        aws_attributes=aws_attributes,        autoscale=auto_scale,        node_type_id=&quot;m4.large&quot;,        spark_version=&quot;10.4.x-scala2.12&quot;,        spark_conf={&quot;spark.speculation&quot;: True},    )    # specify notebook to use and parameters to pass    notebook_task = NotebookTask(        notebook_path=notebook_path,        base_parameters=base_parameters,    )    # compile job task settings    job_task_settings = JobTaskSettings(        new_cluster=new_cluster,        notebook_task=notebook_task,        task_key=&quot;prefect-task&quot;    )    run = jobs_runs_submit_and_wait_for_completion(        databricks_credentials=databricks_credentials,        run_name=&quot;prefect-job&quot;,        tasks=[job_task_settings]    )    return runjobs_runs_submit_flow(    block_name=&quot;BLOCK-NAME-PLACEHOLDER&quot;    notebook_path=&quot;/Users/&lt;EMAIL_ADDRESS_PLACEHOLDER&gt;/example.ipynb&quot;,    name=&quot;Marvin&quot;)```Upon execution, the notebook run should output:```Don't worry Marvin, I got your request! Welcome to prefect-databricks!```!!! info &quot;Input dictionaries in the place of models&quot;        Instead of using the built-in models, you may also input a valid dictionary.        For example, the following are equivalent:    ```python    auto_scale=AutoScale(min_workers=1, max_workers=2)    ```    ```python    auto_scale={&quot;min_workers&quot;: 1, &quot;max_workers&quot;: 2}    ```If you have an existing Databricks job, you can run it using `jobs_runs_submit_by_id_and_wait_for_completion`:```pythonfrom prefect import flowfrom prefect_databricks import DatabricksCredentialsfrom prefect_databricks.flows import (    jobs_runs_submit_by_id_and_wait_for_completion,)@flowdef existing_job_submit(databricks_credentials_block_name: str, job_id):    databricks_credentials = DatabricksCredentials.load(name=block_name)    run = jobs_runs_submit_by_id_and_wait_for_completion(        databricks_credentials=databricks_credentials, job_id=job_id    )    return runexisting_job_submit(databricks_credentials_block_name=&quot;db-creds&quot;, job_id=&quot;YOUR-JOB-NAME&quot;)```## ResourcesFor more tips on how to use tasks and flows in a Collection, check out [Using Collections](https://orion-docs.prefect.io/collections/usage/)!Note, the tasks within this collection were created by a code generator using the service's OpenAPI spec.The service's REST API documentation can be found [here](https://docs.databricks.com/dev-tools/api/latest/index.html).### InstallationInstall `prefect-databricks` with `pip`:```bashpip install prefect-databricks```Requires an installation of Python 3.7+.We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv.These tasks are designed to work with Prefect 2. For more information about how to use Prefect, please refer to the [Prefect documentation](https://orion-docs.prefect.io/).### Saving Credentials to BlockTo use the `load` method on Blocks, you must already have a block document [saved through code](https://orion-docs.prefect.io/concepts/blocks/#saving-blocks) or [saved through the UI](https://orion-docs.prefect.io/ui/blocks/).Below is a walkthrough on saving block documents through code; simply create a short script, replacing the placeholders. 1. Head over to [Databricks](https://accounts.cloud.databricks.com/).2. Login to your Databricks account and select a workspace.3. On the top right side of the nav bar, click on your account name -&gt; User Settings.4. Click Access tokens -&gt; Generate new token -&gt; Generate and copy the token.5. Note down your Databricks instance from the browser URL, formatted like `https://&lt;DATABRICKS-INSTANCE&gt;.cloud.databricks.com/`6. Create a short script, replacing the placeholders.```pythonfrom prefect_databricks import DatabricksCredentialscredentials = DatabricksCredentials(    databricks_instance=&quot;DATABRICKS-INSTANCE-PLACEHOLDER&quot;    token=&quot;TOKEN-PLACEHOLDER&quot;)connector.save(&quot;BLOCK_NAME-PLACEHOLDER&quot;)```Congrats! You can now easily load the saved block, which holds your credentials:```pythonfrom prefect_databricks import DatabricksCredentialsDatabricksCredentials.load(&quot;BLOCK_NAME-PLACEHOLDER&quot;)```!!! info &quot;Registering blocks&quot;    Register blocks in this module to    [view and edit them](https://orion-docs.prefect.io/ui/blocks/)    on Prefect Cloud:    ```bash    prefect block register -m prefect_databricks    ```### FeedbackIf you encounter any bugs while using `prefect-databricks`, feel free to open an issue in the [prefect-databricks](https://github.com/PrefectHQ/prefect-databricks) repository.If you have any questions or issues while using `prefect-databricks`, you can find help in either the [Prefect Discourse forum](https://discourse.prefect.io/) or the [Prefect Slack community](https://prefect.io/slack).Feel free to star or watch [`prefect-databricks`](https://github.com/PrefectHQ/prefect-databricks) for updates too!### ContributingIf you'd like to help contribute to fix an issue or add a feature to `prefect-databricks`, please [propose changes through a pull request from a fork of the repository](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork).Here are the steps:1. [Fork the repository](https://docs.github.com/en/get-started/quickstart/fork-a-repo#forking-a-repository)2. [Clone the forked repository](https://docs.github.com/en/get-started/quickstart/fork-a-repo#cloning-your-forked-repository)3. Install the repository and its dependencies:```pip install -e &quot;.[dev]&quot;```4. Make desired changes5. Add tests6. Insert an entry to [CHANGELOG.md](https://github.com/PrefectHQ/prefect-databricks/blob/main/CHANGELOG.md)7. Install `pre-commit` to perform quality checks prior to commit:```pre-commit install```8. `git commit`, `git push`, and create a pull request</longdescription>
</pkgmetadata>