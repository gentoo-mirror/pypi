<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![llama-small](https://user-images.githubusercontent.com/15543/221690917-1ca1dcb7-4a88-4ef8-842c-98268e3f4e63.jpg)# MicroLlamaThe smallest possible LLM API. Build a question and answer interface to your owncontent in a few minutes. Uses[OpenAI embeddings](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings),[gpt-3.5](https://platform.openai.com/docs/guides/chat) and[Faiss](https://faiss.ai), via[Langchain](https://langchain.readthedocs.io/en/latest/).## Usage1. Combine your source documents into a single JSON file called `source.json`.   It should look like this:```json[    {        &quot;source&quot;: &quot;Reference to the source of your content. Typically a title.&quot;,        &quot;url&quot;: &quot;URL for your source. This key is optional.&quot;,        &quot;content&quot;: &quot;Your content as a single string. If there's a title or summary, put these first, separated by new lines.&quot;    },     ...]```See `example.source.json` for an example.2. Install MicroLlama into a virtual environment:```bashpip install microllama```3. Get an [OpenAI API key](https://platform.openai.com/account/api-keys) and add   it to the environment, e.g. `export OPENAI_API_KEY=sk-etc`. Note that   indexing and querying require OpenAI credits, which   [aren't free](https://openai.com/api/pricing/).4. Run your server with `microllama`. If a vector search index doesn't exist,   it'll be created from your `source.json`, and stored.5. Query your documents at   [/api/ask?your question](http://127.0.0.1:8000/api/ask?your%20question).6. Microllama includes an optional web front-end, which is generated with   `microllama make-front-end`. This command creates a single `index.html` file   which you can edit. It's served at [/](http://127.0.0.1:8000/).## ConfigurationMicrollama is configured through environment variables, with the followingdefaults:- `OPENAI_API_KEY`: required- `FAISS_INDEX_PATH`: &quot;faiss_index&quot;- `SOURCE_JSON`: &quot;source.json&quot;- `MAX_RELATED_DOCUMENTS`: &quot;5&quot;- `EXTRA_CONTEXT`: &quot;Answer in no more than three sentences. If the answer is not  included in the context, say 'Sorry, this is no answer for this in my  sources.'.&quot;- `UVICORN_HOST`: &quot;0.0.0.0&quot;- `UVICORN_PORT`: &quot;8080&quot;## Deploying your APICreate a Dockerfile with `microllama make-dockerfile`. Then:### On Fly.ioSign up for a [Fly.io](https://fly.io) account and[install flyctl](https://fly.io/docs/hands-on/install-flyctl/). Then:```bashfly launch # answer no to Postgres, Redis and deploying now fly secrets set OPENAI_API_KEY=sk-etc fly deploy```### On Google Cloud Run```bashgcloud run deploy --source . --set-env-vars=&quot;OPENAI_API_KEY=sk-etc&quot;```For Cloud Run and other serverless platforms you should generate the FAISS indexat container build time, to reduce startup time. See the two commented lines in`Dockerfile`.You can also generate these commands with `microllama deploy`.## Based on- [Langchain](https://langchain.readthedocs.io/en/latest/)- Simon Willison's  [blog post](https://simonwillison.net/2023/Jan/13/semantic-search-answers/),  [datasette-openai](https://datasette.io/plugins/datasette-openai) and  [datasette-faiss](https://datasette.io/plugins/datasette-faiss).- [FastAPI](https://fastapi.tiangolo.com)- [GPT Index](https://gpt-index.readthedocs.io/en/latest/)- [Dagster blog post](https://dagster.io/blog/chatgpt-langchain)## TODO- [ ] Use splitting which generates more meaningful fragments, e.g.      text_splitter =      `SpacyTextSplitter(chunk_size=700, chunk_overlap=200, separator=&quot; &quot;)`</longdescription>
</pkgmetadata>