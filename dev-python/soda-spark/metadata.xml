<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;&lt;h1&gt;Soda Spark&lt;/h1&gt;&lt;br/&gt;&lt;b&gt;Data testing, monitoring, and profiling for Spark Dataframes.&lt;/b&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://github.com/sodadata/soda-spark/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache%202-blue.svg&quot; alt=&quot;License: Apache 2.0&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://join.slack.com/t/soda-community/shared_invite/zt-m77gajo1-nXJF7JtbbRht2zwaiLb9pg&quot;&gt;&lt;img alt=&quot;Slack&quot; src=&quot;https://img.shields.io/badge/chat-slack-green.svg&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://pypi.org/project/soda-spark/&quot;&gt;&lt;img alt=&quot;Pypi Soda PARK&quot; src=&quot;https://img.shields.io/badge/pypi-soda%20spark-green.svg&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://github.com/sodadata/soda-spark/actions/workflows/build.yml&quot;&gt;&lt;img alt=&quot;Build soda-spark&quot; src=&quot;https://github.com/sodadata/soda-spark/actions/workflows/workflow.yml/badge.svg&quot;&gt;&lt;/a&gt;&lt;/p&gt;Soda Spark is an extension of[Soda SQL](https://docs.soda.io/soda-sql/5_min_tutorial.html) that allows you to run SodaSQL functionality programmatically on a[Spark data frame](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html).Soda SQL is an open-source command-line tool. It utilizes user-defined input to prepare SQL queries that run tests on tables in a data warehouse to find invalid, missing, or unexpected data. When tests fail, they surface &quot;bad&quot; data that you can fix to ensure that downstream analysts are using &quot;good&quot; data to make decisions.## RequirementsSoda Spark has the same requirements as[`soda-sql-spark`](https://docs.soda.io/soda-sql/installation.html).## InstallFrom your shell, execute the following command.``` sh$ pip install soda-spark```## UseFrom your Python prompt, execute the following commands.``` python&gt;&gt;&gt; from pyspark.sql import DataFrame, SparkSession&gt;&gt;&gt; from sodaspark import scan&gt;&gt;&gt;&gt;&gt;&gt; spark_session = SparkSession.builder.getOrCreate()&gt;&gt;&gt;&gt;&gt;&gt; id = &quot;a76824f0-50c0-11eb-8be8-88e9fe6293fd&quot;&gt;&gt;&gt; df = spark_session.createDataFrame([...   {&quot;id&quot;: id, &quot;name&quot;: &quot;Paula Landry&quot;, &quot;size&quot;: 3006},...   {&quot;id&quot;: id, &quot;name&quot;: &quot;Kevin Crawford&quot;, &quot;size&quot;: 7243}... ])&gt;&gt;&gt;&gt;&gt;&gt; scan_definition = (&quot;&quot;&quot;... table_name: demodata... metrics:... - row_count... - max... - min_length... tests:... - row_count &gt; 0... columns:...   id:...     valid_format: uuid...     tests:...     - invalid_percentage == 0... sql_metrics:... - sql: |...     SELECT sum(size) as total_size_us...     FROM demodata...     WHERE country = 'US'...   tests:...   - total_size_us &gt; 5000... &quot;&quot;&quot;)&gt;&gt;&gt; scan_result = scan.execute(scan_definition, df)&gt;&gt;&gt;&gt;&gt;&gt; scan_result.measurements  # doctest: +ELLIPSIS[Measurement(metric='schema', ...), Measurement(metric='row_count', ...), ...]&gt;&gt;&gt; scan_result.test_results  # doctest: +ELLIPSIS[TestResult(test=Test(..., expression='row_count &gt; 0', ...), passed=True, skipped=False, ...)]&gt;&gt;&gt;```Or, use a [scan YAML](https://docs.soda.io/soda-sql/scan-yaml.html) file``` python&gt;&gt;&gt; scan_yml = &quot;static/demodata.yml&quot;&gt;&gt;&gt; scan_result = scan.execute(scan_yml, df)&gt;&gt;&gt;&gt;&gt;&gt; scan_result.measurements  # doctest: +ELLIPSIS[Measurement(metric='schema', ...), Measurement(metric='row_count', ...), ...]&gt;&gt;&gt;```See the[scan result object](https://github.com/sodadata/soda-sql/blob/main/core/sodasql/scan/scan_result.py)for all attributes and methods.Or, return Spark data frames:``` python&gt;&gt;&gt; measurements, test_results, errors = scan.execute(scan_yml, df, as_frames=True)&gt;&gt;&gt;&gt;&gt;&gt; measurements  # doctest: +ELLIPSISDataFrame[metric: string, column_name: string, value: string, ...]&gt;&gt;&gt; test_results  # doctest: +ELLIPSISDataFrame[test: struct&lt;...&gt;, passed: boolean, skipped: boolean, values: map&lt;string,string&gt;, ...]&gt;&gt;&gt;```See the `_to_data_frame` functions in the [`scan.py`](./src/sodaspark/scan.py)to see how the conversion is done.### Send results to Soda cloudSend the scan result to Soda cloud.``` python&gt;&gt;&gt; import os&gt;&gt;&gt; from sodasql.soda_server_client.soda_server_client import SodaServerClient&gt;&gt;&gt;&gt;&gt;&gt; soda_server_client = SodaServerClient(...     host=&quot;cloud.soda.io&quot;,...     api_key_id=os.getenv(&quot;API_PUBLIC&quot;),...     api_key_secret=os.getenv(&quot;API_PRIVATE&quot;),... )&gt;&gt;&gt; scan_result = scan.execute(scan_yml, df, soda_server_client=soda_server_client)&gt;&gt;&gt;```## UnderstandUnder the hood `soda-spark` does the following.1. Setup the scan   * Use the Spark dialect   * Use [Spark session](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html)     as [warehouse](https://docs.soda.io/soda-sql/warehouse.html) connection2. Create (or replace)   [global temporary view](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceGlobalTempView.html)   for the Spark data frame3. Execute the scan on the temporary view</longdescription>
</pkgmetadata>