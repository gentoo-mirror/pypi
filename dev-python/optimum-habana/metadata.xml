<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;!---Copyright 2022 The HuggingFace Team. All rights reserved.Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;![](https://github.com/huggingface/optimum-habana/blob/main/readme_logo.png)# Optimum HabanaðŸ¤— Optimum Habana is the interface between the ðŸ¤— Transformers and Diffusers libraries and [Habana's Gaudi processor (HPU)](https://docs.habana.ai/en/latest/index.html).It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.The list of officially validated models and tasks is available [here](https://github.com/huggingface/optimum-habana#validated-models). Users can try other models and tasks with only few changes.## What is a Habana Processing Unit (HPU)?HPUs offer fast model training and inference as well as a great price-performance ratio.Check out [this blog post about BERT pre-training](https://huggingface.co/blog/pretraining-bert) and [this article benchmarking Habana Gaudi2 versus Nvidia A100 GPUs](https://huggingface.co/blog/habana-gaudi-2-benchmark) for concrete examples.If you are not familiar with HPUs and would like to know more about them, we recommend you take a look at [our conceptual guide](https://huggingface.co/docs/optimum/habana/concept_guides/hpu).## InstallTo install the latest stable release of this package:```bashpip install --upgrade-strategy eager optimum[habana]```The `--upgrade-strategy eager` option is needed to ensure `optimum-habana` is upgraded to the latest stable release.&gt; To use DeepSpeed on HPUs, you also need to run the following command:&gt;```bash&gt;pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.11.0&gt;```Optimum Habana is a fast-moving project, and you may want to install it from source:```bashpip install git+https://github.com/huggingface/optimum-habana.git```Last but not least, don't forget to install the requirements for every example:```bashcd &lt;example-folder&gt;pip install -r requirements.txt```## How to use it?### Quick StartðŸ¤— Optimum Habana was designed with one goal in mind: **to make training and inference straightforward for any ðŸ¤— Transformers and ðŸ¤— Diffusers user while leveraging the complete power of Gaudi processors**.#### Transformers InterfaceThere are two main classes one needs to know:- [GaudiTrainer](https://huggingface.co/docs/optimum/habana/package_reference/trainer): the trainer class that takes care of compiling (lazy or eager mode) and distributing the model to run on HPUs, and performing training and evaluation.- [GaudiConfig](https://huggingface.co/docs/optimum/habana/package_reference/gaudi_config): the class that enables to configure Habana Mixed Precision and to decide whether optimized operators and optimizers should be used or not.The [GaudiTrainer](https://huggingface.co/docs/optimum/habana/package_reference/trainer) is very similar to the [ðŸ¤— Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), and adapting a script using the Trainer to make it work with Gaudi will mostly consist in simply swapping the `Trainer` class for the `GaudiTrainer` one.That's how most of the [example scripts](https://github.com/huggingface/optimum-habana/tree/main/examples) were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).Here is an example:```diff- from transformers import Trainer, TrainingArguments+ from optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArguments- training_args = TrainingArguments(+ training_args = GaudiTrainingArguments(  # training arguments...+ use_habana=True,+ use_lazy_mode=True,  # whether to use lazy or eager mode+ gaudi_config_name=path_to_gaudi_config,)# A lot of code here# Initialize our Trainer- trainer = Trainer(+ trainer = GaudiTrainer(    model=model,    args=training_args,  # Original training arguments.    train_dataset=train_dataset if training_args.do_train else None,    eval_dataset=eval_dataset if training_args.do_eval else None,    compute_metrics=compute_metrics,    tokenizer=tokenizer,    data_collator=data_collator,)```where `gaudi_config_name` is the name of a model from the [Hub](https://huggingface.co/Habana) (Gaudi configurations are stored in model repositories) or a path to a local Gaudi configuration file (you can see [here](https://huggingface.co/docs/optimum/habana/package_reference/gaudi_config) how to write your own).#### Diffusers InterfaceYou can generate images from prompts using Stable Diffusion on Gaudi using the [`GaudiStableDiffusionPipeline`](https://huggingface.co/docs/optimum/habana/package_reference/stable_diffusion_pipeline) class and the [`GaudiDDIMScheduler`] which have been both optimized for HPUs. Here is how to use them and the differences with the ðŸ¤— Diffusers library:```diff- from diffusers import DDIMScheduler, StableDiffusionPipeline+ from optimum.habana.diffusers import GaudiDDIMScheduler, GaudiStableDiffusionPipelinemodel_name = &quot;runwayml/stable-diffusion-v1-5&quot;- scheduler = DDIMScheduler.from_pretrained(model_name, subfolder=&quot;scheduler&quot;)+ scheduler = GaudiDDIMScheduler.from_pretrained(model_name, subfolder=&quot;scheduler&quot;)- pipeline = StableDiffusionPipeline.from_pretrained(+ pipeline = GaudiStableDiffusionPipeline.from_pretrained(    model_name,    scheduler=scheduler,+   use_habana=True,+   use_hpu_graphs=True,+   gaudi_config=&quot;Habana/stable-diffusion&quot;,)outputs = generator(    [&quot;An image of a squirrel in Picasso style&quot;],    num_images_per_prompt=16,+   batch_size=4,)```### DocumentationCheck out [the documentation of Optimum Habana](https://huggingface.co/docs/optimum/habana/index) for more advanced usage.## Validated ModelsThe following model architectures, tasks and device distributions have been validated for ðŸ¤— Optimum Habana:&gt; In the tables below, :heavy_check_mark: means single-card, multi-card and DeepSpeed have all been validated.- Transformers:&lt;div align=&quot;center&quot;&gt;| Architecture | Training | Inference | &lt;center&gt;Tasks&lt;/center&gt; ||--------------|:--------:|:---------:|:-----------------------|| BERT         | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[text classification](https://github.com/huggingface/optimum-habana/tree/main/examples/text-classification)&lt;/li&gt;&lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || RoBERTa | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || ALBERT | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || DistilBERT |:heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || GPT2             | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt;&lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || BLOOM(Z) | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || StarCoder | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;Single card&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || GPT-J | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;/div&gt; | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;Single card&lt;/li&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt;&lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || GPT-NeoX | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;/div&gt; | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt;&lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || OPT | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || Llama 2 / CodeLlama | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;li&gt;LoRA&lt;/li&gt;&lt;/div&gt; | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;DeepSpeed&lt;/li&gt;&lt;li&gt;LoRA&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt;&lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || StableLM | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;Single card&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || Falcon | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;Single card&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || CodeGen | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;Single card&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || MPT | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;Single card&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation)&lt;/li&gt; || T5 | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[summarization](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization)&lt;/li&gt;&lt;li&gt;[translation](https://github.com/huggingface/optimum-habana/tree/main/examples/translation)&lt;/li&gt;&lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering#fine-tuning-t5-on-squad20)&lt;/li&gt; || ViT | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[image classification](https://github.com/huggingface/optimum-habana/tree/main/examples/image-classification)&lt;/li&gt; || Swin | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[image classification](https://github.com/huggingface/optimum-habana/tree/main/examples/image-classification)&lt;/li&gt; || Wav2Vec2 | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[audio classification](https://github.com/huggingface/optimum-habana/tree/main/examples/audio-classification)&lt;/li&gt;&lt;li&gt;[speech recognition](https://github.com/huggingface/optimum-habana/tree/main/examples/speech-recognition)&lt;/li&gt; || CLIP | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[contrastive image-text training](https://github.com/huggingface/optimum-habana/tree/main/examples/contrastive-image-text)&lt;/li&gt; || BridgeTower | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[contrastive image-text training](https://github.com/huggingface/optimum-habana/tree/main/examples/contrastive-image-text)&lt;/li&gt; || ESMFold | :x: | &lt;div style=&quot;text-align:left&quot;&gt;&lt;li&gt;Single card&lt;/li&gt;&lt;/div&gt; | &lt;li&gt;[protein folding](https://github.com/huggingface/optimum-habana/tree/main/examples/protein-folding)&lt;/li&gt; |&lt;/div&gt;- Diffusers:&lt;div align=&quot;center&quot;&gt;| Architecture     | Training | Inference            | Tasks ||------------------|:--------:|:--------------------:|:-----:|| Stable Diffusion | :x:      | &lt;li&gt;Single card&lt;/li&gt; | &lt;li&gt;[text-to-image generation](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion)&lt;/li&gt; || LDM3D            | :x:      | &lt;li&gt;Single card&lt;/li&gt; | &lt;li&gt;[text-to-image generation](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion)&lt;/li&gt; |&lt;/div&gt;Other models and tasks supported by the ðŸ¤— Transformers and ðŸ¤— Diffusers library may also work. You can refer to this [section](https://github.com/huggingface/optimum-habana#how-to-use-it) for using them with ðŸ¤— Optimum Habana. Besides, [this page](https://github.com/huggingface/optimum-habana/tree/main/examples) explains how to modify any [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch) from the ðŸ¤— Transformers library to make it work with ðŸ¤— Optimum Habana.If you find any issues while using those, please open an issue or a pull request.## Gaudi SetupPlease refer to Habana Gaudi's official [installation guide](https://docs.habana.ai/en/latest/Installation_Guide/index.html).&gt; Tests should be run in a Docker container based on Habana Docker images.&gt;&gt; The current version has been validated for SynapseAI 1.11.## DevelopmentCheck the [contributor guide](https://github.com/huggingface/optimum/blob/main/CONTRIBUTING.md) for instructions.</longdescription>
</pkgmetadata>