<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>simpledbf#########*simpledbf* is a Python library for converting basic DBF files (see`Limitations`_) to CSV files, Pandas DataFrames, SQL tables, or HDF5 tables.This package is fully compatible with Python &gt;=3.4, with almost complete`Python 2.7 support`_ as well. The conversion to CSV and SQL (see``to_textsql`` below) is entirely written in Python, so no additionaldependencies are necessary. For other export formats, see `OptionalRequirements`_.  This code was designed to be very simple, fast and memoryefficient for convenient interactive or batch file processing; therefore, itlacks many features, such as the ability to write DBF files, that otherpackages might provide. Bug fixes, questions, and update requests are encouraged and can be filed atthe `GitHub repo`_. This code is derived from an  `ActiveState DBF example`_ that works withPython2 and is distributed under a PSF license... _Optional Requirements:Optional Requirements---------------------* Pandas &gt;= 0.15.2 (Required for DataFrame)* PyTables &gt;= 3.1 (with Pandas required for HDF tables)* SQLalchemy &gt;= 0.9 (with Pandas required for DataFrame-SQL tables)Installation------------The most recent release of *simpledbf* can be installed using ``pip`` or``conda``, if you happen to be using the `Anaconda Python distribution`_.Using ``conda``::    $ conda install -c https://conda.binstar.org/rnelsonchem simpledbfUsing ``pip``::    $ pip install simpledbfThe development version can be installed from GitHub::    $ pip install git+https://github.com/rnelsonchem/simpledbf.gitAs an alternative, this package only contains a single file, so in principle,you could download the ``simpledbf.py`` file from Github and put it in anyfolder of your choosing... _Limitations:DBF File Limitations--------------------This package currently supports a subset of `dBase III through 5`_ DBF files.In particular, support is missing for linked memo (i.e. DBT) files. This ismostly due to limitations in the types of files available to the author.  Feelfree to request an update if you can supply a DBF file with an associated memofile. `DBF version 7`_, the most recent DBF file spec, is not currentlysupported by this package... _Python 2.7 support:Python 2 Support ----------------Except for HDF file export, this code should work fine with Python &gt;=2.7.However, HDF files created in Python3 are compatible with all Python2 HDFpackages, so in principle, you could make any HDF files in a temporary Python3environment. If you are using the `Anaconda Python distribution`_(recommended), then you can make a small Python3 working environment asfollows:.. code::    $ conda create -n dbf python=3 pandas pytables sqlalchemy    # Lots of output...        $ source activate dbf    dbf&gt;$ conda install -c https://conda.binstar.org/rnelsonchem simpledbf    dbf&gt;$ python my_py3_hdf_creation_script.py    # This is using Python3    dbf&gt;$ source deactivate    $ python my_py2_stuff_with_hdf.py    # This is using Python2 againHDF file export is currently broken in Python2 due to a `limitation in PandasHDF export with unicode`_. This issue may be fixed future versions ofPandas/PyTables.Example Usage#############.. _Loading:Load a DBF file---------------This module currently only defines a single class, ``Dbf5``, which isinstantiated with a DBF file name, which can contain path info as well. Anoptional 'codec' keyword argument that controls the codec used forreading/writing files. The default is 'utf-8'. See the documentation forPython's `codec standard library module`_ for more codec options... code::    In : from simpledbf import Dbf5    In : dbf = Dbf5('fake_file_name.dbf', codec='utf-8')The ``Dbf5`` object initially only reads the header information from the file,so you can inspect some of the properties. For example, ``numrec`` is thenumber of records in the DBF file, and ``fields`` is a list of tuples withinformation about the data columns. See the DBF file spec for info on thecolumn type characters. The &quot;DeletionFlag&quot; column is always present as a checkfor deleted records; however, it is never exported during conversion... code::    In : dbf.numrec    Out: 10000    In : dbf.fields    Out: [('DeletionFlag', 'C', 1), ('col_1', 'C', 15), ('col_2', 'N', 2)]The docstring for this object contains a complete listing of attributes andtheir descriptions.The ``mem`` method gives an approximate memory requirement for processing thisDBF file. (~2x the total file size, which could be wildly inaccurate.) Inaddition, all of the output methods in this object take a ``chunksize``keyword argument, which lets you split up the processing of large files intosmaller chunks to limit the total memory usage of the conversion process. Whenthis keyword argument is passed into ``mem``, the approximate memory footprintof the chunk will also be given, which can be useful when trying to determinethe maximum chunksize your memory will allow... code::    In : dbf.mem()    This total process would require more than 350.2 MB of RAM.     In : dbf.mem(chunksize=1000)    Each chunk will require 4.793 MB of RAM.    This total process would require more than 350.2 MB of RAM.Export the Data---------------The ``Ddb5`` object behaves like Python's file object in that it will be&quot;exhausted&quot; after export. To re-export the DBF data to a different format,first create a new ``Dbf5`` instance using the same file name. This procedureis followed in the documentation below.    Note on Empty/Bad Data++++++++++++++++++++++This package attempts to convert most blank strings and poorly formattedvalues to an empty value of your choosing. This is controlled by the ``na``keyword argument to all export functions. The default for CSV is an emptystring (''), and for all other exports, it is 'nan' which converts empty/badvalues to ``float('nan')``. *NOTE* The exception here is that float/intcolumns always use ``float('nan')`` for all missing values forDBF-&gt;SQL-&gt;DataFrame conversion purposes. Pandas has very powerful functionsfor `working with missing data`_, including converting NaN to other values(e.g.  empty strings).         To CSV++++++Use the ``to_csv`` method to export the data to a CSV file. This methodrequires the name of a CSV file as an input. The default behavior is to appendnew data to an existing file, so be careful if the file already exists. The``chunksize`` keyword argument controls the frequency that  the file bufferwill be flushed, which may not be necessary. The ``na`` keyword changes thevalue used for missing/bad entries (default is ''). The keyword ``header`` isa boolean that controls writing of the column names as the first row of theCSV file. The encoding of the resulting CSV file is determined by the codecthat is set when opening the DBF file, see `Loading`_. .. code::    In : dbf = Dbf5('fake_file_name.dbf')    In : dbf.to_csv('junk.csv')If you are unhappy with the default CSV output of this module, Pandas also hasvery `powerful CSV export capabilities`_ for DataFrames.To SQL (CSV-based)++++++++++++++++++Most SQL databases can create tables directly from local CSV files. Thepure-Python ``to_textsql`` method creates two files: 1) a header-less CSV filecontaining the DBF contents, and 2) a SQL file containing the appropriatetable creation and CSV import code. It is up to you to run the SQL file as aseparate step. This function takes two mandatory arguments, which are simplythe names of the SQL and CSV files, respectively. In addition, there are anumber of optional keyword arguments as well. ``sqltype`` controls the outputdialect. The default is 'sqlite', but 'postgres' is also accepted.  ``table``sets the name of the SQL table that will be created. By default, this will bethe name of the DBF file without the file extension. You should escape quotecharacters (&quot;) in the CSV file. This is controlled with the ``escapeqoute``keyword, which defaults to ``'&quot;'``. (This changes '&quot;' in text strings to '&quot;&quot;',which the SQL server should ignore.) The ``chunksize``, ``na``, and ``header``keywords are used to control the CSV file. See above.Here's an example for SQLite:.. code::    In : dbf = Dbf5('fake_file_name.dbf')    In : dbf.to_textsql('junk.sql', 'junk.csv')    # Exit Python    $ sqlite3 junk.db &lt; junk.sqlHere's an example for Postgresql:.. code::    In : dbf = Dbf5('fake_file_name.dbf')    In : dbf.to_textsql('junk.sql', 'junk.csv', sqltype='postgres')    # Exit Python    $ psql -U username -f junk.sql db_nameTo DataFrame ++++++++++++The ``to_dataframe`` method returns the DBF records as a Pandas DataFrame.  Ifthe size of the DBF file exceeds available memory, then passing the``chunksize`` keyword argument will return a generator function. Thisgenerator yields DataFrames of len(&lt;=chunksize) until all of the records havebeen processed. The ``na`` keyword changes the value used for missing/badentries (default is 'nan' which inserts ``float('nan')``)... code::    In : dbf = Dbf5('fake_file_name.dbf')    In : df = dbf.to_dataframe()    # df is a DataFrame with all records    In : dbf = Dbf5('fake_file_name.dbf')    In : for df in dbf.to_dataframe(chunksize=10000)    ....     do_cool_stuff(df)    # Here a generator is returned.. _chunksize issue:Issue with DataFrame Chunksize++++++++++++++++++++++++++++++When a DataFrame is constructed, it attempts to determine the dtype of eachcolumn. If you chunk the DataFrame output, it turns out that the dtype for acolumn can change. For example, if one chunk has a column with all strings,the dtype will be ``np.object``; however, if in the next chunk that samecolumn is full of ``float('nan')``, the resulting dtype will be set as``float``. This has some consequences for writing to SQL and HDF tables aswell. In principle, this behavior could be changed, but it is currentlynon-trivial to set the dtypes for DataFrame columns on construction. Pleasefile a PR through GitHub if this is a big problem.To an SQL Table using Pandas++++++++++++++++++++++++++++The ``to_pandassql`` method will transfer the DBF entries to an SQL databasetable of your choice using a combination of Pandas DataFrames and SQLalchemy.A valid `SQLalchemy engine string`_ argument is required to connect with thedatabase. Database support will be limited to those supported by SQLalchemy.(This has been tested with SQLite and Postgresql.) Note, if you aretransferring a large amount of data, this method will be very slow. If youhave direct access to the SQL server, you might want to use the text-based SQLexport instead... code::    In : dbf = Dbf5('fake_file_name.dbf')    In : dbf.to_pandassql('sqlite:///foo.db')This method accepts three optional arguments. ``table`` is the name of thetable you'd like to use. If this is not passed, your new table will have thesame name as the DBF file without file extension. Again, the default here isto append to an existing table. If you want to start fresh, delete theexisting table before using this function. The ``chunksize`` keyword processesthe DBF file in chunks of records no larger than this size. The ``na`` keywordchanges the value used for missing/bad entries (default is 'nan' which inserts``float('nan')``)... code::    In : dbf = Dbf5('fake_file_name.dbf')    In : dbf.to_pandassql('sqlite:///foo.db', table=&quot;fake_tbl&quot;,    ....                    chunksize=100000)    To an HDF5 Table++++++++++++++++The ``to_pandashdf`` method transfers the DBF entries to an HDF5 table of yourchoice. This method uses a combination of Pandas DataFrames and PyTables, soboth of these packages must be installed. This method requires a file namestring for the HDF file, which will be created if it does not exist.  Again,the default behavior is to append to an existing file of that name, so becareful here.  The HDF file will be created using the highest level ofcompression (9) with the 'blosc' compression lib. This saves an enormousamount of disk space, with little degradation of performance; however, thiscompression library is non-standard, which can cause problems with other HDFlibraries. Compression options are controlled use the ``complib`` and``complevel`` keyword arguments, which are identical to the ones described inthe `Pandas HDF compression docs`_... code::    In : dbf = Dbf5('fake_file_name.dbf')    In : dbf.to_pandashdf('fake.h5')This method uses the same optional arguments, and corresponding defaults, as``to_pandassql`` (see above). A example with ``chunksize`` is shown below. Inaddition, a ``data_columns`` keyword argument is also available, which setsthe columns that will be used as data columns in the HDF table. Data columnscan be used for advanced searching and selection; however, there is somedegredation of preformance for large numbers of data columns. See the `Pandasdata columns docs`_ for a more detailed explanation... code::    In : dbf = Dbf5('fake_file_name.dbf')    In : dbf.to_pandashdf('fake.h5', table=&quot;fake_tbl&quot;, chunksize=100000)See the `chunksize issue`_ for DataFrame export for information on a potentialproblem you may encounter with chunksize.Batch Export++++++++++++Batch file export is trivial using *simpledbf*. For example, the followingcode processes all DBF files in the current directory into separate tables ina single HDF file... code::     In : import os    In : from simpledbf import Dbf5    In : files = os.listdir('.')    In : for f in files:    ....     if f[-3:].lower() == 'dbf':    ....         dbf = Dbf5(f)    ....         dbf.to_pandashdf('all_data.h5')   .. External Hyperlinks.. _ActiveState DBF example: http://code.activestate.com/recipes/        362715-dbf-reader-and-writer/.. _GitHub repo: https://github.com/rnelsonchem/simpledbf.. _dBase III through 5: http://ulisse.elettra.trieste.it/services/doc/        dbase/DBFstruct.htm.. _DBF version 7: http://www.dbase.com/KnowledgeBase/int/db7_file_fmt.htm.. _Anaconda Python distribution: http://continuum.io/downloads.. _limitation in Pandas HDF export with unicode: http://pandas.pydata.org/        pandas-docs/stable/io.html#datatypes.. _codec standard library module: https://docs.python.org/3.4/library/        codecs.html .. _working with missing data: http://pandas.pydata.org/pandas-docs/stable/        missing_data.html.. _powerful CSV export capabilities: http://pandas.pydata.org/pandas-docs/        stable/io.html#writing-to-csv-format.. _SQLalchemy engine string: http://docs.sqlalchemy.org/en/rel_0_9/core/        engines.html.. _Pandas HDF compression docs: http://pandas.pydata.org/pandas-docs/stable/        io.html#compression.. _Pandas data columns docs: http://pandas.pydata.org/pandas-docs/stable/        io.html#query-via-data-columns</longdescription>
</pkgmetadata>