<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># ImageReward&lt;p align=&quot;center&quot;&gt;   ü§ó &lt;a href=&quot;https://huggingface.co/THUDM/ImageReward&quot; target=&quot;_blank&quot;&gt;HF Repo&lt;/a&gt; ‚Ä¢ üê¶ &lt;a href=&quot;https://twitter.com/thukeg&quot; target=&quot;_blank&quot;&gt;Twitter&lt;/a&gt; ‚Ä¢ üìÉ &lt;a href=&quot;https://arxiv.org/abs/2304.05977&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt; ‚Ä¢ üñº &lt;a href=&quot;https://huggingface.co/datasets/THUDM/ImageRewardDB&quot; target=&quot;_blank&quot;&gt;Dataset&lt;/a&gt; ‚Ä¢ üåê &lt;a href=&quot;https://zhuanlan.zhihu.com/p/639494251&quot; target=&quot;_blank&quot;&gt;‰∏≠ÊñáÂçöÂÆ¢&lt;/a&gt; &lt;br&gt;&lt;/p&gt;**ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation**ImageReward is the first general-purpose text-to-image human preference RM, which is trained on in total **137k pairs of expert comparisons**, outperforming existing text-image scoring methods, such as CLIP (by 38.6%), Aesthetic (by 39.6%), and BLIP (by 31.6%), in terms of understanding human preference in text-to-image synthesis.Additionally, we introduce Reward Feedback Learning (ReFL) for direct optimizing a text-to-image diffusion model using ImageReward. ReFL-tuned Stable Diffusion wins against untuned version by 58.4% in human evaluation.Both ImageReward and ReFL are all packed up to Python `image-reward` package now![![PyPI](https://img.shields.io/pypi/v/image-reward)](https://pypi.org/project/image-reward/) [![Downloads](https://static.pepy.tech/badge/image-reward)](https://pepy.tech/project/image-reward)Try `image-reward` package in only 3 lines of code for ImageReward scoring!```python# pip install image-rewardimport ImageReward as RMmodel = RM.load(&quot;ImageReward-v1.0&quot;)rewards = model.score(&quot;&lt;prompt&gt;&quot;, [&quot;&lt;img1_obj_or_path&gt;&quot;, &quot;&lt;img2_obj_or_path&gt;&quot;, ...])```Try `image-reward` package in only 4 lines of code for ReFL fine-tuning!```python# pip install image-reward# pip install diffusers==0.16.0 accelerate==0.16.0 datasets==2.11.0from ImageReward import ReFLargs = ReFL.parse_args()trainer = ReFL.Trainer(&quot;CompVis/stable-diffusion-v1-4&quot;, &quot;data/refl_data.json&quot;, args=args)trainer.train(args=args)```If you find `ImageReward`'s open-source effort useful, please üåü us to encourage our following developement!&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;figures/ImageReward.jpg&quot; width=&quot;700px&quot;&gt;&lt;/p&gt;- [ImageReward](#imagereward)  - [Quick Start](#quick-start)    - [Install Dependency](#install-dependency)    - [Example Use](#example-use)  - [ReFL](#refl)    - [Install Dependency](#install-dependency-1)    - [Example Use](#example-use-1)  - [Demos of ImageReward and ReFL](#demos-of-imagereward-and-refl)  - [Training code for ImageReward](#training-code-for-imagereward)  - [Integration into Stable Diffusion Web UI](#integration-into-stable-diffusion-web-ui)    - [Features](#features)      - [Score generated images and append to image information](#score-generated-images-and-append-to-image-information)        - [Usage:](#usage)        - [Demo video:](#demo-video)      - [Automatically filter out images with low scores](#automatically-filter-out-images-with-low-scores)        - [Usage:](#usage-1)        - [Demo video:](#demo-video-1)      - [View the scores of images that have been scored](#view-the-scores-of-images-that-have-been-scored)        - [Usage:](#usage-2)        - [Example:](#example)      - [Other Features](#other-features)        - [Memory Management](#memory-management)  - [Reproduce Experiments in Table 1](#reproduce-experiments-in-table-1)  - [Reproduce Experiments in Table 3](#reproduce-experiments-in-table-3)  - [Citation](#citation)## Quick Start### Install DependencyWe have integrated the whole repository to a single python package `image-reward`. Following the commands below to prepare the environment:```shell# Clone the ImageReward repository (containing data for testing)git clone https://github.com/THUDM/ImageReward.gitcd ImageReward# Install the integrated package `image-reward`pip install image-reward```### Example UseWe provide example images in the [`assets/images`](assets/images) directory of this repo. The example prompt is:```texta painting of an ocean with clouds and birds, day time, low depth field effect```Use the following code to get the human preference scores from ImageReward:```pythonimport osimport torchimport ImageReward as RMif __name__ == &quot;__main__&quot;:    prompt = &quot;a painting of an ocean with clouds and birds, day time, low depth field effect&quot;    img_prefix = &quot;assets/images&quot;    generations = [f&quot;{pic_id}.webp&quot; for pic_id in range(1, 5)]    img_list = [os.path.join(img_prefix, img) for img in generations]    model = RM.load(&quot;ImageReward-v1.0&quot;)    with torch.no_grad():        ranking, rewards = model.inference_rank(prompt, img_list)        # Print the result        print(&quot;\nPreference predictions:\n&quot;)        print(f&quot;ranking = {ranking}&quot;)        print(f&quot;rewards = {rewards}&quot;)        for index in range(len(img_list)):            score = model.score(prompt, img_list[index])            print(f&quot;{generations[index]:&gt;16s}: {score:.2f}&quot;)```The output should be like as follow (the exact numbers may be slightly different depending on the compute device):```Preference predictions:ranking = [1, 2, 3, 4]rewards = [[0.5811622738838196], [0.2745276093482971], [-1.4131819009780884], [-2.029569625854492]]          1.webp: 0.58          2.webp: 0.27          3.webp: -1.41          4.webp: -2.03```## ReFL### Install Dependency```shellpip install diffusers==0.16.0 accelerate==0.16.0 datasets==2.11.0```### Example UseWe provide example dataset for ReFL in the [`data/refl_data.json`](data/refl_data.json) of this repo. Run ReFL as following:```shellbash scripts/train_refl.sh```## Demos of ImageReward and ReFL&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;figures/Demo.jpg&quot; width=&quot;700px&quot;&gt;&lt;/p&gt;## Training code for ImageReward1. Download data: üñº &lt;a href=&quot;https://huggingface.co/datasets/THUDM/ImageRewardDB&quot; target=&quot;_blank&quot;&gt;Dataset&lt;/a&gt;.2. Make dataset.```shellcd trainpython src/make_dataset.py```3. Set training config: [`train/src/config/config.yaml`](train/src/config/config.yaml)4. One command to train.```shellbash scripts/train_one_node.sh```## Integration into [Stable Diffusion Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)We have developed a **custom script** to integrate ImageReward into SD Web UI for a convenient experience.The script is located at [`sdwebui/image_reward.py`](sdwebui/image_reward.py) in this repository.The **usage** of the script is described as follows:1. **Install**: put the custom script into the [`stable-diffusion-webui/scripts/`](https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/master/scripts) directory2. **Reload**: restart the service, or click the **&quot;Reload custom script&quot;** button at the bottom of the settings tab of SD Web UI. (If the button can't be found, try clicking the **&quot;Show all pages&quot;** button at the bottom of the left sidebar.)3. **Select**: go back to the **&quot;txt2img&quot;/&quot;img2img&quot;** tab, and select **&quot;ImageReward - generate human preference scores&quot;** from the &quot;**Script&quot;** dropdown menu in the lower left corner.4. **Run**: the specific usage varies depending on the functional requirements, as described in the **&quot;Features&quot;** section below.### Features#### Score generated images and append to image information##### Usage:1. **Do not** check the &quot;Filter out images with low scores&quot; checkbox.2. Click the **&quot;Generate&quot;** button to generate images.3. Check the ImageReward at the **bottom** of the image information **below the gallery**.##### Demo video:https://github.com/THUDM/ImageReward/assets/98524878/9d8a036d-1583-4978-aac7-4b758edf9b89#### Automatically filter out images with low scores##### Usage:1. Check the **&quot;Filter out images with low scores&quot;** checkbox.2. Enter the score lower limit in **&quot;Lower score limit&quot;**. (ImageReward roughly follows the standard normal distribution, with a mean of 0 and a variance of 1.)3. Click the **&quot;Generate&quot;** button to generate images.4. Images with scores below the lower limit will be automatically filtered out and **will not appear in the gallery**.5. Check the ImageReward at the **bottom** of the image information **below the gallery**.##### Demo video:https://github.com/THUDM/ImageReward/assets/98524878/b9f01629-87d6-4c92-9990-fe065711b9c6#### View the scores of images that have been scored##### Usage:1. Upload the scored image file in the **&quot;PNG Info&quot;** tab2. Check the image information on the right with the score of the image at the **bottom**.##### Example:&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://user-images.githubusercontent.com/98524878/233829640-12190bff-f62b-4160-b05d-29624fa83677.jpg&quot; width=&quot;700px&quot;&gt;&lt;/p&gt;#### Other Features##### Memory Management- ImageReward model will not be loaded **until first script run**.- **&quot;Reload UI&quot;** will not reload the model nor unload it, but **reuse**s the currently loaded model (if it exists).- A **&quot;Unload Model&quot;** button is provided to manually unload the currently loaded model.## Reproduce Experiments in Table 1&lt;p align=&quot;center&quot;&gt;    &lt;img alt=&quot;Table_1_in_paper&quot; src=&quot;figures/Table_1_in_paper.png&quot; width=&quot;700px&quot;&gt;&lt;/p&gt;**Note:** The experimental results are produced in an environment that satisfies:- (NVIDIA) Driver Version: 515.86.01- CUDA Version: 11.7- `torch` Version: 1.12.1+cu113According to our own reproduction experience, reproducing this experiment in other environments may cause the last decimal place to fluctuate, typically within a range of ¬±0.1.Run the following script to automatically download data, baseline models, and run experiments:```bashbash ./scripts/test-benchmark.sh```Then you can check the results in **`benchmark/results/` or the terminal**.If you want to check the raw data files individually:- Test prompts and corresponding human rankings for images are located in [`benchmark/benchmark-prompts.json`](benchmark/benchmark-prompts.json).- Generated outputs for each prompt (originally from [DiffusionDB](https://github.com/poloclub/diffusiondb)) can be downloaded from [Hugging Face](https://huggingface.co/THUDM/ImageReward/tree/main/generations) or [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/d/8048c335cb464220b663/).    - Each `&lt;model_name&gt;.zip` contains a directory of the same name, in which there are in total 1000 images generated from 100 prompts of 10 images each.    - Every `&lt;model_name&gt;.zip` should be decompressed into `benchmark/generations/` as directory `&lt;model_name&gt;` that contains images.## Reproduce Experiments in Table 3&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;figures/Table_3_in_paper.png&quot; width=&quot;700px&quot;&gt;&lt;/p&gt;Run the following script to automatically download data, baseline models, and run experiments:```bashbash ./scripts/test.sh```If you want to check the raw data files individually:* Test prompts and corresponding human rankings for images are located in [`data/test.json`](data/test.json).* Generated outputs for each prompt (originally from [DiffusionDB](https://github.com/poloclub/diffusiondb)) can be downloaded from [Hugging Face](https://huggingface.co/THUDM/ImageReward/blob/main/test_images.zip) or [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/9bd245027652422499f4/?dl=1). It should be decompressed to `data/test_images`.## Citation```@misc{xu2023imagereward,      title={ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation},      author={Jiazheng Xu and Xiao Liu and Yuchen Wu and Yuxuan Tong and Qinkai Li and Ming Ding and Jie Tang and Yuxiao Dong},      year={2023},      eprint={2304.05977},      archivePrefix={arXiv},      primaryClass={cs.CV}}```</longdescription>
</pkgmetadata>