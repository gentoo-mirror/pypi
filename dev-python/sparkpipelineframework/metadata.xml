<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![Build and Test](https://github.com/imranq2/SparkPipelineFramework/workflows/Build%20and%20Test/badge.svg)![Upload Python Package](https://github.com/imranq2/SparkPipelineFramework/workflows/Upload%20Python%20Package/badge.svg)[![Known Vulnerabilities](https://snyk.io/test/github/imranq2/SparkPipelineFramework/badge.svg?targetFile=requirements.txt)](https://snyk.io/test/github/imranq2/SparkPipelineFramework?targetFile=requirements.txt)# SparkPipelineFrameworkSparkPipelineFramework implements a few design patterns to make it easier to create Spark applications that:1. Separate data transformation logic from the pipeline execution code so you can compose pipelines by just stringing together transformers.  (Based on the SparkML Pipeline class but enhanced to work for both ML and non-ML transformations)2. Enables running SQL transformations without writing any code3. Enables versioning of transformations so different pipelines can use older or newer versions of each transformer.  This enables you to upgrade each pipeline at your own choice4. Enables Autocomplete of transformations when creating pipelines (in PyCharm).5. Implement many separation-of-concerns e.g., logging, performance monitoring, error reporting6. Supports both non-ML, ML and mixed workloads7. Has an additional(optional) library SparkAutoMapper(https://github.com/icanbwell/SparkAutoMapper) that enables data engineers and data analysts to easily map data without writing code.8. Has an additional(optional) library SparkPipelineFramework.Testing (https://github.com/icanbwell/SparkPipelineFramework.Testing) that allows you to create unit tests without writing code; you just put in the input file and confirm the output file# PyPi PackageThis code is available as a package to import into your project.https://pypi.org/project/sparkpipelineframework/# Using it in your project(For an example project that uses SparkPipelineFramework, see https://github.com/imranq2/TestSparkPipelineFramework)1. Add sparkpipelineframework package to your project requirements.txt/Pipefile2. make init. (this will setup Spark, Docker (to run Spark) )3. Create a folder called library in your project## using PycharmYou can run SparkPipelineFrame project from Pycharm1. Add a new Docker Compose interpreter 2. Choose `docker-compose.yml` for the configuration file3. Choose `dev` for the Service 4. Click OK and give Pycharm a couple of minutes to index the content of the docker container5. Right click on the Test folder and click &quot;Run 'pytest in tests'&quot;## To create a new pipeline1. Create a class derived from ```FrameworkPipeline```2. In your __init__ function set self.transformers to the list of transformers to run for this pipeline.For example:```class MyPipeline(FrameworkPipeline):    def __init__(self, parameters: Dict[str, Any], progress_logger: ProgressLogger):        super().__init__(parameters=parameters,                                         progress_logger=progress_logger)        self.transformers = self.create_steps([            FrameworkCsvLoader(                view=&quot;flights&quot;,                path_to_csv=parameters[&quot;flights_path&quot;]            ),            FeaturesCarriers(parameters=parameters).transformers,        ])```## To Add a SQL transformation1. Create a new folder and a .sql file in that folder. This folder should be in the library folder or any subfolder you choose under the library folder.2. The name of the file is the name of the view that will be created/updated to store the result of your sql code e.g., carriers.sql means we will create/update a view called carriers with the results of your sql code.2. Add your sql to it.  This can be any valid SparkSQL and can refer to any view created by the pipeline before this transformer is run.  For example:```SELECT carrier, crsarrtime FROM flights```2. Run the generate_proxies command as shown in the Generating Proxies section below3. Now go to your Pipeline class __init__ and add to self.transformers.  Start the folder name and hit ctrl-space for PyCharm to autocomplete the name4. That's it.  Your sql has been automaticaly wrapped in a Transformer which will do logging, monitor performance and do error checking## To Add a Python transformation1. Create a new folder and .py file in that folder.  This folder should be in the library folder or any subfolder you choose under the library folder.2. In the .py file, create a new class and derive from Transformer (from spark ML).  Implement the _transform() functionFor example:```from typing import Optional, Dict, Anyfrom pyspark import keyword_onlyfrom pyspark.sql.dataframe import DataFramefrom spark_pipeline_framework.progress_logger.progress_logger import ProgressLoggerfrom spark_pipeline_framework.proxy_generator.python_proxy_base import PythonProxyBaseclass FeatureTransformer(PythonProxyBase):    # noinspection PyUnusedLocal    @capture_parameters    def __init__(self,                 name: str = None,                 parameters: Optional[Dict[str, Any]] = None,                 progress_logger: Optional[ProgressLogger] = None,                 verify_count_remains_same: bool = False                 ) -&gt; None:        super(FeatureTransformer, self).__init__(name=name,                                                 parameters=parameters,                                                 progress_logger=progress_logger,                                                 verify_count_remains_same=verify_count_remains_same)    def _transform(self, df: DataFrame) -&gt; DataFrame:        pass```3. Run the generate_proxies command as shown in the Generating Proxies section below3. Now go to your Pipeline class __init__ and add to self.transformers.  Start the folder name and hit ctrl-space for PyCharm to autocomplete the name## To Add a Machine Learning training transformation (called ```fit``` or ```Estimator``` in SparkML lingo)1. Create a new folder and .py file in that folder.  This folder should be in the library folder or any subfolder you choose under the library folder.2. In the .py file, create a new class and derive from Estimator (from spark ML).  Implement the fit() function3. Run the generate_proxies command as shown in the Generating Proxies section below3. Now go to your Pipeline class __init__ and add to self.estimators.  Start the folder name and hit ctrl-space for PyCharm to autocomplete the name## To Add a Machine Learning prediction transformation1. Create a new folder and .py file in that folder.  This folder should be in the library folder or any subfolder you choose under the library folder.2. In the .py file, create a new class and derive from Estimator (from spark ML).  Implement the _transform() function.  Note that that can be the same class you use for training and prediction.3. Run the generate_proxies command as shown in the Generating Proxies section below3. Now go to your Pipeline class __init__ and add to self.transformers.  Start the folder name and hit ctrl-space for PyCharm to autocomplete the name## Including pipelines in other pipelinesPipelines are fully composable so you can include one pipeline as a transformer in another pipeline.For example:```class MyPipeline(FrameworkPipeline):    def __init__(self, parameters: Dict[str, Any], progress_logger: ProgressLogger):        super(MyPipeline, self).__init__(parameters=parameters,                                         progress_logger=progress_logger)        self.transformers = self.create_steps([            FrameworkCsvLoader(                view=&quot;flights&quot;,                path_to_csv=parameters[&quot;flights_path&quot;]            ),            PipelineFoo(parameters=parameters).transformers,            FeaturesCarriers(parameters=parameters).transformers,        ])```## Generating Proxies1. Run the following command to generate proxy classes.  These automatically wrap your sql with a Spark Transformer that can be included in a Pipeline with no additional code. ```python3 spark_pipeline_framework/proxy_generator/generate_proxies.py```.  You can also add this to your project Makefile to make it easier to run: ```.PHONY:proxiesproxies:python3 spark_pipeline_framework/proxy_generator/generate_proxies.py  ```# Testing## Test a pipelineA pipeline can be tested by providing test data in csv (or parquet), running the pipeline and then asserting for data in any view or dataframe.```from pathlib import Pathfrom typing import Dict, Anyfrom pyspark.sql.dataframe import DataFramefrom pyspark.sql.session import SparkSessionfrom pyspark.sql.types import StructTypefrom library.features.carriers.v1.features_carriers_v1 import FeaturesCarriersV1from library.features.carriers_python.v1.features_carriers_python_v1 import FeaturesCarriersPythonV1from spark_pipeline_framework.pipelines.framework_pipeline import FrameworkPipelinefrom spark_pipeline_framework.progress_logger.progress_logger import ProgressLoggerfrom spark_pipeline_framework.transformers.framework_csv_loader import FrameworkCsvLoaderfrom spark_pipeline_framework.utilities.flattener import flattenclass MyPipeline(FrameworkPipeline):    def __init__(self, parameters: Dict[str, Any], progress_logger: ProgressLogger):        super(MyPipeline, self).__init__(parameters=parameters,                                         progress_logger=progress_logger)        self.transformers = self.create_steps([            FrameworkCsvLoader(                view=&quot;flights&quot;,                path_to_csv=parameters[&quot;flights_path&quot;],                progress_logger=progress_logger            ),            FeaturesCarriersV1(parameters=parameters, progress_logger=progress_logger).transformers,            FeaturesCarriersPythonV1(parameters=parameters, progress_logger=progress_logger).transformers        ])def test_can_run_framework_pipeline(spark_session: SparkSession) -&gt; None:    # Arrange    data_dir: Path = Path(__file__).parent.joinpath('./')    flights_path: str = f&quot;file://{data_dir.joinpath('flights.csv')}&quot;    schema = StructType([])    df: DataFrame = spark_session.createDataFrame(        spark_session.sparkContext.emptyRDD(), schema)    spark_session.sql(&quot;DROP TABLE IF EXISTS default.flights&quot;)    # Act    parameters = {        &quot;flights_path&quot;: flights_path    }    with ProgressLogger() as progress_logger:        pipeline: MyPipeline = MyPipeline(parameters=parameters, progress_logger=progress_logger)        transformer = pipeline.fit(df)        transformer.transform(df)    # Assert    result_df: DataFrame = spark_session.sql(&quot;SELECT * FROM flights2&quot;)    result_df.show()    assert result_df.count() &gt; 0```## Testing a single Transformer directlyEach Transformer can be tested individually by setting up the data to pass into it (e.g., loading from csv) and then testing the result of running the transformer.```from pathlib import Pathfrom pyspark.sql.dataframe import DataFramefrom pyspark.sql.session import SparkSessionfrom pyspark.sql.types import StructTypefrom spark_pipeline_framework.transformers.framework_csv_loader import FrameworkCsvLoaderfrom spark_pipeline_framework.utilities.attr_dict import AttrDictfrom library.features.carriers.v1.features_carriers_v1 import FeaturesCarriersV1def test_carriers_v1(spark_session: SparkSession):    # Arrange    data_dir: Path = Path(__file__).parent.joinpath('./')    flights_path: str = f&quot;file://{data_dir.joinpath('flights.csv')}&quot;    schema = StructType([])    df: DataFrame = spark_session.createDataFrame(        spark_session.sparkContext.emptyRDD(), schema)    spark_session.sql(&quot;DROP TABLE IF EXISTS default.flights&quot;)    FrameworkCsvLoader(        view=&quot;flights&quot;,        path_to_csv=flights_path    ).transform(dataset=df)    parameters = {}    FeaturesCarriersV1(parameters=parameters).transformers[0].transform(dataset=df)    result_df: DataFrame = spark_session.sql(&quot;SELECT * FROM flights2&quot;)    result_df.show()    assert result_df.count() &gt; 0```# ContributingRun ```make init```This will install Java, Scala, Spark and other packages# Publishing a new package1. Create a new release3. The GitHub Action should automatically kick in and publish the package4. You can see the status in the Actions tab</longdescription>
</pkgmetadata>