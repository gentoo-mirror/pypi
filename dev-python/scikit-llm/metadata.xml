<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;  &lt;img src=&quot;https://github.com/iryna-kondr/scikit-llm/blob/main/logo.png?raw=true&quot; max-height=&quot;200&quot;/&gt;&lt;/p&gt;# Scikit-LLM: Sklearn Meets Large Language ModelsSeamlessly integrate powerful language models like ChatGPT into scikit-learn for enhanced text analysis tasks.## Installation üíæ```bashpip install scikit-llm```## Support us ü§ùYou can support the project in the following ways:- ‚≠ê Star Scikit-LLM on GitHub (click the star button in the top right corner)- üí° Provide your feedback or propose ideas in the [issues](https://github.com/iryna-kondr/scikit-llm/issues) section or [Discord](https://discord.gg/YDAbwuWK7V)- üì∞ Post about Scikit-LLM on LinkedIn or other platforms- üîó Check out our other projects (cards below are clickable):&lt;a href=&quot;https://github.com/OKUA1/agent_dingo&quot;&gt;&lt;img src=&quot;https://gist.githubusercontent.com/OKUA1/6264a95a8abd225c74411a2b707b0242/raw/1b231aab718fcab624faa33d9c10d0eee17ca160/dingo_light.svg&quot;/&gt;&lt;/a&gt; &lt;br&gt;&lt;a href=&quot;https://github.com/OKUA1/falcon&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/gist/OKUA1/6264a95a8abd225c74411a2b707b0242/raw/3cedb53538cb04656cd9d7d07e697e726896ce9f/falcon_light.svg&quot;/&gt;&lt;/a&gt;## Documentation üìö### Configuring OpenAI API KeyAt the moment the majority of the Scikit-LLM estimators are only compatible with some of the OpenAI models. Hence, a user-provided OpenAI API key is required.```pythonfrom skllm.config import SKLLMConfigSKLLMConfig.set_openai_key(&quot;&lt;YOUR_KEY&gt;&quot;)SKLLMConfig.set_openai_org(&quot;&lt;YOUR_ORGANISATION&gt;&quot;)```**Important notice:**- If you have a free trial OpenAI account, the [rate limits](https://platform.openai.com/docs/guides/rate-limits/overview) are not sufficient (specifically 3 requests per minute). Please switch to the &quot;pay as you go&quot; plan first.- When calling `SKLLMConfig.set_openai_org`, you have to provide your organization ID and **NOT** the name. You can find your ID [here](https://platform.openai.com/account/org-settings).### Using Azure OpenAI```pythonfrom skllm.config import SKLLMConfigSKLLMConfig.set_openai_key(&quot;&lt;YOUR_KEY&gt;&quot;)  # use azure key insteadSKLLMConfig.set_azure_api_base(&quot;&lt;API_BASE&gt;&quot;)# start with &quot;azure::&quot; prefix when setting the model namemodel_name = &quot;azure::&lt;model_name&gt;&quot;# e.g. ZeroShotGPTClassifier(openai_model=&quot;azure::gpt-3.5-turbo&quot;)```Note: Azure OpenAI is not supported by the preprocessors at the moment.### Using GPT4ALLIn addition to OpenAI, some of the models can use [gpt4all](https://gpt4all.io/index.html) as a backend.**This feature is considered higly experimental!**In order to use gpt4all, you need to install the corresponding submodule:```bashpip install &quot;scikit-llm[gpt4all]&quot;```In order to switch from OpenAI to GPT4ALL model, simply provide a string of the format `gpt4all::&lt;model_name&gt;` as an argument. While the model runs completely locally, the estimator still treats it as an OpenAI endpoint and will try to check that the API key is present. You can provide any string as a key.```pythonSKLLMConfig.set_openai_key(&quot;any string&quot;)SKLLMConfig.set_openai_org(&quot;any string&quot;)ZeroShotGPTClassifier(openai_model=&quot;gpt4all::ggml-model-gpt4all-falcon-q4_0.bin&quot;)```When running for the first time, the model file will be downloaded automatially.When using gpt4all please keep the following in mind:1. Not all gpt4all models are commercially licensable, please consult gpt4all website for more details.2. The accuracy of the models may be much lower compared to ones provided by OpenAI (especially gpt-4).3. Not all of the available models were tested, some may not work with scikit-llm at all.### Supported models by a non-standard backendAt the moment only the following estimators support non-standard backends (gpt4all, azure):- `ZeroShotGPTClassifier`- `MultiLabelZeroShotGPTClassifier`- `FewShotGPTClassifier`### Zero-Shot Text ClassificationOne of the powerful ChatGPT features is the ability to perform text classification without being re-trained. For that, the only requirement is that the labels must be descriptive.We provide a class `ZeroShotGPTClassifier` that allows to create such a model as a regular scikit-learn classifier.Example 1: Training as a regular classifier```pythonfrom skllm import ZeroShotGPTClassifierfrom skllm.datasets import get_classification_dataset# demo sentiment analysis dataset# labels: positive, negative, neutralX, y = get_classification_dataset()clf = ZeroShotGPTClassifier(openai_model=&quot;gpt-3.5-turbo&quot;)clf.fit(X, y)labels = clf.predict(X)```Scikit-LLM will automatically query the OpenAI API and transform the response into a regular list of labels.Additionally, Scikit-LLM will ensure that the obtained response contains a valid label. If this is not the case, a label will be selected randomly (label probabilities are proportional to label occurrences in the training set).Example 2: Training without labeled dataSince the training data is not strictly required, it can be fully ommited. The only thing that has to be provided is the list of candidate labels.```pythonfrom skllm import ZeroShotGPTClassifierfrom skllm.datasets import get_classification_datasetX, _ = get_classification_dataset()clf = ZeroShotGPTClassifier()clf.fit(None, [&quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;])labels = clf.predict(X)```**Note:** unlike in a typical supervised setting, the performance of a zero-shot classifier greatly depends on how the label itself is structured. It has to be expressed in natural language, be descriptive and self-explanatory. For example, in the previous semantic classification task, it could be beneficial to transform a label from `&quot;&lt;semantics&gt;&quot;` to `&quot;the semantics of the provided text is &lt;semantics&gt;&quot;`.### Multi-Label Zero-Shot Text ClassificationWith a class `MultiLabelZeroShotGPTClassifier` it is possible to perform the classification in multi-label setting, which means that each sample might be assigned to one or several distinct classes.Example:```pythonfrom skllm import MultiLabelZeroShotGPTClassifierfrom skllm.datasets import get_multilabel_classification_datasetX, y = get_multilabel_classification_dataset()clf = MultiLabelZeroShotGPTClassifier(max_labels=3)clf.fit(X, y)labels = clf.predict(X)```Similarly to the `ZeroShotGPTClassifier` it is sufficient if only candidate labels are provided. However, this time the classifier expects `y` of a type `List[List[str]]`.```pythonfrom skllm import MultiLabelZeroShotGPTClassifierfrom skllm.datasets import get_multilabel_classification_datasetX, _ = get_multilabel_classification_dataset()candidate_labels = [    &quot;Quality&quot;,    &quot;Price&quot;,    &quot;Delivery&quot;,    &quot;Service&quot;,    &quot;Product Variety&quot;,    &quot;Customer Support&quot;,    &quot;Packaging&quot;,    &quot;User Experience&quot;,    &quot;Return Policy&quot;,    &quot;Product Information&quot;,]clf = MultiLabelZeroShotGPTClassifier(max_labels=3)clf.fit(None, [candidate_labels])labels = clf.predict(X)```### Few-Shot Text ClassificationWith `FewShotGPTClassifier` it is possible to perform a few-shot classification, which means that the training samples will be added to prompt and passed to the model.```pythonfrom skllm import FewShotGPTClassifierfrom skllm.datasets import get_classification_datasetX, y = get_classification_dataset()clf = FewShotGPTClassifier(openai_model=&quot;gpt-3.5-turbo&quot;)clf.fit(X, y)labels = clf.predict(X)```While the api remains the same as for the zero shot classifier, there are a few things to take into account:- the &quot;training&quot; requires some labelled training data;- the training set should be small enough to fit into a single prompt (we recommend up to 10 samples per label);- because of the significantly larger prompt, the inference takes longer and consumes higher amount of tokens.Note: as the model is not being re-trained, but uses the training data during inference, one could say that this is still a (different) zero-shot approach.### Dynamic Few-Shot Text Classification_To use this feature, you need to install `annoy` library:_```bashpip install scikit-llm[annoy]````DynamicFewShotGPTClassifier` dynamically selects N samples per class to include in the prompt. This allows the few-shot classifier to scale to datasets that are too large for the standard context window of LLMs._How does it work?_During fitting, the whole dataset is partitioned by class, vectorized, and stored.During inference, the [annoy](https://github.com/spotify/annoy) library is used for fast neighbor lookup, which allows including only the most similar examples in the prompt.```pythonfrom skllm import DynamicFewShotGPTClassifierfrom skllm.datasets import get_classification_datasetX, y = get_classification_dataset()clf = DynamicFewShotGPTClassifier(n_examples=3)clf.fit(X, y)labels = clf.predict(X)```By default the classifier uses kneighbors algorithm from sklearn, which might be slow for large datasets. In this case, it is possible to switch to [annoy](https://github.com/spotify/annoy):```bashpip install scikit-llm[annoy]``````pythonfrom skllm.memory._annoy import AnnoyMemoryIndexfrom skllm.memory.base import IndexConstructorindex = IndexConstructor(AnnoyMemoryIndex)clf = DynamicFewShotGPTClassifier(memory_index=index)```### Text Classification with Google PaLM 2At the moment 3 PaLM based models are available in test mode:- `ZeroShotPaLMClassifier` - zero-shot text classification with PaLM 2;- `PaLMClassifier` - fine-tunable text classifier with PaLM 2;- `PaLM` - fine-tunable estimator that can be trained on arbitrary text input-output pairs.Example:```pythonfrom skllm.models.palm import PaLMClassifierfrom skllm.datasets import get_classification_datasetX, y = get_classification_dataset()clf = PaLMClassifier(n_update_steps=100)clf.fit(X, y)labels = clf.predict(X)```A more detailed documentation will follow soon. For now, please refer to our [official guide on Medium](https://medium.com/@iryna230520/fine-tune-google-palm-2-with-scikit-llm-d41b0aa673a5).### Text VectorizationAs an alternative to using GPT as a classifier, it can be used solely for data preprocessing. `GPTVectorizer` allows to embed a chunk of text of arbitrary length to a fixed-dimensional vector, that can be used with virtually any classification or regression model.Example 1: Embedding the text```pythonfrom skllm.preprocessing import GPTVectorizermodel = GPTVectorizer()vectors = model.fit_transform(X)```Example 2: Combining the Vectorizer with the XGBoost Classifier in a Sklearn Pipeline```pythonfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import LabelEncoderfrom xgboost import XGBClassifierle = LabelEncoder()y_train_encoded = le.fit_transform(y_train)y_test_encoded = le.transform(y_test)steps = [(&quot;GPT&quot;, GPTVectorizer()), (&quot;Clf&quot;, XGBClassifier())]clf = Pipeline(steps)clf.fit(X_train, y_train_encoded)yh = clf.predict(X_test)```### LLM Fine-TuningAt the moment the following scenarios are supported for tuning:- **Text classification**: the model is fine-tuned to predict a single label per sample. The following estimators are supported:  - `skllm.models.palm.PaLMClassifier`  - `skllm.models.gpt.GPTClassifier`- **Text to text**: the model is fine-tuned on arbitrary text input-output pairs. The following estimators are supported:  - `skllm.models.palm.PaLM`  - `skllm.models.gpt.GPT`Example 1: Fine-tuning a PaLM model for text classification```pythonfrom skllm.models.palm import PaLMClassifierclf = PaLMClassifier(n_update_steps=100)clf.fit(X_train, y_train) # y_train is a list of labelslabels = clf.predict(X_test)```Example 2: Fine-tuning a PaLM model for text to text tasks```pythonfrom skllm.models.palm import PaLMclf = PaLM(n_update_steps=100)clf.fit(X_train, y_train) # y_train is any desired output textlabels = clf.predict(X_test)```_Note:_ PaLM models tuning requires a Vertex AI account. Please refer to our [official guide on Medium](https://medium.com/@iryna230520/fine-tune-google-palm-2-with-scikit-llm-d41b0aa673a5) for more details.Example 3: Fine-tuning a GPT model for text classification```pythonfrom skllm.models.gpt import GPTClassifierclf = GPTClassifier(        base_model = &quot;gpt-3.5-turbo-0613&quot;,        n_epochs = None, # int or None. When None, will be determined automatically by OpenAI        default_label = &quot;Random&quot;, # optional)clf.fit(X_train, y_train) # y_train is a list of labelslabels = clf.predict(X_test)```Example 4: Fine-tuning a GPT model for text to text tasks```pythonfrom skllm.models.gpt import GPTCclf = GPT(        base_model = &quot;gpt-3.5-turbo-0613&quot;,        n_epochs = None, # int or None. When None, will be determined automatically by OpenAI        system_msg = &quot;You are a text processing model.&quot;)clf.fit(X_train, y_train) # y_train is any desired output textlabels = clf.predict(X_test)```### Text SummarizationGPT excels at performing summarization tasks. Therefore, we provide `GPTSummarizer` that can be used both as stand-alone estimator, or as a preprocessor (in this case we can make an analogy with a dimensionality reduction preprocessor).Example:```pythonfrom skllm.preprocessing import GPTSummarizerfrom skllm.datasets import get_summarization_datasetX = get_summarization_dataset()s = GPTSummarizer(openai_model=&quot;gpt-3.5-turbo&quot;, max_words=15)summaries = s.fit_transform(X)```Please be aware that the `max_words` hyperparameter sets a soft limit, which is not strictly enforced outside of the prompt. Therefore, in some cases, the actual number of words might be slightly higher.It is possible to generate a summary, emphasizing a specific concept, by providing an optional parameter `focus`:```pythons = GPTSummarizer(openai_model=&quot;gpt-3.5-turbo&quot;, max_words=15, focus=&quot;apples&quot;)```### Text TranslationGPT models have demonstrated their effectiveness in translation tasks by generating accurate translations across various languages. Thus, we added `GPTTranslator` that allows translating an arbitraty text into a language of interest.Example:```pythonfrom skllm.preprocessing import GPTTranslatorfrom skllm.datasets import get_translation_datasetX = get_translation_dataset()t = GPTTranslator(openai_model=&quot;gpt-3.5-turbo&quot;, output_language=&quot;English&quot;)translated_text = t.fit_transform(X)```### CitationYou can cite Scikit-LLM using the following BibTeX:```@software{ScikitLLM,  author = {Iryna Kondrashchenko and Oleh Kostromin},  year = {2023},  publisher = {beastbyte.ai},  address = {Linz, Austria},  title = {Scikit-LLM: Sklearn Meets Large Language Models},  url = {https://github.com/iryna-kondr/scikit-llm }}```</longdescription>
</pkgmetadata>