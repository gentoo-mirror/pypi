<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;h1 align=&quot;center&quot;&gt;  &lt;img src=&quot;https://avatars.githubusercontent.com/u/54333248?s=200&amp;v=4&quot;&gt;    &lt;br&gt;    Pinecone Text Client    &lt;br&gt;&lt;/h1&gt;The Pinecone Text Client is a Python package that provides text utilities designed for seamless integration with Pinecone's [sparse-dense](https://docs.pinecone.io/docs/hybrid-search) (hybrid) semantic search.&gt; **_‚ö†Ô∏è Warning_**&gt;&gt; This is a **public preview** (&quot;Beta&quot;) version.   &gt; For any issues or requests, please reach out to our [support](support@pinecone.io) team.## InstallationTo install the Pinecone Text Client, use the following command:```bashpip install pinecone-text```If you wish to use `SpladeEncoder`, you will need to install the `splade` extra:```bashpip install pinecone-text[splade]```If you wish to use `SentenceTransformerEncoder` dense encoder, you will need to install the `dense` extra:```bashpip install pinecone-text[dense]```## Sparse EncodingTo convert your own text corpus to sparse vectors, you can either use [BM25](https://www.pinecone.io/learn/semantic-search/#bm25) or [SPLADE](https://www.pinecone.io/learn/splade/).### BM25To encode your documents and queries using BM25 as vector for dot product search, you can use the `BM25Encoder` class.&gt; **_üìù NOTE:_**&gt; &gt; Our current implementation of BM25 supports only static document frequency (meaning that the document frequency values are precomputed and fixed, and do not change dynamically based on new documents added to the collection).&gt;&gt; When conducting a search, you may come across queries that contain terms not found in the training corpus but are present in the database. To address this scenario, BM25Encoder uses a default document frequency value of 1 when encoding such terms. #### UsageFor an end-to-end example, you can refer to our Quora dataset generation with BM25 [notebook](https://github.com/pinecone-io/examples/blob/master/pinecone/sparse/bm25/bm25-vector-generation.ipynb).```pythonfrom pinecone_text.sparse import BM25Encodercorpus = [&quot;The quick brown fox jumps over the lazy dog&quot;,          &quot;The lazy dog is brown&quot;,          &quot;The fox is brown&quot;]# Initialize BM25 and fit the corpusbm25 = BM25Encoder()bm25.fit(corpus)# Encode a new document (for upsert to Pinecone index)doc_sparse_vector = bm25.encode_documents(&quot;The brown fox is quick&quot;)# {&quot;indices&quot;: [102, 18, 12, ...], &quot;values&quot;: [0.21, 0.38, 0.15, ...]}# Encode a query (for search in Pinecone index)query_sparse_vector = bm25.encode_queries(&quot;Which fox is brown?&quot;)# {&quot;indices&quot;: [102, 16, 18, ...], &quot;values&quot;: [0.21, 0.11, 0.15, ...]}# store BM25 params as jsonbm25.dump(&quot;bm25_params.json&quot;)# load BM25 params from jsonbm25.load(&quot;bm25_params.json&quot;)```#### Load Default ParametersIf you want to use the default parameters for `BM25Encoder`, you can call the `default` method.The default parameters were fitted on the [MS MARCO](https://microsoft.github.io/msmarco/)  passage ranking dataset.```pythonbm25 = BM25Encoder.default()```#### BM25 ParametersThe `BM25Encoder` class offers configurable parameters to customize the encoding:* `b`: Controls document length normalization (default: 0.75).* `k1`: Controls term frequency saturation (default: 1.2).* Tokenization Options: Allows customization of the tokenization process, including options for handling case, punctuation, stopwords, stemming, and language selection.These parameters can be specified when initializing the BM25Encoder class. Please read the BM25Encoder documentation for more details.### SPLADECurrently the `SpladeEncoder` class supprts only the [naver/splade-cocondenser-ensembledistil](https://huggingface.co/naver/splade-cocondenser-ensembledistil) model, and follows [SPLADE V2](https://arxiv.org/abs/2109.10086) implementation.#### UsageFor an end-to-end example, you can refer to our Quora dataset generation with SPLADE [notebook](https://github.com/pinecone-io/examples/blob/master/pinecone/sparse/splade/splade-vector-generation.ipynb).```pythonfrom pinecone_text.sparse import SpladeEncoder# Initialize Spladesplade = SpladeEncoder()# encode a batch of documentsdocuments = [&quot;The quick brown fox jumps over the lazy dog&quot;,             &quot;The lazy dog is brown&quot;,             &quot;The fox is brown&quot;]document_vectors = splade.encode_documents(documents)# [{&quot;indices&quot;: [102, 18, 12, ...], &quot;values&quot;: [0.21, 0.38, 0.15, ...]}, ...]# encode a queryquery = &quot;Which fox is brown?&quot;query_vectors = splade.encode_queries(query)# {&quot;indices&quot;: [102, 18, 12, ...], &quot;values&quot;: [0.21, 0.38, 0.15, ...]}```## Dense EncodingFor dense embedding we also provide a thin wrapper for the following models:1. All Sentence Transformers models hosted on huggingface [See full list of models](https://huggingface.co/sentence-transformers)2. All OpenAI API supported embedding models [See full list of models](https://platform.openai.com/docs/models/embeddings)### Sentence Transformers modelsWhen using `SentenceTransformerEncoder`, the models are downloaded from huggingface and run locally.#### Usage```pythonfrom pinecone_text.dense.sentence_transformer_encoder import SentenceTransformerEncoderencoder = SentenceTransformerEncoder(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)encoder.encode_documents([&quot;The quick brown fox jumps over the lazy dog&quot;])# [[0.21, 0.38, 0.15, ...]]encoder.encode_queries([&quot;Who jumped over the lazy dog?&quot;])# [[0.11, 0.43, 0.67, ...]]```### OpenAI modelsWhen using the `OpenAIEncoder`, you need to provide an API key for the OpenAI API, and store it in the `OPENAI_API_KEY` environment variable.#### Usage```pythonfrom pinecone_text.dense.openai_encoder import OpenAIEncoderencoder = OpenAIEncoder() # defaults to the recommended model - &quot;text-embedding-ada-002&quot;encoder.encode_documents([&quot;The quick brown fox jumps over the lazy dog&quot;])# [[0.21, 0.38, 0.15, ...]]encoder.encode_queries([&quot;Who jumped over the lazy dog?&quot;])# [[0.11, 0.43, 0.67, ...]]```## Combining Sparse and Dense Encodings for Hybrid SearchTo combine sparse and dense encodings for hybrid search, you can use the `hybrid_convex_scale` method on your query.This method receives both a dense vector and a sparse vector, along with a convex scaling parameter `alpha`. It returns a tuple consisting of the scaled dense and sparse vectors according to the following formula: `alpha * dense_vector + (1 - alpha) * sparse_vector`.```pythonfrom pinecone_text.hybrid import hybrid_convex_scalefrom pinecone_text.sparse import SpladeEncoderfrom pinecone_text.dense.sentence_transformer_encoder import SentenceTransformerEncoder# Initialize Spladesplade = SpladeEncoder()# Initialize Sentence Transformersentence_transformer = SentenceTransformerEncoder(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)# encode a querysparse_vector = splade.encode_queries(&quot;Which fox is brown?&quot;)dense_vector = sentence_transformer.encode_queries(&quot;Which fox is brown?&quot;)# combine sparse and dense vectorshybrid_dense, hybrid_sparse = hybrid_convex_scale(dense_vector, sparse_vector, alpha=0.8)# ([-0.21, 0.38, 0.15, ...], {&quot;indices&quot;: [102, 16, 18, ...], &quot;values&quot;: [0.21, 0.11, 0.15, ...]})```</longdescription>
</pkgmetadata>