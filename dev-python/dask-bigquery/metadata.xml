<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Dask-BigQuery[![Tests](https://github.com/coiled/dask-bigquery/actions/workflows/tests.yml/badge.svg)](https://github.com/coiled/dask-bigquery/actions/workflows/tests.yml) [![Linting](https://github.com/coiled/dask-bigquery/actions/workflows/pre-commit.yml/badge.svg)](https://github.com/coiled/dask-bigquery/actions/workflows/pre-commit.yml)Read/write data from/to [Google BigQuery](https://cloud.google.com/bigquery) with Dask.This package uses the BigQuery Storage API. Please refer to the [data extraction pricing table](https://cloud.google.com/bigquery/pricing#data_extraction_pricing) for associated costs while using Dask-BigQuery.## Installation`dask-bigquery` can be installed with `pip`:```pip install dask-bigquery```or with `conda`:```conda install -c conda-forge dask-bigquery```## Google Cloud permissionsFor reading from BiqQuery, you need the following roles to be enabled on the account:- `BigQuery Read Session User`- `BigQuery Data Viewer`, `BigQuery Data Editor`, or `BigQuery Data Owner`Alternately, `BigQuery Admin` would give you full access to sessions and data.For writing to BigQuery, the following roles are sufficient:- `BigQuery Data Editor`- `Storage Object Creator`The minimal permissions to cover reading and writing:- `BigQuery Data Editor`- `BigQuery Read Session User`- `Storage Object Creator`## AuthenticationBy default, `dask-bigquery` will use the [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc). When running code locally, you can set this to use your user credentials by running```sh$ gcloud auth application-default login```User credentials require interactive login. For settings where this isn't possible, you'll need to create a service account. You can set the Application Default Credentials to the service account key using the `GOOGLE_APPLICATION_CREDENTIALS` environment variable:```sh$ export GOOGLE_APPLICATION_CREDENTIALS=/home/&lt;username&gt;/google.json```For information on obtaining the credentials, use [Google API documentation](https://developers.google.com/workspace/guides/create-credentials).## Example: read from BigQuery`dask-bigquery` assumes that you are already authenticated.```pythonimport dask_bigqueryddf = dask_bigquery.read_gbq(    project_id=&quot;your_project_id&quot;,    dataset_id=&quot;your_dataset&quot;,    table_id=&quot;your_table&quot;,)ddf.head()```## Example: write to BigQueryWith default credentials:```pythonimport daskimport dask_bigqueryddf = dask.datasets.timeseries(freq=&quot;1min&quot;)res = dask_bigquery.to_gbq(    ddf,    project_id=&quot;my_project_id&quot;,    dataset_id=&quot;my_dataset_id&quot;,    table_id=&quot;my_table_name&quot;,)```With explicit credentials:```pythonfrom google.oauth2.service_account import Credentials# credentialscreds_dict = {&quot;type&quot;: ..., &quot;project_id&quot;: ..., &quot;private_key_id&quot;: ...}credentials = Credentials.from_service_account_info(info=creds_dict)res = to_gbq(    ddf,    project_id=&quot;my_project_id&quot;,    dataset_id=&quot;my_dataset_id&quot;,    table_id=&quot;my_table_name&quot;,    credentials=credentials,)```Before loading data into BigQuery, `to_gbq` writes intermediary Parquet to a Google Storage bucket. Default bucket name is `&lt;your_project_id&gt;-dask-bigquery`. You can provide a diferent bucket name by setting the parameter: `bucket=&quot;my-gs-bucket&quot;`. After the job is done, the intermediary data is deleted.If you're using a persistent bucket, we recommend configuring a retention policy that ensures the data is cleaned up even in case of job failures.## Run tests locallyTo run the tests locally you need to be authenticated and have a project created on that account. If you're using a service account, when created you need to select the role of &quot;BigQuery Admin&quot; in the section &quot;Grant this service account access to project&quot;.You can run the tests with`$ pytest dask_bigquery`if your default `gcloud` project is set, or manually specify the project ID with`DASK_BIGQUERY_PROJECT_ID pytest dask_bigquery`## HistoryThis project stems from the discussion in[this Dask issue](https://github.com/dask/dask/issues/3121) and[this initial implementation](https://gist.github.com/bnaul/4819f045ccbee160b60a530b6cfc0c98#file-dask_bigquery-py)developed by [Brett Naul](https://github.com/bnaul), [Jacob Hayes](https://github.com/JacobHayes),and [Steven Soojin Kim](https://github.com/mikss).## License[BSD-3](LICENSE)</longdescription>
</pkgmetadata>