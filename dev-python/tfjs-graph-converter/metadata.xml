<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># TensorFlow.js Graph Model Converter![TFJS Graph Converter Logo](https://github.com/patlevin/tfjs-to-tf/raw/master/docs/logo.png)The purpose of this library is to import TFJS graph models into Tensorflow.This allows you to use TensorFlow.js models with Python in case you don'thave access to the original formats or the models have been created in TFJS.## DisclaimerI'm neither a Python developer, nor do I know TensorFlow or TensorFlow.js.I created this package solely because I ran into an issue when trying to converta pretrained TensorFlow.js model into a different format. I didn't have access tothe pretrained original TF model and didn't have the resources to train it myself.I soon learned that I'm not alone with this [issue](https://github.com/tensorflow/tfjs/issues/1575)so I sat down and wrote this little library.If you find any part of the code to be non-idiomatic or know of a simpler way toachieve certain things, feel free to let me know, since I'm a beginner in bothPython and especially TensorFlow (used it for the very first time in thisvery project).## Prerequisites* tensorflow 2.1+* tensorflowjs 1.5.2+## CompatibilityThe converter has been tested with tensorflowjs v3.13.0, tensorflow v2.8.0and Python 3.9.10.## Installation```shpip install tfjs-graph-converter```## UsageAfter the installation, you can run the packaged `tfjs_graph_converter` binaryfor quick and easy model conversion.### Positional Arguments | Positional Argument | Description | | :--- | :--- | | `input_path` | Path to the TFJS Graph Model directory containing the model.json | | `output_path` | For output format &quot;tf_saved_model&quot;, a SavedModel target directory. For output format &quot;tf_frozen_model&quot;, a frozen model file. |### Options| Option | Description || :--- | :--- || `-h`, `--help` | Show help message and exit || `--output_format` | Use `tf_frozen_model` (the default) to save a Tensorflow frozen model. `tf_saved_model` exports to a Tensorflow _SavedModel_ instead. || `--saved_model_tags` | Specifies the tags of the MetaGraphDef to save, in comma separated string format. Defaults to &quot;serve&quot;. Applicable only if `--output_format` is `tf_saved_model` || `-c`, `--compat_mode` | Sets a compatibility mode forthe coverted model (see below) || `-v`, `--version` | Shows the version of the converter and its dependencies. || `-s`, `--silent` | Suppresses any output besides error messages. |### Compatibility ModesModels are converted to optmimised native Tensorflow operators by default.This can cause problems if the converted model is subseuently converted toanother format (ONNX, TFLite, older TFJS versions, etc.) The `--compat_mode`option can be used to avoid incompatible native operations such as fusedconvolutions. Available options are:| Mode     | Description || :---     | :---        || `none`   | Use all available optimisations and native TF operators || `tfjs`   | Harmonise input types for compatibility with older TFJS versions || `tflite` | Only use TFLite builtins in the converted model |### Advanced OptionsThese options are intended for advanced users who are familiar with the details of TensorFlow and [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).| Option | Description | Example || :--- | :--- | :--- || `--outputs` | Specifies the outputs of the MetaGraphDef to save, in comma separated string format. Applicable only if `--output_format` is `tf_saved_model` | --outputs=Identity || `--signature_key` | Specifies the key for the signature of the MetraGraphDef. Applicable only if `--output_format` is `tf_saved_model`. Requires `--outputs` to be set. | --signature_key=serving_autoencode || `--method_name` | Specifies the method name for the signature of the MetraGraphDef. Applicable only if `--output_format` is `tf_saved_model`. Requires `--outputs` to be set. | --method_name=tensorflow/serving/classify || `--rename` | Specifies a key mapping to change the keys of outputs and inputs in the signature. The format is comma-separated pairs of _old name:new name_. Applicable only if `--output_format` is `tf_saved_model`. Requires `--outputs` to be set. | --rename Identity:scores,model/dense256/BiasAdd:confidence |Specifying ``--outputs`` can be useful for multi-head models to select the defaultoutput for the main signature. The CLI only handles the default signature ofthe model. Multiple signatures can be created using the [API](https://github.com/patlevin/tfjs-to-tf/blob/master/docs/api.rst).The method name must be handled with care, since setting the wrong value mightprevent the signature from being valid for use with TensorFlow Serving.The option is available, because the converter only generates_predict_-signatures. In case the model is a regression model or a classifierwith the matching outputs, the correct method name can be forced using the``--method_name`` option.Alternatively, you can create your own converter programs using the module's API.The API is required to accomplish more complicated tasks, like packaging multipleTensorFlow.js models into a single SavedModel.## ExampleTo convert a TensorFlow.js graph model to a TensorFlow frozen model (i.e. themost common use case?), just specify the directory containing the `model.json`,followed by the path and file name of the frozen model like so:```shtfjs_graph_converter path/to/js/model path/to/frozen/model.pb```## Advanced ExampleConverting to [TF SavedMovel format](https://www.tensorflow.org/guide/saved_model)adds a lot of options for tweaking model signatures. The following exampleconverts a [Posenet](https://github.com/tensorflow/tfjs-models/tree/master/posenet)model, which is a multi-head model.We want to select only two of the four possible outputs and rename them in themodel's signature, as follows:* Input: _input_ (from _sub_2_)* Outputs: _offsets_ and _heatmaps_ (from _float_short_offsets_ and _float_heatmaps_)```shtfjs_graph_converter \    ~/models/posenet/model-stride16.json \    ~/models/posenet_savedmodel \    --output_format tf_saved_model \    --outputs float_short_offsets,float_heatmaps \    --rename float_short_offsets:offsets,float_heatmaps:heatmaps,sub_2:input```After the conversion, we can examine the output and verify the new modelsignature:```shsaved_model_cli show --dir ~/models/posenet_savedmodel --allMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:signature_def['serving_default']:  The given SavedModel SignatureDef contains the following input(s):    inputs['input'] tensor_info:        dtype: DT_FLOAT        shape: (1, -1, -1, 3)        name: sub_2:0  The given SavedModel SignatureDef contains the following output(s):    outputs['heatmaps'] tensor_info:        dtype: DT_FLOAT        shape: (1, -1, -1, 17)        name: float_heatmaps:0    outputs['offsets'] tensor_info:        dtype: DT_FLOAT        shape: (1, -1, -1, 34)        name: float_short_offsets:0  Method name is: tensorflow/serving/predict```## Usage from within PythonThe package installs the module `tfjs_graph_converter`, which contains all thefunctionality used by the converter script.You can leverage the API to either load TensorFlow.js graph models directly foruse with your TensorFlow program (e.g. for inference, fine-tuning, or extending),or use the advanced functionality to combine several TFJS models into a single`SavedModel`.The latter is only supported using the API (it's just a single function call,though, so don't panic ðŸ˜‰)## Important NoteBy default, Python code that includes the library will see CUDA devicesdisabled (i.e. not visible in the program). This is done because the libraryuses some low-level APIs that don't allow disabling GPUs from Python code.Unfortunately some GPUs support CUDA but don't have the compute capabilities orVRAM required to convert certain models. For this reason, CUDA devices aredisabled by default and the converter and scripts using it use CPUs only.This behaviour can now be disabled by calling `enable_cuda()` **before** anyTensorflow or converter function is called. This will re-enable the use ofCUDA-capable devices, but may result in errors during modelloading/conversion depending on the installed GPU hardware.[API Documentation](./docs/modules.rst)</longdescription>
</pkgmetadata>