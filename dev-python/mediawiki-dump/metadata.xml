<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># mediawiki-dump[![PyPI](https://img.shields.io/pypi/v/mediawiki_dump.svg)](https://pypi.python.org/pypi/mediawiki_dump)[![Downloads](https://pepy.tech/badge/mediawiki_dump)](https://pepy.tech/project/mediawiki_dump)[![CI](https://github.com/macbre/mediawiki-dump/actions/workflows/tests.yml/badge.svg)](https://github.com/macbre/mediawiki-dump/actions/workflows/tests.yml)[![Coverage Status](https://coveralls.io/repos/github/macbre/mediawiki-dump/badge.svg?branch=master)](https://coveralls.io/github/macbre/mediawiki-dump?branch=master)```pip install mediawiki_dump```[Python3 package](https://pypi.org/project/mediawiki_dump/) for working with [MediaWiki XML content dumps](https://www.mediawiki.org/wiki/Manual:Backing_up_a_wiki#Backup_the_content_of_the_wiki_(XML_dump)).[Wikipedia](https://dumps.wikimedia.org/) (bz2 compressed) and [Wikia](https://community.fandom.com/wiki/Help:Database_download) (7zip) content dumps are supported.## DependenciesIn order to read 7zip archives (used by Wikia's XML dumps) you need to install [`libarchive`](http://libarchive.org/):```sudo apt install libarchive-dev```## API### TokenizerAllows you to clean up the wikitext:```pythonfrom mediawiki_dump.tokenizer import cleanclean('[[Foo|bar]] is a link')'bar is a link'```And then tokenize the text:```pythonfrom mediawiki_dump.tokenizer import tokenizetokenize('11. juni 2007 varð kunngjørt, at Svínoyar kommuna verður løgd saman við Klaksvíkar kommunu eftir komandi bygdaráðsval.')['juni', 'varð', 'kunngjørt', 'at', 'Svínoyar', 'kommuna', 'verður', 'løgd', 'saman', 'við', 'Klaksvíkar', 'kommunu', 'eftir', 'komandi', 'bygdaráðsval']```### Dump readerFetch and parse dumps (using a local file cache):```pythonfrom mediawiki_dump.dumps import WikipediaDumpfrom mediawiki_dump.reader import DumpReaderdump = WikipediaDump('fo')pages = DumpReader().read(dump)[page.title for page in pages][:10]['Main Page', 'Brúkari:Jon Harald Søby', 'Forsíða', 'Ormurin Langi', 'Regin smiður', 'Fyrimynd:InterLingvLigoj', 'Heimsyvirlýsingin um mannarættindi', 'Bólkur:Kvæði', 'Bólkur:Yrking', 'Kjak:Forsíða']````read` method yields the `DumpEntry` object for each revision.By using `DumpReaderArticles` class you can read article pages only:```pythonimport logging; logging.basicConfig(level=logging.INFO)from mediawiki_dump.dumps import WikipediaDumpfrom mediawiki_dump.reader import DumpReaderArticlesdump = WikipediaDump('fo')reader = DumpReaderArticles()pages = reader.read(dump)print([page.title for page in pages][:25])print(reader.get_dump_language())  # fo```Will give you:```INFO:DumpReaderArticles:Parsing XML dump...INFO:WikipediaDump:Checking /tmp/wikicorpus_62da4928a0a307185acaaa94f537d090.bz2 cache file...INFO:WikipediaDump:Fetching fo dump from &lt;https://dumps.wikimedia.org/fowiki/latest/fowiki-latest-pages-meta-current.xml.bz2&gt;...INFO:WikipediaDump:HTTP 200 (14105 kB will be fetched)INFO:WikipediaDump:Cache set...['WIKIng', 'Føroyar', 'Borðoy', 'Eysturoy', 'Fugloy', 'Forsíða', 'Løgmenn í Føroyum', 'GNU Free Documentation License', 'GFDL', 'Opið innihald', 'Wikipedia', 'Alfrøði', '2004', '20. juni', 'WikiWiki', 'Wiki', 'Danmark', '21. juni', '22. juni', '23. juni', 'Lívfrøði', '24. juni', '25. juni', '26. juni', '27. juni']```## Reading Wikia's dumps ```pythonimport logging; logging.basicConfig(level=logging.INFO)from mediawiki_dump.dumps import WikiaDumpfrom mediawiki_dump.reader import DumpReaderArticlesdump = WikiaDump('plnordycka')pages = DumpReaderArticles().read(dump)print([page.title for page in pages][:25])```Will give you:```INFO:DumpReaderArticles:Parsing XML dump...INFO:WikiaDump:Checking /tmp/wikicorpus_f7dd3b75c5965ee10ae5fe4643fb806b.7z cache file...INFO:WikiaDump:Fetching plnordycka dump from &lt;https://s3.amazonaws.com/wikia_xml_dumps/p/pl/plnordycka_pages_current.xml.7z&gt;...INFO:WikiaDump:HTTP 200 (129 kB will be fetched)INFO:WikiaDump:Cache setINFO:WikiaDump:Reading wikicorpus_f7dd3b75c5965ee10ae5fe4643fb806b file from dump...INFO:DumpReaderArticles:Parsing completed, entries found: 615['Nordycka Wiki', 'Strona główna', '1968', '1948', 'Ormurin Langi', 'Mykines', 'Trollsjön', 'Wyspy Owcze', 'Nólsoy', 'Sandoy', 'Vágar', 'Mørk', 'Eysturoy', 'Rakfisk', 'Hákarl', '1298', 'Sztokfisz', '1978', '1920', 'Najbardziej na północ', 'Svalbard', 'Hamferð', 'Rok w Skandynawii', 'Islandia', 'Rissajaure']```## Fetching full historyPass `full_history` to `BaseDump` constructor to fetch the XML content dump with full history:```pythonimport logging; logging.basicConfig(level=logging.INFO)from mediawiki_dump.dumps import WikiaDumpfrom mediawiki_dump.reader import DumpReaderArticlesdump = WikiaDump('macbre', full_history=True)  # fetch full history, including old revisionspages = DumpReaderArticles().read(dump)print('\n'.join([repr(page) for page in pages]))```Will give you:```INFO:DumpReaderArticles:Parsing completed, entries found: 384&lt;DumpEntry &quot;Macbre Wiki&quot; by Default at 2016-10-12T19:51:06+00:00&gt;&lt;DumpEntry &quot;Macbre Wiki&quot; by Wikia at 2016-10-12T19:51:05+00:00&gt;&lt;DumpEntry &quot;Macbre Wiki&quot; by Macbre at 2016-11-04T10:33:20+00:00&gt;&lt;DumpEntry &quot;Macbre Wiki&quot; by FandomBot at 2016-11-04T10:37:17+00:00&gt;&lt;DumpEntry &quot;Macbre Wiki&quot; by FandomBot at 2017-01-25T14:47:37+00:00&gt;&lt;DumpEntry &quot;Macbre Wiki&quot; by Ryba777 at 2017-04-10T11:20:25+00:00&gt;&lt;DumpEntry &quot;Macbre Wiki&quot; by Ryba777 at 2017-04-10T11:21:20+00:00&gt;&lt;DumpEntry &quot;Macbre Wiki&quot; by Macbre at 2018-03-07T12:51:12+00:00&gt;&lt;DumpEntry &quot;Main Page&quot; by Wikia at 2016-10-12T19:51:05+00:00&gt;&lt;DumpEntry &quot;FooBar&quot; by Anonymous at 2016-11-08T10:15:33+00:00&gt;&lt;DumpEntry &quot;FooBar&quot; by Anonymous at 2016-11-08T10:15:49+00:00&gt;...&lt;DumpEntry &quot;YouTube tag&quot; by FANDOMbot at 2018-06-05T11:45:44+00:00&gt;&lt;DumpEntry &quot;Maps&quot; by Macbre at 2018-06-06T08:51:24+00:00&gt;&lt;DumpEntry &quot;Maps&quot; by Macbre at 2018-06-07T08:17:13+00:00&gt;&lt;DumpEntry &quot;Maps&quot; by Macbre at 2018-06-07T08:17:36+00:00&gt;&lt;DumpEntry &quot;Scary transclusion&quot; by Macbre at 2018-07-24T14:52:20+00:00&gt;&lt;DumpEntry &quot;Lua&quot; by Macbre at 2018-09-11T14:04:15+00:00&gt;&lt;DumpEntry &quot;Lua&quot; by Macbre at 2018-09-11T14:14:24+00:00&gt;&lt;DumpEntry &quot;Lua&quot; by Macbre at 2018-09-11T14:14:37+00:00&gt;```## Reading dumps of selected articlesYou can use [`mwclient` Python library](https://mwclient.readthedocs.io/en/latest/index.html)and fetch &quot;live&quot; dumps of selected articles from any MediaWiki-powered site.```pythonimport mwclientsite = mwclient.Site('vim.fandom.com', path='/')from mediawiki_dump.dumps import MediaWikiClientDumpfrom mediawiki_dump.reader import DumpReaderArticlesdump = MediaWikiClientDump(site, ['Vim documentation', 'Tutorial'])pages = DumpReaderArticles().read(dump)print('\n'.join([repr(page) for page in pages]))```Will give you:```&lt;DumpEntry &quot;Vim documentation&quot; by Anonymous at 2019-07-05T09:39:47+00:00&gt;&lt;DumpEntry &quot;Tutorial&quot; by Anonymous at 2019-07-05T09:41:19+00:00&gt;```## Finding pages with a specific [parser tag](https://www.mediawiki.org/wiki/Manual:Tag_extensions)Let's find pages where no longer supported `&lt;place&gt;` tag is still used:```pythonimport logging; logging.basicConfig(level=logging.INFO)from mediawiki_dump.dumps import WikiaDumpfrom mediawiki_dump.reader import DumpReaderdump = WikiaDump('plpoznan')pages = DumpReader().read(dump)with_places_tag = [    page.title    for page in pages    if '&lt;place ' in page.content]logging.info('Pages found: %d', len(with_places_tag))with open(&quot;pages.txt&quot;, mode=&quot;wt&quot;, encoding=&quot;utf-8&quot;) as fp:    for entry in with_places_tag:        fp.write(entry + &quot;\n&quot;)logging.info(&quot;pages.txt file created&quot;)```</longdescription>
</pkgmetadata>