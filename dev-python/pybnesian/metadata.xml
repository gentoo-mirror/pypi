<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![build](https://img.shields.io/github/workflow/status/davenza/pybnesian/Create%20release)[![Documentation Status](https://readthedocs.org/projects/pybnesian/badge/?version=latest)](https://pybnesian.readthedocs.io/en/latest/?badge=latest)![PyPI](https://img.shields.io/pypi/v/pybnesian?color=blue)# PyBNesian- `PyBNesian` is a Python package that implements Bayesian networks. Currently, it is mainly dedicated to learning Bayesian networks.- `PyBNesian` is implemented in C++, to achieve significant performance gains. It uses [Apache Arrow](https://arrow.apache.org/) to enable fast interoperability between Python and C++. In addition, some parts are implemented in OpenCL to achieve GPU acceleration.- `PyBNesian` allows extending its functionality using Python code, so new research can be easily developed.Implementation=====================Currently `PyBNesian` implements the following features:Models------------ [x] Bayesian networks.- [x] Conditional Bayesian networks (see section 5.6 of [1]).- [x] Dynamic Bayesian networks.which can have different types of CPDs:- [x] Multinomial.- [x] Linear Gaussian.- [x] Conditional kernel density estimation (ratio of two kernel density estimation models). Accelerated with OpenCL.with this combinations of CPDs, we implement the following types of networks (which can also be Conditional or Dynamic):- [x] Discrete networks.- [x] Gaussian networks.- [x] Semiparametric networks.- [x] Hybrid networks (conditional linear Gaussian networks and semiparametric networks).Graphs------------------ [x] DAGs.- [x] Directed graphs.- [x] Undirected graphs.- [x] Partially directed graphs.Graph classes implement useful functionalities for probabilistic graphical models, such as moving between DAG-PDAG representation or fast access to root and leaves.Learning---------------It implements different structure learning algorithms:- [x] Greedy hill-climbing (for Bayesian networks and Conditional Bayesian networks).- [x] PC-stable (for Bayesian networks and Conditional Bayesian networks).- [x] MMPC (for Bayesian networks and Conditional Bayesian networks).- [x] MMHC (for Bayesian networks and conditional Bayesian networks).- [x] DMMHC (for dynamic Bayesian networks).The score and search algorithms can be used with the following scores:- [x] BIC.- [x] BGe.- [x] BDe.- [x] Cross-validation likelihood.- [x] Holdout likelihood.- [x] Cross-validated likelihood with validation dataset. This score combines the cross-validation likelihood with a validation dataset to control the overfitting.and the following the following learning operators:- [x] Arc operations: add arc, remove arc, flip arc.- [x] Change Node Type (for semiparametric Bayesian networks).The following independence tests are implemented for the constraint-based algorithms:- [x] Chi-square test.- [x] partial correlation test t-test.- [x] A likelihood-ratio test based on mutual information assuming a Gaussian distribution for the continuous data.- [x] CMIknn [2].- [x] RCoT [3].It also implements the parameter learning:- [x] Maximum Likelihood Estimator.Inference-----------------------Not implemented right now, as the priority is the learning algorithms. However, all the CPDs and models have a `sample()` method, which can be used to create easily an approximate inference engine based on sampling.Serialization-----------------------All relevant objects (graphs, CPDs, Bayesian networks, etc) can be saved/loaded using the pickle format.Other implementations-----------------`PyBNesian` exposes the implementation of other models or techniques used within the library.- [x] Apply cross-validation to a dataset.- [x] Apply holdout to a dataset.- [x] Kernel Density Estimation. Accelerated with OpenCL.- [ ] K-d Tree. (implemented but not exposed yet).Weighted sums of chi-squared random variables:- [ ] Hall-Buckley-Eagleson approximation. (implemented but not exposed yet).- [ ] Lindsay-Pilla-Basak approximation. (implemented but not exposed yet).Usage example===========================```python&gt;&gt;&gt; from pybnesian import GaussianNetwork, LinearGaussianCPD&gt;&gt;&gt; # Create a GaussianNetwork with 4 nodes and no arcs.&gt;&gt;&gt; gbn = GaussianNetwork(['a', 'b', 'c', 'd'])&gt;&gt;&gt; # Create a GaussianNetwork with 4 nodes and 3 arcs.&gt;&gt;&gt; gbn = GaussianNetwork(['a', 'b', 'c', 'd'], [('a', 'c'), ('b', 'c'), ('c', 'd')])&gt;&gt;&gt; # Return the nodes of the network.&gt;&gt;&gt; print(&quot;Nodes: &quot; + str(gbn.nodes()))Nodes: ['a', 'b', 'c', 'd']&gt;&gt;&gt; # Return the arcs of the network.&gt;&gt;&gt; print(&quot;Arcs: &quot; + str(gbn.nodes()))Arcs: ['a', 'b', 'c', 'd']&gt;&gt;&gt; # Return the parents of c.&gt;&gt;&gt; print(&quot;Parents of c: &quot; + str(gbn.parents('c')))Parents of c: ['b', 'a']&gt;&gt;&gt; # Return the children of c.&gt;&gt;&gt; print(&quot;Children of c: &quot; + str(gbn.children('c')))Children of c: ['d']&gt;&gt;&gt; # You can access to the graph of the network.&gt;&gt;&gt; graph = gbn.graph()&gt;&gt;&gt; # Return the roots of the graph.&gt;&gt;&gt; print(&quot;Roots: &quot; + str(sorted(graph.roots())))Roots: ['a', 'b']&gt;&gt;&gt; # Return the leaves of the graph.&gt;&gt;&gt; print(&quot;Leaves: &quot; + str(sorted(graph.leaves())))Leaves: ['d']&gt;&gt;&gt; # Return the topological sort.&gt;&gt;&gt; print(&quot;Topological sort: &quot; + str(graph.topological_sort()))Topological sort: ['a', 'b', 'c', 'd']&gt;&gt;&gt; # Add an arc.&gt;&gt;&gt; gbn.add_arc('a', 'b')&gt;&gt;&gt; # Flip (reverse) an arc.&gt;&gt;&gt; gbn.flip_arc('a', 'b')&gt;&gt;&gt; # Remove an arc.&gt;&gt;&gt; gbn.remove_arc('b', 'a')&gt;&gt;&gt; # We can also add nodes.&gt;&gt;&gt; gbn.add_node('e')4&gt;&gt;&gt; # We can get the number of nodes&gt;&gt;&gt; assert gbn.num_nodes() == 5&gt;&gt;&gt; # ... and the number of arcs&gt;&gt;&gt; assert gbn.num_arcs() == 3&gt;&gt;&gt; # Remove a node.&gt;&gt;&gt; gbn.remove_node('b')&gt;&gt;&gt; # Each node has an unique index to identify it&gt;&gt;&gt; print(&quot;Indices: &quot; + str(gbn.indices()))Indices: {'e': 4, 'c': 2, 'd': 3, 'a': 0}&gt;&gt;&gt; idx_a = gbn.index('a')&gt;&gt;&gt; # And we can get the node name from the index&gt;&gt;&gt; print(&quot;Node 2: &quot; + str(gbn.name(2)))Node 2: c&gt;&gt;&gt; # The model is not fitted right now.&gt;&gt;&gt; assert gbn.fitted() == False&gt;&gt;&gt; # Create a LinearGaussianCPD (variable, parents, betas, variance)&gt;&gt;&gt; d_cpd = LinearGaussianCPD(&quot;d&quot;, [&quot;c&quot;], [3, 1.2], 0.5)&gt;&gt;&gt; # Add the CPD to the GaussianNetwork&gt;&gt;&gt; gbn.add_cpds([d_cpd])&gt;&gt;&gt; # The CPD is still not fitted because there are 3 nodes without CPD.&gt;&gt;&gt; assert gbn.fitted() == False&gt;&gt;&gt; # Let's generate some random data to fit the model.&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; np.random.seed(1)&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; DATA_SIZE = 100&gt;&gt;&gt; a_array = np.random.normal(3, np.sqrt(0.5), size=DATA_SIZE)&gt;&gt;&gt; c_array = -4.2 - 1.2*a_array + np.random.normal(0, np.sqrt(0.75), size=DATA_SIZE)&gt;&gt;&gt; d_array = 3 + 1.2 * c_array + np.random.normal(0, np.sqrt(0.5), size=DATA_SIZE)&gt;&gt;&gt; e_array = np.random.normal(0, 1, size=DATA_SIZE)&gt;&gt;&gt; df = pd.DataFrame({'a': a_array,...                    'c': c_array,...                    'd': d_array,...                    'e': e_array...                })&gt;&gt;&gt; # Fit the model. You can pass a pandas.DataFrame or a pyarrow.RecordBatch as argument.&gt;&gt;&gt; # This fits the remaining CPDs&gt;&gt;&gt; gbn.fit(df)&gt;&gt;&gt; assert gbn.fitted() == True&gt;&gt;&gt; # Check the learned CPDs.&gt;&gt;&gt; print(gbn.cpd('a'))[LinearGaussianCPD] P(a) = N(3.043, 0.396)&gt;&gt;&gt; print(gbn.cpd('c'))[LinearGaussianCPD] P(c | a) = N(-4.423 + -1.083*a, 0.659)&gt;&gt;&gt; print(gbn.cpd('d'))[LinearGaussianCPD] P(d | c) = N(3.000 + 1.200*c, 0.500)&gt;&gt;&gt; print(gbn.cpd('e'))[LinearGaussianCPD] P(e) = N(-0.020, 1.144)&gt;&gt;&gt; # You can sample some data&gt;&gt;&gt; sample = gbn.sample(50)&gt;&gt;&gt; # Compute the log-likelihood of each instance&gt;&gt;&gt; ll = gbn.logl(sample)&gt;&gt;&gt; # or the sum of log-likelihoods.&gt;&gt;&gt; sll = gbn.slogl(sample)&gt;&gt;&gt; assert np.isclose(ll.sum(), sll)&gt;&gt;&gt; # Save the model, include the CPDs in the file.&gt;&gt;&gt; gbn.save('test', include_cpd=True)&gt;&gt;&gt; # Load the model&gt;&gt;&gt; from pybnesian import load&gt;&gt;&gt; loaded_gbn = load('test.pickle')&gt;&gt;&gt; # Learn the structure using greedy hill-climbing.&gt;&gt;&gt; from pybnesian import hc, GaussianNetworkType&gt;&gt;&gt; # Learn a Gaussian network.&gt;&gt;&gt; learned = hc(df, bn_type=GaussianNetworkType())&gt;&gt;&gt; learned.num_arcs()2```Dependencies============- Python 3.6, 3.7, 3.8 and 3.9.The library has been tested on Ubuntu 16.04/20.04 and Windows 10, but should be compatible with other operating systems.Libraries---------The library depends on [NumPy](https://numpy.org/), [Apache Arrow](https://arrow.apache.org/), and[pybind11](https://github.com/pybind/pybind11).Installation============PyBNesian can be installed with pip:```pip install pybnesian```Build from Source=================Prerequisites-------------- Python 3.6, 3.7, 3.8 or 3.9.- C++17 compatible compiler.- CMake (it is needed to compile [NLopt](https://github.com/stevengj/nlopt)).- OpenCL 1.2 headers/library available.If needed you can select a C++ compiler by setting the environment variable `CC`. For example, in Ubuntu, we can useClang 11 with the following command before installing PyBNesian:```export CC=clang-11```Building--------Clone the repository:```git clone https://github.com/davenza/PyBNesian.gitcd PyBNesiangit checkout v0.1.0 # You can checkout a specific version if you wantpython setup.py install```Testing=========================The library contains tests that can be executed using `pytest`. They also require `scipy` and `pandas` installed.``pip install pytest scipy pandas``Run the tests with:``pytest``## References&lt;a id=&quot;1&quot;&gt;[1]&lt;/a&gt; D. Koller and N. Friedman, Probabilistic Graphical Models: Principles and Techniques,The MIT Press, 2009.&lt;a id=&quot;2&quot;&gt;[2]&lt;/a&gt; J. Runge, Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information. International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 84, 2018, pp. 938–947.&lt;a id=&quot;3&quot;&gt;[3]&lt;/a&gt; E. V. Strobl and K. Zhang and S., Visweswaran. Approximate kernel-based conditional independence tests for fast non-parametric causal discovery. Journal of Causal Inference, 7(1), 2019, pp 1-24.</longdescription>
</pkgmetadata>