<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Text GenerationThe Hugging Face Text Generation Python library provides a convenient way of interfacing with a`text-generation-inference` instance running on[Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints) or on the Hugging Face Hub.## Get Started### Install```shellpip install text-generation```### Inference API Usage```pythonfrom text_generation import InferenceAPIClientclient = InferenceAPIClient(&quot;bigscience/bloomz&quot;)text = client.generate(&quot;Why is the sky blue?&quot;).generated_textprint(text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```or with the asynchronous client:```pythonfrom text_generation import InferenceAPIAsyncClientclient = InferenceAPIAsyncClient(&quot;bigscience/bloomz&quot;)response = await client.generate(&quot;Why is the sky blue?&quot;)print(response.generated_text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;async for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```### Hugging Face Inference Endpoint usage```pythonfrom text_generation import Clientendpoint_url = &quot;https://YOUR_ENDPOINT.endpoints.huggingface.cloud&quot;client = Client(endpoint_url)text = client.generate(&quot;Why is the sky blue?&quot;).generated_textprint(text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```or with the asynchronous client:```pythonfrom text_generation import AsyncClientendpoint_url = &quot;https://YOUR_ENDPOINT.endpoints.huggingface.cloud&quot;client = AsyncClient(endpoint_url)response = await client.generate(&quot;Why is the sky blue?&quot;)print(response.generated_text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;async for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```### Types```python# Prompt tokensclass PrefillToken:    # Token ID from the model tokenizer    id: int    # Token text    text: str    # Logprob    # Optional since the logprob of the first token cannot be computed    logprob: Optional[float]# Generated tokensclass Token:    # Token ID from the model tokenizer    id: int    # Token text    text: str    # Logprob    logprob: float    # Is the token a special token    # Can be used to ignore tokens when concatenating    special: bool# Generation finish reasonclass FinishReason(Enum):    # number of generated tokens == `max_new_tokens`    Length = &quot;length&quot;    # the model generated its end of sequence token    EndOfSequenceToken = &quot;eos_token&quot;    # the model generated a text included in `stop_sequences`    StopSequence = &quot;stop_sequence&quot;# Additional sequences when using the `best_of` parameterclass BestOfSequence:    # Generated text    generated_text: str    # Generation finish reason    finish_reason: FinishReason    # Number of generated tokens    generated_tokens: int    # Sampling seed if sampling was activated    seed: Optional[int]    # Prompt tokens    prefill: List[PrefillToken]    # Generated tokens    tokens: List[Token]# `generate` detailsclass Details:    # Generation finish reason    finish_reason: FinishReason    # Number of generated tokens    generated_tokens: int    # Sampling seed if sampling was activated    seed: Optional[int]    # Prompt tokens    prefill: List[PrefillToken]    # Generated tokens    tokens: List[Token]    # Additional sequences when using the `best_of` parameter    best_of_sequences: Optional[List[BestOfSequence]]# `generate` return valueclass Response:    # Generated text    generated_text: str    # Generation details    details: Details# `generate_stream` detailsclass StreamDetails:    # Generation finish reason    finish_reason: FinishReason    # Number of generated tokens    generated_tokens: int    # Sampling seed if sampling was activated    seed: Optional[int]# `generate_stream` return valueclass StreamResponse:    # Generated token    token: Token    # Complete generated text    # Only available when the generation is finished    generated_text: Optional[str]    # Generation details    # Only available when the generation is finished    details: Optional[StreamDetails]```</longdescription>
</pkgmetadata>