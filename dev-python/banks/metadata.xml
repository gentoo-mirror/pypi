<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># banks[![PyPI - Version](https://img.shields.io/pypi/v/banks.svg)](https://pypi.org/project/banks)[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/banks.svg)](https://pypi.org/project/banks)[![PyPI Release](https://github.com/masci/banks/actions/workflows/release.yml/badge.svg)](https://github.com/masci/banks/actions/workflows/release.yml)[![test](https://github.com/masci/banks/actions/workflows/test.yml/badge.svg)](https://github.com/masci/banks/actions/workflows/test.yml)[Banks](https://en.wikipedia.org/wiki/Arrival_(film)) is the linguist professor who will help you generate meaningful LLM prompts using a template language that makes sense.If you're still using `f-strings` for the job, keep reading.-----**Table of Contents**- [banks](#banks)  - [Installation](#installation)  - [Examples](#examples)    - [Generate a blog writing prompt](#generate-a-blog-writing-prompt)    - [Generate a summarizer prompt](#generate-a-summarizer-prompt)    - [Lemmatize text while processing a template](#lemmatize-text-while-processing-a-template)    - [Use a LLM to generate a text while rendering a prompt](#use-a-llm-to-generate-a-text-while-rendering-a-prompt)    - [Go meta: create a prompt and `generate` its response](#go-meta-create-a-prompt-and-generate-its-response)    - [Go meta(meta): process a LLM response](#go-metameta-process-a-llm-response)    - [Reuse templates from files](#reuse-templates-from-files)  - [License](#license)## Installation```consolepip install banks```## Examples### Generate a blog writing promptGiven a generic template to instruct an LLM to generate a blog article, weuse Banks to generate the actual prompt on our topic of choice, &quot;retrogame computing&quot;:```pyfrom banks import Promptp = Prompt(&quot;Write a 500-word blog post on {{ topic }}.\n\nBlog post:&quot;)topic = &quot;retrogame computing&quot;print(p.text({&quot;topic&quot;: topic}))```This will print the following text, that can be pasted directly into Chat-GPT:```txtWrite a 500-word blog post on retrogame computing.Blog post:```### Generate a summarizer promptInstead of hardcoding the content to summarize in the prompt itself, we can generate itstarting from a generic one:```pyfrom banks import Promptprompt_template = &quot;&quot;&quot;Summarize the following documents:{% for document in documents %}{{ document }}{% endfor %}Summary:&quot;&quot;&quot;# In a real-world scenario, these would be loaded as external resources from files or networkdocuments = [    &quot;A first paragraph talking about AI&quot;,    &quot;A second paragraph talking about climate change&quot;,    &quot;A third paragraph talking about retrogaming&quot;]p = Prompt(prompt_template)print(p.text({&quot;documents&quot;: documents}))```The resulting prompt:```txtSummarize the following documents:A first paragraph talking about AIA second paragraph talking about climate changeA third paragraph talking about retrogamingSummary:```### Lemmatize text while processing a templateBanks comes with predefined filters you can use to process data before generating theprompt. Say you want to use a lemmatizer on a document before summarizing it, firstyou need to install `simplemma`:```shpip install simplemma```then you can use the `lemmatize` filter in your templates like this:```pyfrom banks import Promptprompt_template = &quot;&quot;&quot;Summarize the following document:{{ document | lemmatize }}Summary:&quot;&quot;&quot;p = Prompt(prompt_template)print(p.text({&quot;document&quot;: &quot;The cats are running&quot;}))```the output would be:```txtSummarize the following document:the cat be runSummary:```### Use a LLM to generate a text while rendering a promptSometimes it might be useful to ask another LLM to generate examples for you in afew-shot prompt. Provided you have a valid OpenAI API key stored in an env varcalled `OPENAI_API_KEY` you can ask Banks to do something like this (note we canannotate the prompt using comments - anything within `{# ... #}` will be removedfrom the final prompt):```pyfrom banks import Promptprompt_template = &quot;&quot;&quot;Generate a tweet about the topic {{ topic }} with a positive sentiment.{#    This is for illustration purposes only, there are better and cheaper ways    to generate examples for a few-shots prompt.#}Examples:{% for number in range(3) %}- {% generate &quot;write a tweet with positive sentiment&quot; &quot;gpt-3.5-turbo&quot; %}{% endfor %}&quot;&quot;&quot;p = Prompt(prompt_template)print(p.text({&quot;topic&quot;: &quot;climate change&quot;}))```The output would be something similar to the following:```txtGenerate a tweet about the topic climate change with a positive sentiment.Examples:- &quot;Feeling grateful for the amazing capabilities of #GPT3.5Turbo! It's making my work so much easier and efficient. Thank you, technology!&quot; #positivity #innovation- &quot;Feeling grateful for all the opportunities that come my way! With #GPT3.5Turbo, I am able to accomplish tasks faster and more efficiently. #positivity #productivity&quot;- &quot;Feeling grateful for all the wonderful opportunities and experiences that life has to offer! #positivity #gratitude #blessed #gpt3.5turbo&quot;```If you paste Banks' output into ChatGPT you would get something like this:```txtClimate change is a pressing global issue, but together we can create positive change! Let's embrace renewable energy, protect our planet, and build a sustainable future for generations to come. üåçüíö #ClimateAction #PositiveFuture```### Go meta: create a prompt and `generate` its responseWe can leverage Jinja's macro system to generate a prompt, send the result to OpenAI and get a response.Let's bring back the blog writing example:```pyfrom banks import Promptprompt_template = &quot;&quot;&quot;{% from &quot;banks_macros.jinja&quot; import run_prompt with context %}{%- call run_prompt() -%}Write a 500-word blog post on {{ topic }}Blog post:{%- endcall -%}&quot;&quot;&quot;p = Prompt(prompt_template)print(p.text({&quot;topic&quot;: &quot;climate change&quot;}))```The snippet above won't print the prompt, instead will generate the prompt text```Write a 500-word blog post on climate changeBlog post:```and will send it to OpenAI using the `generate` extension, eventually returning its response:```Climate change is a phenomenon that has been gaining attention in recent years......```### Go meta(meta): process a LLM responseWhen generating a response from a prompt template, we can take a step further andpost-process the LLM response by assinging it to a variable and applying filtersto it:```pyfrom banks import Promptprompt_template = &quot;&quot;&quot;{% from &quot;banks_macros.jinja&quot; import run_prompt with context %}{%- set prompt_result %}{%- call run_prompt() -%}Write a 500-word blog post on {{ topic }}Blog post:{%- endcall -%}{%- endset %}{# nothing is returned at this point: the variable 'prompt_result' contains the result #}{# let's use the prompt_result variable now #}{{ prompt_result | upper }}&quot;&quot;&quot;p = Prompt(prompt_template)print(p.text({&quot;topic&quot;: &quot;climate change&quot;}))```The final answer from the LLM will be printed, this time all in uppercase.### Reuse templates from filesWe can get the same result as the previous example loading the prompt template from fileinstead of hardcoding it into the Python code. For convenience, Banks comes with a fewdefault templates distributed the package. We can load those templates from file like this:```pyfrom banks import Promptp = Prompt.from_template(&quot;blog.jinja&quot;)topic = &quot;retrogame computing&quot;print(p.text({&quot;topic&quot;: topic}))```## License`banks` is distributed under the terms of the [MIT](https://spdx.org/licenses/MIT.html) license.</longdescription>
</pkgmetadata>