<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;a href=&quot;https://explosion.ai&quot;&gt;&lt;img src=&quot;https://explosion.ai/assets/img/logo.svg&quot; width=&quot;125&quot; height=&quot;125&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;# spacy-llm: Integrating LLMs into structured NLP pipelines[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/explosion/spacy-llm/test.yml?branch=main)](https://github.com/explosion/spacy-llm/actions/workflows/test.yml)[![pypi Version](https://img.shields.io/pypi/v/spacy-llm.svg?style=flat-square&amp;logo=pypi&amp;logoColor=white)](https://pypi.org/project/spacy-llm/)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)This package integrates Large Language Models (LLMs) into [spaCy](https://spacy.io), featuring a modular system for **fast prototyping** and **prompting**, and turning unstructured responses into **robust outputs** for various NLP tasks, **no training data** required.- Serializable `llm` **component** to integrate prompts into your pipeline- **Modular functions** to define the [**task**](#tasks) (prompting and parsing) and [**model**](#models)- Support for **hosted APIs** and self-hosted **open-source models**- Integration with [`LangChain`](https://github.com/hwchase17/langchain)- Access to **[OpenAI API](https://platform.openai.com/docs/api-reference/introduction)**, including GPT-4 and various GPT-3 models- Built-in support for **open-source [Dolly](https://huggingface.co/databricks)** models hosted on Hugging Face- Usage examples for **Named Entity Recognition** and **Text Classification**- Easy implementation of **your own functions** via [spaCy's registry](https://spacy.io/api/top-level#registry) for custom prompting, parsing and model integrations## üß† MotivationLarge Language Models (LLMs) feature powerful natural language understanding capabilities. With only a few (and sometimes no) examples, an LLM can be prompted to perform custom NLP tasks such as text categorization, named entity recognition, coreference resolution, information extraction and more.[spaCy](https://spacy.io) is a well-established library for building systems that need to work with language in various ways. spaCy's built-in components are generally powered by supervised learning or rule-based approaches.Supervised learning is much worse than LLM prompting for prototyping, but for many tasks it's much better for production. A transformer model that runs comfortably on a single GPU is extremely powerful, and it's likely to be a better choice for any task for which you have a well-defined output. You train the model with anything from a few hundred to a few thousand labelled examples, and it will learn to do exactly that. Efficiency, reliability and control are all better with supervised learning, and accuracy will generally be higher than LLM prompting as well.`spacy-llm` lets you have **the best of both worlds**. You can quickly initialize a pipeline with components powered by LLM prompts, and freely mix in components powered by other approaches. As your project progresses, you can look at replacing some or all of the LLM-powered components as you require.Of course, there can be components in your system for which the power of an LLM is fully justified. If you want a system that can synthesize information from multiple documents in subtle ways and generate a nuanced summary for you, bigger is better. However, even if your production system needs an LLM for some of the task, that doesn't mean you need an LLM for all of it. Maybe you want to use a cheap text classification model to help you find the texts to summarize, or maybe you want to add a rule-based system to sanity check the output of the summary. These before-and-after tasks are much easier with a mature and well-thought-out library, which is exactly what spaCy provides.## ‚è≥ Install`spacy-llm` will be installed automatically in future spaCy versions. For now, you can run the following in the same virtual environment where you already have `spacy` [installed](https://spacy.io/usage).```bashpython -m pip install spacy-llm```&gt; ‚ö†Ô∏è This package is still experimental and it is possible that changes made to the interface will be breaking in minor version updates.## üêç UsageThe task and the model have to be supplied to the `llm` pipeline component using [spaCy's configsystem](https://spacy.io/api/data-formats#config). This package provides various built-infunctionality, as detailed in the [API](#-api) documentation.### Example 1: Add a text classifier using a GPT-3 model from OpenAICreate a new API key from openai.com or fetch an existing one, and ensure the keys are set as environmental variables.For more background information, see the [OpenAI](#openai) section.Create a config file `config.cfg` containing at least the following(or see the full example [here](usage_examples/textcat_openai)):```ini[nlp]lang = &quot;en&quot;pipeline = [&quot;llm&quot;][components][components.llm]factory = &quot;llm&quot;[components.llm.task]@llm_tasks = &quot;spacy.TextCat.v2&quot;labels = [&quot;COMPLIMENT&quot;, &quot;INSULT&quot;][components.llm.model]@llm_models = &quot;spacy.GPT-3-5.v1&quot;config = {&quot;temperature&quot;: 0.3}```Now run:```pythonfrom spacy_llm.util import assemblenlp = assemble(&quot;config.cfg&quot;)doc = nlp(&quot;You look gorgeous!&quot;)print(doc.cats)```### Example 2: Add NER using an open-source model through Hugging FaceTo run this example, ensure that you have a GPU enabled, and `transformers`, `torch` and CUDA installed.For more background information, see the [DollyHF](#spacydollyhfv1) section.Create a config file `config.cfg` containing at least the following(or see the full example [here](usage_examples/ner_dolly)):```ini[nlp]lang = &quot;en&quot;pipeline = [&quot;llm&quot;][components][components.llm]factory = &quot;llm&quot;[components.llm.task]@llm_tasks = &quot;spacy.NER.v2&quot;labels = [&quot;PERSON&quot;, &quot;ORGANISATION&quot;, &quot;LOCATION&quot;][components.llm.model]@llm_models = &quot;spacy.Dolly.v1&quot;# For better performance, use dolly-v2-12b insteadname = &quot;dolly-v2-3b&quot;```Now run:```pythonfrom spacy_llm.util import assemblenlp = assemble(&quot;config.cfg&quot;)doc = nlp(&quot;Jack and Jill rode up the hill in Les Deux Alpes&quot;)print([(ent.text, ent.label_) for ent in doc.ents])```Note that Hugging Face will download the `&quot;databricks/dolly-v2-3b&quot;` model the first time you use it. You can[define the cached directory](https://huggingface.co/docs/huggingface_hub/main/en/guides/manage-cache)by setting the environmental variable `HF_HOME`.Also, you can upgrade the model to be `&quot;databricks/dolly-v2-12b&quot;` for better performance.### Example 3: Create the component directly in PythonThe `llm` component behaves as any other spaCy component does, so adding it to an existing pipeline follows the samepattern:```pythonimport spacynlp = spacy.blank(&quot;en&quot;)nlp.add_pipe(    &quot;llm&quot;,    config={        &quot;task&quot;: {            &quot;@llm_tasks&quot;: &quot;spacy.NER.v2&quot;,            &quot;labels&quot;: [&quot;PERSON&quot;, &quot;ORGANISATION&quot;, &quot;LOCATION&quot;]        },        &quot;model&quot;: {            &quot;@llm_models&quot;: &quot;spacy.GPT-3-5.v1&quot;,        },    },)nlp.initialize()doc = nlp(&quot;Jack and Jill rode up the hill in Les Deux Alpes&quot;)print([(ent.text, ent.label_) for ent in doc.ents])```Note that for efficient usage of resources, typically you would use [`nlp.pipe(docs)`](https://spacy.io/api/language#pipe)with a batch, instead of calling `nlp(doc)` with a single document.### Example 4: Implement your own custom taskTo write a[`task`](#tasks), youneed to implement two functions: `generate_prompts` that takes a list of spaCy [`Doc`](https://spacy.io/api/doc) objects and transformsthem into a list of prompts, and `parse_responses` that transforms the LLM outputs into annotations on the [`Doc`](https://spacy.io/api/doc), e.g. entity spans, text categories and more.To register your custom task with spaCy, decorate a factory function using the `spacy_llm.registry.llm_tasks` decorator with a custom name that you can refer to in your config.&gt; üìñ For more details, see the [**usage example on writing your own task**](usage_examples/README.md#writing-your-own-task)```pythonfrom typing import Iterable, Listfrom spacy.tokens import Docfrom spacy_llm.registry import registryfrom spacy_llm.util import split_labels@registry.llm_tasks(&quot;my_namespace.MyTask.v1&quot;)def make_my_task(labels: str, my_other_config_val: float) -&gt; &quot;MyTask&quot;:    labels_list = split_labels(labels)    return MyTask(labels=labels_list, my_other_config_val=my_other_config_val)class MyTask:    def __init__(self, labels: List[str], my_other_config_val: float):        ...    def generate_prompts(self, docs: Iterable[Doc]) -&gt; Iterable[str]:        ...    def parse_responses(        self, docs: Iterable[Doc], responses: Iterable[str]    ) -&gt; Iterable[Doc]:        ...``````ini# config.cfg (excerpt)[components.llm.task]@llm_tasks = &quot;my_namespace.MyTask.v1&quot;labels = LABEL1,LABEL2,LABEL3my_other_config_val = 0.3```## Loggingspacy-llm has a built-in logger that can log the prompt sent to the LLM as well as its raw response. This logger uses the debug level and by default has a `logging.NullHandler()` configured.In order to use this logger, you can setup a simple handler like this:```pythonimport loggingimport spacy_llmspacy_llm.logger.addHandler(logging.StreamHandler())spacy_llm.logger.setLevel(logging.DEBUG)```&gt; NOTE: Any `logging` handler will work here so you probably want to use some sort of rotating `FileHandler` as the generated prompts can be quite long, especially for tasks with few-shot examples.Then when using the pipeline you'll be able to view the prompt and response.E.g. with the config and code from [Example 1](##example-1-add-a-text-classifier-using-a-gpt-3-model-from-openai) above:```pythonfrom spacy_llm.util import assemblenlp = assemble(&quot;config.cfg&quot;)doc = nlp(&quot;You look gorgeous!&quot;)print(doc.cats)```You will see `logging` output similar to:```Generated prompt for doc: You look gorgeous!You are an expert Text Classification system. Your task is to accept Text as inputand provide a category for the text based on the predefined labels.Classify the text below to any of the following labels: COMPLIMENT, INSULTThe task is non-exclusive, so you can provide more than one label as long asthey're comma-delimited. For example: Label1, Label2, Label3.Do not put any other text in your answer, only one or more of the provided labels with nothing before or after.If the text cannot be classified into any of the provided labels, answer `==NONE==`.Here is the text that needs classificationText:'''You look gorgeous!'''Model response for doc: You look gorgeous!COMPLIMENT````print(doc.cats)` to standard output should look like:```{'COMPLIMENT': 1.0, 'INSULT': 0.0}```## üìì API`spacy-llm` exposes a `llm` factory that accepts the following configuration options:| Argument         | Type                                        | Description                                                                         || ---------------- | ------------------------------------------- | ----------------------------------------------------------------------------------- || `task`           | `Optional[LLMTask]`                         | An LLMTask can generate prompts and parse LLM responses. See [docs](#tasks).        || `model`          | `Callable[[Iterable[Any]], Iterable[Any]]]` | Callable querying a specific LLM API. See [docs](#models).                          || `cache`          | `Cache`                                     | Cache to use for caching prompts and responses per doc (batch). See [docs](#cache). || `save_io`        | `bool`                                      | Whether to save prompts/responses within `Doc.user_data[&quot;llm_io&quot;]`.                 || `validate_types` | `bool`                                      | Whether to check if signatures of configured model and task are consistent.         |An `llm` component is defined by two main settings:- A [**task**](#tasks), defining the prompt to send to the LLM as well as the functionality to parse the resulting response  back into structured fields on spaCy's [Doc](https://spacy.io/api/doc) objects.- A [**model**](#models) defining the model and how to connect to it. Note that `spacy-llm` supports both access to external  APIs (such as OpenAI) as well as access to self-hosted open-source LLMs (such as using Dolly through Hugging Face).Moreover, `spacy-llm` exposes a customizable [**caching**](#cache) functionality to avoid runningthe same document through an LLM service (be it local or through a REST API) more than once.Finally, you can choose to save a stringified version of LLM prompts/responseswithin the `Doc.user_data[&quot;llm_io&quot;]` attribute by setting `save_io` to `True`.`Doc.user_data[&quot;llm_io&quot;]` is a dictionary containing one entry for every LLM componentwithin the spaCy pipeline. Each entry is itself a dictionary, with two keys:`prompt` and `response`.A note on `validate_types`: by default, `spacy-llm` checks whether the signatures of the `model` and `task` callablesare consistent with each other and emits a warning if they don't. `validate_types` can be set to `False` if you want todisable this behavior.### TasksA _task_ defines an NLP problem or question, that will be sent to the LLM via a prompt. Further, the task defineshow to parse the LLM's responses back into structured information. All tasks are registered in spaCy's `llm_tasks` registry.Practically speaking, a task should adhere to the `Protocol` `LLMTask` defined in [`ty.py`](spacy_llm/ty.py).It needs to define a `generate_prompts` function and a `parse_responses` function.Moreover, the task may define an optional [`scorer` method](https://spacy.io/api/scorer#score).It should accept an iterable of `Example`s as input and return a score dictionary.If the `scorer` method is defined, `spacy-llm` will call it to evaluate the component.#### Providing examples for few-shot promptsAll built-in tasks support few-shot prompts, i. e. including examples in a prompt. Examples can be supplied in two ways:(1) as a separate file containing only examples or (2) by initializing `llm` with a `get_examples()` callback (like anyother spaCy pipeline component).##### (1) Few-shot example fileA file containing examples for few-shot prompting can be configured like this:```ini[components.llm.task]@llm_tasks = &quot;spacy.NER.v2&quot;labels = PERSON,ORGANISATION,LOCATION[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;ner_examples.yml&quot;```The supplied file has to conform to the format expected by the required task (see the task documentation further down).##### (2) Initializing the `llm` component with a `get_examples()` callbackAlternatively, you can initialize your `nlp` pipeline by providing a `get_examples` callback for[`nlp.initialize`](https://spacy.io/api/language#initialize) and setting `n_prompt_examples` to a positive number toautomatically fetch a few examples for few-shot learning. Set `n_prompt_examples` to `-1` to use all examples aspart of the few-shot learning prompt.```ini[initialize.components.llm]n_prompt_examples = 3```#### &lt;kbd&gt;function&lt;/kbd&gt; `task.generate_prompts`Takes a collection of documents, and returns a collection of &quot;prompts&quot;, which can be of type `Any`.Often, prompts are of type `str` - but this is not enforced to allow for maximum flexibility in the framework.| Argument    | Type            | Description            || ----------- | --------------- | ---------------------- || `docs`      | `Iterable[Doc]` | The input documents.   || **RETURNS** | `Iterable[Any]` | The generated prompts. |#### &lt;kbd&gt;function&lt;/kbd&gt; `task.parse_responses`Takes a collection of LLM responses and the original documents, parses the responses into structured information,and sets the annotations on the documents. The `parse_responses` function is free to set the annotations in any way,including `Doc` fields like `ents`, `spans` or `cats`, or using custom defined fields.The `responses` are of type `Iterable[Any]`, though they will often be `str` objects. This depends on thereturn type of the [model](#models).| Argument    | Type            | Description              || ----------- | --------------- | ------------------------ || `docs`      | `Iterable[Doc]` | The input documents.     || `responses` | `Iterable[Any]` | The generated prompts.   || **RETURNS** | `Iterable[Doc]` | The annotated documents. |#### spacy.Summarization.v1The `spacy.Summarization.v1` task supports both zero-shot and few-shot prompting.```ini[components.llm.task]@llm_tasks = &quot;spacy.Summarization.v1&quot;examples = nullmax_n_words = null```| Argument      | Type                                    | Default                                                                | Description                                                                                                                              ||---------------|-----------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|| `template`    | `str`                                   | [summarization.jinja](./spacy_llm/tasks/templates/summarization.jinja) | Custom prompt template to send to LLM backend. Default templates for each task are located in the `spacy_llm/tasks/templates` directory. || `examples`    | `Optional[Callable[[], Iterable[Any]]]` | `None`                                                                 | Optional function that generates examples for few-shot learning.                                                                         || `max_n_words` | `Optional[int]`                         | `None`                                                                 | Maximum number of words to be used in summary. Note that this should not expected to work exactly.                                       || `field`       | `str`                                   | `summary`                                                              | Name of extension attribute to store summary in (i. e. the summary will be available in `doc._.{field}`).                                |The summarization task prompts the model for a concise summary of the provided text. It optionally allows to limit the response to a certain number of tokens - note that this requirement will be included in the prompt, but the task doesn'tperform a hard cut-off. It's hence possible that your summary exceeds `max_n_words`.To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```yaml- text: &gt;    The United Nations, referred to informally as the UN, is an intergovernmental organization whose stated purposes are     to maintain international peace and security, develop friendly relations among nations, achieve international     cooperation, and serve as a centre for harmonizing the actions of nations. It is the world's largest international     organization. The UN is headquartered on international territory in New York City, and the organization has other     offices in Geneva, Nairobi, Vienna, and The Hague, where the International Court of Justice is headquartered.\n\n    The UN was established after World War II with the aim of preventing future world wars, and succeeded the League of     Nations, which was characterized as ineffective.   summary: &quot;The UN is an international organization that promotes global peace, cooperation, and harmony. Established after WWII, its purpose is to prevent future world wars.&quot;``````ini[components.llm.task]@llm_tasks = &quot;spacy.summarization.v1&quot;max_n_words = 20[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;summarization_examples.yml&quot;```#### spacy.NER.v2The built-in NER task supports both zero-shot and few-shot prompting. This version also supports explicitly defining the provided labels with custom descriptions.```ini[components.llm.task]@llm_tasks = &quot;spacy.NER.v2&quot;labels = [&quot;PERSON&quot;, &quot;ORGANISATION&quot;, &quot;LOCATION&quot;]examples = null```| Argument                  | Type                                    | Default                                                  | Description                                                                                                                                           || ------------------------- | --------------------------------------- | -------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- || `labels`                  | `Union[List[str], str]`                 |                                                          | List of labels or str of comma-separated list of labels.                                                                                              || `template`                | `str`                                   | [ner.v2.jinja](./spacy_llm/tasks/templates/ner.v2.jinja) | Custom prompt template to send to LLM model. Default templates for each task are located in the `spacy_llm/tasks/templates` directory.                || `label_definitions`       | `Optional[Dict[str, str]]`              | `None`                                                   | Optional dict mapping a label to a description of that label. These descriptions are added to the prompt to help instruct the LLM on what to extract. || `examples`                | `Optional[Callable[[], Iterable[Any]]]` | `None`                                                   | Optional function that generates examples for few-shot learning.                                                                                      || `normalizer`              | `Optional[Callable[[str], str]]`        | `None`                                                   | Function that normalizes the labels as returned by the LLM. If `None`, defaults to `spacy.LowercaseNormalizer.v1`.                                    || `alignment_mode`          | `str`                                   | `&quot;contract&quot;`                                             | Alignment mode in case the LLM returns entities that do not align with token boundaries. Options are `&quot;strict&quot;`, `&quot;contract&quot;` or `&quot;expand&quot;`.          || `case_sensitive_matching` | `bool`                                  | `False`                                                  | Whether to search without case sensitivity.                                                                                                           || `single_match`            | `bool`                                  | `False`                                                  | Whether to match an entity in the LLM's response only once (the first hit) or multiple times.                                                         |The NER task implementation doesn't currently ask the LLM for specific offsets, but simply expects a list of strings that represent the enties in the document.This means that a form of string matching is required. This can be configured by the following parameters:- The `single_match` parameter is typically set to `False` to allow for multiple matches. For instance, the response from the LLM might only mention the entity &quot;Paris&quot; once, but you'd still  want to mark it every time it occurs in the document.- The case-sensitive matching is typically set to `False` to be robust against case variances in the LLM's output.- The `alignment_mode` argument is used to match entities as returned by the LLM to the tokens from the original `Doc` - specifically it's used as argument  in the call to [`doc.char_span()`](https://spacy.io/api/doc#char_span). The `&quot;strict&quot;` mode will only keep spans that strictly adhere to the given token boundaries.  `&quot;contract&quot;` will only keep those tokens that are fully within the given range, e.g. reducing `&quot;New Y&quot;` to `&quot;New&quot;`.  Finally, `&quot;expand&quot;` will expand the span to the next token boundaries, e.g. expanding `&quot;New Y&quot;` out to `&quot;New York&quot;`.To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```yaml- text: Jack and Jill went up the hill.  entities:    PERSON:      - Jack      - Jill    LOCATION:      - hill- text: Jack fell down and broke his crown.  entities:    PERSON:      - Jack``````ini[components.llm.task]@llm_tasks = &quot;spacy.NER.v2&quot;labels = PERSON,ORGANISATION,LOCATION[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;ner_examples.yml&quot;```You can also write definitions for each label and provide them via the `label_definitions` argument. This lets you tellthe LLM exactly what you're looking for rather than relying on the LLM to interpret its task given just the label name.Label descriptions are freeform so you can write whatever you want here, but through some experiments a briefdescription along with some examples and counter examples seems to work quite well.```ini[components.llm.task]@llm_tasks = &quot;spacy.NER.v2&quot;labels = PERSON,SPORTS_TEAM[components.llm.task.label_definitions]PERSON = &quot;Extract any named individual in the text.&quot;SPORTS_TEAM = &quot;Extract the names of any professional sports team. e.g. Golden State Warriors, LA Lakers, Man City, Real Madrid&quot;```&gt; Label descriptions can also be used with explicit examples to give as much info to the LLM model as possible.#### spacy.NER.v1The original version of the built-in NER task supports both zero-shot and few-shot prompting.```ini[components.llm.task]@llm_tasks = &quot;spacy.NER.v1&quot;labels = PERSON,ORGANISATION,LOCATIONexamples = null```| Argument                  | Type                                    | Default      | Description                                                                                                                                  || ------------------------- | --------------------------------------- | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------- || `labels`                  | `str`                                   |              | Comma-separated list of labels.                                                                                                              || `examples`                | `Optional[Callable[[], Iterable[Any]]]` | `None`       | Optional function that generates examples for few-shot learning.                                                                             || `normalizer`              | `Optional[Callable[[str], str]]`        | `None`       | Function that normalizes the labels as returned by the LLM. If `None`, defaults to `spacy.LowercaseNormalizer.v1`.                           || `alignment_mode`          | `str`                                   | `&quot;contract&quot;` | Alignment mode in case the LLM returns entities that do not align with token boundaries. Options are `&quot;strict&quot;`, `&quot;contract&quot;` or `&quot;expand&quot;`. || `case_sensitive_matching` | `bool`                                  | `False`      | Whether to search without case sensitivity.                                                                                                  || `single_match`            | `bool`                                  | `False`      | Whether to match an entity in the LLM's response only once (the first hit) or multiple times.                                                |The NER task implementation doesn't currently ask the LLM for specific offsets, but simply expects a list of strings that represent the enties in the document.This means that a form of string matching is required. This can be configured by the following parameters:- The `single_match` parameter is typically set to `False` to allow for multiple matches. For instance, the response from the LLM might only mention the entity &quot;Paris&quot; once, but you'd still  want to mark it every time it occurs in the document.- The case-sensitive matching is typically set to `False` to be robust against case variances in the LLM's output.- The `alignment_mode` argument is used to match entities as returned by the LLM to the tokens from the original `Doc` - specifically it's used as argument  in the call to [`doc.char_span()`](https://spacy.io/api/doc#char_span). The `&quot;strict&quot;` mode will only keep spans that strictly adhere to the given token boundaries.  `&quot;contract&quot;` will only keep those tokens that are fully within the given range, e.g. reducing `&quot;New Y&quot;` to `&quot;New&quot;`.  Finally, `&quot;expand&quot;` will expand the span to the next token boundaries, e.g. expanding `&quot;New Y&quot;` out to `&quot;New York&quot;`.To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```yaml- text: Jack and Jill went up the hill.  entities:    PERSON:      - Jack      - Jill    LOCATION:      - hill- text: Jack fell down and broke his crown.  entities:    PERSON:      - Jack``````ini[components.llm.task]@llm_tasks = &quot;spacy.NER.v1&quot;labels = PERSON,ORGANISATION,LOCATION[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;ner_examples.yml&quot;```#### spacy.SpanCat.v2The built-in SpanCat task is a simple adaptation of the NER task tosupport overlapping entities and store its annotations in `doc.spans`.```ini[components.llm.task]@llm_tasks = &quot;spacy.SpanCat.v2&quot;labels = [&quot;PERSON&quot;, &quot;ORGANISATION&quot;, &quot;LOCATION&quot;]examples = null```| Argument                  | Type                                    | Default                                                            | Description                                                                                                                                           || ------------------------- | --------------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- || `labels`                  | `Union[List[str], str]`                 |                                                                    | List of labels or str of comma-separated list of labels.                                                                                              || `template`                | `str`                                   | [`spancat.v2.jinja`](./spacy_llm/tasks/templates/spancat.v2.jinja) | Custom prompt template to send to LLM model. Default templates for each task are located in the `spacy_llm/tasks/templates` directory.                || `label_definitions`       | `Optional[Dict[str, str]]`              | `None`                                                             | Optional dict mapping a label to a description of that label. These descriptions are added to the prompt to help instruct the LLM on what to extract. || `spans_key`               | `str`                                   | `&quot;sc&quot;`                                                             | Key of the `Doc.spans` dict to save the spans under.                                                                                                  || `examples`                | `Optional[Callable[[], Iterable[Any]]]` | `None`                                                             | Optional function that generates examples for few-shot learning.                                                                                      || `normalizer`              | `Optional[Callable[[str], str]]`        | `None`                                                             | Function that normalizes the labels as returned by the LLM. If `None`, defaults to `spacy.LowercaseNormalizer.v1`.                                    || `alignment_mode`          | `str`                                   | `&quot;contract&quot;`                                                       | Alignment mode in case the LLM returns entities that do not align with token boundaries. Options are `&quot;strict&quot;`, `&quot;contract&quot;` or `&quot;expand&quot;`.          || `case_sensitive_matching` | `bool`                                  | `False`                                                            | Whether to search without case sensitivity.                                                                                                           || `single_match`            | `bool`                                  | `False`                                                            | Whether to match an entity in the LLM's response only once (the first hit) or multiple times.                                                         |Except for the `spans_key` parameter, the SpanCat task reuses the configurationfrom the NER task.Refer to [its documentation](#spacynerv2) for more insight.#### spacy.SpanCat.v1The original version of the built-in SpanCat task is a simple adaptation of the v1 NER task tosupport overlapping entities and store its annotations in `doc.spans`.```ini[components.llm.task]@llm_tasks = &quot;spacy.SpanCat.v1&quot;labels = PERSON,ORGANISATION,LOCATIONexamples = null```| Argument                  | Type                                    | Default      | Description                                                                                                                                  || ------------------------- | --------------------------------------- | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------- || `labels`                  | `str`                                   |              | Comma-separated list of labels.                                                                                                              || `spans_key`               | `str`                                   | `&quot;sc&quot;`       | Key of the `Doc.spans` dict to save the spans under.                                                                                         || `examples`                | `Optional[Callable[[], Iterable[Any]]]` | `None`       | Optional function that generates examples for few-shot learning.                                                                             || `normalizer`              | `Optional[Callable[[str], str]]`        | `None`       | Function that normalizes the labels as returned by the LLM. If `None`, defaults to `spacy.LowercaseNormalizer.v1`.                           || `alignment_mode`          | `str`                                   | `&quot;contract&quot;` | Alignment mode in case the LLM returns entities that do not align with token boundaries. Options are `&quot;strict&quot;`, `&quot;contract&quot;` or `&quot;expand&quot;`. || `case_sensitive_matching` | `bool`                                  | `False`      | Whether to search without case sensitivity.                                                                                                  || `single_match`            | `bool`                                  | `False`      | Whether to match an entity in the LLM's response only once (the first hit) or multiple times.                                                |Except for the `spans_key` parameter, the SpanCat task reuses the configurationfrom the NER task.Refer to [its documentation](#spacynerv1) for more insight.#### spacy.TextCat.v3Version 3 (the most recent) of the built-in TextCat task supports both zero-shot and few-shot prompting. It allowssetting definitions of labels. Those definitions are included in the prompt.```ini[components.llm.task]@llm_tasks = &quot;spacy.TextCat.v3&quot;labels = [&quot;COMPLIMENT&quot;, &quot;INSULT&quot;]label_definitions = {    &quot;COMPLIMENT&quot;: &quot;a polite expression of praise or admiration.&quot;,    &quot;INSULT&quot;: &quot;a disrespectful or scornfully abusive remark or act.&quot;}examples = null```| Argument            | Type                                    | Default                                                      | Description                                                                                                                                      || ------------------- | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------ || `labels`            | `Union[List[str], str]`                 |                                                              | List of labels or str of comma-separated list of labels.                                                                                         || `label_definitions` | `Optional[Dict[str, str]]`              | `None`                                                       | Dictionary of label definitions. Included in the prompt, if set.                                                                                 || `template`          | `str`                                   | [`textcat.jinja`](./spacy_llm/tasks/templates/textcat.jinja) | Custom prompt template to send to LLM backend. Default templates for each task are located in the `spacy_llm/tasks/templates` directory.         || `examples`          | `Optional[Callable[[], Iterable[Any]]]` | `None`                                                       | Optional function that generates examples for few-shot learning.                                                                                 || `normalizer`        | `Optional[Callable[[str], str]]`        | `None`                                                       | Function that normalizes the labels as returned by the LLM. If `None`, falls back to `spacy.LowercaseNormalizer.v1`.                             || `exclusive_classes` | `bool`                                  | `False`                                                      | If set to `True`, only one label per document should be valid. If set to `False`, one document can have multiple labels.                         || `allow_none`        | `bool`                                  | `True`                                                       | When set to `True`, allows the LLM to not return any of the given label. The resulting dict in `doc.cats` will have `0.0` scores for all labels. || `verbose`           | `bool`                                  | `False`                                                      | If set to `True`, warnings will be generated when the LLM returns invalid responses.                                                             |To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```json[  {    &quot;text&quot;: &quot;You look great!&quot;,    &quot;answer&quot;: &quot;Compliment&quot;  },  {    &quot;text&quot;: &quot;You are not very clever at all.&quot;,    &quot;answer&quot;: &quot;Insult&quot;  }]``````ini[components.llm.task]@llm_tasks = &quot;spacy.TextCat.v3&quot;labels = [&quot;COMPLIMENT&quot;, &quot;INSULT&quot;]label_definitions = {    &quot;COMPLIMENT&quot;: &quot;a polite expression of praise or admiration.&quot;,    &quot;INSULT&quot;: &quot;a disrespectful or scornfully abusive remark or act.&quot;}[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;textcat_examples.json&quot;```#### spacy.TextCat.v2Version 2 of the built-in TextCat task supports both zero-shot and few-shot prompting and includes an improved prompttemplate.```ini[components.llm.task]@llm_tasks = &quot;spacy.TextCat.v2&quot;labels = [&quot;COMPLIMENT&quot;, &quot;INSULT&quot;]examples = null```| Argument            | Type                                    | Default                                                      | Description                                                                                                                                      || ------------------- | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------ || `labels`            | `Union[List[str], str]`                 |                                                              | List of labels or str of comma-separated list of labels.                                                                                         || `template`          | `str`                                   | [`textcat.jinja`](./spacy_llm/tasks/templates/textcat.jinja) | Custom prompt template to send to LLM model. Default templates for each task are located in the `spacy_llm/tasks/templates` directory.           || `examples`          | `Optional[Callable[[], Iterable[Any]]]` | `None`                                                       | Optional function that generates examples for few-shot learning.                                                                                 || `normalizer`        | `Optional[Callable[[str], str]]`        | `None`                                                       | Function that normalizes the labels as returned by the LLM. If `None`, falls back to `spacy.LowercaseNormalizer.v1`.                             || `exclusive_classes` | `bool`                                  | `False`                                                      | If set to `True`, only one label per document should be valid. If set to `False`, one document can have multiple labels.                         || `allow_none`        | `bool`                                  | `True`                                                       | When set to `True`, allows the LLM to not return any of the given label. The resulting dict in `doc.cats` will have `0.0` scores for all labels. || `verbose`           | `bool`                                  | `False`                                                      | If set to `True`, warnings will be generated when the LLM returns invalid responses.                                                             |To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```json[  {    &quot;text&quot;: &quot;You look great!&quot;,    &quot;answer&quot;: &quot;Compliment&quot;  },  {    &quot;text&quot;: &quot;You are not very clever at all.&quot;,    &quot;answer&quot;: &quot;Insult&quot;  }]``````ini[components.llm.task]@llm_tasks = &quot;spacy.TextCat.v2&quot;labels = [&quot;COMPLIMENT&quot;, &quot;INSULT&quot;][components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;textcat_examples.json&quot;```#### spacy.TextCat.v1Version 1 of the built-in TextCat task supports both zero-shot and few-shot prompting.```ini[components.llm.task]@llm_tasks = &quot;spacy.TextCat.v1&quot;labels = COMPLIMENT,INSULTexamples = null```| Argument            | Type                                    | Default | Description                                                                                                                                      || ------------------- | --------------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------ || `labels`            | str                                     |         | Comma-separated list of labels.                                                                                                                  || `examples`          | `Optional[Callable[[], Iterable[Any]]]` | `None`  | Optional function that generates examples for few-shot learning.                                                                                 || `normalizer`        | `Optional[Callable[[str], str]]`        | `None`  | Function that normalizes the labels as returned by the LLM. If `None`, falls back to `spacy.LowercaseNormalizer.v1`.                             || `exclusive_classes` | `bool`                                  | `False` | If set to `True`, only one label per document should be valid. If set to `False`, one document can have multiple labels.                         || `allow_none`        | `bool`                                  | `True`  | When set to `True`, allows the LLM to not return any of the given label. The resulting dict in `doc.cats` will have `0.0` scores for all labels. || `verbose`           | `bool`                                  | `False` | If set to `True`, warnings will be generated when the LLM returns invalid responses.                                                             |To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```json[  {    &quot;text&quot;: &quot;You look great!&quot;,    &quot;answer&quot;: &quot;Compliment&quot;  },  {    &quot;text&quot;: &quot;You are not very clever at all.&quot;,    &quot;answer&quot;: &quot;Insult&quot;  }]``````ini[components.llm.task]@llm_tasks = &quot;spacy.TextCat.v2&quot;labels = COMPLIMENT,INSULT[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;textcat_examples.json&quot;```#### spacy.REL.v1The built-in REL task supports both zero-shot and few-shot prompting.It relies on an upstream NER component for entities extraction.```ini[components.llm.task]@llm_tasks = &quot;spacy.REL.v1&quot;labels = [&quot;LivesIn&quot;, &quot;Visits&quot;]```| Argument            | Type                                    | Default                                              | Description                                                                                                                            || ------------------- | --------------------------------------- | ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- || `labels`            | `Union[List[str], str]`                 |                                                      | List of labels or str of comma-separated list of labels.                                                                               || `template`          | `str`                                   | [`rel.jinja`](./spacy_llm/tasks/templates/rel.jinja) | Custom prompt template to send to LLM model. Default templates for each task are located in the `spacy_llm/tasks/templates` directory. || `label_description` | `Optional[Dict[str, str]]`              | `None`                                               | Dictionary providing a description for each relation label.                                                                            || `examples`          | `Optional[Callable[[], Iterable[Any]]]` | `None`                                               | Optional function that generates examples for few-shot learning.                                                                       || `normalizer`        | `Optional[Callable[[str], str]]`        | `None`                                               | Function that normalizes the labels as returned by the LLM. If `None`, falls back to `spacy.LowercaseNormalizer.v1`.                   || `verbose`           | `bool`                                  | `False`                                              | If set to `True`, warnings will be generated when the LLM returns invalid responses.                                                   |To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```jsonl{&quot;text&quot;: &quot;Laura bought a house in Boston with her husband Mark.&quot;, &quot;ents&quot;: [{&quot;start_char&quot;: 0, &quot;end_char&quot;: 5, &quot;label&quot;: &quot;PERSON&quot;}, {&quot;start_char&quot;: 24, &quot;end_char&quot;: 30, &quot;label&quot;: &quot;GPE&quot;}, {&quot;start_char&quot;: 48, &quot;end_char&quot;: 52, &quot;label&quot;: &quot;PERSON&quot;}], &quot;relations&quot;: [{&quot;dep&quot;: 0, &quot;dest&quot;: 1, &quot;relation&quot;: &quot;LivesIn&quot;}, {&quot;dep&quot;: 2, &quot;dest&quot;: 1, &quot;relation&quot;: &quot;LivesIn&quot;}]}{&quot;text&quot;: &quot;Michael travelled through South America by bike.&quot;, &quot;ents&quot;: [{&quot;start_char&quot;: 0, &quot;end_char&quot;: 7, &quot;label&quot;: &quot;PERSON&quot;}, {&quot;start_char&quot;: 26, &quot;end_char&quot;: 39, &quot;label&quot;: &quot;LOC&quot;}], &quot;relations&quot;: [{&quot;dep&quot;: 0, &quot;dest&quot;: 1, &quot;relation&quot;: &quot;Visits&quot;}]}``````ini[components.llm.task]@llm_tasks = &quot;spacy.REL.v1&quot;labels = [&quot;LivesIn&quot;, &quot;Visits&quot;][components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;rel_examples.jsonl&quot;```Note: the REL task relies on pre-extracted entities to make its prediction.Hence, you'll need to add a component that populates `doc.ents` with recognizedspans to your spaCy pipeline and put it _before_ the REL component.#### spacy.Lemma.v1The `Lemma.v1` task lemmatizes the provided text and updates the `lemma_` attribute in the doc's tokens accordingly.```ini[components.llm.task]@llm_tasks = &quot;spacy.Lemma.v1&quot;examples = null```| Argument   | Type                                    | Default                                                | Description                                                                                                                            || ---------- | --------------------------------------- | ------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- || `template` | `str`                                   | [lemma.jinja](./spacy_llm/tasks/templates/lemma.jinja) | Custom prompt template to send to LLM model. Default templates for each task are located in the `spacy_llm/tasks/templates` directory. || `examples` | `Optional[Callable[[], Iterable[Any]]]` | `None`                                                 | Optional function that generates examples for few-shot learning.                                                                       |`Lemma.v1` prompts the LLM to lemmatize the passed text and return the lemmatized version as a list of tokens and theircorresponding lemma. E. g. the text`I'm buying ice cream for my friends` should invoke the response```I: I'm: bebuying: buyice: icecream: creamfor: formy: myfriends: friend.: .```If for any given text/doc instance the number of lemmas returned by the LLM doesn't match the number of tokens recognizedby spaCy, no lemmas are stored in the corresponding doc's tokens. Otherwise the tokens `.lemma_` property is updated withthe lemma suggested by the LLM.To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```yaml- text: I'm buying ice cream.  lemmas:    - &quot;I&quot;: &quot;I&quot;    - &quot;'m&quot;: &quot;be&quot;    - &quot;buying&quot;: &quot;buy&quot;    - &quot;ice&quot;: &quot;ice&quot;    - &quot;cream&quot;: &quot;cream&quot;    - &quot;.&quot;: &quot;.&quot;- text: I've watered the plants.  lemmas:    - &quot;I&quot;: &quot;I&quot;    - &quot;'ve&quot;: &quot;have&quot;    - &quot;watered&quot;: &quot;water&quot;    - &quot;the&quot;: &quot;the&quot;    - &quot;plants&quot;: &quot;plant&quot;    - &quot;.&quot;: &quot;.&quot;``````ini[components.llm.task]@llm_tasks = &quot;spacy.Lemma.v1&quot;[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;lemma_examples.yml&quot;```#### spacy.Sentiment.v1Performs sentiment analysis on provided texts. Scores between 0 and 1 are stored in `Doc._.sentiment` - the higher, themore positive. Note in cases of parsing issues (e. g. in case of unexpected LLM responses) the value might be `None`.```ini[components.llm.task]@llm_tasks = &quot;spacy.Sentiment.v1&quot;examples = null```| Argument   | Type                                    | Default                                                        | Description                                                                                                                            || ---------- | --------------------------------------- | -------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- || `template` | `str`                                   | [sentiment.jinja](./spacy_llm/tasks/templates/sentiment.jinja) | Custom prompt template to send to LLM model. Default templates for each task are located in the `spacy_llm/tasks/templates` directory. || `examples` | `Optional[Callable[[], Iterable[Any]]]` | `None`                                                         | Optional function that generates examples for few-shot learning.                                                                       || `field`    | `str`                                   | `sentiment`                                                    | Name of extension attribute to store summary in (i. e. the summary will be available in `doc._.{field}`).                              |To perform few-shot learning, you can write down a few examples in a separate file, and provide these to be injected into the prompt to the LLM.The default reader `spacy.FewShotReader.v1` supports `.yml`, `.yaml`, `.json` and `.jsonl`.```yaml- text: &quot;This is horrifying.&quot;  score: 0- text: &quot;This is underwhelming.&quot;  score: 0.25- text: &quot;This is ok.&quot;  score: 0.5- text: &quot;I'm looking forward to this!&quot;  score: 1.0``````ini[components.llm.task]@llm_tasks = &quot;spacy.Sentiment.v1&quot;[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;sentiment_examples.yml&quot;```#### spacy.NoOp.v1This task is only useful for testing - it tells the LLM to do nothing, and does not set any fields on the `docs`.```ini[components.llm.task]@llm_tasks = &quot;spacy.NoOp.v1&quot;```### ModelsA _model_ defines which LLM model to query, and how to query it. It can be a simple function taking a collectionof prompts (consistent with the output type of `task.generate_prompts()`) and returning a collection of responses(consistent with the expected input of `parse_responses`). Generally speaking, it's a function of type `Callable[[Iterable[Any]], Iterable[Any]]`,but specific implementations can have other signatures, like `Callable[[Iterable[str]], Iterable[str]]`.All built-in models are registered in `llm_models`. If no model is specified, the repo currently connects to the `OpenAI` API by defaultusing REST, and accesses the `&quot;gpt-3.5-turbo&quot;` model.Currently three different approaches to use LLMs are supported:1. `spacy-llm`s native REST backend. This is the default for all hosted models (e. g. OpenAI, Cohere, Anthropic, ...).2. A HuggingFace integration that allows to run a limited set of HF models locally.3. A LangChain integration that allows to run any model supported by LangChain (hosted or locally).Approaches 1. and 2 are the default for hosted model and local models, respectively. Alternatively you can use LangChainto access hosted or local models by specifying one of the models registered with the `langchain.` prefix.&gt; :question: Why LangChain if there are also are a native REST and a HuggingFace backend? When should I use what?&gt;&gt; Third-party libraries like `langchain` focus on prompt management, integration of many different LLM&gt; APIs, and other related features such as conversational memory or agents. `spacy-llm` on the other hand emphasizes&gt; features we consider useful in the context of NLP pipelines utilizing LLMs to process documents (mostly) independent&gt; from each other. It makes sense that the feature sets of such third-party libraries and `spacy-llm` aren't identical -&gt; and users might want to take advantage of features not available in `spacy-llm`.&gt;&gt; The advantage of implementing our own REST and HuggingFace integrations is that we can ensure a larger degree of stability and robustness, as&gt; we can guarantee backwards-compatibility and more smoothly integrated error handling.&gt;&gt; If however there are features or APIs not natively covered by `spacy-llm`, it's trivial to utilize LangChain to cover&gt; this - and easy to customize the prompting mechanism, if so required.Note that when using hosted services, you have to ensure that the proper API keys are set as environment variables asdescribed by the corresponding provider's documentation.E. g. when using OpenAI, you have to get an API key from openai.com, and ensure that the keys are set asenvironmental variables:```shellexport OPENAI_API_KEY=&quot;sk-...&quot;export OPENAI_API_ORG=&quot;org-...&quot;```For Cohere it's```shellexport CO_API_KEY=&quot;...&quot;```and for Anthropic```shellexport ANTHROPIC_API_KEY=&quot;...&quot;```#### spacy.GPT-4.v1OpenAI's `gpt-4` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.GPT-4.v1&quot;name = &quot;gpt-4&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                                            | Default   | Description                                                                                                          || ----------- | --------------------------------------------------------------- | --------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;gpt-4&quot;, &quot;gpt-4-0314&quot;, &quot;gpt-4-32k&quot;, &quot;gpt-4-32k-0314&quot;]` | `&quot;gpt-4&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                                                | `{}`      | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                                          | `True`    | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                                           | `3`       | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                                           | `30`      | Timeout for API request in seconds.                                                                                  |#### spacy.GPT-3-5.v1OpenAI's `gpt-3-5` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.GPT-3-5.v1&quot;name = &quot;gpt-3.5-turbo&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                                                                            | Default           | Description                                                                                                          || ----------- | ----------------------------------------------------------------------------------------------- | ----------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;gpt-3.5-turbo&quot;, &quot;gpt-3.5-turbo-16k&quot;, &quot;gpt-3.5-turbo-0613&quot;, &quot;gpt-3.5-turbo-0613-16k&quot;]` | `&quot;gpt-3.5-turbo&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                                                                                | `{}`              | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                                                                          | `True`            | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                                                                           | `3`               | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                                                                           | `30`              | Timeout for API request in seconds.                                                                                  |#### spacy.Text-Davinci.v1OpenAI's `text-davinci` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Text-Davinci.v1&quot;name = &quot;text-davinci-003&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                              | Default              | Description                                                                                                          || ----------- | ------------------------------------------------- | -------------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;text-davinci-002&quot;, &quot;text-davinci-003&quot;]` | `&quot;text-davinci-003&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                                  | `{}`                 | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                            | `True`               | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                             | `3`                  | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                             | `30`                 | Timeout for API request in seconds.                                                                                  |#### spacy.Code-Davinci.v1OpenAI's `code-davinci` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Code-Davinci.v1&quot;name = &quot;code-davinci-002&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                          | Default              | Description                                                                                                          || ----------- | ----------------------------- | -------------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;code-davinci-002&quot;]` | `&quot;code-davinci-002&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`              | `{}`                 | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                        | `True`               | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                         | `3`                  | Max. number of tries for API request.                                                                                || `timeout`   | `int`                         | `30`                 | Timeout for API request in seconds.                                                                                  |#### spacy.Text-Curie.v1OpenAI's `text-curie` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Text-Curie.v1&quot;name = &quot;text-curie-001&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                        | Default            | Description                                                                                                          || ----------- | --------------------------- | ------------------ | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;text-curie-001&quot;]` | `&quot;text-curie-001&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`            | `{}`               | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                      | `True`             | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                       | `3`                | Max. number of tries for API request.                                                                                || `timeout`   | `int`                       | `30`               | Timeout for API request in seconds.                                                                                  |#### spacy.Text-Babbage.v1OpenAI's `text-babbage` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Text-Babbage.v1&quot;name = &quot;text-babbage-001&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                          | Default              | Description                                                                                                          || ----------- | ----------------------------- | -------------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;text-babbage-001&quot;]` | `&quot;text-babbage-001&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`              | `{}`                 | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                        | `True`               | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                         | `3`                  | Max. number of tries for API request.                                                                                || `timeout`   | `int`                         | `30`                 | Timeout for API request in seconds.                                                                                  |#### spacy.Text-Ada.v1OpenAI's `text-ada` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Text-Ada.v1&quot;name = &quot;text-ada-001&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                      | Default          | Description                                                                                                          || ----------- | ------------------------- | ---------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;text-ada-001&quot;]` | `&quot;text-ada-001&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`          | `{}`             | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                    | `True`           | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                     | `3`              | Max. number of tries for API request.                                                                                || `timeout`   | `int`                     | `30`             | Timeout for API request in seconds.                                                                                  |#### spacy.Davinci.v1OpenAI's `davinci` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Davinci.v1 &quot;name = &quot;davinci&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                 | Default     | Description                                                                                                          || ----------- | -------------------- | ----------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;davinci&quot;]` | `&quot;davinci&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`     | `{}`        | Further configuration passed on to the model.                                                                        || `strict`    | `bool`               | `True`      | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                | `3`         | Max. number of tries for API request.                                                                                || `timeout`   | `int`                | `30`        | Timeout for API request in seconds.                                                                                  |#### spacy.Curie.v1OpenAI's `curie` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Curie.v1 &quot;name = &quot;curie&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type               | Default   | Description                                                                                                          || ----------- | ------------------ | --------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;curie&quot;]` | `&quot;curie&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`   | `{}`      | Further configuration passed on to the model.                                                                        || `strict`    | `bool`             | `True`    | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`              | `3`       | Max. number of tries for API request.                                                                                || `timeout`   | `int`              | `30`      | Timeout for API request in seconds.                                                                                  |#### spacy.Babbage.v1OpenAI's `babbage` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Babbage.v1 &quot;name = &quot;babbage&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                 | Default     | Description                                                                                                          || ----------- | -------------------- | ----------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;babbage&quot;]` | `&quot;babbage&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`     | `{}`        | Further configuration passed on to the model.                                                                        || `strict`    | `bool`               | `True`      | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                | `3`         | Max. number of tries for API request.                                                                                || `timeout`   | `int`                | `30`        | Timeout for API request in seconds.                                                                                  |#### spacy.Ada.v1OpenAI's `ada` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Ada.v1 &quot;name = &quot;ada&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type             | Default | Description                                                                                                          || ----------- | ---------------- | ------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;ada&quot;]` | `&quot;ada&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]` | `{}`    | Further configuration passed on to the model.                                                                        || `strict`    | `bool`           | `True`  | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`            | `3`     | Max. number of tries for API request.                                                                                || `timeout`   | `int`            | `30`    | Timeout for API request in seconds.                                                                                  |#### spacy.Command.v1Cohere's `command` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Command.v1 &quot;name = &quot;command&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                                                              | Default     | Description                                                                                                          || ----------- | --------------------------------------------------------------------------------- | ----------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;command&quot;, &quot;command-light&quot;, &quot;command-light-nightly&quot;, &quot;command-nightly&quot;]` | `&quot;command&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                                                                  | `{}`        | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                                                            | `True`      | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                                                             | `3`         | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                                                             | `30`        | Timeout for API request in seconds.                                                                                  |#### spacy.Claude-1.v1Anthropic's `claude-1` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Claude-1.v1 &quot;name = &quot;claude-1&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                   | Default      | Description                                                                                                          || ----------- | -------------------------------------- | ------------ | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;claude-1&quot;, &quot;claude-1-100k&quot;]` | `&quot;claude-1&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                       | `{}`         | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                 | `True`       | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                  | `3`          | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                  | `30`         | Timeout for API request in seconds.                                                                                  |#### spacy.Claude-instant-1.v1Anthropic's `claude-instant-1` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Claude-instant-1.v1 &quot;name = &quot;claude-instant-1&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                                   | Default              | Description                                                                                                          || ----------- | ------------------------------------------------------ | -------------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;claude-instant-1&quot;, &quot;claude-instant-1-100k&quot;]` | `&quot;claude-instant-1&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                                       | `{}`                 | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                                 | `True`               | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                                  | `3`                  | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                                  | `30`                 | Timeout for API request in seconds.                                                                                  |#### spacy.Claude-instant-1-1.v1Anthropic's `claude-instant-1.1` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Claude-instant-1-1.v1 &quot;name = &quot;claude-instant-1.1&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                                       | Default                | Description                                                                                                          || ----------- | ---------------------------------------------------------- | ---------------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;claude-instant-1.1&quot;, &quot;claude-instant-1.1-100k&quot;]` | `&quot;claude-instant-1.1&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                                           | `{}`                   | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                                     | `True`                 | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                                      | `3`                    | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                                      | `30`                   | Timeout for API request in seconds.                                                                                  |#### spacy.Claude-1-0.v1Anthropic's `claude-1.0` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Claude-1-0.v1 &quot;name = &quot;claude-1.0&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                    | Default        | Description                                                                                                          || ----------- | ----------------------- | -------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;claude-1.0&quot;]` | `&quot;claude-1.0&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`        | `{}`           | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                  | `True`         | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                   | `3`            | Max. number of tries for API request.                                                                                || `timeout`   | `int`                   | `30`           | Timeout for API request in seconds.                                                                                  |#### spacy.Claude-1-2.v1Anthropic's `claude-1.2` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Claude-1-2.v1 &quot;name = &quot;claude-1.2&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                    | Default        | Description                                                                                                          || ----------- | ----------------------- | -------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;claude-1.2&quot;]` | `&quot;claude-1.2&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`        | `{}`           | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                  | `True`         | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                   | `3`            | Max. number of tries for API request.                                                                                || `timeout`   | `int`                   | `30`           | Timeout for API request in seconds.                                                                                  |#### spacy.Claude-1-3.v1Anthropic's `claude-1.3` model family.Example config:```ini[components.llm.model]@llm_models = &quot;spacy.Claude-1-3.v1 &quot;name = &quot;claude-1.3&quot;config = {&quot;temperature&quot;: 0.3}```| Argument    | Type                                       | Default        | Description                                                                                                          || ----------- | ------------------------------------------ | -------------- | -------------------------------------------------------------------------------------------------------------------- || `name`      | `Literal[&quot;claude-1.3&quot;, &quot;claude-1.3-100k&quot;]` | `&quot;claude-1.3&quot;` | Model name, i. e. any supported variant for this particular model.                                                   || `config`    | `Dict[Any, Any]`                           | `{}`           | Further configuration passed on to the model.                                                                        || `strict`    | `bool`                                     | `True`         | If `True`, raises an error if the LLM API returns a malformed response. Otherwise, return the error responses as is. || `max_tries` | `int`                                      | `3`            | Max. number of tries for API request.                                                                                || `timeout`   | `int`                                      | `30`           | Timeout for API request in seconds.                                                                                  |#### spacy.Dolly.v1To use this model, ideally you have a GPU enabled and have installed `transformers`, `torch` and CUDA in your virtual environment.This allows you to have the setting `device=cuda:0` in your config, which ensures that the model is loaded entirely on the GPU (and fails otherwise).You can do so with```shellpython -m pip install &quot;spacy-llm[transformers]&quot; &quot;transformers[sentencepiece]&quot;```If you don't have access to a GPU, you can install `accelerate` and set`device_map=auto` instead, but be aware that this may result in some layers getting distributed to the CPU or even the hard drive,which may ultimately result in extremely slow queries.```shellpython -m pip install &quot;accelerate&gt;=0.16.0,&lt;1.0&quot;```Example config block:```ini[components.llm.model]@llm_models = &quot;spacy.Dolly.v1&quot;name = &quot;dolly-v2-3b&quot;```| Argument      | Type                                                    | Default | Description                                                                                      || ------------- | ------------------------------------------------------- | ------- | ------------------------------------------------------------------------------------------------ || `name`        | `Literal[&quot;dolly-v2-3b&quot;, &quot;dolly-v2-7b&quot;, &quot;dolly-v2-12b&quot;]` |         | The name of a Dolly model that is supported (e. g. &quot;dolly-v2-3b&quot; or &quot;dolly-v2-12b&quot;).             || `config_init` | `Dict[str, Any]`                                        | `{}`    | Further configuration passed on to the construction of the model with `transformers.pipeline()`. || `config_run`  | `Dict[str, Any]`                                        | `{}`    | Further configuration used during model inference.                                               |Supported models (see the [Databricks models page](https://huggingface.co/databricks) on Hugging Face for details):- `&quot;databricks/dolly-v2-3b&quot;`- `&quot;databricks/dolly-v2-7b&quot;`- `&quot;databricks/dolly-v2-12b&quot;`Note that Hugging Face will download this model the first time you use it - you can[define the cached directory](https://huggingface.co/docs/huggingface_hub/main/en/guides/manage-cache)by setting the environmental variable `HF_HOME`.#### spacy.Falcon.v1To use this backend, ideally you have a GPU enabled and have installed `transformers`, `torch` and CUDA in your virtual environment.This allows you to have the setting `device=cuda:0` in your config, which ensures that the model is loaded entirely on the GPU (and fails otherwise).You can do so with```shellpython -m pip install &quot;spacy-llm[transformers]&quot; &quot;transformers[sentencepiece]&quot;```If you don't have access to a GPU, you can install `accelerate` and set`device_map=auto` instead, but be aware that this may result in some layers getting distributed to the CPU or even the hard drive,which may ultimately result in extremely slow queries.```shellpython -m pip install &quot;accelerate&gt;=0.16.0,&lt;1.0&quot;```Example config block:```ini[components.llm.model]@llm_models = &quot;spacy.Falcon.v1&quot;name = &quot;falcon-7b&quot;```| Argument      | Type                                                                                | Default         | Description                                                                                      || ------------- | ----------------------------------------------------------------------------------- | --------------- | ------------------------------------------------------------------------------------------------ || `name`        | `Literal[&quot;falcon-rw-1b&quot;, &quot;falcon-7b&quot;, &quot;falcon-7b-instruct&quot;, &quot;falcon-40b-instruct&quot;]` | `&quot;7b-instruct&quot;` | The name of a Falcon model variant that is supported.                                            || `config_init` | `Dict[str, Any]`                                                                    | `{}`            | Further configuration passed on to the construction of the model with `transformers.pipeline()`. || `config_run`  | `Dict[str, Any]`                                                                    | `{}`            | Further configuration used during model inference.                                               |Note that Hugging Face will download this model the first time you use it - you can[define the cache directory](https://huggingface.co/docs/huggingface_hub/main/en/guides/manage-cache)by setting the environmental variable `HF_HOME`.#### spacy.StableLM.v1To use this model, ideally you have a GPU enabled and have installed `transformers`, `torch` and CUDA in your virtual environment.You can do so with```shellpython -m pip install &quot;spacy-llm[transformers]&quot; &quot;transformers[sentencepiece]&quot;```If you don't have access to a GPU, you can install `accelerate` and set`device_map=auto` instead, but be aware that this may result in some layers getting distributed to the CPU or even the hard drive,which may ultimately result in extremely slow queries.```shellpython -m pip install &quot;accelerate&gt;=0.16.0,&lt;1.0&quot;```Example config block:```ini[components.llm.model]@llm_models = &quot;spacy.StableLM.v1&quot;name = &quot;stablelm-tuned-alpha-7b&quot;```| Argument      | Type                                                                                                                | Default | Description                                                                                                                  || ------------- | ------------------------------------------------------------------------------------------------------------------- | ------- | ---------------------------------------------------------------------------------------------------------------------------- || `name`        | `Literal[&quot;stablelm-base-alpha-3b&quot;, &quot;stablelm-base-alpha-7b&quot;, &quot;stablelm-tuned-alpha-3b&quot;, &quot;stablelm-tuned-alpha-7b&quot;]` |         | The name of a StableLM model that is supported (e. g. &quot;stablelm-tuned-alpha-7b&quot;).                                            || `config_init` | `Dict[str, Any]`                                                                                                    | `{}`    | Further configuration passed on to the construction of the model with `transformers.AutoModelForCausalLM.from_pretrained()`. || `config_run`  | `Dict[str, Any]`                                                                                                    | `{}`    | Further configuration used during model inference.                                                                           |See the [Stability AI StableLM GitHub repo](https://github.com/Stability-AI/StableLM/#stablelm-alpha) for details.Note that Hugging Face will download this model the first time you use it - you can[define the cached directory](https://huggingface.co/docs/huggingface_hub/main/en/guides/manage-cache)by setting the environmental variable `HF_HOME`.#### spacy.OpenLLaMA.v1To use this model, ideally you have a GPU enabled and have installed- `transformers[sentencepiece]`- `torch`- CUDA  in your virtual environment.You can do so with```shellpython -m pip install &quot;spacy-llm[transformers]&quot; &quot;transformers[sentencepiece]&quot;```If you don't have access to a GPU, you can install `accelerate` and set`device_map=auto` instead, but be aware that this may result in some layers getting distributed to the CPU or even the hard drive,which may ultimately result in extremely slow queries.```shellpython -m pip install &quot;accelerate&gt;=0.16.0,&lt;1.0&quot;```Example config block:```ini[components.llm.model]@llm_models = &quot;spacy.OpenLLaMA.v1&quot;name = &quot;open_llama_3b&quot;```| Argument      | Type                                                                                            | Default | Description                                                                                                                  || ------------- |-------------------------------------------------------------------------------------------------| ------- | ---------------------------------------------------------------------------------------------------------------------------- || `name`        | `Literal[&quot;open_llama_3b&quot;, &quot;open_llama_7b&quot;, &quot;open_llama_7b_v2&quot;, &quot;open_llama_13b&quot;]` |         | The name of a OpenLLaMA model that is supported.                                       || `config_init` | `Dict[str, Any]`                                                                                | `{}`    | Further configuration passed on to the construction of the model with `transformers.AutoModelForCausalLM.from_pretrained()`. || `config_run`  | `Dict[str, Any]`                                                                                | `{}`    | Further configuration used during model inference.                                                                           |See the [OpenLM Research OpenLLaMA GitHub repo](https://github.com/openlm-research/open_llama) for details.Note that Hugging Face will download this model the first time you use it - you can[define the cached directory](https://huggingface.co/docs/huggingface_hub/main/en/guides/manage-cache)by setting the environmental variable `HF_HOME`.#### LangChain modelsTo use [LangChain](https://github.com/hwchase17/langchain) for the API retrieval part, make sure you have installed it first:```shellpython -m pip install &quot;langchain==0.0.191&quot;# Or install with spacy-llm directlypython -m pip install &quot;spacy-llm[extras]&quot;```Note that LangChain currently only supports Python 3.9 and beyond.LangChain models in `spacy-llm` work slightly differently. `langchain`'s models are parsed automatically, eachLLM class in `langchain` has one entry in `spacy-llm`'s registry. As `langchain`'s design has one class per API andnot per model, this results in registry entries like `langchain.OpenAI.v1` - i. e. there is one registry entry per APIand not per model (family), as for the REST- and HuggingFace-based entries.The name of the model to be used has to be passed in via the `name` attribute.Example config block:```ini[components.llm.model]@llm_models = &quot;langchain.OpenAI.v1&quot;name = &quot;gpt-3.5-turbo&quot;query = {&quot;@llm_queries&quot;: &quot;spacy.CallLangChain.v1&quot;}config = {&quot;temperature&quot;: 0.3}```| Argument | Type                                                                           | Default | Description                                                                          || -------- | ------------------------------------------------------------------------------ | ------- | ------------------------------------------------------------------------------------ || `name`   | `str`                                                                          |         | The name of a mdodel supported by LangChain for this API.                            || `config` | `Dict[Any, Any]`                                                               | `{}`    | Configuration passed on to the LangChain model.                                      || `query`  | `Optional[Callable[[&quot;langchain.llms.BaseLLM&quot;, Iterable[Any]], Iterable[Any]]]` | `None`  | Function that executes the prompts. If `None`, defaults to `spacy.CallLangChain.v1`. |The default `query` (`spacy.CallLangChain.v1`) executes the prompts by running `model(text)` for each given textual prompt.### CacheInteracting with LLMs, either through an external API or a local instance, is costly.Since developing an NLP pipeline generally means a lot of exploration and prototyping,`spacy-llm` implements a built-in cache to avoid reprocessing the same documents at each runthat keeps batches of documents stored on disk. The cache implementation also ensures that documents in one cache directory were all produced using the same prompt template. This is only possible however if the specified task implements ```python@propertydef prompt_template() -&gt; str:    ...``` which returns the raw prompt template as string. If `prompt_template()` isn't implemented, the cache will emit a warningand not check for prompt template consistency.Example config block:```ini[components.llm.cache]@llm_misc = &quot;spacy.BatchCache.v1&quot;path = &quot;path/to/cache&quot;batch_size = 64max_batches_in_mem = 4```| Argument             | Type                         | Default | Description                                                                                                               || -------------------- | ---------------------------- | ------- | ------------------------------------------------------------------------------------------------------------------------- || `path`               | `Optional[Union[str, Path]]` | `None`  | Cache directory. If `None`, no caching is performed, and this component will act as a NoOp.                               || `batch_size`         | `int`                        | 64      | Number of docs in one batch (file). Once a batch is full, it will be persisted to disk.                                   || `max_batches_in_mem` | `int`                        | 4       | Max. number of batches to hold in memory. Allows you to limit the effect on your memory if you're handling a lot of docs. |When retrieving a document, the `BatchCache` will first figure out what batch the document belongs to. If the batchisn't in memory it will try to load the batch from disk and then move it into memory. Note that since the cache is generated by a registered function, you can also provide your own registered functionreturning your own cache implementation. If you wish to do so, ensure that your cache object adheres to the`Protocol` defined in `spacy_llm.ty.Cache`.### Various functions#### spacy.FewShotReader.v1This function is registered in spaCy's `misc` registry, and reads in examples from a `.yml`, `.yaml`, `.json` or `.jsonl` file.It uses [`srsly`](https://github.com/explosion/srsly) to read in these files and parses them depending on the file extension.```ini[components.llm.task.examples]@misc = &quot;spacy.FewShotReader.v1&quot;path = &quot;ner_examples.yml&quot;```| Argument | Type               | Description                                                                || -------- | ------------------ | -------------------------------------------------------------------------- || `path`   | `Union[str, Path]` | Path to an examples file with suffix `.yml`, `.yaml`, `.json` or `.jsonl`. |#### spacy.FileReader.v1This function is registered in spaCy's `misc` registry, and reads a file provided to the `path` to return a `str`representation of its contents. This function is typically used to read[Jinja](https://jinja.palletsprojects.com/en/3.1.x/) files containing the prompt template.```ini[components.llm.task.template]@misc = &quot;spacy.FileReader.v1&quot;path = &quot;ner_template.jinja2&quot;```| Argument | Type               | Description                  || -------- | ------------------ | ---------------------------- || `path`   | `Union[str, Path]` | Path to the file to be read. |#### Normalizer functionsThese functions provide simple normalizations for string comparisons, e.g. between a list of specified labelsand a label given in the raw text of the LLM response. They are registered in spaCy's `misc` registryand have the signature `Callable[[str], str]`.- `spacy.StripNormalizer.v1`: only apply `text.strip()`- `spacy.LowercaseNormalizer.v1`: applies `text.strip().lower()` to compare strings in a case-insensitive way.## üöÄ Ongoing workIn the near future, we will- Add more example tasks- Support a broader range of models- Provide more example use-cases and tutorials- Make the built-in tasks easier to customize via Jinja templates to define the instructions &amp; examplesPRs are always welcome!## üìùÔ∏è Reporting issuesIf you have questions regarding the usage of `spacy-llm`, or want to give us feedback after giving it a spin, please use the[discussion board](https://github.com/explosion/spaCy/discussions).Bug reports can be filed on the [spaCy issue tracker](https://github.com/explosion/spaCy/issues). Thank you!## Migration guidesPlease refer to our [migration guide](migration_guide.md).</longdescription>
</pkgmetadata>