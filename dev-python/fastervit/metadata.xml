<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># FasterViT: Fast Vision Transformers with Hierarchical AttentionOfficial PyTorch implementation of [**FasterViT: Fast Vision Transformers with Hierarchical Attention**](https://arxiv.org/abs/2306.06189).[Ali Hatamizadeh](https://research.nvidia.com/person/ali-hatamizadeh),[Greg Heinrich](https://developer.nvidia.com/blog/author/gheinrich/),[Hongxu (Danny) Yin](https://scholar.princeton.edu/hongxu),[Andrew Tao](https://developer.nvidia.com/blog/author/atao/),[Jose M. Alvarez](https://alvarezlopezjosem.github.io/),[Jan Kautz](https://jankautz.com/), [Pavlo Molchanov](https://www.pmolchanov.com/).For business inquiries, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)--- FasterViT achieves a new SOTA Pareto-front interms of accuracy vs. image throughput without extra training data !&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/NVlabs/FasterViT/assets/26806394/253d1a2e-b5f5-4a9b-a362-6cdd16bfccc1&quot; width=62% height=62% class=&quot;center&quot;&gt;&lt;/p&gt;## Quick StartWe can import pre-trained FasterViT models with **1 line of code**. First, FasterViT can be simply installed by:```bashpip install fastervit```A pretrained FasterViT model with default hyper-parameters can be created as in the following:```python&gt;&gt;&gt; from fastervit import create_model# Define fastervit-0 model with 224 x 224 resolution&gt;&gt;&gt; model = create_model('faster_vit_0_224',                           pretrained=True,                          model_path=&quot;/tmp/faster_vit_0.pth.tar&quot;)````model_path` is used to set the directory to download the model.We can also simply test the model by passing a dummy input image. The output is the logits:```python&gt;&gt;&gt; import torch&gt;&gt;&gt; image = torch.rand(1, 3, 224, 224)&gt;&gt;&gt; output = model(image) # torch.Size([1, 1000])```We can also use the any-resolution FasterViT model to accommodate arbitrary image resolutions. In the following, we define an any-resolution FasterViT-0model with input resolution of 576 x 960, window sizes of 12 and 6 in 3rd and 4th stages, carrier token size of 2 and embedding dimension of64:```python&gt;&gt;&gt; from fastervit import create_model# Define any-resolution FasterViT-0 model with 576 x 960 resolution&gt;&gt;&gt; model = create_model('faster_vit_0_any_res',                           resolution=[576, 960],                          window_size=[7, 7, 12, 6],                          ct_size=2,                          dim=64,                          pretrained=True)```Note that the above model is intiliazed from the original ImageNet pre-trained FasterViT with original resolution of 224 x 224. As a result, missing keys and mis-matches could be expected since we are addign new layers (e.g. addition of new carrier tokens, etc.) We can simply test the model by passing a dummy input image. The output is the logits:```python&gt;&gt;&gt; import torch&gt;&gt;&gt; image = torch.rand(1, 3, 576, 960)&gt;&gt;&gt; output = model(image) # torch.Size([1, 1000])```--- ## Results + Pretrained Models### ImageNet-1K**FasterViT ImageNet-1K Pretrained Models**&lt;table&gt;  &lt;tr&gt;    &lt;th&gt;Name&lt;/th&gt;    &lt;th&gt;Acc@1(%)&lt;/th&gt;    &lt;th&gt;Acc@5(%)&lt;/th&gt;    &lt;th&gt;Throughput(Img/Sec)&lt;/th&gt;    &lt;th&gt;Resolution&lt;/th&gt;    &lt;th&gt;#Params(M)&lt;/th&gt;    &lt;th&gt;FLOPs(G)&lt;/th&gt;    &lt;th&gt;Download&lt;/th&gt;  &lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-0&lt;/td&gt;    &lt;td&gt;82.1&lt;/td&gt;    &lt;td&gt;95.9&lt;/td&gt;    &lt;td&gt;5802&lt;/td&gt;    &lt;td&gt;224x224&lt;/td&gt;    &lt;td&gt;31.4&lt;/td&gt;    &lt;td&gt;3.3&lt;/td&gt;    &lt;td&gt;&lt;a href=&quot;https://drive.google.com/uc?export=download&amp;id=1twI2LFJs391Yrj8MR4Ui9PfrvWqjE1iB&quot;&gt;model&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-1&lt;/td&gt;    &lt;td&gt;83.2&lt;/td&gt;    &lt;td&gt;96.5&lt;/td&gt;    &lt;td&gt;4188&lt;/td&gt;    &lt;td&gt;224x224&lt;/td&gt;    &lt;td&gt;53.4&lt;/td&gt;    &lt;td&gt;5.3&lt;/td&gt;    &lt;td&gt;&lt;a href=&quot;https://drive.google.com/uc?export=download&amp;id=1r7W10n5-bFtM3sz4bmaLrowN2gYPkLGT&quot;&gt;model&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-2&lt;/td&gt;    &lt;td&gt;84.2&lt;/td&gt;    &lt;td&gt;96.8&lt;/td&gt;    &lt;td&gt;3161&lt;/td&gt;    &lt;td&gt;224x224&lt;/td&gt;    &lt;td&gt;75.9&lt;/td&gt;    &lt;td&gt;8.7&lt;/td&gt;    &lt;td&gt;&lt;a href=&quot;https://drive.google.com/uc?export=download&amp;id=1n_a6s0pgi0jVZOGmDei2vXHU5E6RH5wU&quot;&gt;model&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-3&lt;/td&gt;    &lt;td&gt;84.9&lt;/td&gt;    &lt;td&gt;97.2&lt;/td&gt;    &lt;td&gt;1780&lt;/td&gt;    &lt;td&gt;224x224&lt;/td&gt;    &lt;td&gt;159.5&lt;/td&gt;    &lt;td&gt;18.2&lt;/td&gt;    &lt;td&gt;&lt;a href=&quot;https://drive.google.com/uc?export=download&amp;id=1tvWElZ91Sia2SsXYXFMNYQwfipCxtI7X&quot;&gt;model&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-4&lt;/td&gt;    &lt;td&gt;85.4&lt;/td&gt;    &lt;td&gt;97.3&lt;/td&gt;    &lt;td&gt;849&lt;/td&gt;    &lt;td&gt;224x224&lt;/td&gt;    &lt;td&gt;424.6&lt;/td&gt;    &lt;td&gt;36.6&lt;/td&gt;    &lt;td&gt;&lt;a href=&quot;https://drive.google.com/uc?export=download&amp;id=1gYhXA32Q-_9C5DXel17avV_ZLoaHwdgz&quot;&gt;model&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-5&lt;/td&gt;    &lt;td&gt;85.6&lt;/td&gt;    &lt;td&gt;97.4&lt;/td&gt;    &lt;td&gt;449&lt;/td&gt;    &lt;td&gt;224x224&lt;/td&gt;    &lt;td&gt;975.5&lt;/td&gt;    &lt;td&gt;113.0&lt;/td&gt;    &lt;td&gt;&lt;a href=&quot;https://drive.google.com/uc?export=download&amp;id=1mqpai7XiHLr_n1tjxjzT8q369xTCq_z-&quot;&gt;model&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-6&lt;/td&gt;    &lt;td&gt;85.8&lt;/td&gt;    &lt;td&gt;97.4&lt;/td&gt;    &lt;td&gt;352&lt;/td&gt;    &lt;td&gt;224x224&lt;/td&gt;    &lt;td&gt;1360.0&lt;/td&gt;    &lt;td&gt;142.0&lt;/td&gt;    &lt;td&gt;&lt;a href=&quot;https://drive.google.com/uc?export=download&amp;id=12jtavR2QxmMzcKwPzWe7kw-oy34IYi59&quot;&gt;model&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;### Robustness (ImageNet-A - ImageNet-R - ImageNet-V2)All models use `crop_pct=0.875`. Results are obtained by running inference on ImageNet-1K pretrained models without finetuning.&lt;table&gt;  &lt;tr&gt;    &lt;th&gt;Name&lt;/th&gt;    &lt;th&gt;A-Acc@1(%)&lt;/th&gt;    &lt;th&gt;A-Acc@5(%)&lt;/th&gt;    &lt;th&gt;R-Acc@1(%)&lt;/th&gt;    &lt;th&gt;R-Acc@5(%)&lt;/th&gt;    &lt;th&gt;V2-Acc@1(%)&lt;/th&gt;    &lt;th&gt;V2-Acc@5(%)&lt;/th&gt;  &lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-0&lt;/td&gt;    &lt;td&gt;23.9&lt;/td&gt;    &lt;td&gt;57.6&lt;/td&gt;    &lt;td&gt;45.9&lt;/td&gt;    &lt;td&gt;60.4&lt;/td&gt;    &lt;td&gt;70.9&lt;/td&gt;    &lt;td&gt;90.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-1&lt;/td&gt;    &lt;td&gt;31.2&lt;/td&gt;    &lt;td&gt;63.3&lt;/td&gt;    &lt;td&gt;47.5&lt;/td&gt;    &lt;td&gt;61.9&lt;/td&gt;    &lt;td&gt;72.6&lt;/td&gt;    &lt;td&gt;91.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-2&lt;/td&gt;    &lt;td&gt;38.2&lt;/td&gt;    &lt;td&gt;68.9&lt;/td&gt;    &lt;td&gt;49.6&lt;/td&gt;    &lt;td&gt;63.4&lt;/td&gt;    &lt;td&gt;73.7&lt;/td&gt;    &lt;td&gt;91.6&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-3&lt;/td&gt;    &lt;td&gt;44.2&lt;/td&gt;    &lt;td&gt;73.0&lt;/td&gt;    &lt;td&gt;51.9&lt;/td&gt;    &lt;td&gt;65.6&lt;/td&gt;    &lt;td&gt;75.0&lt;/td&gt;    &lt;td&gt;92.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-4&lt;/td&gt;    &lt;td&gt;49.0&lt;/td&gt;    &lt;td&gt;75.4&lt;/td&gt;    &lt;td&gt;56.0&lt;/td&gt;    &lt;td&gt;69.6&lt;/td&gt;    &lt;td&gt;75.7&lt;/td&gt;    &lt;td&gt;92.7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-5&lt;/td&gt;    &lt;td&gt;52.7&lt;/td&gt;    &lt;td&gt;77.6&lt;/td&gt;    &lt;td&gt;56.9&lt;/td&gt;    &lt;td&gt;70.0&lt;/td&gt;    &lt;td&gt;76.0&lt;/td&gt;    &lt;td&gt;93.0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;FasterViT-6&lt;/td&gt;    &lt;td&gt;53.7&lt;/td&gt;    &lt;td&gt;78.4&lt;/td&gt;    &lt;td&gt;57.1&lt;/td&gt;    &lt;td&gt;70.1&lt;/td&gt;    &lt;td&gt;76.1&lt;/td&gt;    &lt;td&gt;93.0&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;A, R and V2 denote ImageNet-A, ImageNet-R and ImageNet-V2 respectively. ## TrainingPlease see [TRAINING.md](TRAINING.md) for detailed training instructions of all models. ## EvaluationThe FasterViT models can be evaluated on ImageNet-1K validation set using the following: ```python validate.py \--model &lt;model-name&gt;--checkpoint &lt;checkpoint-path&gt;--data_dir &lt;imagenet-path&gt;--batch-size &lt;batch-size-per-gpu``` Here `--model` is the FasterViT variant (e.g. `faster_vit_0_224_1k`), `--checkpoint` is the path to pretrained model weights, `--data_dir` is the path to ImageNet-1K validation set and `--batch-size` is the number of batch size. We also provide a sample script [here](./fastervit/validate.sh). ## ONNX ConversionWe provide ONNX conversion script to enable dynamic batch size inference. For instance, to generate ONNX model for `faster_vit_0_any_res` with resolution 576 x 960 and ONNX opset number 17, the following can be used. ```bash python onnx_convert --model-name faster_vit_0_any_res --resolution-h 576 --resolution-w 960 --onnx-opset 17```## InstallationThe dependencies can be installed by running:```bashpip install -r requirements.txt```## Star History[![Star History Chart](https://api.star-history.com/svg?repos=NVlabs/FasterViT&amp;type=Date)](https://star-history.com/#NVlabs/FasterViT&amp;Date)## Third-party ExtentionsWe always welcome third-party extentions/implementations and usage for other purposes. If you would like your work to be listed in this repository, please raise and issue and provide us with detailed information.  ## CitationPlease consider citing FasterViT if this repository is useful for your work. ```@article{hatamizadeh2023fastervit,  title={FasterViT: Fast Vision Transformers with Hierarchical Attention},  author={Hatamizadeh, Ali and Heinrich, Greg and Yin, Hongxu and Tao, Andrew and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},  journal={arXiv preprint arXiv:2306.06189},  year={2023}}```## LicensesCopyright © 2023, NVIDIA Corporation. All rights reserved.This work is made available under the NVIDIA Source Code License-NC. Click [here](LICENSE) to view a copy of this license.For license information regarding the timm repository, please refer to its [repository](https://github.com/rwightman/pytorch-image-models).For license information regarding the ImageNet dataset, please see the [ImageNet official website](https://www.image-net.org/). ## AcknowledgementThis repository is built on top of the [timm](https://github.com/huggingface/pytorch-image-models) repository. We thank [Ross Wrightman](https://rwightman.com/) for creating and maintaining this high-quality library.  </longdescription>
</pkgmetadata>