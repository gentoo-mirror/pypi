<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># ðŸ¦™ Python Bindings for `llama.cpp`[![Documentation](https://img.shields.io/badge/docs-passing-green.svg)](https://abetlen.github.io/llama-cpp-python)[![Tests](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml)[![PyPI](https://img.shields.io/pypi/v/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)[![PyPI - License](https://img.shields.io/pypi/l/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)Simple Python bindings for **@ggerganov's** [`llama.cpp`](https://github.com/ggerganov/llama.cpp) library.This package provides:- Low-level access to C API via `ctypes` interface.- High-level Python API for text completion  - OpenAI-like API  - LangChain compatibility## Installation from PyPI (recommended)Install from PyPI (requires a c compiler):```bashpip install llama-cpp-python```The above command will attempt to install the package and build build `llama.cpp` from source.This is the recommended installation method as it ensures that `llama.cpp` is built with the available optimizations for your system.### Installation with OpenBLAS / cuBLAS / CLBlast`llama.cpp` supports multiple BLAS backends for faster processing.Use the `FORCE_CMAKE=1` environment variable to force the use of `cmake` and install the pip package for the desired BLAS backend.To install with OpenBLAS, set the `LLAMA_OPENBLAS=1` environment variable before installing:```bashCMAKE_ARGS=&quot;-DLLAMA_OPENBLAS=on&quot; FORCE_CMAKE=1 pip install llama-cpp-python```To install with cuBLAS, set the `LLAMA_CUBLAS=1` environment variable before installing:```bashCMAKE_ARGS=&quot;-DLLAMA_CUBLAS=on&quot; FORCE_CMAKE=1 pip install llama-cpp-python```To install with CLBlast, set the `LLAMA_CLBLAST=1` environment variable before installing:```bashCMAKE_ARGS=&quot;-DLLAMA_CLBLAST=on&quot; FORCE_CMAKE=1 pip install llama-cpp-python```## High-level APIThe high-level API provides a simple managed interface through the `Llama` class.Below is a short example demonstrating how to use the high-level API to generate text:```python&gt;&gt;&gt; from llama_cpp import Llama&gt;&gt;&gt; llm = Llama(model_path=&quot;./models/7B/ggml-model.bin&quot;)&gt;&gt;&gt; output = llm(&quot;Q: Name the planets in the solar system? A: &quot;, max_tokens=32, stop=[&quot;Q:&quot;, &quot;\n&quot;], echo=True)&gt;&gt;&gt; print(output){  &quot;id&quot;: &quot;cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;,  &quot;object&quot;: &quot;text_completion&quot;,  &quot;created&quot;: 1679561337,  &quot;model&quot;: &quot;./models/7B/ggml-model.bin&quot;,  &quot;choices&quot;: [    {      &quot;text&quot;: &quot;Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.&quot;,      &quot;index&quot;: 0,      &quot;logprobs&quot;: None,      &quot;finish_reason&quot;: &quot;stop&quot;    }  ],  &quot;usage&quot;: {    &quot;prompt_tokens&quot;: 14,    &quot;completion_tokens&quot;: 28,    &quot;total_tokens&quot;: 42  }}```## Web Server`llama-cpp-python` offers a web server which aims to act as a drop-in replacement for the OpenAI API.This allows you to use llama.cpp compatible models with any OpenAI compatible client (language libraries, services, etc).To install the server package and get started:```bashpip install llama-cpp-python[server]python3 -m llama_cpp.server --model models/7B/ggml-model.bin```Navigate to [http://localhost:8000/docs](http://localhost:8000/docs) to see the OpenAPI documentation.## Docker imageA Docker image is available on [GHCR](https://ghcr.io/abetlen/llama-cpp-python). To run the server:```bashdocker run --rm -it -p 8000:8000 -v /path/to/models:/models -e MODEL=/models/ggml-model-name.bin ghcr.io/abetlen/llama-cpp-python:latest```## Low-level APIThe low-level API is a direct [`ctypes`](https://docs.python.org/3/library/ctypes.html) binding to the C API provided by `llama.cpp`.The entire lowe-level API can be found in [llama_cpp/llama_cpp.py](https://github.com/abetlen/llama-cpp-python/blob/master/llama_cpp/llama_cpp.py) and directly mirrors the C API in [llama.h](https://github.com/ggerganov/llama.cpp/blob/master/llama.h).Below is a short example demonstrating how to use the low-level API to tokenize a prompt:```python&gt;&gt;&gt; import llama_cpp&gt;&gt;&gt; import ctypes&gt;&gt;&gt; params = llama_cpp.llama_context_default_params()# use bytes for char * params&gt;&gt;&gt; ctx = llama_cpp.llama_init_from_file(b&quot;./models/7b/ggml-model.bin&quot;, params)&gt;&gt;&gt; max_tokens = params.n_ctx# use ctypes arrays for array params&gt;&gt;&gt; tokens = (llama_cppp.llama_token * int(max_tokens))()&gt;&gt;&gt; n_tokens = llama_cpp.llama_tokenize(ctx, b&quot;Q: Name the planets in the solar system? A: &quot;, tokens, max_tokens, add_bos=llama_cpp.c_bool(True))&gt;&gt;&gt; llama_cpp.llama_free(ctx)```Check out the [examples folder](examples/low_level_api) for more examples of using the low-level API.# DocumentationDocumentation is available at [https://abetlen.github.io/llama-cpp-python](https://abetlen.github.io/llama-cpp-python).If you find any issues with the documentation, please open an issue or submit a PR.# DevelopmentThis package is under active development and I welcome any contributions.To get started, clone the repository and install the package in development mode:```bashgit clone --recurse-submodules git@github.com:abetlen/llama-cpp-python.git# Will need to be re-run any time vendor/llama.cpp is updatedpython3 setup.py develop```# How does this compare to other Python bindings of `llama.cpp`?I originally wrote this package for my own use with two goals in mind:- Provide a simple process to install `llama.cpp` and access the full C API in `llama.h` from Python- Provide a high-level Python API that can be used as a drop-in replacement for the OpenAI API so existing apps can be easily ported to use `llama.cpp`Any contributions and changes to this package will be made with these goals in mind.# LicenseThis project is licensed under the terms of the MIT license.</longdescription>
</pkgmetadata>