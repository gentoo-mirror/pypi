<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>* [Overview](#overview)    * [Goal](#goal)    * [Benefit](#benefit)    * [Application](#application)    * [Build your application](#build_your_application)    * [Deploy your application](#deploy_your_application)    * [Run your application](#run_your_application)    * [Supported platforms](#supported_platforms)* [Demos](#demos)* [APIs](#apis)    * [Job Deployer](#job-deployer)    * [Job Submitter](#job-submitter)# Overview## GoalThere are many public clouds provide managed Apache Spark as service, such as databricks, AWS EMR, Oracle OCI DataFlow, see the table below for a detailed list.However, the way to deploy Spark application and launch Spark application are incompatible among different cloud Spark platforms.spark-etl is a python package, provides a standard way for building, deploying and running your Spark application that supports various cloud spark platforms.## BenefitYour application using `spark-etl` can be deployed and launched from different cloud spark platforms without changing the source code.## ApplicationAn application is a python program. It contains:* A `main.py` file which contains the application entry* A `manifest.json` file, which specify the metadata of the application.* A `requirements.txt` file, which specify the application dependency.### Application entry signatureIn your application's `main.py`, you shuold have a `main` function with the following signature:* `spark` is the spark session object* `input_args` a dict, is the argument user specified when running this application.* `sysops` is the system options passed, it is platform specific. Job submitter may inject platform specific object in `sysops` object.* Your `main` function's return value should be a JSON object, it will be returned from the job submitter to the caller.```def main(spark, input_args, sysops={}):    # your code here```[Here](examples/apps/demo01) is an application example.## Build your application`etl -a build -c &lt;config-filename&gt; -p &lt;application-name&gt;`## Deploy your application`etl -a deploy -c &lt;config-filename&gt; -p &lt;application-name&gt; -f &lt;profile-name&gt;`## Run your application`etl -a run -c &lt;config-filename&gt; -p &lt;application-name&gt; -f &lt;profile-name&gt; --run-args &lt;input-filename&gt;`## Supported platforms&lt;table&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img                src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1200px-Apache_Spark_logo.svg.png&quot;                width=&quot;120px&quot;            /&gt;        &lt;/td&gt;        &lt;td&gt;You setup your own Apache Spark Cluster.        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img src=&quot;https://miro.medium.com/max/700/1*qgkjkj6BLVS1uD4mw_sTEg.png&quot; width=&quot;120px&quot; /&gt;        &lt;/td&gt;        &lt;td&gt;            Use &lt;a href=&quot;https://pypi.org/project/pyspark/&quot;&gt;PySpark&lt;/a&gt; package, fully compatible to other spark platform, allows you to test your pipeline in a single computer.        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img src=&quot;https://databricks.com/wp-content/uploads/2019/02/databricks-generic-tile.png&quot; width=&quot;120px&quot;&gt;        &lt;/td&gt;        &lt;td&gt;You host your spark cluster in &lt;a href=&quot;https://databricks.com/&quot;&gt;databricks &lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img                src=&quot;https://blog.ippon.tech/content/images/2019/06/emrlogogo.png&quot;                width=&quot;120px&quot;            /&gt;        &lt;/td&gt;        &lt;td&gt;You host your spark cluster in &lt;a href=&quot;https://aws.amazon.com/emr/&quot;&gt;Amazon AWS EMR&lt;/a&gt;        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img                src=&quot;https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2020/07/100-768x402.jpeg&quot;                width=&quot;120px&quot;            /&gt;        &lt;/td&gt;        &lt;td&gt;You host your spark cluster in &lt;a href=&quot;https://cloud.google.com/dataproc&quot;&gt;Google Cloud&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img                src=&quot;https://apifriends.com/wp-content/uploads/2018/05/HDInsightsDetails.png&quot;                width=&quot;120px&quot;            /&gt;        &lt;/td&gt;        &lt;td&gt;You host your spark cluster in &lt;a href=&quot;https://azure.microsoft.com/en-us/services/hdinsight/&quot;&gt;Microsoft Azure HDInsight&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img                src=&quot;https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRajQVuckGogS3c8Yxa4M-OPd7yFCyWSj4Cpg&amp;usqp=CAU&quot;                width=&quot;120px&quot;            /&gt;        &lt;/td&gt;        &lt;td&gt;            You host your spark cluster in &lt;a href=&quot;https://www.oracle.com/big-data/data-flow/&quot;&gt;Oracle Cloud Infrastructure, Data Flow Service&lt;/a&gt;        &lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;            &lt;img                src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/24/IBM_Cloud_logo.png&quot;                width=&quot;120px&quot;            /&gt;        &lt;/td&gt;        &lt;td&gt;You host your spark cluster in &lt;a href=&quot;https://www.ibm.com/products/big-data-and-analytics&quot;&gt;IBM Cloud&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;&lt;/table&gt;# Demos* [Using local pyspark, access data on local disk](examples/pyspark_local/readme.md)* [Using local pyspark, access data on AWS S3](examples/pyspark_s3/readme.md)* [Using on-premise spark, access data on HDFS](examples/livy_hdfs1/readme.md)* [Using on-premise spark, access data on AWS S3](examples/livy_hdfs2/readme.md)* [Using AWS EMR's spark, access data on AWS S3](examples/aws_emr/readme.md)* [Using Oracle OCI's Dataflow with API key, access data on Object Storage](examples/oci_dataflow1/readme.md)* [Using Oracle OCI's Dataflow with instance principal, access data on Object Storage](examples/oci_dataflow2/readme.md)# APIs[pydocs for APIs](https://stonezhong.github.io/spark_etl/pydocs/spark_etl.html)## Job DeployerFor job deployers, please check the [wiki](https://github.com/stonezhong/spark_etl/wiki#job-deployer-classes) .## Job SubmitterFor job submitters, please check the [wiki](https://github.com/stonezhong/spark_etl/wiki#job-submitter-classes)</longdescription>
</pkgmetadata>