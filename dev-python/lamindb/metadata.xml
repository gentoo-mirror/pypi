<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Stars](https://img.shields.io/github/stars/laminlabs/lamindb?logo=GitHub&amp;color=yellow)](https://github.com/laminlabs/lamindb)[![codecov](https://codecov.io/gh/laminlabs/lamindb/branch/main/graph/badge.svg?token=VKMRJ7OWR3)](https://codecov.io/gh/laminlabs/lamindb)[![pypi](https://img.shields.io/pypi/v/lamindb?color=blue&amp;label=pypi%20package)](https://pypi.org/project/lamindb)# LaminDB: Data lake for biologyLaminDB is an API layer for your existing infrastructure to manage your existing data &amp; analyses.```{warning}Public beta: Currently only recommended for collaborators as we still make breaking changes.Update 2023-06-14:- We completed a major migration from SQLAlchemy/SQLModel to Django, available in 0.42.0.- The last version that is fully compatible with SQLAlchemy/SQLModel is 0.41.2.```## FeaturesFree:- Track data lineage across notebooks, pipelines &amp; apps.- Manage biological registries, ontologies &amp; features.- Persist, load &amp; stream data objects with a single line of code.- Query for anything, define &amp; manage custom schemas.- Manage data on your laptop, server or cloud infra.- Use a mesh of distributed LaminDB instances for different teams and purposes.- Share instances through a Hub akin to GitHub.Enterprise:- Explore &amp; share data, submit samples &amp; track lineage with LaminApp (deployable in your infra).- Receive support, code templates &amp; services for a BioTech data &amp; analytics platform.## How does it work?LaminDB builds semantics of R&amp;D and biology onto well-established tools:- SQLite &amp; Postgres for SQL databases using Django ORM (previously: SQLModel)- S3, GCP &amp; local storage for object storage using fsspec- Configurable storage formats: pyarrow, anndata, zarr, etc.- Biological knowledge sources &amp; ontologies: see [Bionty](https://lamin.ai/docs/bionty)LaminDB is open source. For details, see [Architecture](#architecture).## Installation![pyversions](https://img.shields.io/pypi/pyversions/lamindb)```shellpip install lamindb  # basic data lakepip install 'lamindb[bionty]'  # biological entitiespip install 'lamindb[nbproject]'  # Jupyter notebook trackingpip install 'lamindb[aws]'  # AWS dependencies (s3fs, etc.)pip install 'lamindb[gcp]'  # GCP dependencies (gcfs, etc.)```## Quick setupWhy do I have to sign up?- Data lineage requires a user identity (who modified which data when?).- Collaboration requires a user identity (who shares this with me?).Signing up takes 1 min.We do _not_ store any of your data, but only basic metadata about you (email address, etc.) &amp; your instances (S3 bucket names, etc.).- Sign up: `lamin signup &lt;email&gt;`- Log in: `lamin login &lt;handle&gt;`- Init a data lake: `lamin init --storage &lt;default-storage-location&gt;`## Usage overview```pythonimport lamindb as ln```### Store &amp; load data artifactsStore a `DataFrame` or an `AnnData` in default local or cloud storage:```pythondf = pd.DataFrame({&quot;feat1&quot;: [1, 2], &quot;feat2&quot;: [3, 4]})ln.File(df, name=&quot;My dataframe&quot;).save()  # create a File object and save it```&lt;br&gt;Get it back:```pythonfile = ln.File.select(name=&quot;My dataframe&quot;).one()  # query for itdf = file.load()  # load it into memory```### Track &amp; query data lineage```pythonln.File.select(created_by__handle=&quot;lizlemon&quot;).df()   # all files ingested by lizlemonln.File.select().order_by(&quot;-updated_at&quot;).first()  # latest updated file```#### NotebooksTrack a Jupyter Notebook:```pythonln.track()  # auto-detect notebook metadata, save as a Transform, create a Runln.File(&quot;my_artifact.parquet&quot;).save()  # this file is an output of the notebook run```&lt;br&gt;When you query this file later on, you'll always know where it came from:```pythonfile = ln.File.select(name=&quot;my_artifact.parquet&quot;).one()file.transform  # gives you the notebook with title, filename, version, id, etc.file.run  # gives you the run of the notebook that created the file```&lt;br&gt;Of course, you can also query for notebooks:```pythontransforms = ln.Transform.select(  # all notebooks with 'T cell' in the title created in 2022    name__contains=&quot;T cell&quot;, type=&quot;notebook&quot;, created_at__year=2022).all()ln.File.select(transform__in=transforms).all()  # data artifacts created by these notebooks```#### PipelinesTo save a pipeline (complementary to workflow tools) to the `Transform` registry, call```pythonln.Transform(name=&quot;Awesom-O&quot;, version=&quot;0.41.2&quot;).save()  # save a pipeline```&lt;br&gt;To track a run of a registered pipeline:```pythontransform = ln.Transform.select(name=&quot;Awesom-O&quot;, version=&quot;0.41.2&quot;).one()  # select a pipeline from the registryln.track(transform)  # create a new global run contextln.File(&quot;s3://my_samples01/my_artifact.fastq.gz&quot;).save()  # link file against run &amp; transform```&lt;br&gt;Now, you can query, e.g., for```pythonln.Run.select(transform__name=&quot;Awesom-O&quot;).order_by(&quot;-created_at&quot;).df()  # get the latest pipeline runs```### Lookup categoricals with auto-completeWhen you're unsure about spellings, use a lookup object:```pythonlookup = ln.Transform.lookup()ln.Run.select(transform=lookup.awesome_o)```### Manage biological registries```shelllamin init --storage ./myobjects --schema bionty```&lt;br&gt;...### Track biological features...### Track biological samples...### Manage custom schemas1. Create a GitHub repository with Django ORMs similar to [github.com/laminlabs/lnschema-lamin1](https://github.com/laminlabs/lnschema-lamin1)2. Create &amp; deploy migrations via `lamin migrate create` and `lamin migrate deploy`It's fastest if we do this for you based on our templates within an enterprise plan, but you can fully manage the process yourself.## Notebooks- Find all guide notebooks [here](https://github.com/laminlabs/lamindb/tree/main/docs/guide).- You can run these notebooks in hosted versions of JupyterLab, e.g., [Saturn Cloud](https://github.com/laminlabs/run-lamin-on-saturn), Google Vertex AI, and others or on Google Colab.- Jupyter Lab &amp; Notebook offer a fully interactive experience, VS Code &amp; others require using the CLI (`lamin track my-notebook.ipynb`)## ArchitectureLaminDB consists of the `lamindb` Python package, which builds on a number of open-source packages:- [bionty](https://github.com/laminlabs/bionty): Basic biological entities (usable standalone).- [lamindb-setup](https://github.com/laminlabs/lamindb-setup): Setup &amp; configure LaminDB, client for Lamin Hub.- [lnschema-core](https://github.com/laminlabs/lnschema-core): Core schema, ORMs to model data objects &amp; data lineage.- [lnschema-bionty](https://github.com/laminlabs/lnschema-bionty): Bionty schema, ORMs that are coupled to Bionty's entities.- [lnschema-lamin1](https://github.com/laminlabs/lnschema-lamin1): Exemplary configured schema to track samples, treatments, etc.- [nbproject](https://github.com/laminlabs/nbproject): Parse metadata from Jupyter notebooks.LaminHub &amp; LaminApp are not open-sourced, neither are templates that model lab operations.Lamin's packages build on the infrastructure listed[above](#how-does-it-work). Previously, they were based on SQLAlchemy/SQLModelinstead of Django, and cloudpathlib instead of fsspec.## DocumentationRead the [docs](https://lamin.ai/docs/guide/).</longdescription>
</pkgmetadata>