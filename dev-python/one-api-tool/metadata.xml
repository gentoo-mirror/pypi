<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># OneAPIEasily access multiple ChatGPT or similar APIs with just one line of code/command.Save a significant amount of ‚òïÔ∏è time by avoiding the need to read multiple API documents and test them individually.The currently supported APIs include: - [x] OpenAI Official API.    - [x] ChatGPT: GPT-3.5-turbo/GPT-4.    - [x] Token number counting.    - [x] Embedding generation.    - [x] Function calling. - [x] Microsoft Azure OpenAI Resource endpoint API.    - [x] ChatGPT: GPT-3.5-turbo/GPT-4.    - [x] Token number counting.    - [x] Embedding generation. - [x] Anthropic Claude series model API.    - [x] Claude-v1.3-100k, etc.    - [x] Token number counting.## InstallationRequirements Python &gt;=3.10```shpip install -U one-api-tool```## Usage### 1. Using python.OpenAI config:```json{    &quot;api_key&quot;: &quot;YOUR_API_KEY&quot;,    &quot;api_base&quot;: &quot;https://api.openai.com/v1&quot;,    &quot;api_type&quot;: &quot;open_ai&quot;}```Azure OpenAI config:```json{    &quot;api_key&quot;: &quot;YOUR_API_KEY&quot;,    &quot;api_base&quot;: &quot;Replace with your Azure OpenAI resource's endpoint value.&quot;,    &quot;api_type&quot;: &quot;azure&quot;}```Anthropic config:```json{    &quot;api_key&quot;: &quot;YOUR_API_KEY&quot;,    &quot;api_base&quot;: &quot;https://api.anthropic.com&quot;,    &quot;api_type&quot;: &quot;claude&quot;}````api_key`: Obtain OpenAI API key from the [OpenAI website](https://platform.openai.com/account/api-keys) and Claude API key from the [Anthropic website](https://console.anthropic.com/account/keys).`api_base`: This is the base API that is used to send requests. You can also specify a proxy URL, such as &quot;https://your_proxy_domain/v1&quot;. For example, you can use Cloudflare workers to proxy the OpenAI site.If you are using Azure APIs, you can find relevant information on the Azure resource dashboard. The API format typically follows this pattern: `https://{your_organization}.openai.azure.com/`.`api_type`: Currently supported values are &quot;open_ai&quot;, &quot;azure&quot;, or &quot;claude&quot;.#### Chat example:```pythonfrom oneapi import OneAPIToolimport asyncio# Two ways to initialize the OneAPITool object  # tool = OneAPITool.from_config(api_key, api_base, api_type)tool = OneAPITool.from_config_file(&quot;your_config_file.json&quot;)# Say hello to ChatGPT/Claude/GPT-4res = tool.simple_chat(&quot;Hello AI!&quot;)print(res)# Async chat to ChatGPT/Claude/GPT-4 with `stream=False`res = asyncio.run(tool.asimple_chat('How\'s the weather today?', model='gpt-4', stream=False))# Get embeddings of some sentences for further usage, e.g., clusteringembeddings = tool.get_embeddings([&quot;Hello AI!&quot;, &quot;Hello world!&quot;])print(len(embeddings)))# Count the number of tokensprint(tool.count_tokens([&quot;Hello AI!&quot;, &quot;Hello world!&quot;]))```**Note: Currently, `get_embeddings` only support OpenAI or Microsoft Azure API.**### Batch request with asyncio```pythonfrom oneapi.one_api import batch_chatclaude_config = 'anthropic_config.json'openai_config = 'openapi_config.json'azure_config = 'openapi_azure_config.json'# The coccurent number of requests would be 3, which is the same as the length of the configs list.configs = [claude_config, openai_config, azure_config]prompts = ['How\'s the weather today?', 'How\'s the weather today?', 'How\'s the weather today?']res = asyncio.run(batch_chat(configs, prompts, stream=False))print(res)```#### Simple function calling example:```pythonfrom oneapi import OneAPITooldef get_whether_of_city(city: str, date: str) -&gt; dict:    &quot;&quot;&quot;Get the weather of a city at a date    Args:        city (str): City name        date (str): Date of the weather    Returns:        Dict: Weather information    &quot;&quot;&quot;    return {&quot;city&quot;: city, &quot;date&quot;: date, &quot;weather&quot;: &quot;sunny&quot;, &quot;temperature&quot;: 30, &quot;air_condition&quot;: &quot;good&quot;}# tool = OneAPITool.from_config(api_key, api_base, api_type)tool = OneAPITool.from_config_file(&quot;your_config_file.json&quot;)res = tool.function_chat(&quot;What's the weather like in New York on July 10th?&quot;, functions=[get_whether_of_city])print(res)```&lt;details open&gt; &lt;summary&gt;Output detail&lt;/summary&gt;```textOn July 10th, 2022, the weather in New York is expected to be sunny. The temperature will be around 30 degrees Celsius (86 degrees Fahrenheit). The air quality is expected to be good.```&lt;/details&gt;#### Custom function calling example:```pythonfrom oneapi import OneAPIToolimport jsondef get_whether_of_city(city: str, date: str) -&gt; dict:    &quot;&quot;&quot;Get the weather of a city at a date    Args:        city (str): City name        date (str): Date of the weather    Returns:        Dict: Weather information    &quot;&quot;&quot;    return {&quot;city&quot;: city, &quot;date&quot;: date, &quot;weather&quot;: &quot;sunny&quot;, &quot;temperature&quot;: 30, &quot;air_condition&quot;: &quot;good&quot;}# tool = OneAPITool.from_config(api_key, api_base, api_type)tool = OneAPITool.from_config_file(&quot;your_config_file.json&quot;)msgs = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What's the weather like in New York on July 10th?&quot;}]function_response = tool.simple_chat(msgs, model=&quot;gpt-3.5-turbo-0613&quot;, functions=[get_whether_of_city])print(f&quot;Function response:\n{function_response}&quot;)function_call = function_response[&quot;function_call&quot;]arguments = json.loads(function_call[&quot;arguments&quot;])wether_info = get_whether_of_city(**arguments)print(f&quot;Wether_info:\n{wether_info}&quot;)msgs.append(function_response)msgs.append({&quot;role&quot;: &quot;function&quot;, &quot;name&quot;: function_call[&quot;name&quot;], &quot;content&quot;: json.dumps(wether_info)})second_res = api.simple_chat(msgs, model=&quot;gpt-3.5-turbo-0613&quot;)print(f&quot;Second response:\n{second_res}&quot;)```&lt;details open&gt; &lt;summary&gt;Output detail&lt;/summary&gt;Function response:```json{  &quot;role&quot;: &quot;assistant&quot;,  &quot;content&quot;: null,  &quot;function_call&quot;: {    &quot;name&quot;: &quot;get_whether_of_city&quot;,    &quot;arguments&quot;: &quot;{\n  \&quot;city\&quot;: \&quot;New York\&quot;,\n  \&quot;date\&quot;: \&quot;2022-07-10\&quot;\n}&quot;  }}```Wether_info: ```json{&quot;city&quot;: &quot;New York&quot;, &quot;date&quot;: &quot;July 10th&quot;, &quot;weather&quot;: &quot;sunny&quot;, &quot;temperature&quot;: 30, &quot;air_condition&quot;: &quot;good&quot;}```Second response:```textOn July 10th, 2022, the weather in New York is expected to be sunny with a temperature of 30 degrees Celsius. The air quality is also good.```&lt;/details&gt;### 2. Using command line```shopen-api --config_file CHANGE_TO_YOUR_CONFIG_PATH \--model gpt-3.5-turbo \--prompt &quot;1+1=?&quot; ```&lt;details open&gt;&lt;summary&gt;Output detail&lt;/summary&gt;```text-------------------- prompt detail üöÄ  --------------------1+1=?-------------------- prompt end ---------------------------------------- gpt-3.5-turbo response ‚≠êÔ∏è --------------------2-------------------- response end --------------------```&lt;/details&gt;#### Arguments detail:`--config_file` string ${\color{orange}\text{Required}}$ &lt;br&gt;A local configuration file containing API key information.`--prompt` string ${\color{orange}\text{Required}}$ &lt;br&gt;The question that would be predicted by LLMs, e.g., A math question would be like: &quot;1+1=?&quot;.`--system` string ${\color{grey}\text{Optional}}$  Defaults to null &lt;br&gt; System message to instruct chatGPT, e.g., You are a helpful assistant.`--model` string ${\color{grey}\text{Optional}}$  Defaults to GPT-3.5-turbo or Claude-v1.3 depends on `api_type`&lt;br&gt; Which model to use, e.g., gpt-4.`--temperature` number ${\color{grey}\text{Optional}}$ Defaults to 1 &lt;br&gt;What sampling temperature to use.  Higher values like 0.9 will make the output more random, while lower values like 0.1 will make it more focused and deterministic. `--max_new_tokens` integer ${\color{grey}\text{Optional}}$ Defaults to 2048 &lt;br&gt;The maximum number of tokens to generate in the chat completion.The total length of input tokens and generated tokens is limited by the model's context length.## ToDo- [x] Batch requests.- [x] OpenAI function_call.- [x] Token number counting.- [x] Async requests.- [ ] Custom token budget.</longdescription>
</pkgmetadata>