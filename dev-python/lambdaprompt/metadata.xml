<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![](https://dcbadge.vercel.app/api/server/kW9nBQErGe?compact=true&amp;style=flat)](https://discord.gg/kW9nBQErGe)# Î»prompt - Build, compose and call templated LLM prompts!Write LLM prompts with jinja templates, compose them in python as functions, and call them directly or use them as a webservice!We believe that large language model prompts are a lot like &quot;functions&quot; in a programming sense and would benefit greatly by the power of an interpreted language. lambdaprompt is a library to offer an interface to back that belief up. This library allows for building full large language model based &quot;prompt machines&quot;, including ones that self-edit to correct and even self-write their own execution code. `pip install lambdaprompt`[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/bluecoconut/bc5925d0de83b478852f5457ef8060ad/example-prompt.ipynb)[A webserver (built on `FastAPI`) example repository](https://github.com/approximatelabs/example-lambdaprompt-server)## Environment variables for using hosted modelsFor using openAI, set up API keys as environment variables or set after importing (also easy to just make a `.env` file, since this uses `dotenv` package)`OPENAI_API_KEY=...`## Creating a promptPrompts use JINJA templating to create a string, the string is passed to the LLM for completion.```pythonfrom lambdaprompt import GPT3Promptexample = GPT3Prompt(&quot;Sally had {{ number }} of {{ thing }}. Sally sold &quot;)# then use it as a functionexample(number=12, thing=&quot;apples&quot;)```## Creating ChatGPT3 Conversational promptsEach prompt can be thought of as a parameterizable conversation, and executing the prompt with an input will apply that as &quot;the next line of conversation&quot; and then generate the response. In order to update the memory state of the prompt, call the `.add()` method on the prompt, which can be used to add steps to a conversation and make the prompt &quot;remember&quot; what has been said.```python&gt;&gt;&gt; import lambdaprompt as lp&gt;&gt;&gt; convo = lp.AsyncGPT3Chat([{'system': 'You are a {{ type_of_bot }}'}])&gt;&gt;&gt; await convo(&quot;What should we get for lunch?&quot;, type_of_bot=&quot;pirate&quot;)As a pirate, I would suggest we have some hearty seafood such as fish and chips or a seafood platter. We could also have some rum to wash it down! Arrr!```## General prompt creationYou can also turn any function into a prompt (useful for composing prompts, or creating programs out of prompts. This is commonly called &quot;prompt chaining&quot;. See how you can achieve this with simple python composition.```pythonfrom lambdaprompt import prompt, GPT3Promptgenerate_n_tasks = GPT3Prompt(&quot;Today I will do {{ n }} things (comma separated) [&quot;, stop=&quot;]&quot;)is_happy = GPT3Prompt(&quot;The task {{ task_detail }} is a task that will make me happy? (y/n):&quot;)@promptdef get_tasks_and_rate_is_happy(n=3):    results = []    for task in generate_n_tasks(n=n).split(&quot;,&quot;):        results.append((task, is_happy(task)))    return resultsprint(get_tasks_and_rate_is_happy())```## Async and SyncLambdaprompt works on both sync and async functions, and offers a sync and async templated prompt interface```pythonfrom lambdaprompt import GPT3Prompt, asyncGPT3Prompt#syncfirst = GPT3Prompt(&quot;Sally had {{ number }} of {{ thing }}. Sally sold &quot;)first(number=12, thing=&quot;apples&quot;)#asyncfirst = asyncGPT3Prompt(&quot;Sally had {{ number }} of {{ thing }}. Sally sold &quot;)await first(number=12, thing=&quot;apples&quot;)``````pythonfrom lambdaprompt import prompt@promptdef sync_example(a):    return a + &quot;!&quot;sync_example(&quot;hello&quot;)@promptasync def async_example(a):    return a + &quot;!&quot;await async_example(&quot;hello&quot;)```### Some special propertiesFor templated prompts with only template variable, can directly call with the variable as positional argument (no need to define in kwarg)```pythonbasic_qa = asyncGPT3Prompt(&quot;basic_qa&quot;, &quot;&quot;&quot;What is the answer to the question [{{ question }}]?&quot;&quot;&quot;)await basic_qa(&quot;Is it safe to eat pizza with chopsticks?&quot;)```## Using lambdaprompt as a webserviceSimply `pip install lambdaprompt[server]` and then add `from lambdaprompt.server.main import app` to the top of your file!make a file`app.py`````pythonfrom lambdaprompt import AsyncGPT3Prompt, promptfrom lambdaprompt.server.main import appAsyncGPT3Prompt(    &quot;&quot;&quot;Rewrite the following as a {{ target_author }}. ```{{ source_text }}```Output:```&quot;&quot;&quot;,    name=&quot;rewrite_as&quot;,    stop=&quot;```&quot;,)````Then run```uvicorn app:app --reload```browse to `http://localhost:8000/docs` to see the swagger docs generated for the prompts!## Running inside dockerFirst, create an .env file with your OpenAI API key: (like `OPENAI_API_KEY=sk-dskj32094klsaj9024lkjsa`)```docker build . -t lambdaprompt:0.0.1docker run -it --env-file .env lambdaprompt:0.0.1  bash -c &quot;python two.py&quot;```This will output something like this:```docker run -it --env-file .env lambdaprompt:0.0.1  bash -c &quot;python two.py&quot;[('example: go for a walk', '\n\nYes. Going for a walk can be a great way to boost your mood and get some fresh air.'), (' read a book', '\n\nYes'), (' call a friend', '\n\nYes')]docker run -it --env-file .env lambdaprompt:0.0.1  bash -c &quot;python two.py&quot;[(' edit ', '\n\nNo. Editing can be a tedious and time-consuming task, so it may not necessarily make you happy.')]```## Design Patterns (TODO)- Response Optimization  - [Ideation, Scoring and Selection](link)  - [Error Correcting Language Loops](link)- Summarization and Aggregations  - [Rolling](link)  - [Fan-out-tree](link)- [Meta-Prompting](link)</longdescription>
</pkgmetadata>