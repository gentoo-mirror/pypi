<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># What is this?`bareunpy` is the python 3 library for bareun.Bareun is a Korean NLP,which provides tokenizing, POS tagging for Korean.## How to install```shellpip3 install bareunpy```## How to get bareun- Go to https://bareun.ai/.  - With registration, for the first time, you can get a API-KEY to use it freely.  - With API-KEY, you can install the `bareun1` server.  - Or you can make a call to use this `bareunpy` library to any servers.- Or use docker image. See https://hub.docker.com/r/bareunai/bareun```shelldocker pull bareunai/bareun:latest```## How to use, tagger```pythonimport sysimport google.protobuf.text_format as tffrom bareunpy import Tagger## you can API-KEY from https://bareun.ai/#API_KEY=&quot;koba-42CXULQ-SDPU6ZA-RQ6QPBQ-4BMZCOA&quot;# If you have your own localhost bareun.my_tagger = Tagger(API_KEY, 'localhost')# or if you have your own bareun which is running on 10.8.3.211:15656.my_tagger = Tagger(API_KEY, '10.8.3.211', 15656)# print results. res = tagger.tags([&quot;안녕하세요.&quot;, &quot;반가워요!&quot;])# get protobuf message.m = res.msg()tf.PrintMessage(m, out=sys.stdout, as_utf8=True)print(tf.MessageToString(m, as_utf8=True))print(f'length of sentences is {len(m.sentences)}')## output : 2print(f'length of tokens in sentences[0] is {len(m.sentences[0].tokens)}')print(f'length of morphemes of first token in sentences[0] is {len(m.sentences[0].tokens[0].morphemes)}')print(f'lemma of first token in sentences[0] is {m.sentences[0].tokens[0].lemma}')print(f'first morph of first token in sentences[0] is {m.sentences[0].tokens[0].morphemes[0]}')print(f'tag of first morph of first token in sentences[0] is {m.sentences[0].tokens[0].morphemes[0].tag}')## Advanced usage.for sent in m.sentences:    for token in sent.tokens:        for m in token.morphemes:            print(f'{m.text.content}/{m.tag}:{m.probability}:{m.out_of_vocab})# get json objectjo = res.as_json()print(jo)# get tuple of pos tagging.pa = res.pos()print(pa)# another methodsma = res.morphs()print(ma)na = res.nouns()print(na)va = res.verbs()print(va)# custom dictionarycust_dic = tagger.custom_dict(&quot;my&quot;)cust_dic.copy_np_set({'내고유명사', '우리집고유명사'})cust_dic.copy_cp_set({'코로나19'})cust_dic.copy_cp_caret_set({'코로나^백신', '&quot;독감^백신'})cust_dic.update()# laod prev custom dictcust_dict2 = tagger.custom_dict(&quot;my&quot;)cust_dict2.load()tagger.set_domain('my')tagger.pos('코로나19는 언제 끝날까요?')```## How to use, tokenizer```pythonimport sysimport google.protobuf.text_format as tffrom bareunpy import Tokenizer# If you have your own localhost bareun.my_tokenizer = Tokenizer(API_KEY, 'localhost')# or if you have your own bareun which is running on 10.8.3.211:15656.my_tokenizer = Tagger(API_KEY, '10.8.3.211', 15656)# print results. tokenized = tokenizer.tokenize_list([&quot;안녕하세요.&quot;, &quot;반가워요!&quot;])# get protobuf message.m = tokenized.msg()tf.PrintMessage(m, out=sys.stdout, as_utf8=True)print(tf.MessageToString(m, as_utf8=True))print(f'length of sentences is {len(m.sentences)}')## output : 2print(f'length of tokens in sentences[0] is {len(m.sentences[0].tokens)}')print(f'length of segments of first token in sentences[0] is {len(m.sentences[0].tokens[0].segments)}')print(f'tagged of first token in sentences[0] is {m.sentences[0].tokens[0].tagged}')print(f'first segment of first token in sentences[0] is {m.sentences[0].tokens[0].segments[0]}')print(f'hint of first morph of first token in sentences[0] is {m.sentences[0].tokens[0].segments[0].hint}')## Advanced usage.for sent in m.sentences:    for token in sent.tokens:        for m in token.segments:            print(f'{m.text.content}/{m.hint})# get json objectjo = tokenized.as_json()print(jo)# get tuple of segmentsss = tokenized.segments()print(ss)ns = tokenized.nouns()print(ns)vs = tokenized.verbs()print(vs)# postpositions: 조사ps = tokenized.postpositions()print(ps)# Adverbs, 부사ass = tokenized.adverbs()print(ass)ss = tokenized.symbols()print(ss)```</longdescription>
</pkgmetadata>