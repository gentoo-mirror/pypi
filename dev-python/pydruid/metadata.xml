<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pydruidpydruid exposes a simple API to create, execute, and analyze [Druid](http://druid.io/) queries. pydruid can parse query results into [Pandas](http://pandas.pydata.org/) DataFrame objects for subsequent data analysis -- this offers a tight integration between [Druid](http://druid.io/), the [SciPy](http://www.scipy.org/stackspec.html) stack (for scientific computing) and [scikit-learn](http://scikit-learn.org/stable/) (for machine learning). pydruid can export query results into TSV or JSON for further processing with your favorite tool, e.g., R, Julia, Matlab, Excel. It provides both synchronous and asynchronous clients.Additionally, pydruid implements the [Python DB API 2.0](https://www.python.org/dev/peps/pep-0249/), a [SQLAlchemy dialect](http://docs.sqlalchemy.org/en/latest/dialects/), and a provides a command line interface to interact with Druid.To install:```pythonpip install pydruid# or, if you intend to use asynchronous clientpip install pydruid[async]# or, if you intend to export query results into pandaspip install pydruid[pandas]# or, if you intend to do bothpip install pydruid[async, pandas]# or, if you want to use the SQLAlchemy enginepip install pydruid[sqlalchemy]# or, if you want to use the CLIpip install pydruid[cli]```Documentation: https://pythonhosted.org/pydruid/.# examplesThe following exampes show how to execute and analyze the results of three types of queries: timeseries, topN, and groupby. We will use these queries to ask simple questions about twitter's public data set.## timeseriesWhat was the average tweet length, per day, surrounding the 2014 Sochi olympics?```pythonfrom pydruid.client import *from pylab import pltquery = PyDruid(druid_url_goes_here, 'druid/v2')ts = query.timeseries(    datasource='twitterstream',    granularity='day',    intervals='2014-02-02/p4w',    aggregations={'length': doublesum('tweet_length'), 'count': doublesum('count')},    post_aggregations={'avg_tweet_length': (Field('length') / Field('count'))},    filter=Dimension('first_hashtag') == 'sochi2014')df = query.export_pandas()df['timestamp'] = df['timestamp'].map(lambda x: x.split('T')[0])df.plot(x='timestamp', y='avg_tweet_length', ylim=(80, 140), rot=20,        title='Sochi 2014')plt.ylabel('avg tweet length (chars)')plt.show()```![alt text](https://github.com/metamx/pydruid/raw/master/docs/figures/avg_tweet_length.png &quot;Avg. tweet length&quot;)## topNWho were the top ten mentions (@user_name) during the 2014 Oscars?```pythontop = query.topn(    datasource='twitterstream',    granularity='all',    intervals='2014-03-03/p1d',  # utc time of 2014 oscars    aggregations={'count': doublesum('count')},    dimension='user_mention_name',    filter=(Dimension('user_lang') == 'en') &amp; (Dimension('first_hashtag') == 'oscars') &amp;           (Dimension('user_time_zone') == 'Pacific Time (US &amp; Canada)') &amp;           ~(Dimension('user_mention_name') == 'No Mention'),    metric='count',    threshold=10)df = query.export_pandas()print df   count                 timestamp user_mention_name0   1303  2014-03-03T00:00:00.000Z      TheEllenShow1     44  2014-03-03T00:00:00.000Z        TheAcademy2     21  2014-03-03T00:00:00.000Z               MTV3     21  2014-03-03T00:00:00.000Z         peoplemag4     17  2014-03-03T00:00:00.000Z               THR5     16  2014-03-03T00:00:00.000Z      ItsQueenElsa6     16  2014-03-03T00:00:00.000Z           eonline7     15  2014-03-03T00:00:00.000Z       PerezHilton8     14  2014-03-03T00:00:00.000Z     realjohngreen9     12  2014-03-03T00:00:00.000Z       KevinSpacey```## groupbyWhat does the social network of users replying to other users look like?```pythonfrom igraph import *from cairo import *from pandas import concatgroup = query.groupby(    datasource='twitterstream',    granularity='hour',    intervals='2013-10-04/pt12h',    dimensions=[&quot;user_name&quot;, &quot;reply_to_name&quot;],    filter=(~(Dimension(&quot;reply_to_name&quot;) == &quot;Not A Reply&quot;)) &amp;           (Dimension(&quot;user_location&quot;) == &quot;California&quot;),    aggregations={&quot;count&quot;: doublesum(&quot;count&quot;)})df = query.export_pandas()# map names to categorical variables with a lookup tablenames = concat([df['user_name'], df['reply_to_name']]).unique()nameLookup = dict([pair[::-1] for pair in enumerate(names)])df['user_name_lookup'] = df['user_name'].map(nameLookup.get)df['reply_to_name_lookup'] = df['reply_to_name'].map(nameLookup.get)# create the graph with igraphg = Graph(len(names), directed=False)vertices = zip(df['user_name_lookup'], df['reply_to_name_lookup'])g.vs[&quot;name&quot;] = namesg.add_edges(vertices)layout = g.layout_fruchterman_reingold()plot(g, &quot;tweets.png&quot;, layout=layout, vertex_size=2, bbox=(400, 400), margin=25, edge_width=1, vertex_color=&quot;blue&quot;)```![alt text](https://github.com/metamx/pydruid/raw/master/docs/figures/twitter_graph.png &quot;Social Network&quot;)# asynchronous client```pydruid.async_client.AsyncPyDruid``` implements an asynchronous client. To achieve that, it utilizes an asynchronousHTTP client from ```Tornado``` framework. The asynchronous client is suitable for use with async frameworks such as Tornadoand provides much better performance at scale. It lets you serve multiple requests at the same time, without blocking onDruid executing your queries.## example```pythonfrom tornado import genfrom pydruid.async_client import AsyncPyDruidfrom pydruid.utils.aggregators import longsumfrom pydruid.utils.filters import Dimensionclient = AsyncPyDruid(url_to_druid_broker, 'druid/v2')@gen.coroutinedef your_asynchronous_method_serving_top10_mentions_for_day(day    top_mentions = yield client.topn(        datasource='twitterstream',        granularity='all',        intervals=&quot;%s/p1d&quot; % (day, ),        aggregations={'count': doublesum('count')},        dimension='user_mention_name',        filter=(Dimension('user_lang') == 'en') &amp; (Dimension('first_hashtag') == 'oscars') &amp;               (Dimension('user_time_zone') == 'Pacific Time (US &amp; Canada)') &amp;               ~(Dimension('user_mention_name') == 'No Mention'),        metric='count',        threshold=10)    # asynchronously return results    # can be simply ```return top_mentions``` in python 3.x    raise gen.Return(top_mentions)```# thetaSketchesTheta sketch Post aggregators are built slightly differently to normal Post Aggregators, as they have different operators.Note: you must have the ```druid-datasketches``` extension loaded into your Druid cluster in order to use these.See the [Druid datasketches](http://druid.io/docs/latest/development/extensions-core/datasketches-aggregators.html) documentation for details.```pythonfrom pydruid.client import *from pydruid.utils import aggregatorsfrom pydruid.utils import filtersfrom pydruid.utils import postaggregatorquery = PyDruid(url_to_druid_broker, 'druid/v2')ts = query.groupby(    datasource='test_datasource',    granularity='all',    intervals='2016-09-01/P1M',    filter = ( filters.Dimension('product').in_(['product_A', 'product_B'])),    aggregations={        'product_A_users': aggregators.filtered(            filters.Dimension('product') == 'product_A',            aggregators.thetasketch('user_id')            ),        'product_B_users': aggregators.filtered(            filters.Dimension('product') == 'product_B',            aggregators.thetasketch('user_id')            )    },    post_aggregations={        'both_A_and_B': postaggregator.ThetaSketchEstimate(            postaggregator.ThetaSketch('product_A_users') &amp; postaggregator.ThetaSketch('product_B_users')            )    })```# DB API```pythonfrom pydruid.db import connectconn = connect(host='localhost', port=8082, path='/druid/v2/sql/', scheme='http')curs = conn.cursor()curs.execute(&quot;&quot;&quot;    SELECT place,           CAST(REGEXP_EXTRACT(place, '(.*),', 1) AS FLOAT) AS lat,           CAST(REGEXP_EXTRACT(place, ',(.*)', 1) AS FLOAT) AS lon      FROM places     LIMIT 10&quot;&quot;&quot;)for row in curs:    print(row)```# SQLAlchemy```pythonfrom sqlalchemy import *from sqlalchemy.engine import create_enginefrom sqlalchemy.schema import *engine = create_engine('druid://localhost:8082/druid/v2/sql/')  # uses HTTP by default :(# engine = create_engine('druid+http://localhost:8082/druid/v2/sql/')# engine = create_engine('druid+https://localhost:8082/druid/v2/sql/')places = Table('places', MetaData(bind=engine), autoload=True)print(select([func.count('*')], from_obj=places).scalar())```## Column headersIn version 0.13.0 Druid SQL added support for including the column names in theresponse which can be requested via the &quot;header&quot; field in the request. Thishelps to ensure that the cursor description is defined (which is a requirementfor SQLAlchemy query statements) regardless on whether the result set containsany rows. Historically this was problematic for result sets which contained norows at one could not infer the expected column names.Enabling the header can be configured via the SQLAlchemy URI by using the queryparameter, i.e.,```pythonengine = create_engine('druid://localhost:8082/druid/v2/sql?header=true')```Note the current default is `false` to ensure backwards compatibility but shouldbe set to `true` for Druid versions &gt;= 0.13.0.# Command line```bash$ pydruid http://localhost:8082/druid/v2/sql/&gt; SELECT COUNT(*) AS cnt FROM places  cnt-----12345&gt; SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES;TABLE_NAME----------test_tableCOLUMNSSCHEMATATABLES&gt; BYE;GoodBye!```# ContributingContributions are welcomed of course. We like to use `black` and `flake8`.```bashpip install -r requirements-dev.txt  # installs useful dev depspre-commit install  # installs useful commit hooks```</longdescription>
</pkgmetadata>