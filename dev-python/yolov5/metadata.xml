<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;h1 align=&quot;center&quot;&gt;  packaged ultralytics/yolov5&lt;/h1&gt;&lt;h4 align=&quot;center&quot;&gt;  pip install yolov5&lt;/h4&gt;&lt;div align=&quot;center&quot;&gt;  &lt;a href=&quot;https://pepy.tech/project/yolov5&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/yolov5&quot; alt=&quot;total downloads&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://pepy.tech/project/yolov5&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/yolov5/month&quot; alt=&quot;monthly downloads&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://twitter.com/fcakyon&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/twitter-fcakyon_-blue?logo=twitter&amp;style=flat&quot; alt=&quot;fcakyon twitter&quot;&gt;&lt;/a&gt;  &lt;br&gt;  &lt;a href=&quot;https://badge.fury.io/py/yolov5&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/yolov5.svg?kill_cache=1&quot; alt=&quot;pypi version&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://github.com/fcakyon/yolov5-pip/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/fcakyon/yolov5-pip/actions/workflows/ci.yml/badge.svg&quot; alt=&quot;ci testing&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://github.com/fcakyon/yolov5-pip/actions/workflows/package_testing.yml&quot;&gt;&lt;img src=&quot;https://github.com/fcakyon/yolov5-pip/actions/workflows/package_testing.yml/badge.svg&quot; alt=&quot;package testing&quot;&gt;&lt;/a&gt;&lt;/div&gt;## &lt;div align=&quot;center&quot;&gt;Overview&lt;/div&gt;&lt;div align=&quot;center&quot;&gt;You can finally install &lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;YOLOv5 object detector&lt;/a&gt; using &lt;a href=&quot;https://pypi.org/project/yolov5/&quot;&gt;pip&lt;/a&gt; and integrate into your project easily.&lt;img src=&quot;https://user-images.githubusercontent.com/26833433/136901921-abcfcd9d-f978-4942-9b97-0e3f202907df.png&quot; width=&quot;1000&quot;&gt;&lt;/div&gt;&lt;br&gt;This yolov5 package contains everything from ultralytics/yolov5 &lt;a href=&quot;https://github.com/ultralytics/yolov5/tree/5deff1471dede726f6399be43e7073ee7ed3a7d4&quot;&gt;at this commit&lt;/a&gt; plus:&lt;br&gt;1. Easy installation via pip: &lt;b&gt;pip install yolov5&lt;/b&gt;&lt;br&gt;2. Full CLI integration with &lt;a href=&quot;https://github.com/google/python-fire&quot;&gt;fire&lt;/a&gt; package&lt;br&gt;3. COCO dataset format support (for training)&lt;br&gt;4. Full &lt;a href=&quot;https://huggingface.co/models?other=yolov5&quot;&gt;ðŸ¤— Hub&lt;/a&gt; integration&lt;br&gt;5. &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;S3&lt;/a&gt; support (model and dataset upload)&lt;br&gt;6. &lt;a href=&quot;https://neptune.ai/&quot;&gt;NeptuneAI&lt;/a&gt; logger support (metric, model and dataset logging)&lt;br&gt;7. Classwise AP logging during experiments## &lt;div align=&quot;center&quot;&gt;Install&lt;/div&gt;Install yolov5 using pip (for Python &gt;=3.7)```consolepip install yolov5```## &lt;div align=&quot;center&quot;&gt;Model Zoo&lt;/div&gt;&lt;div align=&quot;center&quot;&gt;Effortlessly explore and use finetuned YOLOv5 models with one line of code: &lt;a href=&quot;https://github.com/keremberke/awesome-yolov5-models&quot;&gt;awesome-yolov5-models&lt;/a&gt;&lt;a href=&quot;https://github.com/keremberke/awesome-yolov5-models&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/34196005/210134158-108b24f4-2b8e-43ea-95c8-44731625cde2.gif&quot; width=&quot;640&quot;&gt;&lt;/a&gt;&lt;/div&gt;## &lt;div align=&quot;center&quot;&gt;Use from Python&lt;/div&gt;```pythonimport yolov5# load pretrained modelmodel = yolov5.load('yolov5s.pt')# or load custom modelmodel = yolov5.load('train/best.pt')  # set model parametersmodel.conf = 0.25  # NMS confidence thresholdmodel.iou = 0.45  # NMS IoU thresholdmodel.agnostic = False  # NMS class-agnosticmodel.multi_label = False  # NMS multiple labels per boxmodel.max_det = 1000  # maximum number of detections per image# set imageimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'# perform inferenceresults = model(img)# inference with larger input sizeresults = model(img, size=1280)# inference with test time augmentationresults = model(img, augment=True)# parse resultspredictions = results.pred[0]boxes = predictions[:, :4] # x1, y1, x2, y2scores = predictions[:, 4]categories = predictions[:, 5]# show detection bounding boxes on imageresults.show()# save results into &quot;results/&quot; folderresults.save(save_dir='results/')```&lt;details closed&gt;&lt;summary&gt;Train/Detect/Test/Export&lt;/summary&gt;- You can directly use these functions by importing them:```pythonfrom yolov5 import train, val, detect, export# from yolov5.classify import train, val, predict# from yolov5.segment import train, val, predicttrain.run(imgsz=640, data='coco128.yaml')val.run(imgsz=640, data='coco128.yaml', weights='yolov5s.pt')detect.run(imgsz=640)export.run(imgsz=640, weights='yolov5s.pt')```- You can pass any argument as input:```pythonfrom yolov5 import detectimg_url = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'detect.run(source=img_url, weights=&quot;yolov5s6.pt&quot;, conf_thres=0.25, imgsz=640)```&lt;/details&gt;## &lt;div align=&quot;center&quot;&gt;Use from CLI&lt;/div&gt;You can call `yolov5 train`, `yolov5 detect`, `yolov5 val` and `yolov5 export` commands after installing the package via `pip`:&lt;details open&gt;&lt;summary&gt;Training&lt;/summary&gt;- Finetune one of the pretrained YOLOv5 models using your custom `data.yaml`:```bash$ yolov5 train --data data.yaml --weights yolov5s.pt --batch-size 16 --img 640                                          yolov5m.pt              8                                          yolov5l.pt              4                                          yolov5x.pt              2```- Start a training using a COCO formatted dataset:```yaml# data.ymltrain_json_path: &quot;train.json&quot;train_image_dir: &quot;train_image_dir/&quot;val_json_path: &quot;val.json&quot;val_image_dir: &quot;val_image_dir/&quot;``````bash$ yolov5 train --data data.yaml --weights yolov5s.pt```- Train your model using [Roboflow Universe](https://universe.roboflow.com/) datasets (roboflow&gt;=0.2.29 required):```bash$ yolov5 train --data DATASET_UNIVERSE_URL --weights yolov5s.pt --roboflow_token YOUR_ROBOFLOW_TOKEN```Where `DATASET_UNIVERSE_URL` must be in `https://universe.roboflow.com/workspace_name/project_name/project_version` format.- Visualize your experiments via [Neptune.AI](https://neptune.ai/) (neptune-client&gt;=0.10.10 required):```bash$ yolov5 train --data data.yaml --weights yolov5s.pt --neptune_project NAMESPACE/PROJECT_NAME --neptune_token YOUR_NEPTUNE_TOKEN```- Automatically upload weights to [Huggingface Hub](https://huggingface.co/models?other=yolov5):```bash$ yolov5 train --data data.yaml --weights yolov5s.pt --hf_model_id username/modelname --hf_token YOUR-HF-WRITE-TOKEN```- Automatically upload weights and datasets to AWS S3 (with Neptune.AI artifact tracking integration):```bashexport AWS_ACCESS_KEY_ID=YOUR_KEYexport AWS_SECRET_ACCESS_KEY=YOUR_KEY``````bash$ yolov5 train --data data.yaml --weights yolov5s.pt --s3_upload_dir YOUR_S3_FOLDER_DIRECTORY --upload_dataset```- Add `yolo_s3_data_dir` into `data.yaml` to match Neptune dataset with a present dataset in S3.```yaml# data.ymltrain_json_path: &quot;train.json&quot;train_image_dir: &quot;train_image_dir/&quot;val_json_path: &quot;val.json&quot;val_image_dir: &quot;val_image_dir/&quot;yolo_s3_data_dir: s3://bucket_name/data_dir/```&lt;/details&gt;&lt;details open&gt;&lt;summary&gt;Inference&lt;/summary&gt;yolov5 detect command runs inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases) and saving results to `runs/detect`.```bash$ yolov5 detect --source 0  # webcam                         file.jpg  # image                         file.mp4  # video                         path/  # directory                         path/*.jpg  # glob                         rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa  # rtsp stream                         rtmp://192.168.1.105/live/test  # rtmp stream                         http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8  # http stream```&lt;/details&gt;&lt;details open&gt;&lt;summary&gt;Export&lt;/summary&gt;You can export your fine-tuned YOLOv5 weights to any format such as `torchscript`, `onnx`, `coreml`, `pb`, `tflite`, `tfjs`:```bash$ yolov5 export --weights yolov5s.pt --include torchscript,onnx,coreml,pb,tfjs```&lt;/details&gt;&lt;details open&gt;&lt;summary&gt;Classify&lt;/summary&gt;Train/Val/Predict with YOLOv5 image classifier:```bash$ yolov5 classify train --img 640 --data mnist2560 --weights yolov5s-cls.pt --epochs 1``````bash$ yolov5 classify predict --img 640 --weights yolov5s-cls.pt --source images/```&lt;/details&gt;&lt;details open&gt;&lt;summary&gt;Segment&lt;/summary&gt;Train/Val/Predict with YOLOv5 instance segmentation model:```bash$ yolov5 segment train --img 640 --weights yolov5s-seg.pt --epochs 1``````bash$ yolov5 segment predict --img 640 --weights yolov5s-seg.pt --source images/```&lt;/details&gt;</longdescription>
</pkgmetadata>