<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;544px&quot; src=&quot;logo.svg&quot; /&gt;&lt;/p&gt;&lt;h3 align=&quot;center&quot;&gt;Extensible and Fault-Tolerant Hyperparameter Management&lt;/h3&gt;HParams is a thoughtful approach to configuration management for machine learning projects. Itenables you to externalize your hyperparameters into a configuration file. In doing so, you canreproduce experiments, iterate quickly, and reduce errors.**Features:**- Approachable and easy-to-use API- Battle-tested over three years- Fast with little to no runtime overhead (&lt; 3e-05 seconds) per configured function- Robust to most use cases with 100% test coverage and 75 tests- Lightweight with only one dependency![PyPI - Python Version](https://img.shields.io/pypi/pyversions/hparams.svg?style=flat-square)[![Codecov](https://img.shields.io/codecov/c/github/PetrochukM/HParams/master.svg?style=flat-square)](https://codecov.io/gh/PetrochukM/HParams)[![Downloads](http://pepy.tech/badge/hparams)](http://pepy.tech/project/hparams)[![Build Status](https://img.shields.io/travis/PetrochukM/HParams/master.svg?style=flat-square)](https://travis-ci.org/PetrochukM/HParams)[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg?style=flat-square)](https://opensource.org/licenses/MIT)[![Twitter: PetrochukM](https://img.shields.io/twitter/follow/MPetrochuk.svg?style=social)](https://twitter.com/MPetrochuk)_Logo by [Chloe Yeo](http://www.yeochloe.com/), Corporate Sponsorship by [WellSaid Labs](https://wellsaidlabs.com/)_## InstallationMake sure you have Python 3. You can then install `hparams` using `pip`:```bashpip install hparams```Install the latest code via:```bashpip install git+https://github.com/PetrochukM/HParams.git```## Oops üêõWith HParams, you will avoid common but needless hyperparameter mistakes. It will throw a warningor error if:- A hyperparameter is overwritten.- A hyperparameter is declared but not set.- A hyperparameter is set but not declared.- A hyperparameter type is incorrect.Finally, HParams is built with developer experience in mind. HParams includes 13 errors and 6 warningsto help catch and resolve issues quickly.## ExamplesAdd HParams to your project by following one of these common use cases:### Configure Training ü§óConfigure your training run, like so:```python# main.pyfrom hparams import configurable, add_config, HParams, HParamfrom typing import Union@configurabledef train(batch_size: Union[int, HParam]=HParam(int)):    passclass Model():    @configurable    def __init__(self, hidden_size=HParam(int), dropout=HParam(float)):        passadd_config({ 'main': {    'train': HParams(batch_size=32),    'Model.__init__': HParams(hidden_size=1024, dropout=0.25),}})```HParams supports optional configuration typechecking to help you find bugs! üêõ### Set DefaultsConfigure PyTorch and Tensorflow defaults to match via:```pythonfrom torch.nn import BatchNorm1dfrom hparams import configurable, add_config, HParams# NOTE: `momentum=0.01` to match Tensorflow defaultsBatchNorm1d.__init__ = configurable(BatchNorm1d.__init__)add_config({ 'torch.nn.BatchNorm1d.__init__': HParams(momentum=0.01) })```Configure your random seed globally, like so:```python# config.pyimport randomfrom hparams import configurable, add_config, HParamsrandom.seed = configurable(random.seed)add_config({'random.seed': HParams(a=123)})``````python# main.pyimport configimport randomrandom.seed()```### CLIExperiment with hyperparameters through your command line, for example:```consolefoo@bar:~$ file.py --torch.optim.adam.Adam.__init__ 'HParams(lr=0.1,betas=(0.999,0.99))'``````pythonimport sysfrom torch.optim import Adamfrom hparams import configurable, add_config, parse_hparam_argsAdam.__init__ = configurable(Adam.__init__)parsed = parse_hparam_args(sys.argv[1:])  # Parse command line argumentsadd_config(parsed)```### Hyperparameter optimizationHyperparameter optimization is easy to-do, check this out:```pythonimport itertoolsfrom torch.optim import Adamfrom hparams import configurable, add_config, HParamsAdam.__init__ = configurable(Adam.__init__)def train():  # Train the model and return the loss.    passfor betas in itertools.product([0.999, 0.99, 0.9], [0.999, 0.99, 0.9]):    add_config({Adam.__init__: HParams(betas=betas)})  # Grid search over the `betas`    train()```### Track HyperparametersEasily track your hyperparameters using tools like [Comet](comet.ml).```pythonfrom comet_ml import Experimentfrom hparams import get_configexperiment = Experiment()experiment.log_parameters(get_config())```### Multiprocessing: Partial SupportExport a Python `functools.partial` to use in another process, like so:```pythonfrom hparams import configurable, HParam@configurabledef func(hparam=HParam()):    passpartial = func.get_configured_partial()```With this approach, you don't have to transfer the global state to the new process. To transfer theglobal state, you'll want to use `get_config` and `add_config`.## Docs üìñThe complete documentation for HParams is available [here](./DOCS.md).## ContributingWe've released HParams because a lack of hyperparameter management solutions. We hope thatother people can benefit from the project. We are thankful for any contributions from thecommunity.### Contributing GuideRead our [contributing guide](https://github.com/PetrochukM/HParams/blob/master/CONTRIBUTING.md) tolearn about our development process, how to propose bugfixes and improvements, and how to build andtest your changes to HParams.## Authors- [Michael Petrochuk](https://github.com/PetrochukM/) ‚Äî Developer- [Chloe Yeo](http://www.yeochloe.com/) ‚Äî Logo Design## CitingIf you find HParams useful for an academic publication, then please use the following BibTeX tocite it:```latex@misc{hparams,author = {Petrochuk, Michael},title = {HParams: Hyperparameter management solution},year = {2019},publisher = {GitHub},journal = {GitHub repository},howpublished = {\url{https://github.com/PetrochukM/HParams}},}```</longdescription>
</pkgmetadata>