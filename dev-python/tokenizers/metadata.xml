<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;    &lt;br&gt;    &lt;img src=&quot;https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png&quot; width=&quot;600&quot;/&gt;    &lt;br&gt;&lt;p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://badge.fury.io/py/tokenizers&quot;&gt;         &lt;img alt=&quot;Build&quot; src=&quot;https://badge.fury.io/py/tokenizers.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/huggingface/tokenizers/blob/master/LICENSE&quot;&gt;        &lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue&quot;&gt;    &lt;/a&gt;&lt;/p&gt;&lt;br&gt;# TokenizersProvides an implementation of today's most used tokenizers, with a focus on performance andversatility.Bindings over the [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers) implementation.If you are interested in the High-level design, you can go check it there.Otherwise, let's dive in!## Main features: - Train new vocabularies and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3   most common BPE versions). - Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes   less than 20 seconds to tokenize a GB of text on a server's CPU. - Easy to use, but also extremely versatile. - Designed for research and production. - Normalization comes with alignments tracking. It's always possible to get the part of the   original sentence that corresponds to a given token. - Does all the pre-processing: Truncate, Pad, add the special tokens your model needs.### Installation#### With pip:```bashpip install tokenizers```#### From sources:To use this method, you need to have the Rust installed:```bash# Install with:curl https://sh.rustup.rs -sSf | sh -s -- -yexport PATH=&quot;$HOME/.cargo/bin:$PATH&quot;```Once Rust is installed, you can compile doing the following```bashgit clone https://github.com/huggingface/tokenizerscd tokenizers/bindings/python# Create a virtual env (you can use yours as well)python -m venv .envsource .env/bin/activate# Install `tokenizers` in the current virtual envpip install -e .```### Load a pretrained tokenizer from the Hub```pythonfrom tokenizers import Tokenizertokenizer = Tokenizer.from_pretrained(&quot;bert-base-cased&quot;)```### Using the provided TokenizersWe provide some pre-build tokenizers to cover the most common cases. You can easily load one ofthese using some `vocab.json` and `merges.txt` files:```pythonfrom tokenizers import CharBPETokenizer# Initialize a tokenizervocab = &quot;./path/to/vocab.json&quot;merges = &quot;./path/to/merges.txt&quot;tokenizer = CharBPETokenizer(vocab, merges)# And then encode:encoded = tokenizer.encode(&quot;I can feel the magic, can you?&quot;)print(encoded.ids)print(encoded.tokens)```And you can train them just as simply:```pythonfrom tokenizers import CharBPETokenizer# Initialize a tokenizertokenizer = CharBPETokenizer()# Then train it!tokenizer.train([ &quot;./path/to/files/1.txt&quot;, &quot;./path/to/files/2.txt&quot; ])# Now, let's use it:encoded = tokenizer.encode(&quot;I can feel the magic, can you?&quot;)# And finally save it somewheretokenizer.save(&quot;./path/to/directory/my-bpe.tokenizer.json&quot;)```#### Provided Tokenizers - `CharBPETokenizer`: The original BPE - `ByteLevelBPETokenizer`: The byte level version of the BPE - `SentencePieceBPETokenizer`: A BPE implementation compatible with the one used by SentencePiece - `BertWordPieceTokenizer`: The famous Bert tokenizer, using WordPieceAll of these can be used and trained as explained above!### Build your ownWhenever these provided tokenizers don't give you enough freedom, you can build your own tokenizer,by putting all the different parts you need together.You can check how we implemented the [provided tokenizers](https://github.com/huggingface/tokenizers/tree/master/bindings/python/py_src/tokenizers/implementations) and adapt them easily to your own needs.#### Building a byte-level BPEHere is an example showing how to build your own byte-level BPE by putting all the different piecestogether, and then saving it to a single file:```pythonfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors# Initialize a tokenizertokenizer = Tokenizer(models.BPE())# Customize pre-tokenization and decodingtokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)tokenizer.decoder = decoders.ByteLevel()tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)# And then traintrainer = trainers.BpeTrainer(    vocab_size=20000,    min_frequency=2,    initial_alphabet=pre_tokenizers.ByteLevel.alphabet())tokenizer.train([    &quot;./path/to/dataset/1.txt&quot;,    &quot;./path/to/dataset/2.txt&quot;,    &quot;./path/to/dataset/3.txt&quot;], trainer=trainer)# And Save ittokenizer.save(&quot;byte-level-bpe.tokenizer.json&quot;, pretty=True)```Now, when you want to use this tokenizer, this is as simple as:```pythonfrom tokenizers import Tokenizertokenizer = Tokenizer.from_file(&quot;byte-level-bpe.tokenizer.json&quot;)encoded = tokenizer.encode(&quot;I can feel the magic, can you?&quot;)```</longdescription>
</pkgmetadata>