<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># ðŸ“ˆ Chat2Plot - interactive &amp; safe text-to-visualization with LLMThis library uses LLMs to generate:1. Visualization 2. High-level chart specifications in json (You can choose between a simple or vega-lite format)3. Explanationfrom natural language requests for given data.Chat2Plot does not generate executable code or SQL from the LLM, so you can safely generate visualizations.demo: https://chat2plot-sample.streamlit.app/![sample](sample.png)## Quick Start```shellpip install chat2plot``````Pythonimport osimport pandas as pdfrom chat2plot import chat2plot# 1. Set api-keyos.environ[&quot;OPENAI_API_KEY&quot;] = &quot;...&quot;df = pd.read_csv(...)# 2. Pass a dataframe to drawc2p = chat2plot(df)# 3. Make a question about the dataresult = c2p(&quot;average target over countries&quot;)result.figure.show()  # draw a plotprint(result.config)  # get a config (json / dataclass)print(result.explanation)  # see the explanation generated by LLM# you can make follow-up request to refine the chartresult = c2p(&quot;change to horizontal-bar chart&quot;)result.figure.show()```## Why Chat2Plot?Inside Chat2Plot, LLM does not generate Python code,but generates plot specifications in json.The declarative visualization specification in json is transformed into actual charts in Chat2Plot using plotly or altair, but users can also use json directly in their own applications.This design limits the visualization expression compared to Python code generation (such as ChatGPT's Code Interpreter Plugin), but has the following practical advantages:- Secure    - More secure execution, as LLM does not directly generate code.- Language-independent    - Declarative data structures are language-agnostic, making it easy to plot in non-Python environments.- Interactive    - Declarative data can be modified by the user to improve plots through collaborative work between the user and LLM.By default, chat2plot uses [function calling API](https://openai.com/blog/function-calling-and-other-api-updates).## Examples### Custom language models`gpt-3.5-turbo-0613` is used by default, but you can use other language models.```pythonimport pandas as pdfrom langchain.chat_models import AzureChatOpenAIfrom chat2plot import chat2plotplot = chat2plot(pd.DataFrame(), chat=AzureChatOpenAI())ret = plot.query(&quot;&lt;your query&gt;&quot;)```### Vega-lite format```pythonimport pandas as pdfrom chat2plot import chat2plotplot = chat2plot(pd.DataFrame(), schema_definition=&quot;vega&quot;)ret = plot.query(&quot;&lt;your query&gt;&quot;)assert isinstance(ret.config, dict)  # vega-lite formatprint(ret.config)```### Custom chart definition```pythonimport pydanticimport pandas as pdfrom chat2plot import chat2plotclass CustomChartConfig(pydantic.BaseModel):    chart_type: str    x_axis_name: str    y_axis_name: str    y_axis_aggregate: strplot = chat2plot(pd.DataFrame(), schema_definition=CustomChartConfig)ret = plot.query(&quot;&lt;your query&gt;&quot;)# chat2plot treats the data type you pass as a chart settingassert isinstance(ret.config, CustomChartConfig)```### Specifying output languageYou can specify in which language the chart explanations should be output. If not specified, it will return as much as possible in the same language as the user's question, but this option is often useful if you always want output in a specific language.```pythonimport pandas as pdfrom chat2plot import chat2plotplot = chat2plot(pd.DataFrame(), language=&quot;Chinese&quot;)ret = plot.query(&quot;&lt;your query&gt;&quot;)print(ret.explanation)  # explanation ```### Privacy preservingWhen `description_strategy=&quot;dtypes&quot;` is specified, chat2plot will not send the data content (but just column names) to LLM.```pythonimport pandas as pdfrom langchain.chat_models import AzureChatOpenAIfrom chat2plot import chat2plotplot = chat2plot(pd.DataFrame(), description_strategy=&quot;dtypes&quot;)ret = plot.query(&quot;&lt;your query&gt;&quot;)```## APIA `Chat2Plot` instance can be created using the `chat2plot` function.```Pythondef chat2plot(    df: pd.DataFrame,    schema_definition: Literal[&quot;simple&quot;, &quot;vega&quot;] | Type[pydantic.BaseModel] = &quot;simple&quot;,    chat: BaseChatModel | None = None,    function_call: bool | Literal[&quot;auto&quot;] = &quot;auto&quot;,    language: str | None = None,    description_strategy: str = &quot;head&quot;,    custom_deserializer: ModelDeserializer | None = None,    verbose: bool = False,) -&gt; Chat2PlotBase:```- **df** - Data source for visualization.- **schema_definition** (optional) - Type of json format.  - `vega` - A vega-lite compliant format  - `simple` - chat2plot's built-in format, parsed as `chat2plot.PlotConfig`  If you want chat2plot to generate chart definitions according to your own defined schema,   you can pass any type that extends pydantic.BaseModel instead of these two options.- **chat** (optional) - The chat instance for interaction with LLMs.  If omitted, `ChatOpenAI(temperature=0, model_name=&quot;gpt-3.5-turbo-0613&quot;)` will be used.- **function_call** (optional) - Specifies whether to use the [function calling API](https://openai.com/blog/function-calling-and-other-api-updates).  If omitted, it is automatically determined based on the underlying model type. - **language** (optional) - Language of explanations. If not specified, it will be automatically inferred from user prompts.- **description_strategy** (optional) - Type of how the information in the dataset is embedded in the prompt.  - `head` - send `df.head(5)` to LLMs.  - `dtypes` - send `df.dtypes` to LLMs. This can be used when you do not want to send contents of `df` to LLMs.- **custom_deserializer** (optional) - Specifies a custom deserializer to convert json returned from the LLM into a chart configuration.- **verbose** (optional) - If `True`, chat2plot will output logs.Once an instance is created, a graph generation request can be made by calling `query` method, or simply passing the same arguments to the instance (`__call__`).```Pythondef query(self, q: str, config_only: bool = False, show_plot: bool = False) -&gt; Plot:```- **q** - A query string.- **config_only** - If `True`, skip generating figure.- **show_plot** - If `True`, showing the generated figure is also performed inside chat2plot.The default behavior is `config_only=False` and `show_plot=False`, i.e. Chat2Plot generates a figure as well as configuration, but does not draw it.The `query` method returns `Plot` object, and this has the following properties:```Python@dataclass(frozen=True)class Plot:    figure: alt.Chart | Figure | None    config: PlotConfig | dict[str, Any] | None    response_type: ResponseType    explanation: str    conversation_history: list[langchain.schema.BaseMessage] | None```- **figure** - The generated figure (chart).- **config** - The chart specification. Returns dict for `vega` mode and `PlotConfig` for `simple` mode.- **response_type** - The result code of the response.- **explanation** - Reason for the chart setup, generated by LLMs.- **conversation_history** - Full history of conversation (including retry prompt).</longdescription>
</pkgmetadata>