<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Torchsort![Tests](https://github.com/teddykoker/torchsort/workflows/Tests/badge.svg)Fast, differentiable sorting and ranking in PyTorch.Pure PyTorch implementation of [Fast Differentiable Sorting andRanking](https://arxiv.org/abs/2002.08871) (Blondel et al.). Much of the code iscopied from the original Numpy implementation at[google-research/fast-soft-sort](https://github.com/google-research/fast-soft-sort),with the isotonic regression solver rewritten as a PyTorch C++ and CUDAextension.## Install```bashpip install torchsort```To build the CUDA extension you will need the CUDA toolchain installed. If youwant to build in an environment without a CUDA runtime (e.g. docker), you willneed to export the environment variable`TORCH_CUDA_ARCH_LIST=&quot;Pascal;Volta;Turing;Ampere&quot;` before installing.&lt;details&gt;&lt;summary&gt;&lt;strong&gt;Conda Installation&lt;/strong&gt;&lt;/summary&gt;On some systems the package my not compile with `pip` install in condaenvironments. If this happens you may need to:     1. Install g++ with `conda install -c conda-forge gxx_linux-64` 2. Set export variable `export CXX=/path/to/miniconda3/envs/env_name/bin/x86_64-conda_cos6-linux-gnu-g++` 3. If still failing, export variable `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/miniconda3/lib`Thanks to @levnikmyskin for pointing this out!&lt;/details&gt;## Usage`torchsort` exposes two functions: `soft_rank` and `soft_sort`, each withparameters `regularization` (`&quot;l2&quot;` or `&quot;kl&quot;`) and `regularization_strength` (ascalar value). Each will rank/sort the last dimension of a 2-d tensor, with anaccuracy dependant upon the regularization strength:```pythonimport torchimport torchsortx = torch.tensor([[8, 0, 5, 3, 2, 1, 6, 7, 9]])torchsort.soft_sort(x, regularization_strength=1.0)# tensor([[0.5556, 1.5556, 2.5556, 3.5556, 4.5556, 5.5556, 6.5556, 7.5556, 8.5556]])torchsort.soft_sort(x, regularization_strength=0.1)# tensor([[-0., 1., 2., 3., 5., 6., 7., 8., 9.]])torchsort.soft_rank(x)# tensor([[8., 1., 5., 4., 3., 2., 6., 7., 9.]])```Both operations are fully differentiable, on CPU or GPU:```pythonx = torch.tensor([[8., 0., 5., 3., 2., 1., 6., 7., 9.]], requires_grad=True).cuda()y = torchsort.soft_sort(x)torch.autograd.grad(y[0, 0], x)# (tensor([[0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111]],#         device='cuda:0'),)```## Example### Spearman's Rank Coefficient[Spearman's rankcoefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)is a very useful metric for measuring how monotonically related two variablesare. We can use Torchsort to create a differentiable Spearman's rank coefficientfunction so that we can optimize a model directly for this metric:```pythonimport torchimport torchsortdef spearmanr(pred, target, **kw):    pred = torchsort.soft_rank(pred, **kw)    target = torchsort.soft_rank(target, **kw)    pred = pred - pred.mean()    pred = pred / pred.norm()    target = target - target.mean()    target = target / target.norm()    return (pred * target).sum()pred = torch.tensor([[1., 2., 3., 4., 5.]], requires_grad=True)target = torch.tensor([[5., 6., 7., 8., 7.]])spearman = spearmanr(pred, target)# tensor(0.8321)torch.autograd.grad(spearman, pred)# (tensor([[-5.5470e-02,  2.9802e-09,  5.5470e-02,  1.1094e-01, -1.1094e-01]]),)```## Benchmark![Benchmark](https://github.com/teddykoker/torchsort/raw/main/extra/benchmark.png)`torchsort` and `fast_soft_sort` each operate with a time complexity of *O(n logn)*, each with some additional overhead when compared to the built-in`torch.sort`. With a batch size of 1 (see left), the Numba JIT'd forward pass of`fast_soft_sort` performs about on-par with the `torchsort` CPU kernel, howeverits backward pass still relies on some Python code, which greatly penalizes itsperformance. Furthermore, the `torchsort` kernel supports batches, and yields much betterperformance than `fast_soft_sort` as the batch size increases.![Benchmark](https://github.com/teddykoker/torchsort/raw/main/extra/benchmark_cuda.png)The `torchsort` CUDA kernel performs quite well with sequence lengths under~2000, and scales to extremely large batch sizes. In the future theCUDA kernel can likely be further optimized to achieve performance closer to that of thebuilt in `torch.sort`.## Reference```bibtex@inproceedings{blondel2020fast,  title={Fast differentiable sorting and ranking},  author={Blondel, Mathieu and Teboul, Olivier and Berthet, Quentin and Djolonga, Josip},  booktitle={International Conference on Machine Learning},  pages={950--959},  year={2020},  organization={PMLR}}```</longdescription>
</pkgmetadata>