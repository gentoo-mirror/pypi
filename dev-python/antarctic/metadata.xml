<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Antarctic[![Release](https://github.com/tschm/antarctic/workflows/Release/badge.svg)](https://github.com/tschm/antarctic/actions/)[![DeepSource](https://deepsource.io/gh/tschm/antarctic.svg/?label=active+issues&amp;show_trend=true&amp;token=Ap44D1XBPLUb19JqC763UIWf)](https://deepsource.io/gh/tschm/antarctic/?ref=repository-badge)Project to persist Pandas data structures in a MongoDB database. ## Installation```pythonpip install antarctic```##  UsageThis project (unless the popular arctic project which I admire) is based on top of [MongoEngine](https://pypi.org/project/mongoengine/).MongoEngine is an ORM for MongoDB. MongoDB stores documents. We introduce a new field and extend the Document class to make Antarctic a convenient choice for storing Pandas (time series) data. ### FieldsWe introduce first a new field --- the PandasField.```pythonfrom mongoengine import Document, connectfrom antarctic.pandas_fields import PandasField# connect with your existing MongoDB (here I am using a popular interface mocking a MongoDB)client = connect(db=&quot;test&quot;, host=&quot;mongomock://localhost&quot;)# Define the blueprint for a portfolio documentclass Portfolio(Document):nav = PandasField()weights = PandasField()prices = PandasField()```The portfolio objects works exactly the way you think it works```pythonp = Portfolio()p.nav = pd.Series(...)p.prices = pd.DataFrame(...)p.save()print(p.nav)print(p.prices)```Behind the scenes we convert the both Series and Frame objects into parquet bytestreams andstore them in a MongoDB database.The format should also be readable by R. #### DocumentsIn most cases we have copies of very similar documents, e.g. we store Portfolios and Symbols rather than just a Portfolio or a Symbol.For this purpose we have developed the abstract `XDocument` class relying on the Document class of MongoEngine.It provides some convenient tools to simplify looping over all or a subset of Documents of the same type, e.g.```pythonfrom antarctic.document import XDocumentfrom antarctic.pandas_fields import PandasFieldclient = connect(db=&quot;test&quot;, host=&quot;mongodb://localhost&quot;)class Symbol(XDocument):price = PandasField()```We define a bunch of symbols and assign a price for each (or some of it):```pythons1 = Symbol(name=&quot;A&quot;, price=pd.Series(...)).save()s2 = Symbol(name=&quot;B&quot;, price=pd.Series(...)).save()# We can access subsets likefor symbol in Symbol.subset(names=[&quot;B&quot;]):print(symbol)# often we need a dictionary of Symbols:Symbol.to_dict(objects=[s1, s2])# Each XDocument also provides a field for reference data:s1.reference[&quot;MyProp1&quot;] = &quot;ABC&quot;s2.reference[&quot;MyProp2&quot;] = &quot;BCD&quot;# You can loop over (subsets) of Symbols and extract reference and/or series dataprint(Symbol.reference_frame(objects=[s1, s2]))print(Symbol.series(series=&quot;price&quot;))print(Symbol.apply(func=lambda x: x.price.mean(), default=np.nan))```The XDocument class is exposing DataFrames both for reference and time series data.There is an `apply` method for using a function on (subset) of documents. ### Database vs. DatastoreStoring json or bytestream representations of Pandas objects is not exactly a database. Appending is rather expensive as one would haveto extract the original Pandas object, append to it and convert the new object back into a json or bytestream representation.Clever sharding can mitigate such effects but at the end of the day you shouldn't update such objects too often. Often practitionersuse a small database for recording (e.g. over the last 24h) and update the MongoDB database once a day. It's extremely fast to read the Pandas objectsout of such a construction.Often such concepts are called DataStores.</longdescription>
</pkgmetadata>