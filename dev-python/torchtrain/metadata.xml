<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># ðŸ”¥ torchtrain ðŸ’ªA small tool for PyTorch training.## Features- Avoid boilerplate code for training.- Stepwise training.- Automatic TensorBoard logging, and tqdm bar.- Count model parameters and save hyperparameters.- DataParallel.- Early stop.- Save and load checkpoint. Continue training.- Catch out of memory exceptions to avoid breaking training.- Gradient accumulation.- Gradient clipping.- Only run few epochs, steps and batches for code test.## Install```pip install torchtrain```## ExampleCheck doc string of [`Trainer` class](https://github.com/idorce/torchtrain/blob/master/torchtrain/trainer.py) for detailed configurations.An incomplete minimal example:```pythondata_iter = get_data()model = Bert()optimizer = Adam(model.parameters(), lr=cfg[&quot;lr&quot;])criteria = {&quot;loss&quot;: AverageAggregator(BCELoss())}trainer = Trainer(model, data_iter, criteria, cfg, optimizer)trainer.train(stepwise=True)```Or this version:```pythonfrom argparse import ArgumentParserfrom sklearn.model_selection import ParameterGridfrom torch.optim import Adamfrom torch.optim.lr_scheduler import LambdaLRfrom transformers import AutoModel, BertTokenizerfrom data.load import get_batch_size, get_datafrom metrics import BCELossfrom models import BertSumExtfrom torchtrain import Trainerfrom torchtrain.metrics import AverageAggregatorfrom torchtrain.utils import set_random_seedsdef get_args():    parser = ArgumentParser()    parser.add_argument(&quot;--seed&quot;, type=int, default=233666)    parser.add_argument(&quot;--run_ckp&quot;, default=&quot;&quot;)    parser.add_argument(&quot;--run_dataset&quot;, default=&quot;val&quot;)    parser.add_argument(&quot;--batch_size&quot;, type=int, default=64)    parser.add_argument(&quot;--warmup&quot;, type=int, default=10000)    parser.add_argument(&quot;--stepwise&quot;, action=&quot;store_false&quot;)    # torchtrain cfgs    parser.add_argument(&quot;--max_n&quot;, type=int, default=50000)    parser.add_argument(&quot;--val_step&quot;, type=int, default=1000)    parser.add_argument(&quot;--save_path&quot;, default=&quot;/tmp/runs&quot;)    parser.add_argument(&quot;--model_name&quot;, default=&quot;BertSumExt&quot;)    parser.add_argument(&quot;--cuda_list&quot;, default=&quot;2,3&quot;)    parser.add_argument(&quot;--grad_accum_batch&quot;, type=int, default=1)    parser.add_argument(&quot;--train_few&quot;, action=&quot;store_true&quot;)    return vars(parser.parse_args())def get_param_grid():    param_grid = [        {&quot;pretrained_model_name&quot;: [&quot;voidful/albert_chinese_tiny&quot;], &quot;lr&quot;: [6e-5]},    ]    return ParameterGrid(param_grid)def get_cfg(args={}, params={}):    cfg = {**args, **params}    # other cfgs    return cfgdef run(cfg):    set_random_seeds(cfg[&quot;seed&quot;])    tokenizer = BertTokenizer.from_pretrained(cfg[&quot;pretrained_model_name&quot;])    bert = AutoModel.from_pretrained(cfg[&quot;pretrained_model_name&quot;])    data_iter = get_data(        cfg[&quot;batch_size&quot;], tokenizer, bert.config.max_position_embeddings    )    model = BertSumExt(bert)    optimizer = Adam(model.parameters(), lr=cfg[&quot;lr&quot;])    scheduler = LambdaLR(        optimizer,        lambda step: min(step ** (-0.5), step * (cfg[&quot;warmup&quot;] ** (-1.5)))        if step &gt; 0        else 0,    )    criteria = {&quot;loss&quot;: AverageAggregator(BCELoss())}    trainer = Trainer(        model,        data_iter,        criteria,        cfg,        optimizer,        scheduler,        get_batch_size=get_batch_size,    )    if cfg[&quot;run_ckp&quot;]:        return trainer.test(cfg[&quot;run_ckp&quot;], cfg[&quot;run_dataset&quot;])    return trainer.train(stepwise=cfg[&quot;stepwise&quot;])def main():    param_grid = get_param_grid()    for i, params in enumerate(param_grid):        print(&quot;Config&quot;, str(i + 1), &quot;/&quot;, str(len(param_grid)))        cfg = get_cfg(get_args(), params)        metrics = run(cfg)        print(&quot;Best metrics:&quot;, metrics)if __name__ == &quot;__main__&quot;:    main()```</longdescription>
</pkgmetadata>