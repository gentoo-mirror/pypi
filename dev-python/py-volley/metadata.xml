<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![VolleyFull-Horizontal](https://user-images.githubusercontent.com/81711984/149005139-f0441dcf-c76e-4112-baf1-998d0a6abdbb.png)Documentation: https://shipt.github.io/py-volley/Volley makes building event stream applications easier and more accessible. Use Volley if you need to quickly develop an application to consume messages, processes them (and do other things), then publish results to one or many places. Volley was inspired ease of use and developer experience provided by the [Flask](https://github.com/pallets/flask) and [FastAPI](https://github.com/tiangolo/fastapi) projects, and aims to make working with queue based and event driven system as accessible as REST APIs.Volley handles a number of operations that need to happen before and after processing a message. Reading the data, serialization, data validation, all need to happen before data reaches your application. If these fail, Volley can route the message to a dead-letter-queue. After processing, Volley again handles data validation, serialization, and the writing/publishing of data to any number of output queues. Finally, upon successfully delivery of that message to the target queue, Volley handles marking it as read or deleting it from the input queue.All of Volley's major operations (connectors, serializers, data validation/model handling) can be extended with plugins, and comes with built in support for queues-like technologies [Apache Kafka](https://kafka.apache.org/) and [RSMQ](https://github.com/mlasevich/PyRSMQ) (Redis Simple Message Queue). There is a plugin built for a Postgres queue in our [examples](./example/plugins/my_plugin.py).[![Build Status](https://drone.shipt.com/api/badges/shipt/py-volley/status.svg?ref=refs/heads/main)](https://drone.shipt.com/shipt/py-volley)[![Coverage](https://sonarqube.shipt.com/api/project_badges/measure?project=shipt_py-volley_AYImTs5MsYUjTdFQ7Awt&amp;metric=coverage&amp;token=squ_e98968a6b1bce0281e001fd0e70e538f6228b47f)](https://sonarqube.shipt.com/dashboard?id=shipt_py-volley_AYImTs5MsYUjTdFQ7Awt)# InstallationRequires Python &gt;= 3.8```bashpip install py-volley[all]```You can also limit the dependencies by:```bashpip install py-volley[kafka]  # Kafka dependenciespip install py-volley[rsmq]  # RSMQ dependenciespip install py-volley[zmq]  # ZeroMQ dependencies```## Features- Built in support for [Apache Kafka](https://kafka.apache.org/), [RSMQ](https://github.com/smrchy/rsmq), [ZeroMQ](https://zeromq.org/)- [Prometheus](https://prometheus.io/) metrics for all operations such as function processing time, and consumption and production count.- Serialization in JSON and [MessagePack](https://msgpack.org/index.html)- Data validation via [Pydantic](https://pydantic-docs.helpmanual.io/)- Optionally configured integration with dead-letter-queues- Extendible connectors (consumer/producers), serializers, model handlers, and model handlers via plugins.## Getting startedVolley handles the process of consuming/producing by providing developers with extendible interfaces and handlers:- connectors - consumer and producer interfaces which define how the application should read messages, write messages, and what actions to take when a message is successfully or fails processing.- serializers - handlers and interface which describe the behavior for reading an byte objects from connectors. For example, Json or MessagePack serializers.- model_handler - handler and interface which works very closely with serializers. Typically used to turn serialized data into a structured Python data model. Pydantic is Volley's most supported data_model and can handler serialization itself.- data_model - When your application receives data from a queue, what schema and object do you expect it in? The data_model is provided by the user. And the `model_handler` describes how to construct your `data_model`.To demonstrate, let's create an application with two worker nodes. One consumes from Kafka, finds the maximum value in a list then publishes it to Redis. The other consumes the message from Redis - if the max value is &gt; 10, it logs to console otherwise it constructs a new list and publishes to the same Kafka topic. ```mermaidflowchart LRA[(Kafka)] --&gt; |consume| B[Worker 1]B --&gt; |publish| C[(Redis)]C --&gt; |consume| D[Worker 2]D --&gt; E{&gt;10}E --&gt; | no | AE --&gt; | yes | F[Log to Console]```You can skip the details and just run `make intro.start`, which runs this example through `./example/intro/docker-compose.yml`1. start Kafka and Redis instance```docker run -d -p 6379:6379 redis:5.0.0docker run -d -p 9092:9092 bashj79/kafka-kraft```2. Configure the queues and data models. Let's put this in `./my_config.py`.```python# ./my_config.pyfrom typing import List, Tuplefrom pydantic import BaseModelfrom volley import Engine, QueueConfig# define the schemas for the first and second worker nodes.class InputMessage(BaseModel):  my_values: List[float]class OutputMessage(BaseModel):  the_max: float# define the configurations for the two queues, one in Kafka and the other in Redis.queue_config = [  QueueConfig(    name=&quot;my-kafka-input&quot;,    value=&quot;my.kafka.topic.name&quot;,    profile=&quot;confluent&quot;,    data_model=InputMessage,    config={      &quot;group.id&quot;: &quot;my.consumer.group&quot;,      &quot;bootstrap.servers&quot;: &quot;localhost:9092&quot;,    }  ),  QueueConfig(    name=&quot;my-redis-output&quot;,    value=&quot;my.redis.output.queue.name&quot;,    profile=&quot;rsmq&quot;,    data_model=OutputMessage,    config={      &quot;host&quot;: &quot;localhost&quot;,      &quot;port&quot;: 6379,    }  )]```3. Build the first worker node - consume from Kafka, find the max value, publish to Redis```python# ./app_0.pyfrom typing import List, Tuplefrom volley import Enginefrom my_config import queue_config, InputMessage, OutputMessage# the first node - reads from kafka and writes to redisapp_0 = Engine(  app_name=&quot;app_0&quot;,  input_queue=&quot;my-kafka-input&quot;,  # one input  output_queues=[&quot;my-redis-output&quot;],  # zero to many outputs  queue_config=queue_config)@app_0.stream_appdef kafka_to_redis(msg: InputMessage) -&gt; List[Tuple[str, OutputMessage]]:  print(f&quot;Received {msg.model_dump_json()}&quot;)  max_val = max(msg.my_values)  out = OutputMessage(the_max=max_val)  print(out)  return [(&quot;my-redis-output&quot;, out)]  # a list of one or many output targets and messagesif __name__ == &quot;__main__&quot;:  kafka_to_redis()```4. Run the first application in a terminal```bashpython app_0.py```5. Build the second worker node - consume from Redis, determine if we log to console or recycle the message as a new list.```python# ./app_1.pyfrom typing import List, Tuple, Unionfrom volley import Enginefrom my_config import OutputMessage, queue_config, InputMessage# the second nodeapp_1 = Engine(  app_name=&quot;app_1&quot;,  input_queue=&quot;my-redis-output&quot;,  output_queues=[&quot;my-kafka-input&quot;],  queue_config=queue_config,  metrics_port=None)@app_1.stream_appdef redis_to_kafka(msg: OutputMessage) -&gt; Union[bool, List[Tuple[str, InputMessage]]]:  print(f&quot;The maximum: {msg.the_max}&quot;)  if msg.the_max &gt; 10:    print(&quot;That's it, we are done!&quot;)    return True  else:    out = InputMessage(my_values=[msg.the_max, msg.the_max+1, msg.the_max+2])    return [(&quot;my-kafka-input&quot;, out)]  # a list of one or many output targets and messagesif __name__ == &quot;__main__&quot;:    redis_to_kafka()```6. Run the second worker node in another terminal```bashpython app_1.py```7. Finally, let's manually publish a message to the input kafka topic:```pythonfrom confluent_kafka import Producerimport jsonproducer = Producer({&quot;bootstrap.servers&quot;: &quot;localhost:9092&quot;})producer.produce(topic=&quot;my.kafka.topic.name&quot;, value=json.dumps({&quot;my_values&quot;:[1,2,3]}))producer.flush(5)```You should see the following in your two terminals__./app_0.py__```Received {&quot;my_values&quot;: [1.0, 2.0, 3.0]}the_max=3.0Received {&quot;my_values&quot;: [3.0, 4.0, 5.0]}the_max=5.0Received {&quot;my_values&quot;: [5.0, 6.0, 7.0]}the_max=7.0Received {&quot;my_values&quot;: [7.0, 8.0, 9.0]}the_max=9.0Received {&quot;my_values&quot;: [9.0, 10.0, 11.0]}the_max=11.0```__./app_1.py__```The maximum: 3.0The maximum: 5.0The maximum: 7.0The maximum: 9.0The maximum: 11.0That's it, we are done!```# Complete exampleClone this repo and `make run.example` to see a complete example of:- consuming a message from a Kafka topic- producing to RSMQ- consuming from RSMQ and publishing to Kafka and Postgres using custom plugin for Postgres.# ContributingSee our [contributing guide](./CONTRIBUTING.md).Thanks goes to great [projects](./ATTRIBUTIONS.md) and these incredible people.&lt;a href=&quot;https://github.com/shipt/py-volley/graphs/contributors&quot;&gt;  &lt;img src=&quot;https://contrib.rocks/image?repo=shipt/py-volley&quot; /&gt;&lt;/a&gt;</longdescription>
</pkgmetadata>