<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># RLax![CI status](https://github.com/deepmind/rlax/workflows/ci/badge.svg)![docs](https://readthedocs.org/projects/rlax/badge/?version=latest)![pypi](https://img.shields.io/pypi/v/rlax)RLax (pronounced &quot;relax&quot;) is a library built on top of JAX that exposesuseful building blocks for implementing reinforcement learning agents. Fulldocumentation can be found at [rlax.readthedocs.io](https://rlax.readthedocs.io/en/latest/index.html).## InstallationYou can install the latest released version of RLax from PyPI via:```shpip install rlax```or you can install the latest development version from GitHub:```shpip install git+https://github.com/deepmind/rlax.git```All RLax code may then be just in time compiled for different hardware(e.g. CPU, GPU, TPU) using `jax.jit`.In order to run the `examples/` you will also need to clone the repo andinstall the additional requirements:[optax](https://github.com/deepmind/optax),[haiku](https://github.com/deepmind/haiku), and[bsuite](https://github.com/deepmind/bsuite).## ContentThe operations and functions provided are not complete algorithms, butimplementations of reinforcement learning specific mathematical operations thatare needed when building fully-functional agents capable of learning:* Values, including both state and action-values;* Values for Non-linear generalizations of the Bellman equations.* Return Distributions, aka distributional value functions;* General Value Functions, for cumulants other than the main reward;* Policies, via policy-gradients in both continuous and discrete action spaces.The library supports both on-policy and off-policy learning (i.e. learning fromdata sampled from a policy different from the agent's policy).See file-level and function-level doc-strings for the documentation of thesefunctions and for references to the papers that introduced and/or used them.## UsageSee `examples/` for examples of using some of the functions in RLax toimplement a few simple reinforcement learning agents, and demonstrate learningon BSuite's version of the Catch environment (a common unit-test foragent development in the reinforcement learning literature):Other examples of JAX reinforcement learning agents using `rlax` can be found in[bsuite](https://github.com/deepmind/bsuite/tree/master/bsuite/baselines).## BackgroundReinforcement learning studies the problem of a learning system (the *agent*),which must learn to interact with the universe it is embedded in (the*environment*).Agent and environment interact on discrete steps. On each step the agent selectsan *action*, and is provided in return a (partial) snapshot of the state of theenvironment (the *observation*), and a scalar feedback signal (the *reward*).The behaviour of the agent is characterized by a probability distribution overactions, conditioned on past observations of the environment (the *policy*). Theagents seeks a policy that, from any given step, maximises the discountedcumulative reward that will be collected from that point onwards (the *return*).Often the agent policy or the environment dynamics itself are stochastic. Inthis case the return is a random variable, and the optimal agent's policy istypically more precisely specified as a policy that maximises the expectation ofthe return (the *value*), under the agent's and environment's stochasticity.## Reinforcement Learning AlgorithmsThere are three prototypical families of reinforcement learning algorithms:1.  those that estimate the value of states and actions, and infer a policy by    *inspection* (e.g. by selecting the action with highest estimated value)2.  those that learn a model of the environment (capable of predicting the    observations and rewards) and infer a policy via *planning*.3.  those that parameterize a policy that can be directly *executed*,In any case, policies, values or models are just functions. In deepreinforcement learning such functions are represented by a neural network.In this setting, it is common to formulate reinforcement learning updates asdifferentiable pseudo-loss functions (analogously to (un-)supervised learning).Under automatic differentiation, the original update rule is recovered.Note however, that in particular, the updates are only valid if the input datais sampled in the correct manner. For example, a policy gradient loss is onlyvalid if the input trajectory is an unbiased sample from the current policy;i.e. the data are on-policy. The library cannot check or enforce suchconstraints. Links to papers describing how each operation is used are howeverprovided in the functions' doc-strings.## Naming Conventions and Developer GuidelinesWe define functions and operations for agents interacting with a single streamof experience. The JAX construct `vmap` can be used to apply these samefunctions to batches (e.g. to support *replay* and *parallel* data generation).Many functions consider policies, actions, rewards, values, in consecutivetimesteps in order to compute their outputs. In this case the suffix `_t` and`tm1` is often to clarify on which step each input was generated, e.g:*   `q_tm1`: the action value in the `source` state of a transition.*   `a_tm1`: the action that was selected in the `source` state.*   `r_t`: the resulting rewards collected in the `destination` state.*   `discount_t`: the `discount` associated with a transition.*   `q_t`: the action values in the `destination` state.Extensive testing is provided for each function. All tests should also verifythe output of `rlax` functions when compiled to XLA using `jax.jit` and whenperforming batch operations using `jax.vmap`.## Citing RLaxRLax is part of the [DeepMind JAX Ecosystem], to cite RLax please usethe [DeepMind JAX Ecosystem citation].[DeepMind JAX Ecosystem]: https://deepmind.com/blog/article/using-jax-to-accelerate-our-research &quot;DeepMind JAX Ecosystem&quot;[DeepMind JAX Ecosystem citation]: https://github.com/deepmind/jax/blob/main/deepmind2020jax.txt &quot;Citation&quot;</longdescription>
</pkgmetadata>