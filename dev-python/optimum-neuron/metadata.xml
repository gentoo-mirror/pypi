<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;!---Copyright 2023 The HuggingFace Team. All rights reserved.Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;# Optimum Neuronü§ó Optimum Neuron is the interface between the ü§ó Transformers library and AWS Accelerators¬†including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks.The list of officially validated models and tasks is available [here](TODO:). Users can try other models and tasks with only few changes.## InstallTo install the latest release of this package:```bashpip install optimum[neuron]```Optimum Neuron is a fast-moving project, and you may want to install it from source:```bashpip install git+https://github.com/huggingface/optimum-neuron.git```&gt; Alternatively, you can install the package without pip as follows:&gt; ```bash&gt; git clone https://github.com/huggingface/optimum-neuron.git&gt; cd optimum-neuron&gt; python setup.py install&gt; ```Last but not least, don't forget to install the requirements for every example:```bashcd &lt;example-folder&gt;pip install -r requirements.txt```## How to use it?### Quick Startü§ó Optimum Neuron was designed with one goal in mind: **to make training and inference straightforward for any ü§ó Transformers user while leveraging the complete power of AWS Accelerators**.#### Transformers InterfaceThere are two main classes one needs to know:- TrainiumArgumentParser: inherits the original [HfArgumentParser](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.HfArgumentParser) in Transformers with additional checks on the argument values to make sure that they will work well with AWS Trainium instances.- [TrainiumTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer): this version trainer takes care of doing the proper checks and changes to the supported models to make them trainable on AWS Trainium instances.The [TrainiumTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer) is very similar to the [ü§ó Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), and adapting a script using the Trainer to make it work with Trainium will mostly consist in simply swapping the Trainer class for the TrainiumTrainer one.That's how most of the [example scripts](https://github.com/huggingface/optimum-neuron/tree/main/examples) were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).```difffrom transformers import TrainingArguments+from optimum.neuron import TrainiumTrainer as Trainertraining_args = TrainingArguments(  # training arguments...)# A lot of code here# Initialize our Trainertrainer = Trainer(    model=model,    args=training_args,  # Original training arguments.    train_dataset=train_dataset if training_args.do_train else None,    eval_dataset=eval_dataset if training_args.do_eval else None,    compute_metrics=compute_metrics,    tokenizer=tokenizer,    data_collator=data_collator,)```### DocumentationCheck out [the documentation of Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index) for more advanced usage.&lt;!---## Validated ModelsThe following model architectures, tasks and device distributions have been validated for ü§ó Optimum Neuron:&lt;div align=&quot;center&quot;&gt;| Architecture     | State | &lt;center&gt;Tasks&lt;/center&gt;                                                                                                                                                                                                                                                                                                                                 || ---------------- | ----- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ || BERT             | ‚úÖ     | &lt;li&gt;[text classification](https://github.com/huggingface/optimum-neuron/tree/main/examples/text-classification)&lt;/li&gt;&lt;li&gt;[question answering](https://github.com/huggingface/optimum-neuron/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-neuron/tree/main/examples/language-modeling)&lt;/li&gt; || RoBERTa          | ‚ùå     | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-neuron/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-neuron/tree/main/examples/language-modeling)&lt;/li&gt;                                                                                                                     || ALBERT           | ‚ùå     | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-neuron/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-neuron/tree/main/examples/language-modeling)&lt;/li&gt;                                                                                                                     || DistilBERT       | ‚ùå     | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-neuron/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-neuron/tree/main/examples/language-modeling)&lt;/li&gt;                                                                                                                     || GPT2             | ‚ùå     | &lt;li&gt;[language modeling](https://github.com/huggingface/optimum-neuron/tree/main/examples/language-modeling)&lt;/li&gt;                                                                                                                                                                                                                                       || T5               | ‚ùå     | &lt;li&gt;[summarization](https://github.com/huggingface/optimum-neuron/tree/main/examples/summarization)&lt;/li&gt;&lt;li&gt;[translation](https://github.com/huggingface/optimum-neuron/tree/main/examples/translation)&lt;/li&gt;                                                                                                                                           || ViT              | ‚ùå     | &lt;li&gt;[image classification](https://github.com/huggingface/optimum-neuron/tree/main/examples/image-classification)&lt;/li&gt;                                                                                                                                                                                                                                 || Swin             | ‚ùå     | &lt;li&gt;[image classification](https://github.com/huggingface/optimum-neuron/tree/main/examples/image-classification)&lt;/li&gt;                                                                                                                                                                                                                                 || Wav2Vec2         | ‚ùå     | &lt;li&gt;[audio classification](https://github.com/huggingface/optimum-neuron/tree/main/examples/audio-classification)&lt;/li&gt;&lt;li&gt;[speech recognition](https://github.com/huggingface/optimum-neuron/tree/main/examples/speech-recognition)&lt;/li&gt;                                                                                                               || Stable Diffusion | ‚ùå     | &lt;li&gt;[text-to-image generation](https://github.com/huggingface/optimum-neuron/tree/main/examples/stable-diffusion)&lt;/li&gt;                                                                                                                                                                                                                                 || CLIP             | ‚ùå     | &lt;li&gt;[contrastive image-text training](https://github.com/huggingface/optimum-neuron/tree/main/examples/contrastive-image-text)&lt;/li&gt;                                                                                                                                                                                                                    |&lt;/div&gt;Other models and tasks supported by the ü§ó Transformers library may also work. You can refer to this [section](https://github.com/huggingface/optimum-neuron#how-to-use-it) for using them with ü§ó Optimum Neuron. Besides, [this page](https://github.com/huggingface/optimum-neuron/tree/main/examples) explains how to modify any [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch) from the ü§ó Transformers library to make it work with ü§ó Optimum Neuron.--&gt;If you find any issue while using those, please open an issue or a pull request.</longdescription>
</pkgmetadata>