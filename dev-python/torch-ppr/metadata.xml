<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;!--&lt;p align=&quot;center&quot;&gt;  &lt;img src=&quot;https://github.com/mberr/torch-ppr/raw/main/docs/source/logo.png&quot; height=&quot;150&quot;&gt;&lt;/p&gt;--&gt;&lt;h1 align=&quot;center&quot;&gt;  torch-ppr&lt;/h1&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://github.com/mberr/torch-ppr/actions?query=workflow%3ATests&quot;&gt;        &lt;img alt=&quot;Tests&quot; src=&quot;https://github.com/mberr/torch-ppr/workflows/Tests/badge.svg&quot; /&gt;    &lt;/a&gt;    &lt;a href=&quot;https://pypi.org/project/torch_ppr&quot;&gt;        &lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/torch_ppr&quot; /&gt;    &lt;/a&gt;    &lt;a href=&quot;https://pypi.org/project/torch_ppr&quot;&gt;        &lt;img alt=&quot;PyPI - Python Version&quot; src=&quot;https://img.shields.io/pypi/pyversions/torch_ppr&quot; /&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/mberr/torch-ppr/blob/main/LICENSE&quot;&gt;        &lt;img alt=&quot;PyPI - License&quot; src=&quot;https://img.shields.io/pypi/l/torch_ppr&quot; /&gt;    &lt;/a&gt;    &lt;a href='https://torch_ppr.readthedocs.io/en/latest/?badge=latest'&gt;        &lt;img src='https://readthedocs.org/projects/torch_ppr/badge/?version=latest' alt='Documentation Status' /&gt;    &lt;/a&gt;    &lt;a href=&quot;https://codecov.io/gh/mberr/torch-ppr/branch/main&quot;&gt;        &lt;img src=&quot;https://codecov.io/gh/mberr/torch-ppr/branch/main/graph/badge.svg&quot; alt=&quot;Codecov status&quot; /&gt;    &lt;/a&gt;      &lt;a href=&quot;https://github.com/cthoyt/cookiecutter-python-package&quot;&gt;        &lt;img alt=&quot;Cookiecutter template from @cthoyt&quot; src=&quot;https://img.shields.io/badge/Cookiecutter-snekpack-blue&quot; /&gt;     &lt;/a&gt;    &lt;a href='https://github.com/psf/black'&gt;        &lt;img src='https://img.shields.io/badge/code%20style-black-000000.svg' alt='Code style: black' /&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/mberr/torch-ppr/blob/main/.github/CODE_OF_CONDUCT.md&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg&quot; alt=&quot;Contributor Covenant&quot;/&gt;    &lt;/a&gt;&lt;/p&gt;This package allows calculating page-rank and personalized page-rank via power iteration with PyTorch,which also supports calculation on GPU (or other accelerators).## üí™ Getting StartedAs a simple example, consider this simple graph with five nodes.&lt;p align=&quot;center&quot;&gt;  &lt;img src=&quot;docs/source/img/small_graph.svg&quot; height=&quot;300&quot;&gt;&lt;/p&gt;Its edge list is given as```python-console&gt;&gt;&gt; import torch&gt;&gt;&gt; edge_index = torch.as_tensor(data=[(0, 1), (1, 2), (1, 3), (2, 4)]).t()```We can use```python-console&gt;&gt;&gt; from torch_ppr import page_rank&gt;&gt;&gt; page_rank(edge_index=edge_index)tensor([0.1269, 0.3694, 0.2486, 0.1269, 0.1281])```to calculate the page rank, i.e., a measure of global importance.We notice that the central node receives the largest importance score,while all other nodes have lower importance. Moreover, the twoindistinguishable nodes `0` and `3` receive the same page rank.We can also calculate *personalized* page rank which measures importancefrom the perspective of a single node.For instance, for node `2`, we have```python-console&gt;&gt;&gt; from torch_ppr import personalized_page_rank&gt;&gt;&gt; personalized_page_rank(edge_index=edge_index, indices=[2])tensor([[0.1103, 0.3484, 0.2922, 0.1103, 0.1388]])```Thus, the most important node is the central node `1`, nodes `0` and `3` receivethe same importance value which is below the value of the direct neighbor `4`.By the virtue of using PyTorch, the code seamlessly works on GPUs, too, andsupports auto-grad differentiation. Moreover, the calculation of personalizedpage rank supports automatic batch size optimization via[`torch_max_mem`](https://github.com/mberr/torch-max-mem).## üöÄ InstallationThe most recent release can be installed from[PyPI](https://pypi.org/project/torch_ppr/) with:```bash$ pip install torch_ppr```The most recent code and data can be installed directly from GitHub with:```bash$ pip install git+https://github.com/mberr/torch-ppr.git```## üëê ContributingContributions, whether filing an issue, making a pull request, or forking, are appreciated. See[CONTRIBUTING.md](https://github.com/mberr/torch-ppr/blob/master/.github/CONTRIBUTING.md) for more information on getting involved.## üëã Attribution### ‚öñÔ∏è LicenseThe code in this package is licensed under the MIT License.&lt;!--### üìñ CitationCitation goes here!--&gt;&lt;!--### üéÅ SupportThis project has been supported by the following organizations (in alphabetical order):- [Harvard Program in Therapeutic Science - Laboratory of Systems Pharmacology](https://hits.harvard.edu/the-program/laboratory-of-systems-pharmacology/)--&gt;&lt;!--### üí∞ FundingThis project has been supported by the following grants:| Funding Body                                             | Program                                                                                                                       | Grant           ||----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|-----------------|| DARPA                                                    | [Automating Scientific Knowledge Extraction (ASKE)](https://www.darpa.mil/program/automating-scientific-knowledge-extraction) | HR00111990009   |--&gt;### üç™ CookiecutterThis package was created with [@audreyfeldroy](https://github.com/audreyfeldroy)'s[cookiecutter](https://github.com/cookiecutter/cookiecutter) package using [@cthoyt](https://github.com/cthoyt)'s[cookiecutter-snekpack](https://github.com/cthoyt/cookiecutter-snekpack) template.## üõ†Ô∏è For Developers&lt;details&gt;  &lt;summary&gt;See developer instructions&lt;/summary&gt;The final section of the README is for if you want to get involved by making a code contribution.### Development InstallationTo install in development mode, use the following:```bash$ git clone git+https://github.com/mberr/torch-ppr.git$ cd torch-ppr$ pip install -e .```### ü•º TestingAfter cloning the repository and installing `tox` with `pip install tox`, the unit tests in the `tests/` folder can berun reproducibly with:```shell$ tox```Additionally, these tests are automatically re-run with each commit in a [GitHub Action](https://github.com/mberr/torch-ppr/actions?query=workflow%3ATests).### üìñ Building the DocumentationThe documentation can be built locally using the following:```shell$ git clone git+https://github.com/mberr/torch-ppr.git$ cd torch-ppr$ tox -e docs$ open docs/build/html/index.html``` The documentation automatically installs the package as well as the `docs`extra specified in the [`setup.cfg`](setup.cfg). `sphinx` pluginslike `texext` can be added there. Additionally, they need to be added to the`extensions` list in [`docs/source/conf.py`](docs/source/conf.py).### üì¶ Making a ReleaseAfter installing the package in development mode and installing`tox` with `pip install tox`, the commands for making a new release are contained within the `finish` environmentin `tox.ini`. Run the following from the shell:```shell$ tox -e finish```This script does the following:1. Uses [Bump2Version](https://github.com/c4urself/bump2version) to switch the version number in the `setup.cfg`,   `src/torch_ppr/version.py`, and [`docs/source/conf.py`](docs/source/conf.py) to not have the `-dev` suffix2. Packages the code in both a tar archive and a wheel using [`build`](https://github.com/pypa/build)3. Uploads to PyPI using [`twine`](https://github.com/pypa/twine). Be sure to have a `.pypirc` file configured to avoid the need for manual input at this   step4. Push to GitHub. You'll need to make a release going with the commit where the version was bumped.5. Bump the version to the next patch. If you made big changes and want to bump the version by minor, you can   use `tox -e bumpversion minor` after.&lt;/details&gt;</longdescription>
</pkgmetadata>