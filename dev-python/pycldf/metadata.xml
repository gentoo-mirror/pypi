<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pycldfA python package to read and write [CLDF](http://cldf.clld.org) datasets.[![Build Status](https://github.com/cldf/pycldf/workflows/tests/badge.svg)](https://github.com/cldf/pycldf/actions?query=workflow%3Atests)[![Documentation Status](https://readthedocs.org/projects/pycldf/badge/?version=latest)](https://pycldf.readthedocs.io/en/latest/?badge=latest)[![PyPI](https://img.shields.io/pypi/v/pycldf.svg)](https://pypi.org/project/pycldf)## InstallInstall `pycldf` from [PyPI](https://pypi.org/project/pycldf):```shellpip install pycldf```## Command line usageInstalling the `pycldf` package will also install a command line interface `cldf`, which provides some sub-commands to manage CLDF datasets.### Dataset discovery`cldf` subcommands support dataset discovery as specified in the [standard](https://github.com/cldf/cldf/blob/master/extensions/discovery.md).So a typical workflow involving a remote dataset could look as follows.Create a local directory to which to download the dataset (ideally including version info):```shell$ mkdir wacl-1.0.0```Validating a dataset from Zenodo will implicitly download it, so running```shell$ cldf validate https://zenodo.org/record/7322688#rdf:ID=wacl --download-dir wacl-1.0.0/```will download the dataset to `wacl-1.0.0`.Subsequently we can access the data locally for better performance:```shell$ cldf stats wacl-1.0.0/#rdf:ID=wacl&lt;cldf:v1.0:StructureDataset at wacl-1.0.0/cldf&gt;                          value------------------------  --------------------------------------------------------------------dc:bibliographicCitation  Her, One-Soon, Harald Hammarström and Marc Allassonnière-Tang. 2022.dc:conformsTo             http://cldf.clld.org/v1.0/terms.rdf#StructureDatasetdc:identifier             https://wacl.clld.orgdc:license                https://creativecommons.org/licenses/by/4.0/dc:source                 sources.bibdc:title                  World Atlas of Classifier Languagesdcat:accessURL            https://github.com/cldf-datasets/waclrdf:ID                    waclrdf:type                  http://www.w3.org/ns/dcat#Distribution                Type              Rows--------------  --------------  ------values.csv      ValueTable        3338parameters.csv  ParameterTable       1languages.csv   LanguageTable     3338codes.csv       CodeTable            2sources.bib     Sources           2000```(Note that locating datasets on Zenodo requires installation of [cldfzenodo](https:pypi.org/project/cldfzenodo).)### Summary statistics```shell$ cldf stats mydataset/Wordlist-metadata.json &lt;cldf:v1.0:Wordlist at mydataset&gt;Path                   Type          Rows---------------------  ----------  ------forms.csv              Form Table       1mydataset/sources.bib  Sources          1```### ValidationArguably the most important functionality of `pycldf` is validating CLDF datasets.By default, data files are read in strict-mode, i.e. invalid rows will result in an exceptionbeing raised. To validate a data file, it can be read in validating-mode.For example the following output is generated```sh$ cldf validate mydataset/forms.csvWARNING forms.csv: duplicate primary key: (u'1',)WARNING forms.csv:4:Source missing source key: Mei2005```when reading the file```ID,Language_ID,Parameter_ID,Value,Segments,Comment,Source1,abcd1234,1277,word,,,Meier2005[3-7]1,stan1295,1277,hand,,,Meier2005[3-7]2,stan1295,1277,hand,,,Mei2005[3-7]```### Extracting human readable metadataThe information in a CLDF metadata file can be converted to [markdown](https://en.wikipedia.org/wiki/Markdown)(a human readable markup language) running```shellcldf markdown PATH/TO/metadata.json```A typical usage of this feature is to create a `README.md` for your dataset(which, when uploaded to e.g. GitHub will be rendered nicely in the browser).### Downloading media listed in a dataset's MediaTableTypically, CLDF datasets only reference media items. The *MediaTable* provides enough information, though,to download and save an item's content. This can be done running```shellcldf downloadmedia PATH/TO/metadata.json PATH/TO/DOWNLOAD/DIR```To minimize bandwidth usage, relevant items can be filtered by passing selection criteria in the form`COLUMN_NAME=SUBSTRING` as optional arguments. E.g. downloading could be limited to audio files passing`Media_Type=audio/` (provided, `Media_Type` is the name of the column with `propertyUrl` http://cldf.clld.org/v1.0/terms.rdf#mediaType)### Converting a CLDF dataset to an SQLite databaseA very useful feature of CSVW in general and CLDF in particular is that itprovides enough metadata for a set of CSV files to load them into a relationaldatabase - including relations between tables. This can be done running the`cldf createdb` command:```shell script$ cldf createdb -husage: cldf createdb [-h] [--infer-primary-keys] DATASET SQLITE_DB_PATHLoad a CLDF dataset into a SQLite DBpositional arguments:  DATASET               Dataset specification (i.e. path to a CLDF metadata                        file or to the data file)  SQLITE_DB_PATH        Path to the SQLite db file```For a specification of the resulting database schema refer to the documentation in[`src/pycldf/db.py`](src/pycldf/db.py).## Python APIFor a detailed documentation of the Python API, refer to the[docs on ReadTheDocs](https://pycldf.readthedocs.io/en/latest/index.html).### Reading CLDFAs an example, we'll read data from [WALS Online, v2020](https://github.com/cldf-datasets/wals/tree/v2020):```python&gt;&gt;&gt; from pycldf import Dataset&gt;&gt;&gt; wals2020 = Dataset.from_metadata('https://raw.githubusercontent.com/cldf-datasets/wals/v2020/cldf/StructureDataset-metadata.json')```For exploratory purposes, accessing a remote dataset over HTTP is fine. But for real analysis, you'd want to downloadthe datasets first and then access them locally, passing a local file path to `Dataset.from_metadata`.Let's look at what we got:```python&gt;&gt;&gt; print(wals2020)&lt;cldf:v1.0:StructureDataset at https://raw.githubusercontent.com/cldf-datasets/wals/v2020/cldf/StructureDataset-metadata.json&gt;&gt;&gt;&gt; for c in wals2020.components:  ...     print(c)...ValueTableParameterTableCodeTableLanguageTableExampleTable```As expected, we got a [StructureDataset](https://github.com/cldf/cldf/tree/master/modules/StructureDataset), and inaddition to the required `ValueTable`, we also have a couple more [components](https://github.com/cldf/cldf#cldf-components).We can investigate the values using [`pycldf`'s ORM](src/pycldf/orm.py) functionality, i.e. mapping rows in the CLDFdata files to convenient python objects. (Take note of the limitations describe in [orm.py](src/pycldf/orm.py), though.)```python&gt;&gt;&gt; for value in wals2020.objects('ValueTable'):  ...     break...&gt;&gt;&gt; value&lt;pycldf.orm.Value id=&quot;81A-aab&quot;&gt;&gt;&gt;&gt; value.language&lt;pycldf.orm.Language id=&quot;aab&quot;&gt;&gt;&gt;&gt; value.language.cldfNamespace(glottocode=None, id='aab', iso639P3code=None, latitude=Decimal('-3.45'), longitude=Decimal('142.95'), macroarea=None, name='Arapesh (Abu)')&gt;&gt;&gt; value.parameter&lt;pycldf.orm.Parameter id=&quot;81A&quot;&gt;&gt;&gt;&gt; value.parameter.cldfNamespace(description=None, id='81A', name='Order of Subject, Object and Verb')&gt;&gt;&gt; value.references(&lt;Reference Nekitel-1985[94]&gt;,)&gt;&gt;&gt; value.references[0]&lt;Reference Nekitel-1985[94]&gt;&gt;&gt;&gt; print(value.references[0].source.bibtex())@misc{Nekitel-1985,    olac_field = {syntax; general_linguistics; typology},    school     = {Australian National University},    title      = {Sociolinguistic Aspects of Abu', a Papuan Language of the Sepik Area, Papua New Guinea},    wals_code  = {aab},    year       = {1985},    author     = {Nekitel, Otto I. M. S.}}```If performance is important, you can just read rows of data as python `dict`s, in which case the references betweentables must be resolved &quot;by hand&quot;:```python&gt;&gt;&gt; params = {r['id']: r for r in wals2020.iter_rows('ParameterTable', 'id', 'name')}&gt;&gt;&gt; for v in wals2020.iter_rows('ValueTable', 'parameterReference'):    ...     print(params[v['parameterReference']]['name'])...     break...Order of Subject, Object and Verb```Note that we passed names of CLDF terms to `Dataset.iter_rows` (e.g. `id`) specifying which columns we want to access by CLDF term - rather than by the column names they are mapped to in the dataset.## Writing CLDF**Warning:** Writing CLDF with `pycldf` does not automatically result in valid CLDF!It does result in data that can be checked via `cldf validate` (see [below](#validation)),though, so you should always validate after writing.```pythonfrom pycldf import Wordlist, Sourcedataset = Wordlist.in_dir('mydataset')dataset.add_sources(Source('book', 'Meier2005', author='Hans Meier', year='2005', title='The Book'))dataset.write(FormTable=[    {        'ID': '1',         'Form': 'word',         'Language_ID': 'abcd1234',         'Parameter_ID': '1277',         'Source': ['Meier2005[3-7]'],    }])```results in```$ ls -1 mydataset/forms.csvsources.bibWordlist-metadata.json```- `mydataset/forms.csv````ID,Language_ID,Parameter_ID,Value,Segments,Comment,Source1,abcd1234,1277,word,,,Meier2005[3-7]```- `mydataset/sources.bib````bibtex@book{Meier2005,    author = {Meier, Hans},    year = {2005},    title = {The Book}}```- `mydataset/Wordlist-metadata.json`### Advanced writingTo add predefined CLDF components to a dataset, use the `add_component` method:```pythonfrom pycldf import StructureDataset, term_uridataset = StructureDataset.in_dir('mydataset')dataset.add_component('ParameterTable')dataset.write(    ValueTable=[{'ID': '1', 'Language_ID': 'abc', 'Parameter_ID': '1', 'Value': 'x'}],ParameterTable=[{'ID': '1', 'Name': 'Grammatical Feature'}])```It is also possible to add generic tables:```pythondataset.add_table('contributors.csv', term_uri('id'), term_uri('name'))```which can also be linked to other tables:```pythondataset.add_columns('ParameterTable', 'Contributor_ID')dataset.add_foreign_key('ParameterTable', 'Contributor_ID', 'contributors.csv', 'ID')```### Addressing tables and columnsTables in a dataset can be referenced using a `Dataset`'s `__getitem__` method,passing- a full CLDF Ontology URI for the corresponding component,- the local name of the component in the CLDF Ontology,- the `url` of the table.Columns in a dataset can be referenced using a `Dataset`'s `__getitem__` method,passing a tuple `(&lt;TABLE&gt;, &lt;COLUMN&gt;)` where `&lt;TABLE&gt;` specifies a table as explainedabove and `&lt;COLUMN&gt;` is- a full CLDF Ontolgy URI used as `propertyUrl` of the column,- the `name` property of the column.See also https://pycldf.readthedocs.io/en/latest/dataset.html#accessing-schema-objects-components-tables-columns-etc## Object oriented access to CLDF dataThe [`pycldf.orm`](src/pycldf/orm.py) module implements functionalityto access CLDF data via an [ORM](https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping).See https://pycldf.readthedocs.io/en/latest/orm.html fordetails.## Accessing CLDF data via SQLThe [`pycldf.db`](src/pycldf/db.py) module implements functionalityto load CLDF data into a [SQLite](https://sqlite.org) database. See https://pycldf.readthedocs.io/en/latest/ext_sql.htmlfor details.## See also- https://github.com/frictionlessdata/datapackage-py</longdescription>
</pkgmetadata>