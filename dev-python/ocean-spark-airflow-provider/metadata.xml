<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Airflow connector to Ocean for Apache SparkAn Airflow plugin and provider to launch and monitor Sparkapplications on [Ocean forApache Spark](https://spot.io/products/ocean-apache-spark/).## Installation```pip install ocean-spark-airflow-provider```## UsageFor general usage of Ocean for Apache Spark, refer to the [officialdocumentation](https://docs.spot.io/ocean-spark/getting-started/?id=get-started-with-ocean-for-apache-spark).### Setting up the connectionIn the connection menu, register a new connection of type **Ocean forApache Spark**. The default connection name is `ocean_spark_default`. You willneed to have: - The Ocean Spark cluster ID of the cluster you just created (of the   format `osc-e4089a00`). You can find it in the Spot console in the   [list of   clusters](https://docs.spot.io/ocean-spark/product-tour/manage-clusters),   or by using the [Cluster   List](https://docs.spot.io/api/#operation/OceanSparkClusterList) API. - [A Spot   token](https://docs.spot.io/administration/api/create-api-token?id=create-an-api-token)   to interact with the Spot API. ![connection setup dialog](./images/connection_setup.png) The **Ocean for Apache Spark** connection type is not available for Airflow1, instead create an **HTTP** connection and fill your cluster id as**host**, and your API token as **password**.You will need to create a separate connection for each Ocean Sparkcluster that you want to use with Airflow.  In the`OceanSparkOperator`, you can select which Ocean Spark connection touse with the `connection_name` argument (defaults to`ocean_spark_default`). For example, you may choose to have one Ocean Spark cluster per environment (dev, staging, prod), and youcan easily target an environment by picking the correct Airflow connection.### Using the Spark operator```pythonfrom ocean_spark.operators import OceanSparkOperator    # DAG creation    spark_pi_task = OceanSparkOperator(    job_id=&quot;spark-pi&quot;,    task_id=&quot;compute-pi&quot;,    dag=dag,    config_overrides={        &quot;type&quot;: &quot;Scala&quot;,        &quot;sparkVersion&quot;: &quot;3.2.0&quot;,        &quot;image&quot;: &quot;gcr.io/datamechanics/spark:platform-3.2-latest&quot;,        &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;,        &quot;mainClass&quot;: &quot;org.apache.spark.examples.SparkPi&quot;,        &quot;mainApplicationFile&quot;: &quot;local:///opt/spark/examples/jars/examples.jar&quot;,        &quot;arguments&quot;: [&quot;10000&quot;],        &quot;driver&quot;: {            &quot;cores&quot;: 1,            &quot;spot&quot;: false        },        &quot;executor&quot;: {            &quot;cores&quot;: 4,            &quot;instances&quot;: 1,            &quot;spot&quot;: true,            &quot;instanceSelector&quot;: &quot;r5&quot;        },    },)```### Using the Spark Connect operator```pythonfrom airflow import DAG, utilsfrom ocean_spark.connect_operator import (    OceanSparkConnectOperator,)args = {    &quot;owner&quot;: &quot;airflow&quot;,    &quot;depends_on_past&quot;: False,    &quot;start_date&quot;: utils.dates.days_ago(0, second=1),}dag = DAG(dag_id=&quot;spark-connect-task&quot;, default_args=args, schedule_interval=None)spark_pi_task = OceanSparkConnectOperator(    task_id=&quot;spark-connect&quot;,    dag=dag,)```### Trigger the DAG with config, such as```json{  &quot;app_id&quot;: &quot;spark-connect-123&quot;,  &quot;sql&quot;: &quot;select random()&quot;}```more examples are available for [Airflow 2](./deploy/airflow2/dags).## Test locallyYou can test the plugin locally using the docker compose setup in thisrepository. Run `make serve_airflow` at the root of the repository tolaunch an instance of Airflow 2 with the provider already installed.</longdescription>
</pkgmetadata>