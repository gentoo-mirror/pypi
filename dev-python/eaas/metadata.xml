<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Evaluation-as-a-Service for NLP&lt;p align=&quot;center&quot;&gt;    &lt;br&gt;    &lt;img src=&quot;docs/Resources/figs/logo.png&quot; width=&quot;400&quot;/&gt;    &lt;br&gt;  &lt;a href=&quot;https://github.com/ExpressAI/eaas_client/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/inspired-co/eaas_client&quot; /&gt;&lt;/a&gt;  &lt;a href=&quot;https://github.com/expressai/eaas_client/stargazers&quot;&gt;&lt;img alt=&quot;GitHub stars&quot; src=&quot;https://img.shields.io/github/stars/inspired-co/eaas_client&quot; /&gt;&lt;/a&gt;  &lt;a href=&quot;https://pypi.org/project//&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/eaas&quot; /&gt;&lt;/a&gt;  &lt;a href=&quot;.github/workflows/ci.yml&quot;&gt;&lt;img alt=&quot;Integration Tests&quot; src=&quot;https://github.com/inspired-co/eaas_client/actions/workflows/ci.yml/badge.svg?event=push&quot; /&gt;&lt;/a&gt;&lt;/p&gt;## UsageBefore using EaaS, please see the [terms of use](TERMS.md).Detailed documentation can be found [here](https://inspired-co.github.io/eaas_client/). To install the EaaS, simply run```bashpip install eaas```## Run your &quot;Hello, world&quot;A minimal EaaS application looks something like this:```pythonfrom eaas import Config, Clientclient = Client(Config())inputs = [{    &quot;source&quot;: &quot;Hello, my world&quot;,    &quot;references&quot;: [&quot;Hello, world&quot;, &quot;Hello my world&quot;],    &quot;hypothesis&quot;: &quot;Hi, my world&quot;}]metrics = [&quot;rouge1&quot;, &quot;bleu&quot;, &quot;chrf&quot;]score_dic = client.score(inputs, metrics=metrics)```If `eaas` has been installed successfully, you should get the resultsbelow by printing `score_dic`. Each entry corresponds to the metrics passedto `metrics` (in the same order). The `corpus` entry indicates the corpus-levelscore, `sample` entry is a list of sample-level scores: ```pythonscore_dic = {'scores':     [         {'corpus': 0.6666666666666666, 'sample': [0.6666666666666666]},         {'corpus': 0.35355339059327373, 'sample': [0.35355339059327373]},         {'corpus': 0.4900623006253688, 'sample': [0.4900623006253688]}     ]}```Notably: * To use this API for scoring, you need to format your input as list of dictionary. * Each dictionary consists of `source` (string, optional), `references` (list of string, optional) and `hypothesis` (string, required). `source` and `references` are optional based on the metrics you want to use. * Please do not conduct any preprocessing on `source`, `references` or `hypothesis`. * We expect normal-cased detokenized texts. All the preprocessing steps are taken by the metrics.  ## Supported MetricsCurrently, EaaS supports the following metrics:* `bart_score_en_ref`: [BARTScore](https://arxiv.org/abs/2106.11520) is a sequence to sequence framework based on pre-trained language model BART.  `bart_score_cnn_hypo_ref` uses the CNNDM finetuned BART. It calculates the average generation score of `Score(hypothesis|reference)` and `Score(reference|hypothesis)`.* `bart_score_en_src`: [BARTScore](https://arxiv.org/abs/2106.11520) using the CNNDM finetuned BART. It calculates `Score(hypothesis|source)`.* `bert_score_p`: [BERTScore](https://arxiv.org/abs/1904.09675) is a metric designed for evaluating translated text using BERT-based matching framework. `bert_score_p` calculates the BERTScore precision.* `bert_score_r`: [BERTScore](https://arxiv.org/abs/1904.09675) recall.* `bert_score_f`: [BERTScore](https://arxiv.org/abs/1904.09675) f score.* `bleu`: [BLEU](https://aclanthology.org/P02-1040.pdf) measures modified ngram matches between each candidate translation and the reference translations. * `chrf`: [CHRF](https://aclanthology.org/W15-3049/) measures the character-level ngram matches between hypothesis and reference.* `comet`: [COMET](https://aclanthology.org/2020.emnlp-main.213/) is a neural framework for training multilingual machine translation evaluation models. `comet` uses the `wmt20-comet-da` checkpoint which utilizes source, hypothesis and reference.* `comet_qe`: [COMET](https://aclanthology.org/2020.emnlp-main.213/) for quality estimation. `comet_qe` uses the `wmt20-comet-qe-da` checkpoint which utilizes only source and hypothesis.* `mover_score`: [MoverScore](https://arxiv.org/abs/1909.02622) is a metric similar to BERTScore. Different from BERTScore, it uses the Earth Moverâ€™s Distance instead of the Euclidean Distance.* `prism`: [PRISM](https://arxiv.org/abs/2004.14564) is a sequence to sequence framework trained from scratch. `prism` calculates the average generation score of `Score(hypothesis|reference)` and `Score(reference|hypothesis)`.* `prism_qe`: [PRISM](https://arxiv.org/abs/2004.14564) for quality estimation. It calculates `Score(hypothesis| source)`.* `rouge1`: [ROUGE-1](https://aclanthology.org/W04-1013/) refers to the overlap of unigram (each word) between the system and reference summaries.* `rouge2`: [ROUGE-2](https://aclanthology.org/W04-1013/) refers to the overlap of bigrams between the system and reference summaries.* `rougeL`: [ROUGE-L](https://aclanthology.org/W04-1013/) refers to the longest common subsequence between the system and reference summaries.The default configurations for each metric can refer to this [doc](docs/default_config.md)## Asynchronous RequestsIf you want to make a call to the EaaS server to calculate some metrics and continuelocal computation while waiting for the result, you can do so as follows:```pythonfrom eaas import Configfrom eaas.async_client import AsyncClientconfig = Config()client = AsyncClient(config)inputs = ...req = client.async_score(inputs, metrics=[&quot;bleu&quot;])# do some other computationresult = req.get_result()```</longdescription>
</pkgmetadata>