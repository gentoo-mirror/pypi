<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;a href=&quot;https://explosion.ai&quot;&gt;&lt;img src=&quot;https://explosion.ai/assets/img/logo.svg&quot; width=&quot;125&quot; height=&quot;125&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;# spaCy + StanfordNLPThis package wraps the [StanfordNLP](https://github.com/stanfordnlp/stanfordnlp)library, so you can use Stanford's models as a [spaCy](https://spacy.io)pipeline. The Stanford models achieved top accuracy in the CoNLL 2017 and 2018shared task, which involves tokenization, part-of-speech tagging, morphologicalanalysis, lemmatization and labelled dependency parsing in 58 languages.[![PyPi](https://img.shields.io/pypi/v/spacy-stanfordnlp.svg?style=flat-square)](https://pypi.python.org/pypi/spacy-stanfordnlp)[![GitHub](https://img.shields.io/github/release/explosion/spacy-stanfordnlp/all.svg?style=flat-square)](https://github.com/explosion/spacy-stanfordnlp)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)Using this wrapper, you'll be able to use the following annotations, computed byyour pretrained `stanfordnlp` model:- Statistical tokenization (reflected in the `Doc` and its tokens)- Lemmatization (`token.lemma` and `token.lemma_`)- Part-of-speech tagging (`token.tag`, `token.tag_`, `token.pos`, `token.pos_`)- Dependency parsing (`token.dep`, `token.dep_`, `token.head`)- Sentence segmentation (`doc.sents`)## Ô∏èÔ∏èÔ∏è‚åõÔ∏è Installation```bashpip install spacy-stanfordnlp```Make sure to also install one of the[pre-trained StanfordNLP models](https://stanfordnlp.github.io/stanfordnlp/installation_download.html). It's recommended to run StanfordNLP on Python 3.6.8+ or Python 3.7.2+.## üìñ Usage &amp; ExamplesThe `StanfordNLPLanguage` class can be initialized with a loaded StanfordNLPpipeline and returns a spaCy [`Language` object](https://spacy.io/api/language),i.e. the `nlp` object you can use to process text and create a[`Doc` object](https://spacy.io/api/doc).```pythonimport stanfordnlpfrom spacy_stanfordnlp import StanfordNLPLanguagesnlp = stanfordnlp.Pipeline(lang=&quot;en&quot;)nlp = StanfordNLPLanguage(snlp)doc = nlp(&quot;Barack Obama was born in Hawaii. He was elected president in 2008.&quot;)for token in doc:    print(token.text, token.lemma_, token.pos_, token.dep_)```If language data for the given language is available in spaCy, the respectivelanguage class will be used as the base for the `nlp` object ‚Äì for example,`English()`. This lets you use spaCy's lexical attributes like `is_stop` or`like_num`. The `nlp` object follows the same API as any other spaCy `Language`class ‚Äì so you can visualize the `Doc` objects with displaCy, add customcomponents to the pipeline, use the rule-based matcher and do pretty muchanything else you'd normally do in spaCy.```python# Access spaCy's lexical attributesprint([token.is_stop for token in doc])print([token.like_num for token in doc])# Visualize dependenciesfrom spacy import displacydisplacy.serve(doc)  # or displacy.render if you're in a Jupyter notebook# Efficient processing with nlp.pipefor doc in nlp.pipe([&quot;Lots of texts&quot;, &quot;Even more texts&quot;, &quot;...&quot;]):    print(doc.text)# Combine with your own custom pipeline componentsdef custom_component(doc):    # Do something to the doc here    return docnlp.add_pipe(custom_component)# Serialize it to a numpy arraynp_array = doc.to_array(['ORTH', 'LEMMA', 'POS'])```### Experimental: Mixing and matching pipeline componentsBy default, the `nlp` object's pipeline will be empty, because all attributesare computed once and set in the custom[`Tokenizer`](spacy_stanfordnlp/language.py). But since it's a regular `nlp`object, you can add your own components to the pipeline.For example, the entity recognizer from one of spaCy's pre-trained models:```pythonimport spacyimport spacy_stanfordnlpimport stanfordnlpsnlp = stanfordnlp.Pipeline(lang=&quot;en&quot;, models_dir=&quot;./models&quot;)nlp = StanfordNLPLanguage(snlp)# Load spaCy's pre-trained en_core_web_sm model, get the entity recognizer and# add it to the StanfordNLP model's pipelinespacy_model = spacy.load(&quot;en_core_web_sm&quot;)ner = spacy_model.get_pipe(&quot;ner&quot;)nlp.add_pipe(ner)doc = nlp(&quot;Barack Obama was born in Hawaii. He was elected president in 2008.&quot;)print([(ent.text, ent.label_) for ent in doc.ents])# [('Barack Obama', 'PERSON'), ('Hawaii', 'GPE'), ('2008', 'DATE')]```You could also add and train[your own custom text classification component](https://spacy.io/usage/training#textcat).### Advanced: serialization and entry pointsThe spaCy `nlp` object created by `StanfordNLPLanguage` exposes its language as`stanfordnlp_xx`.```pythonfrom spacy.util import get_lang_classlang_cls = get_lang_class(&quot;stanfordnlp_en&quot;)```Normally, the above would fail because spaCy doesn't include a language class`stanfordnlp_en`. But because this package exposes a `spacy_languages` entrypoint in its [`setup.py`](setup.py) that points to `StanfordNLPLanguage`, spaCyknows how to initialize it.This means that saving to and loading from disk works:```pythonsnlp = stanfordnlp.Pipeline(lang=&quot;en&quot;)nlp = StanfordNLPLanguage(snlp)nlp.to_disk(&quot;./stanfordnlp-spacy-model&quot;)```Additional arguments on `spacy.load` are automatically passed down to thelanguage class and pipeline components. So when loading the saved model, you canpass in the `snlp` argument:```pythonsnlp = stanfordnlp.Pipeline(lang=&quot;en&quot;)nlp = spacy.load(&quot;./stanfordnlp-spacy-model&quot;, snlp=snlp)```Note that this **will not save any model data by default**. The StanfordNLPmodels are very large, so for now, this package expects that you load themseparately.</longdescription>
</pkgmetadata>