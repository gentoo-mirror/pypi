<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://i.imgur.com/7eR7Pan.png&quot; width=&quot;400&quot;&gt;&lt;br&gt;    Run large language models at home, BitTorrent-style.&lt;br&gt;    Fine-tuning and inference &lt;a href=&quot;https://github.com/bigscience-workshop/petals#benchmarks&quot;&gt;up to 10x faster&lt;/a&gt; than offloading&lt;br&gt;&lt;br&gt;    &lt;a href=&quot;https://pypi.org/project/petals/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/petals.svg?color=green&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;Generate text with distributed [LLaMA 2](https://ai.meta.com/llama/) ([70B](https://huggingface.co/meta-llama/Llama-2-70b-hf), [70B-Chat](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)), [LLaMA-65B](https://github.com/facebookresearch/llama/blob/llama_v1/MODEL_CARD.md), [Guanaco-65B](https://huggingface.co/timdettmers/guanaco-65b) or [BLOOM-176B](https://huggingface.co/bigscience/bloom) and fine‚Äëtune them for your own tasks &amp;mdash; right from your desktop computer or Google Colab:```pythonfrom transformers import AutoTokenizerfrom petals import AutoDistributedModelForCausalLMmodel_name = &quot;enoch/llama-65b-hf&quot;# You can also use &quot;meta-llama/Llama-2-70b-hf&quot;, &quot;meta-llama/Llama-2-70b-chat-hf&quot;,# &quot;bigscience/bloom&quot;, or &quot;bigscience/bloomz&quot;tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoDistributedModelForCausalLM.from_pretrained(model_name)# Embeddings &amp; prompts are on your device, transformer blocks are distributed across the Internetinputs = tokenizer(&quot;A cat sat&quot;, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]outputs = model.generate(inputs, max_new_tokens=5)print(tokenizer.decode(outputs[0]))  # A cat sat on a mat...```&lt;p align=&quot;center&quot;&gt;    üöÄ &amp;nbsp;&lt;b&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing&quot;&gt;Try now in Colab&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;ü¶ô **Want to run LLaMA 2?** Request access to its weights at the ‚ôæÔ∏è [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and ü§ó [Model Hub](https://huggingface.co/meta-llama/Llama-2-70b-hf), then run `huggingface-cli login` in the terminal before loading the model. Or just try it in our [chatbot app](https://chat.petals.dev).üìã **Terms of use.** Make sure you follow the model license (see the ones for [LLaMA 2](https://bit.ly/llama2-license), [LLaMA](https://bit.ly/llama-license) and [BLOOM](https://bit.ly/bloom-license)).üîè **Privacy.** Your data will be processed by other people in the public swarm. Learn more about privacy [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety). For sensitive data, you can set up a [private swarm](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm) among people you trust.### Connect your GPU and increase Petals capacityPetals is a community-run system &amp;mdash; we rely on people sharing their GPUs. You can check out available servers on our [swarm monitor](https://health.petals.dev) and connect your GPU to help serving one of the models!üêç **Linux + Anaconda.** Run these commands:```bashconda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidiapip install git+https://github.com/bigscience-workshop/petalspython -m petals.cli.run_server enoch/llama-65b-hf --adapters timdettmers/guanaco-65b```ü™ü **Windows + WSL.** Follow the guide on our [Wiki](https://github.com/bigscience-workshop/petals/wiki/Run-Petals-server-on-Windows).üêã **Any OS + Docker.** Run our [Docker](https://www.docker.com) image:```bashsudo docker run -p 31330:31330 --ipc host --gpus all --volume petals-cache:/cache --rm learningathome/petals:main \    python -m petals.cli.run_server --port 31330 enoch/llama-65b-hf --adapters timdettmers/guanaco-65b```These commands host a part of LLaMA-65B with optional [Guanaco](https://huggingface.co/timdettmers/guanaco-65b) adapters on your machine. You can also host `meta-llama/Llama-2-70b-hf`, `meta-llama/Llama-2-70b-chat-hf`, `bigscience/bloom`, `bigscience/bloomz`, and other compatible models from ü§ó [Model Hub](https://huggingface.co/models), or [add support](https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals) for new model architectures.ü¶ô **Want to host LLaMA 2?** Request access to its weights at the ‚ôæÔ∏è [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and ü§ó [Model Hub](https://huggingface.co/meta-llama/Llama-2-70b-hf), generate an üîë [access token](https://huggingface.co/settings/tokens), then use this command for `petals.cli.run_server`:```bashpython -m petals.cli.run_server meta-llama/Llama-2-70b-chat-hf --token YOUR_TOKEN_HERE```üí¨ **FAQ.** Check out our [Wiki](https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-server) to learn how to use multple GPUs, restart the server on reboot, etc. If you have any issues or feedback, ping us in [our Discord](https://discord.gg/D9MwApKgWa)!üîí **Security.** Hosting a server does not allow others to run custom code on your computer. Learn more [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety).üèÜ **Thank you!** Once you load and host 10+ blocks, we can show your name or link on the [swarm monitor](https://health.petals.dev) as a way to say thanks. You can specify them with `--public_name YOUR_NAME`.### Check out tutorials, examples, and moreBasic tutorials:- Getting started: [tutorial](https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing)- Prompt-tune LLaMA-65B for text semantic classification: [tutorial](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb)- Prompt-tune BLOOM to create a personified chatbot: [tutorial](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb)Useful tools and advanced guides:- [Chatbot web app](https://chat.petals.dev) (connects to Petals via an HTTP/WebSocket endpoint): [source code](https://github.com/borzunov/chat.petals.dev)- [Monitor](https://health.petals.dev) for the public swarm: [source code](https://github.com/borzunov/health.petals.dev)- Launch your own swarm: [guide](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm)- Run a custom foundation model: [guide](https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals)Learning more:- Frequently asked questions: [FAQ](https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions)- In-depth system description: [paper](https://arxiv.org/abs/2209.01188)## How does it work?- Petals runs large language models like [LLaMA](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) and [BLOOM](https://huggingface.co/bigscience/bloom) **collaboratively** ‚Äî you load a small part of the model, then team up with people serving the other parts to run inference or fine-tuning.- Single-batch inference runs at up to 6 steps/sec for LLaMA 2 (70B) and &amp;approx; 1 step/sec for BLOOM-176B. This is [up to 10x faster](https://github.com/bigscience-workshop/petals#benchmarks) than offloading, enough for [chatbots](https://chat.petals.dev) and other interactive apps. Parallel inference reaches hundreds of tokens/sec.- Beyond classic language model APIs ‚Äî you can employ any fine-tuning and sampling methods, execute custom paths through the model, or see its hidden states. You get the comforts of an API with the flexibility of PyTorch.&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://i.imgur.com/RTYF3yW.png&quot; width=&quot;800&quot;&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    üìö &amp;nbsp;&lt;b&gt;&lt;a href=&quot;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions&quot;&gt;See FAQ&lt;/a&gt;&lt;/b&gt;    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    üìú &amp;nbsp;&lt;b&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.01188.pdf&quot;&gt;Read paper&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;## InstallationHere's how to install Petals with [Anaconda](https://www.anaconda.com/products/distribution) on Linux:```bashconda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidiapip install git+https://github.com/bigscience-workshop/petals```If you don't use Anaconda, you can install PyTorch in [any other way](https://pytorch.org/get-started/locally/). If you want to run models with 8-bit weights, please install PyTorch with CUDA 11.x or newer for compatility with [bitsandbytes](https://github.com/timDettmers/bitsandbytes).See the instructions for macOS and Windows, the full requirements, and troubleshooting advice in our [FAQ](https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-client).## BenchmarksThe benchmarks below are for BLOOM-176B:&lt;table align=&quot;center&quot;&gt;  &lt;tr&gt;    &lt;th colspan=&quot;2&quot;&gt;Network&lt;/th&gt;    &lt;th colspan=&quot;2&quot;&gt;Single-batch inference&lt;br&gt;(steps/s)&lt;/th&gt;    &lt;th colspan=&quot;2&quot;&gt;Parallel forward&lt;br&gt;(tokens/s)&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th rowspan=&quot;2&quot;&gt;Bandwidth&lt;/th&gt;    &lt;th rowspan=&quot;2&quot;&gt;Round-trip&lt;br&gt;latency&lt;/th&gt;    &lt;th colspan=&quot;2&quot;&gt;Sequence length&lt;/th&gt;    &lt;th colspan=&quot;2&quot;&gt;Batch size&lt;/th&gt;  &lt;/tr&gt;  &lt;tr align=&quot;center&quot;&gt;    &lt;td&gt;128&lt;/td&gt;    &lt;td&gt;2048&lt;/td&gt;    &lt;td&gt;1&lt;/td&gt;    &lt;td&gt;64&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;6&quot;&gt;Offloading, max. possible speed on 1x A100 &lt;sup&gt;1&lt;/sup&gt;&lt;/th&gt;  &lt;/tr&gt;  &lt;tr align=&quot;center&quot;&gt;    &lt;td&gt;256 Gbit/s&lt;/td&gt;    &lt;td&gt;&lt;/td&gt;    &lt;td&gt;0.18&lt;/td&gt;    &lt;td&gt;0.18&lt;/td&gt;    &lt;td&gt;2.7&lt;/td&gt;    &lt;td&gt;170.3&lt;/td&gt;  &lt;/tr&gt;  &lt;tr align=&quot;center&quot;&gt;    &lt;td&gt;128 Gbit/s&lt;/td&gt;    &lt;td&gt;&lt;/td&gt;    &lt;td&gt;0.09&lt;/td&gt;    &lt;td&gt;0.09&lt;/td&gt;    &lt;td&gt;2.4&lt;/td&gt;    &lt;td&gt;152.8&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;6&quot;&gt;Petals on 14 heterogeneous servers across Europe and North America &lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt;  &lt;/tr&gt;  &lt;tr align=&quot;center&quot;&gt;    &lt;td colspan=&quot;2&quot;&gt;Real world&lt;/td&gt;    &lt;td&gt;0.83&lt;/td&gt;    &lt;td&gt;0.79&lt;/td&gt;    &lt;td&gt;32.6&lt;/td&gt;    &lt;td&gt;179.4&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;6&quot;&gt;Petals on 3 servers, with one A100 each &lt;sup&gt;3&lt;/sup&gt;&lt;/th&gt;  &lt;/tr&gt;  &lt;tr align=&quot;center&quot;&gt;    &lt;td&gt;1 Gbit/s&lt;/td&gt;    &lt;td&gt;&amp;lt; 5 ms&lt;/td&gt;    &lt;td&gt;1.71&lt;/td&gt;    &lt;td&gt;1.54&lt;/td&gt;    &lt;td&gt;70.0&lt;/td&gt;    &lt;td&gt;253.6&lt;/td&gt;  &lt;/tr&gt;  &lt;tr align=&quot;center&quot;&gt;    &lt;td&gt;100 Mbit/s&lt;/td&gt;    &lt;td&gt;&amp;lt; 5 ms&lt;/td&gt;    &lt;td&gt;1.66&lt;/td&gt;    &lt;td&gt;1.49&lt;/td&gt;    &lt;td&gt;56.4&lt;/td&gt;    &lt;td&gt;182.0&lt;/td&gt;  &lt;/tr&gt;  &lt;tr align=&quot;center&quot;&gt;    &lt;td&gt;100 Mbit/s&lt;/td&gt;    &lt;td&gt;100 ms&lt;/td&gt;    &lt;td&gt;1.23&lt;/td&gt;    &lt;td&gt;1.11&lt;/td&gt;    &lt;td&gt;19.7&lt;/td&gt;    &lt;td&gt;112.2&lt;/td&gt;  &lt;/tr&gt;&lt;/table&gt;&lt;sup&gt;1&lt;/sup&gt; **An upper bound for offloading performance.** We base our offloading numbers on the best possible hardware setup for offloading: CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes per GPU and PCIe switches for pairs of GPUs. We assume zero latency for the upper bound estimation. In 8-bit, the model uses 1 GB of memory per billion parameters. PCIe 4.0 with 16 lanes has a throughput of 256 Gbit/s, so offloading 176B parameters takes 5.5 seconds. The throughput is twice as slow (128 Gbit/s) if we have two GPUs behind the same PCIe switch.&lt;sup&gt;2&lt;/sup&gt; **A real-world distributed setting** with 14 servers holding 2√ó RTX 3060, 4√ó 2080Ti, 2√ó 3090, 2√ó A4000, and 4√ó A5000 GPUs. These are personal servers and servers from university labs, spread across Europe and North America and connected to the Internet at speeds of 100‚Äì1000 Mbit/s. 4 servers operate from under firewalls.&lt;sup&gt;3&lt;/sup&gt; **An optimistic setup** that requires least communication. The client nodes have 8 CPU cores and no GPU.We provide more evaluations and discuss these results in more detail in **Section 3.3** of our [paper](https://arxiv.org/pdf/2209.01188.pdf).## üõ†Ô∏è ContributingPlease see our [FAQ](https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#contributing) on contributing.## üìú CitationAlexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel.[Petals: Collaborative Inference and Fine-tuning of Large Models.](https://arxiv.org/abs/2209.01188)_arXiv preprint arXiv:2209.01188,_ 2022.```bibtex@article{borzunov2022petals,  title = {Petals: Collaborative Inference and Fine-tuning of Large Models},  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Ryabinin, Max and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},  journal = {arXiv preprint arXiv:2209.01188},  year = {2022},  url = {https://arxiv.org/abs/2209.01188}}```--------------------------------------------------------------------------------&lt;p align=&quot;center&quot;&gt;    This project is a part of the &lt;a href=&quot;https://bigscience.huggingface.co/&quot;&gt;BigScience&lt;/a&gt; research workshop.&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://petals.dev/bigscience.png&quot; width=&quot;150&quot;&gt;&lt;/p&gt;</longdescription>
</pkgmetadata>