<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://pl-public-data.s3.amazonaws.com/assets_lightning/fabric_logo.png&quot; width=&quot;400px&quot;&gt;**Fabric is the fast and lightweight way to scale PyTorch models without boilerplate**______________________________________________________________________&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://lightning.ai/&quot;&gt;Website&lt;/a&gt; •  &lt;a href=&quot;https://lightning.ai/docs/fabric/&quot;&gt;Docs&lt;/a&gt; •  &lt;a href=&quot;#getting-started&quot;&gt;Getting started&lt;/a&gt; •  &lt;a href=&quot;#faq&quot;&gt;FAQ&lt;/a&gt; •  &lt;a href=&quot;#asking-for-help&quot;&gt;Help&lt;/a&gt; •  &lt;a href=&quot;https://discord.gg/VptPCZkGNa&quot;&gt;Discord&lt;/a&gt;&lt;/p&gt;[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/lightning_fabric)](https://pypi.org/project/lightning_fabric/)[![PyPI Status](https://badge.fury.io/py/lightning_fabric.svg)](https://badge.fury.io/py/lightning_fabric)[![PyPI Status](https://pepy.tech/badge/lightning_fabric)](https://pepy.tech/project/lightning_fabric)[![Conda](https://img.shields.io/conda/v/conda-forge/lightning_fabric?label=conda&amp;color=success)](https://anaconda.org/conda-forge/lightning_fabric)&lt;/div&gt;## Maximum flexibility, minimum code changesWith just a few code changes, run any PyTorch model on any distributed hardware, no boilerplate!- Easily switch from running on CPU to GPU (Apple Silicon, CUDA, …), TPU, multi-GPU or even multi-node training- Use state-of-the-art distributed training strategies (DDP, FSDP, DeepSpeed) and mixed precision out of the box- All the device logic boilerplate is handled for you- Designed with multi-billion parameter models in mind- Build your own custom Trainer using Fabric primitives for training checkpointing, logging, and more```diff+ import lightning as L  import torch  import torch.nn as nn  from torch.utils.data import DataLoader, Dataset  class PyTorchModel(nn.Module):      ...  class PyTorchDataset(Dataset):      ...+ fabric = L.Fabric(accelerator=&quot;cuda&quot;, devices=8, strategy=&quot;ddp&quot;)+ fabric.launch()- device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu  model = PyTorchModel(...)  optimizer = torch.optim.SGD(model.parameters())+ model, optimizer = fabric.setup(model, optimizer)  dataloader = DataLoader(PyTorchDataset(...), ...)+ dataloader = fabric.setup_dataloaders(dataloader)  model.train()  for epoch in range(num_epochs):      for batch in dataloader:          input, target = batch-         input, target = input.to(device), target.to(device)          optimizer.zero_grad()          output = model(input)          loss = loss_fn(output, target)-         loss.backward()+         fabric.backward(loss)          optimizer.step()          lr_scheduler.step()```______________________________________________________________________# Getting started## Install Lightning&lt;details&gt;&lt;summary&gt;Prerequisites&lt;/summary&gt;&gt; TIP: We strongly recommend creating a virtual environment first.&gt; Don’t know what this is? Follow our [beginner guide here](https://lightning.ai/docs/stable/install/installation.html).- Python 3.8.x or later (3.8.x, 3.9.x, 3.10.x, ...)&lt;/details&gt;```bashpip install -U lightning```## Convert your PyTorch to Fabric1. Create the `Fabric` object at the beginning of your training code.   ```   import Lightning as L   fabric = L.Fabric()   ```1. Call `setup()` on each model and optimizer pair and `setup_dataloaders()` on all your data loaders.   ```   model, optimizer = fabric.setup(model, optimizer)   dataloader = fabric.setup_dataloaders(dataloader)   ```1. Remove all `.to` and `.cuda` calls -&gt; Fabric will take care of it.   ```diff   - model.to(device)   - batch.to(device)   ```1. Replace `loss.backward()` by `fabric.backward(loss)`.   ```diff   - loss.backward()   + fabric.backward(loss)   ```1. Run the script from the terminal with   ```bash   lightning run model path/to/train.py   ```or use the launch() method in a notebook. Learn more about [launching distributed training](https://lightning.ai/docs/fabric/stable/fundamentals/launch.html).______________________________________________________________________# FAQ## When to use Fabric?- **Minimum code changes**- You want to scale your PyTorch model to use multi-GPU or use advanced strategies like DeepSpeed without having to refactor. You don’t care about structuring your code- you just want to scale it as fast as possible.- **Maxmium control**- Write your own training and/or inference logic down to the individual optimizer calls. You aren’t forced to conform to a standardized epoch-based training loop like the one in Lightning Trainer. You can do flexible iteration based training, meta-learning, cross-validation and other types of optimization algorithms without digging into framework internals. This also makes it super easy to adopt Fabric in existing PyTorch projects to speed-up and scale your models without the compromise on large refactors. Just remember: With great power comes a great responsibility.- **Maxmium flexibility**- You want to have full control over your entire training- in Fabric all features are opt-in, and it provides you with a tool box of primitives so you can build your own Trainer.## When to use the [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html)?- You want to have all the engineering boilerplate handled for you - dozens of features like checkpointing, logging and early stopping out of the box. Less hassle, less error prone, easy to try different techniques and features.- You want to have good defaults chosen for you - so you can have a better starting point.- You want your code to be modular, readable and well structured - easy to share between projects and with collaborators.## Can I use Fabric with my LightningModule or Lightning Callback?Yes :) Fabric works with PyTorch LightningModules and Callbacks, so you can choose how to structure your code and reuse existing models and callbacks as you wish. Read more [here](https://lightning.ai/docs/fabric/stable/fundamentals/code_structure.html).&lt;img src=&quot;https://pl-public-data.s3.amazonaws.com/assets_lightning/continuum.png&quot; width=&quot;800px&quot;&gt;______________________________________________________________________# Examples- [GAN](https://github.com/Lightning-AI/lightning/tree/master/examples/fabric/dcgan)- [Meta learning](https://github.com/Lightning-AI/lightning/tree/master/examples/fabric/meta_learning)- [Reinforcement learning](https://github.com/Lightning-AI/lightning/tree/master/examples/fabric/reinforcement_learning)- [K-Fold cross validation](https://github.com/Lightning-AI/lightning/tree/master/examples/fabric/kfold_cv)______________________________________________________________________## Asking for helpIf you have any questions please:1. [Read the docs](https://lightning.ai/docs/fabric).1. [Ask a question in our forum](https://lightning.ai/forums/).1. [Join our discord community](https://discord.com/invite/tfXFetEZxv).</longdescription>
</pkgmetadata>