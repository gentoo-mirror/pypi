<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># *** ATTENTION ***Don't immidiately run `pip install mlregression`. See Section _Installation_.# Machine learning regression (mlregression)Machine Learning Regression (mlregrresion) is an off-the-shelf implementation of the most popular ML methods that automatically takes care of fitting and parameter tuning.Currently, the __fully__ implemented models include:- Ensemble trees (Random forests, XGBoost, LightGBM, GradientBoostingRegressor, ExtraTreesRegressor)- Penalized regression (Ridge, Lasso, ElasticNet, Lars, LassoLars) - Neural nets (Simple neural nets with 1-5 hidden layers, rely activation, and early stopping)_NB!_ When using penalized regressions, consider using the native CV-implementation from scikit-learn for speed, e.g., simply set `estimator=&quot;LassoCV&quot;` similar to Example 1.Scikit-learn regressors (together with `XGBoost` and `LightGBM`) can be estimated by setting the `estimator`-argument equal to the name (string) as in Example 1 (`estimator=&quot;RandomForestRegressor&quot;`).Alternatively, one can provide an instance of an estimator, e.g., `estimator=RandomForestRegressor()`. Again, this is fully automated for most Scikit-learn regressors, but for non-standard methods, one would have to provide a parameter grid as well, e.g., `param_grid={...}`.Please contact the authors below if you find any bugs or have any suggestions for improvement. Thank you!Author: Nicolaj Søndergaard Mühlbach (n.muhlbach at gmail dot com, muhlbach at mit dot edu) ## Code dependenciesThis code has the following dependencies:- Python &gt;=3.6- numpy &gt;=1.19- pandas &gt;=1.3- scikit-learn &gt;=1- scikit-learn-intelex &gt;= 2021.3- daal &gt;= 2021.3- daal4py &gt;= 2021.3- tbb &gt;= 2021.4- xgboost &gt;=1.5- lightgbm &gt;=3.2## InstallationBefore calling `pip install mlregression`, we recommend using `conda` to install the dependencies. In our experience, calling the following command works like a charm:```conda install -c conda-forge numpy&quot;&gt;=1.19&quot; pandas&quot;&gt;=1.3&quot; scikit-learn&quot;&gt;=1&quot; scikit-learn-intelex&quot;&gt;=2021.3&quot; daal&quot;&gt;=2021.3&quot; daal4py&quot;&gt;=2021.3&quot; tbb&quot;&gt;=2021.4&quot; xgboost&quot;&gt;=1.5&quot; lightgbm&quot;&gt;=3.2&quot; --force-reinstall```After this, install `mlregression` by calling `pip install mlregression`.Note that without installing the dependensies, the package will not work. As of now, it does not work when installing the dependensies via `pip install`. The reason is that we are using the Intel® Extension for Scikit-learn to massively speed up computations, but the dependensies are not properly installed via `pip install`.## UsageWe demonstrate the use of __mlregression__ below, using random forests, xgboost, and lightGBM as underlying regressors.```python#------------------------------------------------------------------------------# Libraries#------------------------------------------------------------------------------# Standardfrom sklearn.datasets import make_regressionfrom sklearn.model_selection import train_test_split# This libraryfrom mlregression.mlreg import MLRegressor#------------------------------------------------------------------------------# Data#------------------------------------------------------------------------------# Generate dataX, y = make_regression(n_samples=500,                       n_features=10,                        n_informative=5,                       n_targets=1,                       bias=0.0,                       coef=False,                       random_state=1991)X_train, X_test, y_train, y_test = train_test_split(X, y)#------------------------------------------------------------------------------# Example 1: Prediction#------------------------------------------------------------------------------# Specify any of the following estimators:&quot;&quot;&quot;&quot;LinearRegression&quot;,&quot;RidgeCV&quot;, &quot;LassoCV&quot;, &quot;ElasticNetCV&quot;,&quot;RandomForestRegressor&quot;,&quot;ExtraTreesRegressor&quot;, &quot;GradientBoostingRegressor&quot;,&quot;XGBRegressor&quot;, &quot;LGBMegressor&quot;,&quot;MLPRegressor&quot;,&quot;&quot;&quot;# For instance, pick &quot;RandomForestRegressor&quot;estimator = &quot;RandomForestRegressor&quot;# Note that the 'estimator' may also be an instance of a class, e.g., RandomForestRegressor(), conditional on being imported first, e.g. from sklearn.ensemble import RandomForestRegressor# Instantiate model and choose the number of parametrizations to examine using cross-validation ('max_n_models') and the number of cross-validation folds ('n_cv_folds')mlreg = MLRegressor(estimator=estimator,                    n_cv_folds=5,                    max_n_models=2)# Fitmlreg.fit(X=X_train, y=y_train)# Predicty_hat = mlreg.predict(X=X_test)# Access all the usual attributesmlreg.best_score_mlreg.best_estimator_# Compute the scoremlreg.score(X=X_test,y=y_test)#------------------------------------------------------------------------------# Example 2: Cross-fitting#------------------------------------------------------------------------------# Instantiate model and choose the number of parametrizations to examine using cross-validation ('max_n_models'), the number of cross-validation folds ('n_cv_folds'), AND the number of cross-fitting folds ('n_cf_folds')mlreg = MLRegressor(estimator=estimator,                    n_cv_folds=5,                    max_n_models=2,                    n_cf_folds=2)# Cross fitmlreg.cross_fit(X=X_train, y=y_train)# Extract in-sample that are estimated in an out-of-sample way (e.g., via cross-fitting)y_hat = mlreg.y_pred_cf_# Likewise, extract the residualized outcomes used in e.g., double machine learning. This is \tilde{Y} = Y - E[Y|X=x]y_res = mlreg.y_res_cf_```&lt;!-- ## ExampleWe provide an example script in `demo.py`. --&gt;</longdescription>
</pkgmetadata>