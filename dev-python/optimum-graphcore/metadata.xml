<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![examples](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-examples.yml/badge.svg)](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-examples.yml) [![pipelines](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-pipelines.yml/badge.svg)](https://github.com/huggingface/optimum-graphcore/actions/workflows/test-pipelines.yml)&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;readme_logo.png&quot; /&gt;&lt;/p&gt;# Optimum Graphcoreü§ó Optimum Graphcore is the interface between the ü§ó Transformers library and [Graphcore IPUs](https://www.graphcore.ai/products/ipu).It provides a set of tools enabling model parallelization and loading on IPUs, training, fine-tuning and inference on all the tasks already supported by ü§ó Transformers while being compatible with the ü§ó Hub and every model available on it out of the box.## What is an Intelligence Processing Unit (IPU)?Quote from the Hugging Face [blog post](https://huggingface.co/blog/graphcore#what-is-an-intelligence-processing-unit):&gt;IPUs are the processors that power Graphcore‚Äôs IPU-POD datacenter compute systems. This new type of processor is designed to support the very specific computational requirements of AI and machine learning. Characteristics such as fine-grained parallelism, low precision arithmetic, and the ability to handle sparsity have been built into our silicon.&gt; Instead of adopting a SIMD/SIMT architecture like GPUs, Graphcore‚Äôs IPU uses a massively parallel, MIMD architecture, with ultra-high bandwidth memory placed adjacent to the processor cores, right on the silicon die.&gt; This design delivers high performance and new levels of efficiency, whether running today‚Äôs most popular models, such as BERT and EfficientNet, or exploring next-generation AI applications.## Poplar SDK setupA Poplar SDK environment needs to be enabled to use this library. Please refer to Graphcore's [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) guides.## InstallTo install the latest release of this package:`pip install optimum-graphcore`Optimum Graphcore is a fast-moving project, and you may want to install from source.`pip install git+https://github.com/huggingface/optimum-graphcore.git`### Installing in developer modeIf you are working on the `optimum-graphcore` code then you should use an editable installby cloning and installing `optimum` and `optimum-graphcore`:```git clone https://github.com/huggingface/optimum --branch v1.6.1-releasegit clone https://github.com/huggingface/optimum-graphcorepip install -e optimum -e optimum-graphcore```Now whenever you change the code, you'll be able to run with those changes instantly.## Running the examplesThere are a number of examples provided in the `examples` directory. Each of these contains a README with command lines for running them on IPUs with Optimum Graphcore.Please install the requirements for every example:```cd &lt;example-folder&gt;pip install -r requirements.txt```## How to use Optimum Graphcoreü§ó Optimum Graphcore was designed with one goal in mind: **make training and evaluation straightforward for any ü§ó Transformers user while leveraging the complete power of IPUs.**It requires minimal changes if you are already using ü§ó Transformers.To immediately use a model on a given input (text, image, audio, ...), we support the `pipeline` API:```diff-&gt;&gt;&gt; from transformers import pipeline+&gt;&gt;&gt; from optimum.graphcore import pipeline# Allocate a pipeline for sentiment-analysis-&gt;&gt;&gt; classifier = pipeline('sentiment-analysis', model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)+&gt;&gt;&gt; classifier = pipeline('sentiment-analysis', model=&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;, ipu_config = &quot;Graphcore/distilbert-base-ipu&quot;)&gt;&gt;&gt; classifier('We are very happy to introduce pipeline to the transformers repository.')[{'label': 'POSITIVE', 'score': 0.9996947050094604}]```It is also super easy to use the `Trainer` API:```diff-from transformers import Trainer, TrainingArguments+from optimum.graphcore import IPUConfig, IPUTrainer, IPUTrainingArguments-training_args = TrainingArguments(+training_args = IPUTrainingArguments(     per_device_train_batch_size=4,     learning_rate=1e-4,+    # Any IPUConfig on the Hub or stored locally+    ipu_config_name=&quot;Graphcore/bert-base-ipu&quot;,+)++# Loading the IPUConfig needed by the IPUTrainer to compile and train the model on IPUs+ipu_config = IPUConfig.from_pretrained(+    training_args.ipu_config_name, ) # Initialize our Trainer-trainer = Trainer(+trainer = IPUTrainer(     model=model,+    ipu_config=ipu_config,     args=training_args,     train_dataset=train_dataset if training_args.do_train else None,     ...  # Other arguments```For more information, refer to the full [ü§ó Optimum Graphcore documentation](https://huggingface.co/docs/optimum/graphcore_index).## Supported modelsThe following model architectures and tasks are currently supported by ü§ó Optimum Graphcore:|            | Pre-Training | Masked LM | Causal LM | Seq2Seq LM (Summarization, Translation, etc) | Sequence Classification | Token Classification | Question Answering | Multiple Choice | Image Classification | CTC ||------------|--------------|-----------|-----------|----------------------------------------------|-------------------------|----------------------|--------------------|-----------------|----------------------| ------------ || BART       | ‚úÖ            |           | ‚ùå         | ‚úÖ                                            | ‚úÖ                       |                      | ‚ùå                  |                 |                      |             || BERT       | ‚úÖ            | ‚úÖ         | ‚ùå         |                                              | ‚úÖ                       | ‚úÖ                    | ‚úÖ                  | ‚úÖ               |                      |             || ConvNeXt   | ‚úÖ            |           |           |                                              |                         |                      |                    |                 | ‚úÖ                    |             || DeBERTa    | ‚úÖ            | ‚úÖ         |           |                                              | ‚úÖ                       | ‚úÖ                    | ‚úÖ                  |                 |                      |             || DistilBERT | ‚ùå            | ‚úÖ         |           |                                              | ‚úÖ                       | ‚úÖ                    | ‚úÖ                  | ‚úÖ               |                      |             || GPT-2      | ‚úÖ            |           | ‚úÖ         |                                              | ‚úÖ                       | ‚úÖ                    |                    |                 |                      |             || [GroupBERT](https://arxiv.org/abs/2106.05822)   | ‚úÖ            | ‚úÖ         | ‚ùå         |                                              | ‚úÖ                       | ‚úÖ                    | ‚úÖ                  | ‚úÖ               |                      |             || HuBERT     | ‚ùå            |           |           |                                              | ‚úÖ                       |                      |                    |                 |                      |       ‚úÖ      || LXMERT     | ‚ùå            |           |           |                                              |                         |                      | ‚úÖ                  |                 |                      |             || RoBERTa    | ‚úÖ            | ‚úÖ         | ‚ùå         |                                              | ‚úÖ                       | ‚úÖ                    | ‚úÖ                  | ‚úÖ               |                      |             || T5         | ‚úÖ            |           |           | ‚úÖ                                            |                         |                      |                    |                 |                      |             || ViT        | ‚ùå            |           |           |                                              |                         |                      |                    |                 | ‚úÖ                    |             || Wav2Vec2   | ‚úÖ            |           |           |                                              |                         |                      |                    |                 |                      |      ‚úÖ        || Whisper   |    ‚ùå          |           |           |                    ‚úÖ                           |                          |                      |                    |                 |                      |              |If you find any issue while using those, please open an issue or a pull request.</longdescription>
</pkgmetadata>