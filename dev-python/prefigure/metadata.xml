<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># prefigure&gt; Run-configuration management utils: combines configparser, argparse, and wandb.APICapabilities for archiving run settings and pulling configurations from previous runs.  With just 3 lines of code ðŸ˜Ž : the import, the arg setup, &amp; the wandb push.  Combines argparse, configparser, and wandb.API.  WandB logging is done via `pytorch_lightning`'s [WandBLogger](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html). ## Install:```bashpip install prefigure wandb pytorch_lightning```## Instructions:All your usual command line args (with the exception of `--name` and `--training-dir`) are now to be specified in a `defaults.ini` file -- see `examples/` for an example.  A different `.ini` file can be specified via  `--config-file`.&gt; Versions 0.0.9 and later: A `.gin` can be instead be used for `--config-file`, in which case the sytem *only runs gin and nothing else*.The option `--wandb-config &lt;url&gt;` pulls previous runs' configs off wandb, where `&lt;url&gt; is the url of any one of your runs to override those defaults:e.g. `--wandb-config='https://wandb.ai/drscotthawley/delete-me/runs/1m2gh3o1?workspace=user-drscotthawley'`(i.e., whatever URL you grab from your browser window when looking at an individual run.)  **NOTE: the `--wandb-config` thing can only pull from WandB runs that used prefigure, i.e. that have logged a &quot;wandb config push&quot;.**Any command line args you specify will override any settings from WandB and/or the `.ini` file.The order of precedence is &quot;command line args override WandB, which overrides the .ini file&quot;.### 1st line to addIn your run/training code, add this near the top:```Pythonfrom prefigure import get_all_args, push_wandb_config```### 2nd line to addNear the top of your `main()`, add this:```Pythonargs = get_all_args()```Further down in your code, comment-out (or delete) *all* your command-line arguments (e.g. ArgParse calls). If you want different command-line arguments, then add or change them in defaults.ini.  The 'help' string for these is provided via  comment in the line preceding your variable. See [examples/defaults.ini](https://github.com/drscotthawley/prefigure/blob/main/examples/defaults.ini) for examples.### 3rd line to addand then right after you define the [wandb logger](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html), run```Pythonpush_wandb_config(wandb_logger, args)```### (Optional:) 4th &amp; 5ths line to add: OFC```Pythonfrom prefigure import OFC...ofc = OFC(args)```Starting with `prefigure` v0.0.8, there is an On-the-Fly Control (OFC, [pronounced like](https://getyarn.io/yarn-clip/f9a780c2-0690-4cc5-ba0f-139ef8a637a3) what you say when you realize you forget to set a variable properly). This tracks any changes to arguments made to a separate file (by default `ofc.ini`) andupdates those args dyanmically when changes to that file are made. It can also (optionally) log those changes to WandB (and when they occur); see sample usage below.## Sample usage:Here's a rough outline of some pytorch code. ```Pythonimport torchimport torch.utils.data as datafrom prefigure import get_all_args, push_wandb_config, OFCimport pytorch_lightning as pldef main():    # Config setup. Order of preference will be:    #   1. Default settings are in defaults.ini file or whatever you specify via --config-file    #   2. if --wandb-config is given, pull config from wandb to override defaults    #   3. Any new command-line arguments override whatever was set earlier    args = get_all_args()    ofc = OFC(args)  # optional    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    torch.manual_seed(args.seed)    train_set = SampleDataset([args.training_dir], args)    train_dl = data.DataLoader(train_set, args.batch_size, shuffle=True,                               num_workers=args.num_workers, persistent_workers=True, pin_memory=True)    wandb_logger = pl.loggers.WandbLogger(project=args.name)    # push config to wandb for archiving, but don't push --training-dir value to WandB    push_wandb_config(wandb_logger, args, omit=['training_dir'])     demo_dl = data.DataLoader(train_set, args.num_demos, shuffle=True)    ...        #inside training loop        # OFC usage (optional)        if hasattr(args,'check_ofc_every') and (step &gt; 0) and (step % args.check_ofc_every == 0):            changes_dict = ofc.update()   # check for changes. NOTE: all &quot;args&quot; updated automatically            if {} != changes_dict:        # other things to do with changes: log to wandb                wandb.log({'args/'+k:v for k,v in changes_dict.items()}, step=step)         # For easy drop-in OFC capability, keep using args.XXXX for all variables....)        if (step &gt; 0) and (step % args.checkpoint_every == 0):...             do_stuff()```</longdescription>
</pkgmetadata>