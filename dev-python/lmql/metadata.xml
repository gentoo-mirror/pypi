<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;  &lt;a href=&quot;https://lmql.ai&quot;&gt;    &lt;img src=&quot;https://raw.githubusercontent.com/eth-sri/lmql/web/lmql.svg&quot; alt=&quot;Logo&quot; width=&quot;80&quot; height=&quot;80&quot;&gt;  &lt;/a&gt;  &lt;h3 align=&quot;center&quot;&gt;LMQL&lt;/h3&gt;  &lt;p align=&quot;center&quot;&gt;    A query language for programming (large) language models.    &lt;br /&gt;    &lt;a href=&quot;https://docs.lmql.ai&quot;&gt;&lt;strong&gt;Documentation »&lt;/strong&gt;&lt;/a&gt;    &lt;br /&gt;    &lt;br /&gt;    &lt;a href=&quot;https://lmql.ai&quot;&gt;Explore Examples&lt;/a&gt;    ·    &lt;a href=&quot;https://lmql.ai/playground&quot;&gt;Playground IDE&lt;/a&gt;    ·    &lt;a href=&quot;https://github.com/eth-sri/lmql/issues&quot;&gt;Report Bug&lt;/a&gt;    &lt;br/&gt;    &lt;br/&gt;  &lt;/p&gt;&lt;/div&gt;LMQL is a query language for large language models (LLMs). It facilitates LLM interaction by combining the benefits of natural language prompting with the expressiveness of Python. With only a few lines of LMQL code, users can express advanced, multi-part and tool-augmented LM queries, which then are optimized by the LMQL runtime to run efficiently as part of the LM decoding loop.![lmql-overview](https://user-images.githubusercontent.com/17903049/222918379-84a00b9a-1ef0-45bf-9384-15a20f2874f0.png)&lt;p align=&quot;center&quot;&gt;Example of a simple LMQL program.&lt;/p&gt;## Getting StartedTo install the latest version of LMQL run the following command with Python &gt;=3.10 installed.```pip install lmql```**Local GPU Support:** If you want to run models on a local GPU, make sure to install LMQL in an environment with a GPU-enabled installation of PyTorch &gt;= 1.11 (cf. https://pytorch.org/get-started/locally/).### Running LMQL ProgramsAfter installation, you can launch the LMQL playground IDE with the following command:```lmql playground```&gt; Using the LMQL playground requires an installation of Node.js. If you are in a conda-managed environment you can install node.js via `conda install nodejs=14.20 -c conda-forge`. Otherwise, please see the official Node.js website https://nodejs.org/en/download/ for instructions how to install it on your system.This launches a browser-based playground IDE, including a showcase of many exemplary LMQL programs. If the IDE does not launch automatically, go to `http://localhost:3000`.Alternatively, `lmql run` can be used to execute local `.lmql` files. Note that when using local HuggingFace Transformers models in the Playground IDE or via `lmql run`, you have to first launch an instance of the LMQL Inference API for the corresponding model via the command `lmql serve-model`.### Configuring OpenAI API CredentialsIf you want to use OpenAI models, you have to configure your API credentials. To do so, create a file `api.env` in the active working directory, with the following contents.```openai-org: &lt;org identifier&gt;openai-secret: &lt;api secret&gt;```For system-wide configuration, you can also create an `api.env` file at `$HOME/.lmql/api.env` or at the project root of your LMQL distribution (e.g. `src/` in a development copy).## Setting Up a Development EnvironmentTo setup a `conda` environment for local LMQL development with GPU support, run the following commands:```# prepare conda environmentconda env create -f scripts/conda/requirements.yml -n lmqlconda activate lmql# registers the `lmql` command in the current shellsource scripts/activate-dev.sh```&gt; **Operating System**: The GPU-enabled version of LMQL was tested to work on Ubuntu 22.04 with CUDA 12.0 and Windows 10 via WSL2 and CUDA 11.7. The no-GPU version (see below) was tested to work on Ubuntu 22.04 and macOS 13.2 Ventura or Windows 10 via WSL2.### Development without GPUThis section outlines how to setup an LMQL development environment without local GPU support. Note that LMQL without local GPU support only supports the use of API-integrated models like `openai/text-davinci-003`. Please see the OpenAI API documentation (https://platform.openai.com/docs/models/gpt-3-5) to learn more about the set of available models.To setup a `conda` environment for LMQL with GPU support, run the following commands:```# prepare conda environmentconda env create -f scripts/conda/requirements-no-gpu.yml -n lmql-no-gpuconda activate lmql-no-gpu# registers the `lmql` command in the current shellsource scripts/activate-dev.sh```</longdescription>
</pkgmetadata>