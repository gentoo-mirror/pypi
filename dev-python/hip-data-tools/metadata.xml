<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Link to data-tools in hipages Developer Portal, Component: data-tools](https://backyard.k8s.hipages.com.au/api/badges/entity/default/Component/data-tools/badge/pingback &quot;Link to data-tools in hipages Developer Portal&quot;)](https://backyard.k8s.hipages.com.au/catalog/default/Component/data-tools)[![Entity owner badge, owner: data-platform](https://backyard.k8s.hipages.com.au/api/badges/entity/default/Component/data-tools/badge/owner &quot;Entity owner badge&quot;)](https://backyard.k8s.hipages.com.au/catalog/default/Component/data-tools)# hip-data-toolsÂ© Hipages Group Pty Ltd 2019-2022[![PyPI version](https://badge.fury.io/py/hip-data-tools.svg)](https://pypi.org/project/hip-data-tools/#history) [![CircleCI](https://circleci.com/gh/hipagesgroup/data-tools/tree/master.svg?style=svg)](https://circleci.com/gh/hipagesgroup/data-tools/tree/master)[![Maintainability](https://api.codeclimate.com/v1/badges/bb4c3f9ce84ccec71c76/maintainability)](https://codeclimate.com/repos/5d53b4c199b9430177008586/maintainability)[![Test Coverage](https://api.codeclimate.com/v1/badges/bb4c3f9ce84ccec71c76/test_coverage)](https://codeclimate.com/repos/5d53b4c199b9430177008586/test_coverage)Common Python tools and utilities for data engineering, ETL, Exploration, etc. The package is uploaded to PyPi for easy drop and use in various environmnets, such as (but not limited to):1. Running production workloads2. ML Training in Jupyter like notebooks3. Local machine for dev and exploration ## InstallationInstall from PyPi repo:```bashpip3 install hip-data-tools```Install from source```bashpip3 install .```## MacOS Dependencies```brew install libevbrew install librdkafka```## Connect to aws You will need to instantiate an AWS Connection:```pythonfrom hip_data_tools.aws.common import AwsConnectionManager, AwsConnectionSettings, AwsSecretsManager# to connect using an aws cli profileconn = AwsConnectionManager(AwsConnectionSettings(region=&quot;ap-southeast-2&quot;, secrets_manager=None, profile=&quot;default&quot;))# OR if you want to connect using the standard aws environment variablesconn = AwsConnectionManager(settings=AwsConnectionSettings(region=&quot;ap-southeast-2&quot;, secrets_manager=AwsSecretsManager(), profile=None))# OR if you want custom set of env vars to connectconn = AwsConnectionManager(    settings=AwsConnectionSettings(        region=&quot;ap-southeast-2&quot;,        secrets_manager=AwsSecretsManager(            access_key_id_var=&quot;SOME_CUSTOM_AWS_ACCESS_KEY_ID&quot;,            secret_access_key_var=&quot;SOME_CUSTOM_AWS_SECRET_ACCESS_KEY&quot;,            use_session_token=True,            aws_session_token_var=&quot;SOME_CUSTOM_AWS_SESSION_TOKEN&quot;            ),        profile=None,        )    )```Using this connection to object you can use the aws utilities, for example aws Athena:```pythonfrom hip_data_tools.aws.athena import AthenaUtilau = AthenaUtil(database=&quot;default&quot;, conn=conn, output_bucket=&quot;example&quot;, output_key=&quot;tmp/scratch/&quot;)result = au.run_query(&quot;SELECT * FROM temp limit 10&quot;, return_result=True)print(result)```## Connect to Cassandra ```pythonfrom cassandra.policies import DCAwareRoundRobinPolicyfrom cassandra.cqlengine import columnsfrom cassandra.cqlengine.management import sync_tablefrom cassandra.cqlengine.models import Modelfrom cassandra import ConsistencyLevelload_balancing_policy = DCAwareRoundRobinPolicy(local_dc='AWS_VPC_AP_SOUTHEAST_2')conn = CassandraConnectionManager(    settings = CassandraConnectionSettings(        cluster_ips=[&quot;1.1.1.1&quot;, &quot;2.2.2.2&quot;],        port=9042,        load_balancing_policy=load_balancing_policy,    ),    consistency_level=ConsistencyLevel.LOCAL_QUORUM)conn = CassandraConnectionManager(    CassandraConnectionSettings(        cluster_ips=[&quot;1.1.1.1&quot;, &quot;2.2.2.2&quot;],        port=9042,        load_balancing_policy=load_balancing_policy,        secrets_manager=CassandraSecretsManager(        username_var=&quot;MY_CUSTOM_USERNAME_ENV_VAR&quot;),    ),    consistency_level=ConsistencyLevel.LOCAL_ONE)# For running Cassandra model operationsconn.setup_connection(&quot;dev_space&quot;)class ExampleModel(Model):    example_type    = columns.Integer(primary_key=True)    created_at      = columns.DateTime()    description     = columns.Text(required=False)sync_table(ExampleModel)```## Connect to Google Sheets#### How to connectYou need to go to Google developer console and get credentials. Then the Google sheet need to be shared with client email. GoogleApiConnectionSettings need to be provided with the Google API credentials key json. Then you can access the Google sheet by using the workbook_url and the sheet name.#### How to instantiate Sheet UtilYou can instantiate Sheet Util by providing GoogleSheetConnectionManager, workbook_url and the sheet name.```pythonsheet_util = SheetUtil(    conn_manager=GoogleSheetConnectionManager(self.settings.source_connection_settings),    workbook_url='https://docs.google.com/spreadsheets/d/cKyrzCBLfsQM/edit?usp=sharing',    sheet='Sheet1')```#### How to read a dataframe using SheetUtilYou can get the data in the Google sheet as a Pandas DataFrame using the SheetUtil. We have defined a template for the Google sheet to use with this utility. ![alt text](https://img.techpowerup.org/200311/screen-shot-2020-03-11-at-4-08-25-pm.png)You need to provide the &quot;field_names_row_number&quot; and &quot;field_types_row_number&quot; to call &quot;get_dataframe()&quot; method in SheetUtil.```pythonsheet_data = sheet_util.get_data_frame(                field_names_row_number=8,                field_types_row_number=7,                row_range=&quot;12:20&quot;,                data_start_row_number=9)```You can use load_sheet_to_athena() function to load Google sheet data into an Athena table.```pythonGoogleSheetToAthena(GoogleSheetsToAthenaSettings(        source_workbook_url='https://docs.google.com/spreadsheets/d/cKyrzCBLfsQM/edit?usp=sharing',        source_sheet='spec_example',        source_row_range=None,        source_fields=None,        source_field_names_row_number=5,        source_field_types_row_number=4,        source_data_start_row_number=6,        source_connection_settings=get_google_connection_settings(gcp_conn_id=GCP_CONN_ID),        manual_partition_key_value={&quot;column&quot;: &quot;start_date&quot;, &quot;value&quot;: START_DATE},        target_database=athena_util.database,        target_table_name=TABLE_NAME,        target_s3_bucket=s3_util.bucket,        target_s3_dir=s3_dir,        target_connection_settings=get_aws_connection_settings(aws_conn_id=AWS_CONN_ID),        target_table_ddl_progress=False    )).load_sheet_to_athena()```There is an integration test called &quot;integration_test_should__load_sheet_to_athena__when_using_sheetUtil&quot; to test this functionality. You can simply run it by removing the &quot;integration_&quot; prefix.</longdescription>
</pkgmetadata>