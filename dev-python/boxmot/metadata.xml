<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># BoxMOT: pluggable SOTA tracking modules for object detectors&lt;div align=&quot;center&quot;&gt;  &lt;p&gt;  &lt;img src=&quot;assets/images/track_all_seg_1280_025conf.gif&quot; width=&quot;400&quot;/&gt;  &lt;/p&gt;  &lt;br&gt;  &lt;div&gt;  &lt;a href=&quot;https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/mikel-brostrom/yolov8_tracking/actions/workflows/ci.yml/badge.svg&quot; alt=&quot;CI CPU testing&quot;&gt;&lt;/a&gt;  &lt;a href=&quot;https://pepy.tech/project/boxmot&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/boxmot&quot;&gt;&lt;/a&gt;  &lt;br&gt;  &lt;a href=&quot;https://colab.research.google.com/drive/18nIqkBr68TkK8dHdarxTco6svHUJGggY?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://doi.org/10.5281/zenodo.8132989&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.8132989.svg&quot; alt=&quot;DOI&quot;&gt;&lt;/a&gt;  &lt;/div&gt;&lt;/div&gt;## IntroductionThis repo contains a collections of pluggable state-of-the-art multi-object trackers for object detectors. For the methods using appearance description, both heavy ([CLIPReID](https://arxiv.org/pdf/2211.13977.pdf)) and lightweight state-of-the-art ReID models ([LightMBN](https://arxiv.org/pdf/2101.10774.pdf), [OSNet](https://arxiv.org/pdf/1905.00953.pdf) and more) are downloaded automatically. We provide examples on how to use this package together with popular object detection models such as: [Yolov8](https://github.com/ultralytics), [Yolo-NAS](https://github.com/Deci-AI/super-gradients) and [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX).&lt;div align=&quot;center&quot;&gt;|  Tracker | HOTA↑ | MOTA↑ | IDF1↑ || -------- | ----- | ----- | ----- || [BoTSORT](https://arxiv.org/pdf/2206.14651.pdf)    | 77.8 | 78.9 | 88.9 || [HybridSORT](https://arxiv.org/pdf/2308.00783.pdf) | 77.3 | 77.9 | 88.8 || [OCSORT](https://arxiv.org/pdf/2203.14360.pdf)     | 76.9 | 78.0 | 88.5 || [DeepOCSORT](https://arxiv.org/pdf/2302.11813.pdf) | 75.8 | 74.4 | 86.3 || [ByteTrack](https://arxiv.org/pdf/2110.06864.pdf)  | 75.6 | 74.6 | 86.0 || [StrongSORT](https://arxiv.org/pdf/2202.13514.pdf) |      | | || &lt;img width=200/&gt;                                   | &lt;img width=100/&gt; | &lt;img width=100/&gt; | &lt;img width=100/&gt; |&lt;sub&gt; NOTES: performed on the 10 first frames of each MOT17 sequence. The detector used is ByteTrack's YoloXm, trained on: CrowdHuman, MOT17, Cityperson and ETHZ.&lt;/sub&gt;&lt;/div&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Tutorials&lt;/summary&gt;* [Yolov8 training (link to external repository)](https://docs.ultralytics.com/modes/train/)&amp;nbsp;* [Deep appearance descriptor training (link to external repository)](https://kaiyangzhou.github.io/deep-person-reid/user_guide.html)&amp;nbsp;* [ReID model export to ONNX, OpenVINO, TensorRT and TorchScript](https://github.com/mikel-brostrom/yolo_tracking/wiki/ReID-multi-framework-model-export)&amp;nbsp;* [Evaluation on custom tracking dataset](https://github.com/mikel-brostrom/yolo_tracking/wiki/How-to-evaluate-on-custom-tracking-dataset)&amp;nbsp;* [ReID inference acceleration with Nebullvm](https://colab.research.google.com/drive/1APUZ1ijCiQFBR9xD0gUvFUOC8yOJIvHm?usp=sharing)&amp;nbsp;  &lt;/details&gt;&lt;details&gt;&lt;summary&gt;Experiments&lt;/summary&gt;In inverse chronological order:* [Evaluation of the params evolved for first half of MOT17 on the complete MOT17](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Evaluation-of-the-params-evolved-for-first-half-of-MOT17-on-the-complete-MOT17)* [Segmentation model vs object detetion model on MOT metrics](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Segmentation-model-vs-object-detetion-model-on-MOT-metrics)* [Effect of masking objects before feature extraction](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Masked-detection-crops-vs-regular-detection-crops-for-ReID-feature-extraction)* [conf-thres vs HOTA, MOTA and IDF1](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/conf-thres-vs-MOT-metrics)* [Effect of KF updates ahead for tracks with no associations on MOT17](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-KF-updates-ahead-for-tracks-with-no-associations,-on-MOT17)* [Effect of full images vs 1280 input to StrongSORT on MOT17](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Effect-of-passing-full-image-input-vs-1280-re-scaled-to-StrongSORT-on-MOT17)* [Effect of different OSNet architectures on MOT16](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/OSNet-architecture-performances-on-MOT16)* [Yolov5 StrongSORT vs BoTSORT vs OCSORT](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/StrongSORT-vs-BoTSORT-vs-OCSORT)    * Yolov5 [BoTSORT](https://arxiv.org/abs/2206.14651) branch: https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/tree/botsort* [Yolov5 StrongSORT OSNet vs other trackers MOT17](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-17-evaluation-(private-detector))&amp;nbsp;* [StrongSORT MOT16 ablation study](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/Yolov5DeepSORTwithOSNet-vs-Yolov5StrongSORTwithOSNet-ablation-study-on-MOT16)&amp;nbsp;* [Yolov5 StrongSORT OSNet vs other trackers MOT16 (deprecated)](https://github.com/mikel-brostrom/Yolov5_StrongSORT_OSNet/wiki/MOT-16-evaluation)&amp;nbsp;  &lt;/details&gt;#### News* HybridSORT available (August 2023)* SOTA CLIP-ReID people and vehicle models available (August 2023)## Why using this tracking toolbox?Everything is designed with simplicity and flexibility in mind. We don't hyperfocus on results on a single dataset, we prioritize real-world results. If you don't get good tracking results on your custom dataset with the out-of-the-box tracker configurations, use the `examples/evolve.py` script for tracker hyperparameter tuning.## InstallationStart with [**Python&gt;=3.8**](https://www.python.org/) environment.If you want to run the YOLOv8, YOLO-NAS or YOLOX examples:```git clone https://github.com/mikel-brostrom/yolo_tracking.gitpip install -v -e .```but if you only want to import the tracking modules you can simply:```pip install boxmot```## YOLOv8 | YOLO-NAS | YOLOX examples&lt;details&gt;&lt;summary&gt;Tracking&lt;/summary&gt;&lt;details&gt;&lt;summary&gt;Yolo models&lt;/summary&gt;```bash$ python examples/track.py --yolo-model yolov8n       # bboxes only  python examples/track.py --yolo-model yolo_nas_s    # bboxes only  python examples/track.py --yolo-model yolox_n       # bboxes only                                        yolov8n-seg   # bboxes + segmentation masks                                        yolov8n-pose  # bboxes + pose estimation```  &lt;/details&gt;&lt;details&gt;&lt;summary&gt;Tracking methods&lt;/summary&gt;```bash$ python examples/track.py --tracking-method deepocsort                                             strongsort                                             ocsort                                             bytetrack                                             botsort```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Tracking sources&lt;/summary&gt;Tracking can be run on most video formats```bash$ python examples/track.py --source 0                               # webcam                                    img.jpg                         # image                                    vid.mp4                         # video                                    path/                           # directory                                    path/*.jpg                      # glob                                    'https://youtu.be/Zgi9g1ksQHc'  # YouTube                                    'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Select ReID model&lt;/summary&gt;Some tracking methods combine appearance description and motion in the process of tracking. For those which use appearance, you can choose a ReID model based on your needs from this [ReID model zoo](https://kaiyangzhou.github.io/deep-person-reid/MODEL_ZOO). These model can be further optimized for you needs by the [reid_export.py](https://github.com/mikel-brostrom/yolo_tracking/blob/master/boxmot/deep/reid_export.py) script```bash$ python examples/track.py --source 0 --reid-model lmbn_n_cuhk03_d.pt               # lightweight                                                   osnet_x0_25_market1501.pt                                                   mobilenetv2_x1_4_msmt17.engine                                                   resnet50_msmt17.onnx                                                   osnet_x1_0_msmt17.pt                                                   clip_market1501.pt               # heavy                                                   clip_vehicleid.pt                                                   ...```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Filter tracked classes&lt;/summary&gt;By default the tracker tracks all MS COCO classes.If you want to track a subset of the classes that you model predicts, add their corresponding index after the classes flag,```bashpython examples/track.py --source 0 --yolo-model yolov8s.pt --classes 16 17  # COCO yolov8 model. Track cats and dogs, only```[Here](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) is a list of all the possible objects that a Yolov8 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero&lt;/details&gt;&lt;details&gt;&lt;summary&gt;MOT compliant results&lt;/summary&gt;Can be saved to your experiment folder `runs/track/exp*/` by```bashpython examples/track.py --source ... --save-mot```&lt;/details&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Evaluation&lt;/summary&gt;Evaluate a combination of detector, tracking method and ReID model on standard MOT dataset or you custom one by```bash$ python3 examples/val.py --yolo-model yolo_nas_s.pt --reid-model osnetx1_0_dukemtcereid.pt --tracking-method deepocsort --benchmark MOT16                          --yolo-model yolox_n.pt    --reid-model osnet_ain_x1_0_msmt17.pt  --tracking-method ocsort     --benchmark MOT17                          --yolo-model yolov8s.pt    --reid-model lmbn_n_market.pt          --tracking-method strongsort --benchmark &lt;your-custom-dataset&gt;```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Evolution&lt;/summary&gt;We use a fast and elitist multiobjective genetic algorithm for tracker hyperparameter tuning. By default the objectives are: HOTA, MOTA, IDF1. Run it by```bash$ python examples/evolve.py --tracking-method strongsort --benchmark MOT17 --n-trials 100  # tune strongsort for MOT17                            --tracking-method ocsort     --benchmark &lt;your-custom-dataset&gt; --objective HOTA # tune ocsort for maximizing HOTA on your custom tracking dataset```The set of hyperparameters leading to the best HOTA result are written to the tracker's config file.&lt;/details&gt;## Custom object detection model example&lt;details&gt;&lt;summary&gt;Minimalistic&lt;/summary&gt;```pythonimport cv2import numpy as npfrom pathlib import Pathfrom boxmot import DeepOCSORTtracker = DeepOCSORT(    model_weights=Path('osnet_x0_25_msmt17.pt'), # which ReID model to use    device='cuda:0',    fp16=False,)vid = cv2.VideoCapture(0)while True:    ret, im = vid.read()    # substitute by your object detector, output has to be N X (x, y, x, y, conf, cls)    dets = np.array([[144, 212, 578, 480, 0.82, 0],                    [425, 281, 576, 472, 0.56, 65]])    tracks = tracker.update(dets, im) # --&gt; (x, y, x, y, id, conf, cls, ind)```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Complete&lt;/summary&gt;```pythonimport cv2import numpy as npfrom pathlib import Pathfrom boxmot import DeepOCSORTtracker = DeepOCSORT(    model_weights=Path('osnet_x0_25_msmt17.pt'), # which ReID model to use    device='cuda:0',    fp16=True,)vid = cv2.VideoCapture(0)color = (0, 0, 255)  # BGRthickness = 2fontscale = 0.5while True:    ret, im = vid.read()    # substitute by your object detector, input to tracker has to be N X (x, y, x, y, conf, cls)    dets = np.array([[144, 212, 578, 480, 0.82, 0],                    [425, 281, 576, 472, 0.56, 65]])    tracks = tracker.update(dets, im) # --&gt; (x, y, x, y, id, conf, cls, ind)    xyxys = tracks[:, 0:4].astype('int') # float64 to int    ids = tracks[:, 4].astype('int') # float64 to int    confs = tracks[:, 5]    clss = tracks[:, 6].astype('int') # float64 to int    inds = tracks[:, 7].astype('int') # float64 to int    # in case you have segmentations or poses alongside with your detections you can use    # the ind variable in order to identify which track is associated to each seg or pose by:    # segs = segs[inds]    # poses = poses[inds]    # you can then zip them together: zip(tracks, poses)    # print bboxes with their associated id, cls and conf    if tracks.shape[0] != 0:        for xyxy, id, conf, cls in zip(xyxys, ids, confs, clss):            im = cv2.rectangle(                im,                (xyxy[0], xyxy[1]),                (xyxy[2], xyxy[3]),                color,                thickness            )            cv2.putText(                im,                f'id: {id}, conf: {conf}, c: {cls}',                (xyxy[0], xyxy[1]-10),                cv2.FONT_HERSHEY_SIMPLEX,                fontscale,                color,                thickness            )    # show image with bboxes, ids, classes and confidences    cv2.imshow('frame', im)    # break on pressing q    if cv2.waitKey(1) &amp; 0xFF == ord('q'):        breakvid.release()cv2.destroyAllWindows()```&lt;/details&gt;## ContactFor Yolo tracking bugs and feature requests please visit [GitHub Issues](https://github.com/mikel-brostrom/yolo_tracking/issues).For business inquiries or professional support requests please send an email to: yolov5.deepsort.pytorch@gmail.com</longdescription>
</pkgmetadata>