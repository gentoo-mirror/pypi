<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Keras Transformer[![Version](https://img.shields.io/pypi/v/keras-transformer.svg)](https://pypi.org/project/keras-transformer/)![License](https://img.shields.io/pypi/l/keras-transformer.svg) \[[‰∏≠Êñá](https://github.com/CyberZHG/keras-transformer/blob/master/README.zh-CN.md)|[English](https://github.com/CyberZHG/keras-transformer/blob/master/README.md)\]Implementation of [transformer](https://arxiv.org/pdf/1706.03762.pdf) for seq2seq tasks.## Install```bashpip install keras-transformer```## Usage### Train```pythonimport numpy as npfrom keras_transformer import get_model# Build a small toy token dictionarytokens = 'all work and no play makes jack a dull boy'.split(' ')token_dict = {    '&lt;PAD&gt;': 0,    '&lt;START&gt;': 1,    '&lt;END&gt;': 2,}for token in tokens:    if token not in token_dict:        token_dict[token] = len(token_dict)# Generate toy dataencoder_inputs_no_padding = []encoder_inputs, decoder_inputs, decoder_outputs = [], [], []for i in range(1, len(tokens) - 1):    encode_tokens, decode_tokens = tokens[:i], tokens[i:]    encode_tokens = ['&lt;START&gt;'] + encode_tokens + ['&lt;END&gt;'] + ['&lt;PAD&gt;'] * (len(tokens) - len(encode_tokens))    output_tokens = decode_tokens + ['&lt;END&gt;', '&lt;PAD&gt;'] + ['&lt;PAD&gt;'] * (len(tokens) - len(decode_tokens))    decode_tokens = ['&lt;START&gt;'] + decode_tokens + ['&lt;END&gt;'] + ['&lt;PAD&gt;'] * (len(tokens) - len(decode_tokens))    encode_tokens = list(map(lambda x: token_dict[x], encode_tokens))    decode_tokens = list(map(lambda x: token_dict[x], decode_tokens))    output_tokens = list(map(lambda x: [token_dict[x]], output_tokens))    encoder_inputs_no_padding.append(encode_tokens[:i + 2])    encoder_inputs.append(encode_tokens)    decoder_inputs.append(decode_tokens)    decoder_outputs.append(output_tokens)# Build the modelmodel = get_model(    token_num=len(token_dict),    embed_dim=30,    encoder_num=3,    decoder_num=2,    head_num=3,    hidden_dim=120,    attention_activation='relu',    feed_forward_activation='relu',    dropout_rate=0.05,    embed_weights=np.random.random((13, 30)),)model.compile(    optimizer='adam',    loss='sparse_categorical_crossentropy',)model.summary()# Train the modelmodel.fit(    x=[np.asarray(encoder_inputs * 1000), np.asarray(decoder_inputs * 1000)],    y=np.asarray(decoder_outputs * 1000),    epochs=5,)```### Predict```pythonfrom keras_transformer import decodedecoded = decode(    model,    encoder_inputs_no_padding,    start_token=token_dict['&lt;START&gt;'],    end_token=token_dict['&lt;END&gt;'],    pad_token=token_dict['&lt;PAD&gt;'],    max_len=100,)token_dict_rev = {v: k for k, v in token_dict.items()}for i in range(len(decoded)):    print(' '.join(map(lambda x: token_dict_rev[x], decoded[i][1:-1])))```### Translation```pythonimport numpy as npfrom keras_transformer import get_model, decodesource_tokens = [    'i need more power'.split(' '),    'eat jujube and pill'.split(' '),]target_tokens = [    list('ÊàëË¶ÅÊõ¥Â§öÁöÑÊäõÁì¶'),    list('ÂêÉÊû£üíä'),]# Generate dictionariesdef build_token_dict(token_list):    token_dict = {        '&lt;PAD&gt;': 0,        '&lt;START&gt;': 1,        '&lt;END&gt;': 2,    }    for tokens in token_list:        for token in tokens:            if token not in token_dict:                token_dict[token] = len(token_dict)    return token_dictsource_token_dict = build_token_dict(source_tokens)target_token_dict = build_token_dict(target_tokens)target_token_dict_inv = {v: k for k, v in target_token_dict.items()}# Add special tokensencode_tokens = [['&lt;START&gt;'] + tokens + ['&lt;END&gt;'] for tokens in source_tokens]decode_tokens = [['&lt;START&gt;'] + tokens + ['&lt;END&gt;'] for tokens in target_tokens]output_tokens = [tokens + ['&lt;END&gt;', '&lt;PAD&gt;'] for tokens in target_tokens]# Paddingsource_max_len = max(map(len, encode_tokens))target_max_len = max(map(len, decode_tokens))encode_tokens = [tokens + ['&lt;PAD&gt;'] * (source_max_len - len(tokens)) for tokens in encode_tokens]decode_tokens = [tokens + ['&lt;PAD&gt;'] * (target_max_len - len(tokens)) for tokens in decode_tokens]output_tokens = [tokens + ['&lt;PAD&gt;'] * (target_max_len - len(tokens)) for tokens in output_tokens]encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens]decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]# Build &amp; fit modelmodel = get_model(    token_num=max(len(source_token_dict), len(target_token_dict)),    embed_dim=32,    encoder_num=2,    decoder_num=2,    head_num=4,    hidden_dim=128,    dropout_rate=0.05,    use_same_embed=False,  # Use different embeddings for different languages)model.compile('adam', 'sparse_categorical_crossentropy')model.summary()model.fit(    x=[np.array(encode_input * 1024), np.array(decode_input * 1024)],    y=np.array(decode_output * 1024),    epochs=10,    batch_size=32,)# Predictdecoded = decode(    model,    encode_input,    start_token=target_token_dict['&lt;START&gt;'],    end_token=target_token_dict['&lt;END&gt;'],    pad_token=target_token_dict['&lt;PAD&gt;'],)print(''.join(map(lambda x: target_token_dict_inv[x], decoded[0][1:-1])))print(''.join(map(lambda x: target_token_dict_inv[x], decoded[1][1:-1])))```### DecodeIn `decode`, the word with top probability is selected as the predicted token by default. You can add randomness by setting `top_k` and `temperature`:```pythondecoded = decode(    model,    encode_input,    start_token=target_token_dict['&lt;START&gt;'],    end_token=target_token_dict['&lt;END&gt;'],    pad_token=target_token_dict['&lt;PAD&gt;'],    top_k=10,    temperature=1.0,)print(''.join(map(lambda x: target_token_dict_inv[x], decoded[0][1:-1])))print(''.join(map(lambda x: target_token_dict_inv[x], decoded[1][1:-1])))```</longdescription>
</pkgmetadata>