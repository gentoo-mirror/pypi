<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;h1&gt;pytest-codspeed&lt;/h1&gt;[![CI](https://github.com/CodSpeedHQ/pytest-codspeed/actions/workflows/ci.yml/badge.svg)](https://github.com/CodSpeedHQ/pytest-codspeed/actions/workflows/ci.yml)&lt;a href=&quot;https://pypi.org/project/pytest-codspeed&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/pytest-codspeed?color=%2334D058&amp;label=pypi&quot; alt=&quot;Package version&quot;&gt;&lt;/a&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.7%20|%203.8%20|%203.9%20|%203.10%20|%203.11%20|%203.12-informational.svg&quot; alt=&quot;python-3.7-3.8-3.9-3.10-3.11-3.12&quot;&gt;[![Discord](https://img.shields.io/badge/chat%20on-discord-7289da.svg)](https://discord.com/invite/MxpaCfKSqF)Pytest plugin to create CodSpeed benchmarks&lt;/div&gt;## Requirements**Python**: 3.7 and later**pytest**: any recent version## Installation```shellpip install pytest-codspeed```## Usage### Creating benchmarksCreating benchmarks with `pytest-codspeed` is compatible with the standard `pytest-benchmark` API. So if you already have benchmarks written with it, you can start using `pytest-codspeed` right away.#### Marking a whole test function as a benchmark with `pytest.mark.benchmark````pythonimport pytestfrom statistics import median@pytest.mark.benchmarkdef test_median_performance():    return median([1, 2, 3, 4, 5])```#### Benchmarking selected lines of a test function with the `benchmark` fixture```pythonimport pytestfrom statistics import meandef test_mean_performance(benchmark):    # Precompute some data useful for the benchmark but that should not be    # included in the benchmark time    data = [1, 2, 3, 4, 5]    # Benchmark the execution of the function    benchmark(lambda: mean(data))def test_mean_and_median_performance(benchmark):    # Precompute some data useful for the benchmark but that should not be    # included in the benchmark time    data = [1, 2, 3, 4, 5]    # Benchmark the execution of the function:    # The `@benchmark` decorator will automatically call the function and    # measure its execution    @benchmark    def bench():        mean(data)        median(data)```### Running benchmarks#### Testing the benchmarks locallyIf you want to run only the benchmarks tests locally, you can use the `--codspeed` pytest flag:```shellpytest tests/ --codspeed```&gt; **Note:** Running `pytest-codspeed` locally will not produce any performance reporting. It's only useful for making sure that your benchmarks are working as expected. If you want to get performance reporting, you should run the benchmarks in your CI.#### In your CIYou can use the [CodSpeedHQ/action](https://github.com/CodSpeedHQ/action) to run the benchmarks in Github Actions and upload the results to CodSpeed.Example workflow:```yamlname: benchmarkson:  push:    branches:      - &quot;main&quot; # or &quot;master&quot;  pull_request:jobs:  benchmarks:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v3      - uses: actions/setup-python@v3        with:          python-version: &quot;3.9&quot;      - name: Install dependencies        run: pip install -r requirements.txt      - name: Run benchmarks        uses: CodSpeedHQ/action@v1        with:          token: ${{ secrets.CODSPEED_TOKEN }}          run: pytest tests/ --codspeed```</longdescription>
</pkgmetadata>