<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># BunkaTopicsBunkaTopics is a Topic Modeling package that leverages Embeddings and focuses on Topic Representation to extract meaningful and interpretable topics from a list of documents.## InstallationBefore installing bunkatopics, please install the following packages:Load the spacy language models```bashpython -m spacy download fr_core_news_lg``````bashpython -m spacy download en_core_web_sm```Eventually, install bunkatopic using pip```bashpip install bunkatopics```## Quick Start with BunkaTopics```pythonfrom bunkatopics import BunkaTopicsimport pandas as pddata = pd.read_csv('data/imdb.csv', index_col = [0])data = data.sample(2000, random_state = 42)# Instantiate the model, extract ther terms and Embed the documentsmodel = BunkaTopics(data, # dataFrame                    text_var = 'description', # Text Columns                    index_var = 'imdb',  # Index Column (Mandatory)                    extract_terms=True, # extract Terms ?                    terms_embeddings=True, # extract terms Embeddings?                    docs_embeddings=True, # extract Docs Embeddings?                    embeddings_model=&quot;distiluse-base-multilingual-cased-v1&quot;, # Chose an embeddings Model                    multiprocessing=True, # Multiprocessing of Embeddings                    language=&quot;en&quot;, # Chose between English &quot;en&quot; and French &quot;fr&quot;                    sample_size_terms = len(data),                    terms_limit=10000, # Top Terms to Output                    terms_ents=True, # Extract entities                    terms_ngrams=(1, 2), # Chose Ngrams to extract                    terms_ncs=True, # Extract Noun Chunks                    terms_include_pos=[&quot;NOUN&quot;, &quot;PROPN&quot;, &quot;ADJ&quot;], # Include Part-of-Speech                    terms_include_types=[&quot;PERSON&quot;, &quot;ORG&quot;]) # Include Entity Types# Extract the topicstopics = model.get_clusters(topic_number= 15, # Number of Topics                    top_terms_included = 1000, # Compute the specific terms from the top n terms                    top_terms = 5, # Most specific Terms to describe the topics                    term_type = &quot;lemma&quot;, # Use &quot;lemma&quot; of &quot;text&quot;                    ngrams = [1, 2], # N-grams for Topic Representation                    clusterer = 'hdbscan') # Chose between Kmeans and HDBSCAN# Visualize the clusters. It is adviced to choose less that 5 terms - top_terms = 5 - to avoid overchanging the Figurefig = model.visualize_clusters(search = None, width=1000, height=1000, fit_clusters=True,  # Fit Umap to well visually separate clustersdensity_plot=False) # Plot a density map to get a territory overviewfig.show()centroid_documents = model.get_centroid_documents(top_elements=2)```</longdescription>
</pkgmetadata>