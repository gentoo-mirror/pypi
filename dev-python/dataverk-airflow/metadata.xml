<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Dataverk airflowEnkelt wrapperbibliotek rundt [KubernetesPodOperator](https://airflow.apache.org/docs/stable/kubernetes.html) som lager Airflow task som kjører i en Kubernetes pod.## Våre operatorsAlle våre operators lar deg klone et repo på forhånd, bare legg det til med `repo=&quot;navikt/&lt;repo&gt;`.Vi har også støtte for å installere Python pakker ved oppstart av Airflow task, spesifiser `requirements.txt`-filen din med `requirements_path=&quot;/path/to/requirements.txt&quot;`.### Quarto operatorDenne kjører Quarto render for deg.Man finner Quarto-token for ditt teamet i [Datamarkedsplassen](https://data.intern.nav.no/user/tokens). I eksempelt under lagrere vi tokenet i en Airflow variable som så brukes i DAG tasken under.Se offisiell [Airflow dokumentasjon](https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html) for hvordan man bruker `Variable.get()´ i en task.```pythonfrom airflow import DAGfrom airflow.utils.dates import days_agofrom airflow.models import Variablefrom dataverk_airflow import quarto_operatorwith DAG('navn-dag', start_date=days_ago(1), schedule_interval=&quot;*/10 * * * *&quot;) as dag:    t1 = quarto_operator(dag=dag,                         name=&quot;&lt;navn-på-task&gt;&quot;,                         repo=&quot;navikt/&lt;repo&gt;&quot;,                         quarto={                             &quot;path&quot;: &quot;/path/to/index.qmd&quot;,                             &quot;env&quot;: &quot;dev/prod&quot;,                             &quot;id&quot;:&quot;uuid&quot;,                             &quot;token&quot;: Variable.get(&quot;quarto_token&quot;),                         },                         slack_channel=&quot;&lt;#slack-alarm-kanal&gt;&quot;)```### Notebook operatorDenne lar deg kjøre en Jupyter notebook.```pythonfrom airflow import DAGfrom airflow.utils.dates import days_agofrom dataverk_airflow import notebook_operatorwith DAG('navn-dag', start_date=days_ago(1), schedule_interval=&quot;*/10 * * * *&quot;) as dag:    t1 = notebook_operator(dag=dag,                           name=&quot;&lt;navn-på-task&gt;&quot;,                           repo=&quot;navikt/&lt;repo&gt;&quot;,                           nb_path=&quot;/path/to/notebook.ipynb&quot;,                           slack_channel=&quot;&lt;#slack-alarm-kanal&gt;&quot;)```### Python operatorDenne lar deg kjøre vilkårlig Python-scripts.```pythonfrom airflow import DAGfrom airflow.utils.dates import days_agofrom dataverk_airflow import python_operatorwith DAG('navn-dag', start_date=days_ago(1), schedule_interval=&quot;*/10 * * * *&quot;) as dag:    t1 = python_operator(dag=dag,                         name=&quot;&lt;navn-på-task&gt;&quot;,                         repo=&quot;navikt/&lt;repo&gt;&quot;,                         script_path=&quot;/path/to/script.py&quot;,                         slack_channel=&quot;&lt;#slack-alarm-kanal&gt;&quot;)```### Kubernetes operatorVi tilbyr også vår egen Kubernetes operator som kloner et valg repo inn i containeren.```pythonfrom airflow import DAGfrom airflow.utils.dates import days_agofrom dataverk_airflow import kubernetes_operatorwith DAG('navn-dag', start_date=days_ago(1), schedule_interval=&quot;*/10 * * * *&quot;) as dag:    t1 = kubernetes_operator(dag=dag,                             name=&quot;&lt;navn-på-task&gt;&quot;,                             repo=&quot;navikt/&lt;repo&gt;&quot;,                             cmds=[&quot;/path/to/bin/&quot;, &quot;script-name.sh&quot;, &quot;argument1&quot;, &quot;argument2&quot;],                             image=&quot;europe-north1-docker.pkg.dev/nais-management-233d/ditt-team/ditt-image:din-tag&quot;,                             slack_channel=&quot;&lt;#slack-alarm-kanal&gt;&quot;)```## Sette resource requirementsVi har støtte for å sette `requests` og `limits` for hver operator.Merk at man ikke trenger å sette `limits` på CPU da dette blir automatisk løst av plattformen.Ved å bruke `ephemeral-storage` kan man be om ekstra diskplass for lagring i en task.```pythonfrom airflow import DAGfrom airflow.utils.dates import days_agofrom dataverk_airflow import python_operatorwith DAG('navn-dag', start_date=days_ago(1), schedule_interval=&quot;*/10 * * * *&quot;) as dag:    t1 = python_operator(dag=dag,                         name=&quot;&lt;navn-på-task&gt;&quot;,                         repo=&quot;navikt/&lt;repo&gt;&quot;,                         script_path=&quot;/path/to/script.py&quot;,                         resources={                             &quot;requests&quot;: {                                 &quot;memory&quot;: &quot;50Mi&quot;,                                 &quot;cpu&quot;: &quot;100m&quot;,                                 &quot;ephemeral-storage&quot;: &quot;1Gi&quot;                             },                             &quot;limits&quot;: {                                 &quot;memory&quot;: &quot;100Mi&quot;                             }                         })```</longdescription>
</pkgmetadata>