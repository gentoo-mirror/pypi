<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pydbrDatabricks client SDK for Python with command line interface for Databricks REST APIs.{:toc}## IntroductionPydbr (short of Python-Databricks) package provides python SDK for Databricks REST API:* dbfs* workspace* jobs* runsThe package also comes with a useful CLI which might be very helpful in automation.## Installation```bash$ pip install pydbr```## Databricks CLIDatabricks command line client provides convenient way to interact with Databricks cluster at the command line. A very popular use of such approach in in automation tasks, like DevOps pipelines or third party workflow managers.You can call the Databricks CLI using convenient shell command `pydbr`:```bash$ pydbr --help``` or using python module:```bash$ python -m pydbr.cli --help```To connect to the Databricks cluster, you can supply arguments at the command line:* `--bearer-token`* `--url`* `--cluster-id`Alternatively, you can define environment variables. Command line arguments take precedence.```bashexport DATABRICKS_URL='https://westeurope.azuredatabricks.net/'export DATABRICKS_BEARER_TOKEN='dapixyz89u9ufsdfd0'export DATABRICKS_CLUSTER_ID='1234-456778-abc234'export DATABRICKS_ORG_ID='87287878293983984'```### DBFS#### List DBFS items```bash# List items on DBFSpydbr dbfs ls --json-indent 3 FileStore/movielens``````bash[   {      &quot;path&quot;: &quot;/FileStore/movielens/ml-latest-small&quot;,      &quot;is_dir&quot;: true,      &quot;file_size&quot;: 0,      &quot;is_file&quot;: false,      &quot;human_size&quot;: &quot;0 B&quot;   }]```#### Download file from DBFS```bash# Download a file and print to STDOUTpydbr dbfs get ml-latest-small/movies.csv```#### Download directory from DBFS```bash# Download recursively entire directory and store locallypydbr dbfs get -o ml-local ml-latest-small```### WorkspaceDatabricks workspace contains notebooks and other items.#### List workspace```bash##################### List workspace# Default path is root - '/'$ pydbr workspace ls# auto-add leading '/'$ pydbr workspace ls 'Users'# Space-indentend json output with number of spaces$ pydbr workspace --json-indent 4 ls# Custom indent string$ pydbr workspace ls --json-indent='&gt;'```#### Export items from Databricks workspace```bash###################### Export workspace items# Export everything in source format using defaults: format=SOURCE, path=/pydbr workspace export -o ./.dev/export# Export everything in DBC formatpydbr workspace export -f DBC -o ./.dev/export.# When path is folder, export is recursivepydbr workspace export -o ./.dev/export-utils 'Utils'# Export single ITEMpydbr workspace export -o ./.dev/GetML 'Utils/Download MovieLens.py'```### RunsThis command group implements the [`jobs/runs` Databricks REST API](https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-submit).#### Submit a notebookImplements: [https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-submit](https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-submit)```bash$ pydbr runs submit &quot;Utils/Download MovieLens&quot;``````{&quot;run_id&quot;: 4}```You can retrieve the job information using `runs get`:```bash$ pydbr runs get 4 -i 3```If you need to pass parameters, use the `--parameters` or `-p` option and specify JSON text.```bash$ pydbr runs submit -p '{&quot;run_tag&quot;:&quot;20250103&quot;}' &quot;Utils/Download MovieLens&quot;```You can refer also to parameters in JSON file:```bash$ pydbr runs submit -p '@params.json' &quot;Utils/Download MovieLens&quot;```You can use the parameters in the notebook and will also be able to see them in the run metadata:```bashpydbr runs get-output -i 3 8``````json{   &quot;notebook_output&quot;: {      &quot;result&quot;: &quot;Downloaded files (tag: 20250103): README.txt, links.csv, movies.csv, ratings.csv, tags.csv&quot;,      &quot;truncated&quot;: false   },   &quot;error&quot;: null,   &quot;metadata&quot;: {      &quot;job_id&quot;: 8,      &quot;run_id&quot;: 8,      &quot;creator_user_name&quot;: &quot;your.name@gmail.com&quot;,      &quot;number_in_job&quot;: 1,      &quot;original_attempt_run_id&quot;: null,      &quot;state&quot;: {         &quot;life_cycle_state&quot;: &quot;TERMINATED&quot;,         &quot;result_state&quot;: &quot;SUCCESS&quot;,         &quot;state_message&quot;: &quot;&quot;      },      &quot;schedule&quot;: null,      &quot;task&quot;: {         &quot;notebook_task&quot;: {            &quot;notebook_path&quot;: &quot;/Utils/Download MovieLens&quot;,            &quot;base_parameters&quot;: {               &quot;run_tag&quot;: &quot;20250103&quot;            }         }      },      &quot;cluster_spec&quot;: {         &quot;existing_cluster_id&quot;: &quot;xxxx-yyyyyy-zzzzzz&quot;      },      &quot;cluster_instance&quot;: {         &quot;cluster_id&quot;: &quot;xxxx-yyyyyy-zzzzzzzz&quot;,         &quot;spark_context_id&quot;: &quot;8734983498349834&quot;      },      &quot;overriding_parameters&quot;: null,      &quot;start_time&quot;: 1592067357734,      &quot;setup_duration&quot;: 0,      &quot;execution_duration&quot;: 11000,      &quot;cleanup_duration&quot;: 0,      &quot;trigger&quot;: null,      &quot;run_name&quot;: &quot;pydbr-1592067355&quot;,      &quot;run_page_url&quot;: &quot;https://westeurope.azuredatabricks.net/?o=89349849834#job/8/run/1&quot;,      &quot;run_type&quot;: &quot;SUBMIT_RUN&quot;   }}```#### Get run metadataImplements: [Databricks REST runs/get](https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-get) ```bash$ pydbr runs get -i 3 6``````json{   &quot;job_id&quot;: 6,   &quot;run_id&quot;: 6,   &quot;creator_user_name&quot;: &quot;your.name@gmail.com&quot;,   &quot;number_in_job&quot;: 1,   &quot;original_attempt_run_id&quot;: null,   &quot;state&quot;: {      &quot;life_cycle_state&quot;: &quot;TERMINATED&quot;,      &quot;result_state&quot;: &quot;SUCCESS&quot;,      &quot;state_message&quot;: &quot;&quot;   },   &quot;schedule&quot;: null,   &quot;task&quot;: {      &quot;notebook_task&quot;: {         &quot;notebook_path&quot;: &quot;/Utils/Download MovieLens&quot;      }   },   &quot;cluster_spec&quot;: {      &quot;existing_cluster_id&quot;: &quot;xxxx-yyyyy-zzzzzz&quot;   },   &quot;cluster_instance&quot;: {      &quot;cluster_id&quot;: &quot;xxxx-yyyyy-zzzzzz&quot;,      &quot;spark_context_id&quot;: &quot;783487348734873873&quot;   },   &quot;overriding_parameters&quot;: null,   &quot;start_time&quot;: 1592062497162,   &quot;setup_duration&quot;: 0,   &quot;execution_duration&quot;: 11000,   &quot;cleanup_duration&quot;: 0,   &quot;trigger&quot;: null,   &quot;run_name&quot;: &quot;pydbr-1592062494&quot;,   &quot;run_page_url&quot;: &quot;https://westeurope.azuredatabricks.net/?o=398348734873487#job/6/run/1&quot;,   &quot;run_type&quot;: &quot;SUBMIT_RUN&quot;}```#### List RunsImplements: [Databricks REST runs/list](https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-list)```bash$ pydbr runs ls```To get only the runs for a particular job:```bash# Get job with job-id=4$ pydbr runs ls 4 -i 3``````json{   &quot;runs&quot;: [      {         &quot;job_id&quot;: 4,         &quot;run_id&quot;: 4,         &quot;creator_user_name&quot;: &quot;your.name@gmail.com&quot;,         &quot;number_in_job&quot;: 1,         &quot;original_attempt_run_id&quot;: null,         &quot;state&quot;: {            &quot;life_cycle_state&quot;: &quot;PENDING&quot;,            &quot;state_message&quot;: &quot;&quot;         },         &quot;schedule&quot;: null,         &quot;task&quot;: {            &quot;notebook_task&quot;: {               &quot;notebook_path&quot;: &quot;/Utils/Download MovieLens&quot;            }         },         &quot;cluster_spec&quot;: {            &quot;existing_cluster_id&quot;: &quot;xxxxx-yyyy-zzzzzzz&quot;         },         &quot;cluster_instance&quot;: {            &quot;cluster_id&quot;: &quot;xxxxx-yyyy-zzzzzzz&quot;         },         &quot;overriding_parameters&quot;: null,         &quot;start_time&quot;: 1592058826123,         &quot;setup_duration&quot;: 0,         &quot;execution_duration&quot;: 0,         &quot;cleanup_duration&quot;: 0,         &quot;trigger&quot;: null,         &quot;run_name&quot;: &quot;pydbr-1592058823&quot;,         &quot;run_page_url&quot;: &quot;https://westeurope.azuredatabricks.net/?o=abcdefghasdf#job/4/run/1&quot;,         &quot;run_type&quot;: &quot;SUBMIT_RUN&quot;      }   ],   &quot;has_more&quot;: false}```#### Export run Implements: [Databricks REST runs/export](https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-export)```bash$ pydbr runs export --content-only 4 &gt; .dev/run-view.html```#### Get run outputImplements: [Databricks REST runs/get-output](https://docs.databricks.com/dev-tools/api/latest/jobs.html#runs-get-output)```bash$ pydbr runs get-output -i 3 6``````json{   &quot;notebook_output&quot;: {      &quot;result&quot;: &quot;Downloaded files: README.txt, links.csv, movies.csv, ratings.csv, tags.csv&quot;,      &quot;truncated&quot;: false   },   &quot;error&quot;: null,   &quot;metadata&quot;: {      &quot;job_id&quot;: 5,      &quot;run_id&quot;: 5,      &quot;creator_user_name&quot;: &quot;your.name@gmail.com&quot;,      &quot;number_in_job&quot;: 1,      &quot;original_attempt_run_id&quot;: null,      &quot;state&quot;: {         &quot;life_cycle_state&quot;: &quot;TERMINATED&quot;,         &quot;result_state&quot;: &quot;SUCCESS&quot;,         &quot;state_message&quot;: &quot;&quot;      },      &quot;schedule&quot;: null,      &quot;task&quot;: {         &quot;notebook_task&quot;: {            &quot;notebook_path&quot;: &quot;/Utils/Download MovieLens&quot;         }      },      &quot;cluster_spec&quot;: {         &quot;existing_cluster_id&quot;: &quot;xxxx-yyyyy-zzzzzzz&quot;      },      &quot;cluster_instance&quot;: {         &quot;cluster_id&quot;: &quot;xxxx-yyyyy-zzzzzzz&quot;,         &quot;spark_context_id&quot;: &quot;8973498743973498&quot;      },      &quot;overriding_parameters&quot;: null,      &quot;start_time&quot;: 1592062147101,      &quot;setup_duration&quot;: 1000,      &quot;execution_duration&quot;: 11000,      &quot;cleanup_duration&quot;: 0,      &quot;trigger&quot;: null,      &quot;run_name&quot;: &quot;pydbr-1592062135&quot;,      &quot;run_page_url&quot;: &quot;https://westeurope.azuredatabricks.net/?o=89798374987987#job/5/run/1&quot;,      &quot;run_type&quot;: &quot;SUBMIT_RUN&quot;   }}```To get only the exit output:```bash$ pydbr runs get-output -r 6``````Downloaded files: README.txt, links.csv, movies.csv, ratings.csv, tags.csv```## Python Client SDK for Databricks REST APIsTo implement your own Databricks REST API client, you can use the Python Client SDK for Databricks REST APIs.### Create Databricks connection```python# Get Databricks workspace connectiondbc = pydbr.connect(        bearer_token='dapixyzabcd09rasdf',        url='https://westeurope.azuredatabricks.net')```### DBFS```python# Get list of items at path /FileStoredbc.dbfs.ls('/FileStore')# Check if file or directory existsdbc.dbfs.exists('/path/to/heaven')# Make a directory and it's parentsdbc.dbfs.mkdirs('/path/to/heaven')# Delete a directory recusivelydbc.dbfs.rm('/path', recursive=True)# Download file block starting 1024 with size 2048dbc.dbfs.read('/data/movies.csv', 1024, 2048)# Download entire filedbc.dbfs.read_all('/data/movies.csv')```### Databricks workspace```python# List root workspace directorydbc.workspace.ls('/')# Check if workspace item existsdbc.workspace.exists('/explore')# Check if workspace item is a directorydbc.workspace.is_directory('/')# Export notebook in default (SOURCE) formatdbc.workspace.export('/my_notebook')# Export notebook in HTML formatdbc.workspace.export('/my_notebook', 'HTML')```## Build and publish```bashpip install wheel twinepython setup.py sdist bdist_wheelpython -m twine upload dist/*```</longdescription>
</pkgmetadata>