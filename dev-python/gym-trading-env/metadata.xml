<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Crypto-Trading-Env&lt;img alt=&quot;Render example&quot; src =&quot;https://github.com/ClementPerroud/Gym-Trading-Env/blob/main/readme_images/render.gif?raw=true&quot; width = &quot;800&quot;/&gt;An OpenAI Gym environment for simulating stocks and train Reinforcement Learning (RL) trading agents.Designed to be **FAST** and **CUSTOMIZABLE** for easy RL trading algorythms implementation.## Install and import```pip install gym-trading-env```Then import :```pythonfrom gym_trading_env.environments import TradingEnv```## Environment Properties### Actions space : positionsGithub is full of environments that consider actions such as **BUY**, **SELL**. In my opinion, it is a real mistake to consider a reinforcement learning agent in the same way as a trader. Traders make trade and to do so, they place orders on the market (eg. Buy X of stock Y). But what really matter is the position reached. Now, imagine we labelled each position by a number :- ```1``` : All of our portfolio is converted into stock Y. (=*BUY ALL*)- ```0``` : All of our portfolio is converted into our fiat currency. (=*SELL ALL*)Now, we can imagine half position and other variants :- ```0.5``` : 50% in stock Y &amp; 50% in currency- Even : ```0.1``` : 10% in stock Y &amp; 90% in currency....In fact, it is way simpler for a RL-agent to work with positions. This way, it can easily make complex operation with a simple action space.But if you want to use a really basic environment, you can use only 2 positions : ```1``` and ```0``` which is more of less equivalent to **BUY ALL** and **SELL ALL** actions.### Complex positionsThis environment supports more complex positions such as:- ```-1``` : Bet 100% of the portfolio value on the decline of asset Y (=SHORT). To perform this action, the environment borrows 100% of the portfolio valuation as stock Y to an imaginary person, and immediately sells it. When the agent closes this position, the environment buys the owed amount of stock Y and repays the imaginary person with it. If the price has fallen during the operation, we buy cheaper than we sold what we need to repay : the difference is our gain. The imaginary person is paid a small rent (parameter : ```borrow_interest_rate```)- ```+2``` : Bet 100% of the portfolio value of the rise of asset Y. We use the same mechanism explained above, but we borrow currency and buy stock Y.- ```-10``` ? : We can BUT ...  We need to borrow 1000% of the portfolio valuation as asset Y. You need to understand that such a &quot;leverage&quot; is very risky. As if the stock price rise by 10%, you need to repay the original 1000% of your portfolio valuation at 1100% (1000%*1.10) of your current portfolio valuation. Well, 100% (1100% - 1000%) of your portfolio is used to repay your debt. **GAME OVER, you have 0$ left**. The leverage is very useful but also risky, as it increases your **gains** AND your **losses**. Always keep in mind that you can lose everything.### How to use ?**1 - Import and clean your data**. They need to be ordered by ascending date. Index must be DatetimeIndex. Your DataFrame needs to contain a close price labelled ```close``` to run. If you want to render your results, your DataFrame needs to contain open, high, low, volume features respectively labelled ```open```, ```high```, ```low```, ```volume```.```python# Available in the github repo : test/data/BTC_USD-Hourly.csvdf = pd.read_csv(&quot;data/BTC_USD-Hourly.csv&quot;, parse_dates=[&quot;date&quot;], index_col= &quot;date&quot;)df.sort_index(inplace= True)df.dropna(inplace= True)df.drop_duplicates(inplace=True)```**1.1 (Optional) Download data** : The package provides an easy way to download data (works with CCTX and uses asyncio for FAST download) :Indeed, data is KING. To train RL Agent, you will need a big amount of good quality data. ```pythonfrom gym_trading_env.downloader import downloadimport datetimedownload(    exchange_names = [&quot;binance&quot;, &quot;bitfinex2&quot;, &quot;huobi&quot;],    symbols= [&quot;BTC/USDT&quot;, &quot;ETH/USDT&quot;],    timeframe= &quot;30m&quot;,    dir = &quot;test/data&quot;,    since= datetime.datetime(year= 2019, month= 1, day=1),    until = datetime.datetime(year= 2023, month= 1, day=1),)# It will download and save in folder 'test/data' both 'BTC/USDT' and 'ETH/USDT' historical data from all the exchanges mentionned.```This function uses pickle format to save the OHLCV data. You will need to import the dataset with ```pd.read_pickle('... .pkl', ...)```. The function supports exchange_names ```binance```, ```biftfinex2``` (API v2) and ```huobi```. **2 - Create your features**. Your RL-agent will need some good, preprocessed features. It is your job to make sure it has everything it needs.**The feature column names need to contain the keyword 'feature'. The environment will automatically detect them !**```pythondf[&quot;feature_close&quot;] = df[&quot;close&quot;].pct_change()df[&quot;feature_open&quot;] = df[&quot;open&quot;]/df[&quot;close&quot;]df[&quot;feature_high&quot;] = df[&quot;high&quot;]/df[&quot;close&quot;]df[&quot;feature_low&quot;] = df[&quot;low&quot;]/df[&quot;close&quot;]df[&quot;feature_volume&quot;] = df[&quot;Volume USD&quot;] / df[&quot;Volume USD&quot;].rolling(7*24).max()df.dropna(inplace= True) # Clean your data !```**(Optional)3 - Create your own reward function**. Use the history object described below to create your own ! For example : ```pythonimport numpy as npdef reward_function(history):    return np.log(history[&quot;portfolio_valuation&quot;, -1] / history[&quot;portfolio_valuation&quot;, -2]) #log (p_t / p_t-1 )```The history object is similar to a DataFrame. It uses timestep and/or columns to access its values. You can use it this way :&gt;- ```history['column name', t]``` returns the a *scalar* value of the metrics 'column name' at time step t.&gt;- ```history['column name']``` returns a *numpy array* with all the values from timestep 0 to current timestep.&gt;- ```history[t]``` returns a *dictionnary* with of the metrics as keys with the associated values.Accessible columns of history object :&gt;- ```step``` : Step = t.&gt;- ```date``` : Date at step t, datetime.&gt;- ```reward``` : Reward at step t.&gt;- ```position_index``` : Index of the position at step t amoung your position argument.&gt;- ```position``` : Portfolio position at step t.&gt;- ```portfolio_valuation```: Global valuation of the portfolio.&gt;- It gathers every data (not used as features) from your DataFrame and labels them with 'data_{column}'. For example :```data_close```, ```data_open```,  ```data_high```....&gt;- It stores the distribution of the portfolio : ```portfolio_distribution_asset``` the amount of owned asset (stock), ```portfolio_distribution_fiat``` the amount of owned fiat currency, ```portfolio_distribution_borrowed_asset``` amount of borrowed asset, ```portfolio_distribution_borrowed_fiat``` the amount of borrowed fiat currency, ```portfolio_distribution_interest_asset``` the total of cumalated interest generated by the borrowed asset, ```portfolio_distribution_interest_fiat``` the total of cumalated interest generated by the borrowed fiat currency.**4 - Initiate the environment**```pythonenv = TradingEnv(        name= &quot;BTCUSD&quot;,        df = df,        windows= 5,        positions = [ -1, -0.5, 0, 0.5, 1], # From -1 (=SHORT), to +1 (=LONG)        initial_position = 0,        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (= 1h here)        reward_function = reward_function,        portfolio_initial_value = 1000, # here, in USDT    )```Parameters :&gt;- ```name``` (required) : Name of your asset / symbol&gt;- ```df``` (required) : DataFrame containing technical indicators ```open```, ```high```, ```low```, ```close```, ```volume```, and the features you want to be returned as observations (containing ```feature``` in their column names).&gt;- ```windows```(optional, default : None), If None, observation at t are the features at step t  (classic mode). If windows = i (int),  observation at t are the features from steps [t-i+1 :  t] (useful for reccurent models)&gt;- ```positions``` (optional, default : [0, 1]). Positions that the agent can choose (Explained in &quot;Actions space : positions&quot;)&gt;- ```initial_position``` (optional, default : 0). Initial position of the portfolio&gt;- ```trading_fees``` (optional, default : 0). Trading fee for buy and sell.&gt;- ```borrow_interest_rate``` (optional, default : 0). Borrow interest rate PER STEP.&gt;- ```reward_function``` (Optional, default : the reward function used above). Reward function.&gt;- ```portfolio_initial_value``` (optional, default : 1000). Initial value of the portfolio (in FIAT currency)**5 - Run the environment**```pythontruncated, done = False, Falseobservation, info = env.reset()while not truncated and not done:    action = env.action_space.sample()    observation, reward, done, truncated, info = env.step(action)```&gt;- ```observation``` : Returns a dict with items :&gt;    - ```features``` : Contains the features created. If windows is None, it contains the features of the current step (shape = (n_features,)). If windows is i (int), it contains the features the last i steps (shape = (5, n_features)).&gt;    - ```position``` : The last position of the environments. It can be useful to include this to the features, so the agent knows which position it is.&gt;- ```reward``` : The step reward following the action taken.&gt;- ```done```: Always False.&gt;- ```truncated``` : is true if the end of the DataFrame is reached.&gt;- ```info``` : Returns the last history step of the object &quot;history&quot; presented above (in &quot;3 - Create your own reward function&quot;)### 6 - Render performed with Flask (local app).&lt;img alt=&quot;Render example&quot; src =&quot;https://github.com/ClementPerroud/Gym-Trading-Env/blob/main/readme_images/render.gif?raw=true&quot; width = &quot;800&quot;/&gt;For the render not to perturb the training, it needs to be performed in a separate python script. This way you have plenty of time to perform analysis on your results. First, you need to save your results at the end of every episode you want to render with ```env.save_for_render(...)```. And decide which file you want your logs to be stored in with paramter ```dir```. For example :```python...# At the end of the episode you want to renderenv.save_for_render(dir = &quot;render_logs&quot;)```Then in the separated render script. You can import and initiate a renderer object, and run the render in a localhost web app :```pythonfrom gym_trading_env.renderer import Rendererrenderer = Renderer(render_logs_dir=&quot;render_logs&quot;)renderer.run()```#### Custom renderYou can add **metrics** and plot **lines** with :```pythonrenderer = Renderer(render_logs_dir=&quot;render_logs&quot;)# Add Custom Lines (Simple Moving Average)renderer.add_line( name= &quot;sma10&quot;, function= lambda df : df[&quot;close&quot;].rolling(10).mean(), line_options ={&quot;width&quot; : 1, &quot;color&quot;: &quot;purple&quot;})renderer.add_line( name= &quot;sma20&quot;, function= lambda df : df[&quot;close&quot;].rolling(20).mean(), line_options ={&quot;width&quot; : 1, &quot;color&quot;: &quot;blue&quot;})# Add Custom Metrics (Annualized metrics)renderer.add_metric(    name = &quot;Annual Market Return&quot;,    function = lambda df : f&quot;{ ((df['close'].iloc[-1] / df['close'].iloc[0])**(pd.Timedelta(days=365)/(df.index.values[-1] - df.index.values[0]))-1)*100:0.2f}%&quot;)renderer.add_metric(        name = &quot;Annual Portfolio Return&quot;,        function = lambda df : f&quot;{((df['portfolio_valuation'].iloc[-1] / df['portfolio_valuation'].iloc[0])**(pd.Timedelta(days=365)/(df.index.values[-1] - df.index.values[0]))-1)*100:0.2f}%&quot;)renderer.run()```&lt;img alt=&quot;Render example&quot; src =&quot;https://github.com/ClementPerroud/Gym-Trading-Env/blob/main/readme_images/render_customization.gif?raw=true&quot; width = &quot;800&quot;/&gt;&gt;```.add_line``` takes arguments :&gt;- ```name``` (*required*): The name of the scatter&gt;- ```function``` (*required*): The function used to compute the line. The function must take an argument ```df``` which is a DateFrame and return a Series, 1D-Array or list.&gt;- ```line_options``` : Can contain a dict with keys ```color``` and ```width```&gt;&gt;&gt;```.add_metric``` takes arguments :&gt;- ```name``` : The name of the metric&gt;- ```function``` : The function used to compute the line. The function must take an argument ```df``` which is a DateFrame and return a **string** !Enjoy :)</longdescription>
</pkgmetadata>