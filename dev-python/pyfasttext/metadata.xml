<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>pyfasttext==========*Warning!* ``pyfasttext`` **is no longer maintained: use the official Python binding from the fastText repository:** https://github.com/facebookresearch/fastText/tree/master/pythonYet another Python binding for`fastText &lt;https://github.com/facebookresearch/fastText&gt;`__.The binding supports Python 2.6, 2.7 and Python 3. It requires`Cython &lt;http://cython.org/&gt;`__.`Numpy &lt;http://www.numpy.org/&gt;`__ and`cysignals &lt;http://cysignals.readthedocs.io/en/latest/&gt;`__ are alsodependencies, but are optional.| ``pyfasttext`` has been tested successfully on Linux and Mac OS X.| *Warning*: if you want to compile ``pyfasttext`` on Windows, do not  compile with the ``cysignals`` module because it does not support this  platform.Table of Contents=================-  `pyfasttext &lt;#pyfasttext&gt;`__-  `Table of Contents &lt;#table-of-contents&gt;`__   -  `Installation &lt;#installation&gt;`__      -  `Simplest way to install pyfasttext: use         pip &lt;#simplest-way-to-install-pyfasttext-use-pip&gt;`__         -  `Possible compilation error &lt;#possible-compilation-error&gt;`__      -  `Cloning &lt;#cloning&gt;`__      -  `Requirements for Python 2.7 &lt;#requirements-for-python-27&gt;`__      -  `Building and installing         manually &lt;#building-and-installing-manually&gt;`__         -  `Building and installing without optional            dependencies &lt;#building-and-installing-without-optional-dependencies&gt;`__   -  `Usage &lt;#usage&gt;`__      -  `How to load the library? &lt;#how-to-load-the-library&gt;`__      -  `How to load an existing         model? &lt;#how-to-load-an-existing-model&gt;`__      -  `Word representation         learning &lt;#word-representation-learning&gt;`__         -  `Training using Skipgram &lt;#training-using-skipgram&gt;`__         -  `Training using CBoW &lt;#training-using-cbow&gt;`__      -  `Word vectors &lt;#word-vectors&gt;`__         -  `Word vectors access &lt;#word-vectors-access&gt;`__         -  `Vector for a given word &lt;#vector-for-a-given-word&gt;`__            -  `Numpy ndarray &lt;#numpy-ndarray&gt;`__         -  `Words for a given vector &lt;#words-for-a-given-vector&gt;`__         -  `Get the number of words in the            model &lt;#get-the-number-of-words-in-the-model&gt;`__         -  `Get all the word vectors in a            model &lt;#get-all-the-word-vectors-in-a-model&gt;`__            -  `Numpy ndarray &lt;#numpy-ndarray-1&gt;`__         -  `Misc operations with word            vectors &lt;#misc-operations-with-word-vectors&gt;`__         -  `Word similarity &lt;#word-similarity&gt;`__         -  `Most similar words &lt;#most-similar-words&gt;`__         -  `Analogies &lt;#analogies&gt;`__      -  `Text classification &lt;#text-classification&gt;`__         -  `Supervised learning &lt;#supervised-learning&gt;`__         -  `Get all the labels &lt;#get-all-the-labels&gt;`__         -  `Get the number of labels &lt;#get-the-number-of-labels&gt;`__         -  `Prediction &lt;#prediction&gt;`__         -  `Labels and probabilities &lt;#labels-and-probabilities&gt;`__            -  `Normalized probabilities &lt;#normalized-probabilities&gt;`__         -  `Labels only &lt;#labels-only&gt;`__         -  `Quantization &lt;#quantization&gt;`__         -  `Is a model quantized? &lt;#is-a-model-quantized&gt;`__      -  `Subwords &lt;#subwords&gt;`__         -  `Get the subwords &lt;#get-the-subwords&gt;`__         -  `Get the subword vectors &lt;#get-the-subword-vectors&gt;`__      -  `Sentence and text vectors &lt;#sentence-and-text-vectors&gt;`__         -  `Unsupervised models &lt;#unsupervised-models&gt;`__         -  `Supervised models &lt;#supervised-models&gt;`__      -  `Misc utilities &lt;#misc-utilities&gt;`__         -  `Show the module version &lt;#show-the-module-version&gt;`__         -  `Show fastText version &lt;#show-fasttext-version&gt;`__         -  `Show the model            (hyper)parameters &lt;#show-the-model-hyperparameters&gt;`__         -  `Show the model version            number &lt;#show-the-model-version-number&gt;`__         -  `Extract labels or classes from a            dataset &lt;#extract-labels-or-classes-from-a-dataset&gt;`__         -  `Extract labels &lt;#extract-labels&gt;`__         -  `Extract classes &lt;#extract-classes&gt;`__      -  `Exceptions &lt;#exceptions&gt;`__      -  `Interruptible operations &lt;#interruptible-operations&gt;`__Installation------------To compile ``pyfasttext``, make sure you have the following compiler: \*GCC (``g++``) with C++11 support. \* LLVM (``clang++``) with (at least)partial C++17 support.Simplest way to install pyfasttext: use pip~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Just type these lines:.. code:: bash    pip install cython    pip install pyfasttextPossible compilation error^^^^^^^^^^^^^^^^^^^^^^^^^^If you have a compilation error, you can try to install ``cysignals``manually:.. code:: bash    pip install cysignalsThen, retry to install ``pyfasttext`` with the already mentioned ``pip``command.Cloning~~~~~~~| ``pyfasttext`` uses git  `submodules &lt;https://git-scm.com/book/en/v2/Git-Tools-Submodules&gt;`__.| So, you need to add the ``--recursive`` option when you clone the  repository... code:: bash    git clone --recursive https://github.com/vrasneur/pyfasttext.git    cd pyfasttextRequirements for Python 2.7~~~~~~~~~~~~~~~~~~~~~~~~~~~| Python 2.7 support relies on the `future &lt;http://python-future.org&gt;`__  module: ``pyfasttext`` needs ``bytes`` objects, which are not  available natively in Python2.| You can install the ``future`` module with ``pip``... code:: bash    pip install futureBuilding and installing manually~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~First, install all the requirements:.. code:: bash    pip install -r requirements.txtThen, build and install with ``setup.py``:.. code:: bash    python setup.py installBuilding and installing without optional dependencies^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``pyfasttext`` can export word vectors as ``numpy`` ``ndarray``\ s,however this feature can be disabled at compile time.To compile without ``numpy``, pyfasttext has a ``USE_NUMPY`` environmentvariable. Set this variable to 0 (or empty), like this:.. code:: bash    USE_NUMPY=0 python setup.py installIf you want to compile without ``cysignals``, likewise, you can set the``USE_CYSIGNALS`` environment variable to 0 (or empty).Usage-----How to load the library?~~~~~~~~~~~~~~~~~~~~~~~~.. code:: python    &gt;&gt;&gt; from pyfasttext import FastTextHow to load an existing model?~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~.. code:: python    &gt;&gt;&gt; model = FastText('/path/to/model.bin')or.. code:: python    &gt;&gt;&gt; model = FastText()    &gt;&gt;&gt; model.load_model('/path/to/model.bin')Word representation learning~~~~~~~~~~~~~~~~~~~~~~~~~~~~| You can use all the options provided by the ``fastText`` binary  (``input``, ``output``, ``epoch``, ``lr``, ...).| Just use keyword arguments in the training methods of the ``FastText``  object.Training using Skipgram^^^^^^^^^^^^^^^^^^^^^^^.. code:: python    &gt;&gt;&gt; model = FastText()    &gt;&gt;&gt; model.skipgram(input='data.txt', output='model', epoch=100, lr=0.7)Training using CBoW^^^^^^^^^^^^^^^^^^^.. code:: python    &gt;&gt;&gt; model = FastText()    &gt;&gt;&gt; model.cbow(input='data.txt', output='model', epoch=100, lr=0.7)Word vectors~~~~~~~~~~~~Word vectors access^^^^^^^^^^^^^^^^^^^Vector for a given word'''''''''''''''''''''''By default, a single word vector is returned as a regular Python arrayof floats... code:: python    &gt;&gt;&gt; model['dog']    array('f', [-1.308749794960022, -1.8326224088668823, ...])Numpy ndarray             The ``model.get_numpy_vector(word)`` method returns the word vector as a``numpy`` ``ndarray``... code:: python    &gt;&gt;&gt; model.get_numpy_vector('dog')    array([-1.30874979, -1.83262241, ...], dtype=float32)If you want a normalized vector (*i.e.* the vector divided by its norm),there is an optional boolean parameter named ``normalized``... code:: python    &gt;&gt;&gt; model.get_numpy_vector('dog', normalized=True)    array([-0.07084749, -0.09920666, ...], dtype=float32)Words for a given vector''''''''''''''''''''''''| The inverse operation of ``model[word]`` or  ``model.get_numpy_vector(word)`` is  ``model.words_for_vector(vector, k)``.| It returns a list of the ``k`` words closest to the provided vector.  The default value for ``k`` is 1... code:: python    &gt;&gt;&gt; king = model.get_numpy_vector('king')    &gt;&gt;&gt; man = model.get_numpy_vector('man')    &gt;&gt;&gt; woman = model.get_numpy_vector('woman')    &gt;&gt;&gt; model.words_for_vector(king + woman - man, k=1)    [('queen', 0.77121970653533936)]Get the number of words in the model''''''''''''''''''''''''''''''''''''.. code:: python    &gt;&gt;&gt; model.nwords    500000Get all the word vectors in a model'''''''''''''''''''''''''''''''''''.. code:: python    &gt;&gt;&gt; for word in model.words:    ...   print(word, model[word])Numpy ndarray             If you want all the word vectors as a big ``numpy`` ``ndarray``, you canuse the ``numpy_normalized_vectors`` member. Note that all these vectorsare *normalized*... code:: python    &gt;&gt;&gt; model.nwords    500000    &gt;&gt;&gt; model.numpy_normalized_vectors    array([[-0.07549749, -0.09407753, ...],           [ 0.00635979, -0.17272158, ...],           ...,            [-0.01009259,  0.14604086, ...],           [ 0.12467574, -0.0609326 , ...]], dtype=float32)    &gt;&gt;&gt; model.numpy_normalized_vectors.shape    (500000, 100) # (number of words, dimension)Misc operations with word vectors^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Word similarity'''''''''''''''.. code:: python    &gt;&gt;&gt; model.similarity('dog', 'cat')    0.75596606254577637Most similar words''''''''''''''''''.. code:: python    &gt;&gt;&gt; model.nearest_neighbors('dog', k=2)    [('dogs', 0.7843924736976624), ('cat', 75596606254577637)]Analogies'''''''''The ``model.most_similar()`` method works similarly as the one in`gensim &lt;https://radimrehurek.com/gensim/models/keyedvectors.html&gt;`__... code:: python    &gt;&gt;&gt; model.most_similar(positive=['woman', 'king'], negative=['man'], k=1)    [('queen', 0.77121970653533936)]Text classification~~~~~~~~~~~~~~~~~~~Supervised learning^^^^^^^^^^^^^^^^^^^.. code:: python    &gt;&gt;&gt; model = FastText()    &gt;&gt;&gt; model.supervised(input='/path/to/input.txt', output='/path/to/model', epoch=100, lr=0.7)Get all the labels^^^^^^^^^^^^^^^^^^.. code:: python    &gt;&gt;&gt; model.labels    ['LABEL1', 'LABEL2', ...]Get the number of labels^^^^^^^^^^^^^^^^^^^^^^^^.. code:: python    &gt;&gt;&gt; model.nlabels    100Prediction^^^^^^^^^^| To obtain the ``k`` most likely labels from test sentences, there are  multiple ``model.predict_*()`` methods.| The default value for ``k`` is 1. If you want to obtain all the  possible labels, use ``None`` for ``k``.Labels and probabilities''''''''''''''''''''''''If you have a list of strings (or an iterable object), use this:.. code:: python    &gt;&gt;&gt; model.predict_proba(['first sentence\n', 'second sentence\n'], k=2)    [[('LABEL1', 0.99609375), ('LABEL3', 1.953126549381068e-08)], [('LABEL2', 1.0), ('LABEL3', 1.953126549381068e-08)]]If you want to test a single string, use this:.. code:: python    &gt;&gt;&gt; model.predict_proba_single('first sentence\n', k=2)    [('LABEL1', 0.99609375), ('LABEL3', 1.953126549381068e-08)]**WARNING**: In order to get the same probabilities as the ``fastText``binary, you have to add a newline (``\n``) at the end of each string.If your test data is stored inside a file, use this:.. code:: python    &gt;&gt;&gt; model.predict_proba_file('/path/to/test.txt', k=2)    [[('LABEL1', 0.99609375), ('LABEL3', 1.953126549381068e-08)], [('LABEL2', 1.0), ('LABEL3', 1.953126549381068e-08)]]Normalized probabilities                        For performance reasons, fastText probabilities often do not sum up to1.0.If you want normalized probabilities (where the sum is closer to 1.0than the original probabilities), you can use the ``normalized=True``parameter in all the methods that output probabilities(``model.predict_proba()``, ``model.predict_proba_file()`` and``model.predict_proba_single()``)... code:: python    &gt;&gt;&gt; sum(proba for label, proba in model.predict_proba_single('this is a sentence that needs to be classified\n', k=None))    0.9785203068801335    &gt;&gt;&gt; sum(proba for label, proba in model.predict_proba_single('this is a sentence that needs to be classified\n', k=None, normalized=True))    0.9999999999999898Labels only'''''''''''If you have a list of strings (or an iterable object), use this:.. code:: python    &gt;&gt;&gt; model.predict(['first sentence\n', 'second sentence\n'], k=2)    [['LABEL1', 'LABEL3'], ['LABEL2', 'LABEL3']]If you want to test a single string, use this:.. code:: python    &gt;&gt;&gt; model.predict_single('first sentence\n', k=2)    ['LABEL1', 'LABEL3']**WARNING**: In order to get the same probabilities as the ``fastText``binary, you have to add a newline (``\n``) at the end of each string.If your test data is stored inside a file, use this:.. code:: python    &gt;&gt;&gt; model.predict_file('/path/to/test.txt', k=2)    [['LABEL1', 'LABEL3'], ['LABEL2', 'LABEL3']]Quantization^^^^^^^^^^^^Use keyword arguments in the ``model.quantize()`` method... code:: python    &gt;&gt;&gt; model.quantize(input='/path/to/input.txt', output='/path/to/model')You can load quantized models using the ``FastText`` constructor or the``model.load_model()`` method.Is a model quantized?'''''''''''''''''''''If you want to know if a model has been quantized before, use the``model.quantized`` attribute... code:: python    &gt;&gt;&gt; model = FastText('/path/to/model.bin')    &gt;&gt;&gt; model.quantized    False    &gt;&gt;&gt; model = FastText('/path/to/model.ftz')    &gt;&gt;&gt; model.quantized    TrueSubwords~~~~~~~~fastText can use subwords (*i.e.* character ngrams) when doingunsupervised or supervised learning.You can access the subwords, and their associated vectors, using``pyfasttext``.Get the subwords^^^^^^^^^^^^^^^^fastText's word embeddings can be augmented with subword-levelinformation. It is possible to retrieve the subwords and theirassociated vectors from a model using ``pyfasttext``.To retrieve all the subwords for a given word, use the``model.get_all_subwords(word)`` method... code:: python    &gt;&gt;&gt; model.args.get('minn'), model.args.get('maxn')    (2, 4)    &gt;&gt;&gt; model.get_all_subwords('hello') # word + subwords from 2 to 4 characters    ['hello', '&lt;h', '&lt;he', '&lt;hel', 'he', 'hel', 'hell', 'el', 'ell', 'ello', 'll', 'llo', 'llo&gt;', 'lo', 'lo&gt;', 'o&gt;']For fastText, ``&lt;`` means &quot;beginning of a word&quot; and ``&gt;`` means &quot;end ofa word&quot;.As you can see, fastText includes the full word. You can omit it usingthe ``omit_word=True`` keyword argument... code:: python    &gt;&gt;&gt; model.get_all_subwords('hello', omit_word=True)    ['&lt;h', '&lt;he', '&lt;hel', 'he', 'hel', 'hell', 'el', 'ell', 'ello', 'll', 'llo', 'llo&gt;', 'lo', 'lo&gt;', 'o&gt;']When a model is quantized, fastText may *prune* some subwords. If youwant to see only the subwords that are really used when computing a wordvector, you should use the ``model.get_subwords(word)`` method... code:: python    &gt;&gt;&gt; model.quantized    True    &gt;&gt;&gt; model.get_subwords('beautiful')    ['eau', 'aut', 'ful', 'ul']    &gt;&gt;&gt; model.get_subwords('hello')    ['hello'] # fastText will not use any subwords when computing the word vector, only the full wordGet the subword vectors^^^^^^^^^^^^^^^^^^^^^^^To get the individual vectors given the subwords, use the``model.get_numpy_subword_vectors(word)`` method... code:: python    &gt;&gt;&gt; model.get_numpy_subword_vectors('beautiful') # 4 vectors, so 4 rows    array([[ 0.49022141,  0.13586822,  ..., -0.14065443,  0.89617103], # subword &quot;eau&quot;           [-0.42594951,  0.06260503,  ..., -0.18182631,  0.34219387], # subword &quot;aut&quot;           [ 0.49958718,  2.93831301,  ..., -1.97498322, -1.16815805], # subword &quot;ful&quot;           [-0.4368791 , -1.92924356,  ...,  1.62921488, 1.90240896]], dtype=float32) # subword &quot;ul&quot;In fastText, the final word vector is the average of these individualvectors... code:: python    &gt;&gt;&gt; import numpy as np    &gt;&gt;&gt; vec1 = model.get_numpy_vector('beautiful')    &gt;&gt;&gt; vecs2 = model.get_numpy_subword_vectors('beautiful')    &gt;&gt;&gt; np.allclose(vec1, np.average(vecs2, axis=0))    TrueSentence and text vectors~~~~~~~~~~~~~~~~~~~~~~~~~To compute the vector of a sequence of words (*i.e.* a sentence),fastText uses two different methods: \* one for unsupervised models \*another one for supervised modelsWhen fastText computes a word vector, recall that it uses the average ofthe following vectors: the word itself and its subwords.Unsupervised models^^^^^^^^^^^^^^^^^^^For unsupervised models, the representation of a sentence for fastTextis the average of the normalized word vectors.| To get the resulting vector as a regular Python array, use the  ``model.get_sentence_vector(line)`` method.| To get the resulting vector as a ``numpy`` ``ndarray``, use the  ``model.get_numpy_sentence_vector(line)`` method... code:: python    &gt;&gt;&gt; vec = model.get_numpy_sentence_vector('beautiful cats')    &gt;&gt;&gt; vec1 = model.get_numpy_vector('beautiful', normalized=True)    &gt;&gt;&gt; vec2 = model.get_numpy_vector('cats', normalized=True)    &gt;&gt;&gt; np.allclose(vec, np.average([vec1, vec2], axis=0)    TrueSupervised models^^^^^^^^^^^^^^^^^For supervised models, fastText uses the regular word vectors, as wellas vectors computed using word ngrams (*i.e.* shorter sequences of wordsfrom the sentence). When computing the average, these vectors are notnormalized.| To get the resulting vector as a regular Python array, use the  ``model.get_text_vector(line)`` method.| To get the resulting vector as a ``numpy`` ``ndarray``, use the  ``model.get_numpy_text_vector(line)`` method... code:: python    &gt;&gt;&gt; model.get_numpy_sentence_vector('beautiful cats') # for an unsupervised model    array([-0.20266785,  0.3407566 ,  ...,  0.03044436,  0.39055538], dtype=float32)    &gt;&gt;&gt; model.get_numpy_text_vector('beautiful cats') # for a supervised model    array([-0.20840774,  0.4289546 ,  ..., -0.00457615,  0.52417743], dtype=float32)Misc utilities~~~~~~~~~~~~~~Show the module version^^^^^^^^^^^^^^^^^^^^^^^.. code:: python    &gt;&gt;&gt; import pyfasttext    &gt;&gt;&gt; pyfasttext.__version__    '0.4.3'Show fastText version^^^^^^^^^^^^^^^^^^^^^As there is no version number in fastText, we use the latest fastTextcommit hash (from ``HEAD``) as a substitute... code:: python    &gt;&gt;&gt; import pyfasttext    &gt;&gt;&gt; pyfasttext.__fasttext_version__    '431c9e2a9b5149369cc60fb9f5beba58dcf8ca17'Show the model (hyper)parameters^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^.. code:: python    &gt;&gt;&gt; model.args    {'bucket': 11000000,     'cutoff': 0,     'dim': 100,     'dsub': 2,     'epoch': 100,    ...    }Show the model version number^^^^^^^^^^^^^^^^^^^^^^^^^^^^^fastText uses a versioning scheme for its generated models. You canretrieve the model version number using the ``model.version`` attribute.+----------------+------------------------+| version number | description            |+================+========================+| -1             | for really old models  ||                | with no version number |+----------------+------------------------+| 11             | first version number   ||                | added by fastText      |+----------------+------------------------+| 12             | for models generated   ||                | after fastText added   ||                | support for subwords   ||                | in supervised learning |+----------------+------------------------+.. code:: python    &gt;&gt;&gt; model.version    12Extract labels or classes from a dataset^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^You can use the ``FastText`` object to extract labels or classes from adataset. The label prefix (which is ``__label__`` by default) is setusing the ``label`` parameter in the constructor.If you load an existing model, the label prefix will be the one definedin the model... code:: python    &gt;&gt;&gt; model = FastText(label='__my_prefix__')Extract labels''''''''''''''There can be multiple labels per line... code:: python    &gt;&gt;&gt; model.extract_labels('/path/to/dataset1.txt')    [['LABEL2', 'LABEL5'], ['LABEL1'], ...]Extract classes'''''''''''''''There can be only one class per line... code:: python    &gt;&gt;&gt; model.extract_classes('/path/to/dataset2.txt')    ['LABEL3', 'LABEL1', 'LABEL2', ...]Exceptions~~~~~~~~~~The ``fastText`` source code directly calls exit() when something wronghappens (*e.g.* a model file does not exist, ...).Instead of exiting, ``pyfasttext`` raises a Python exception(``RuntimeError``)... code:: python    &gt;&gt;&gt; import pyfasttext    &gt;&gt;&gt; model = pyfasttext.FastText('/path/to/non-existing_model.bin')    Model file cannot be opened for loading!    Traceback (most recent call last):      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;      File &quot;src/pyfasttext.pyx&quot;, line 124, in pyfasttext.FastText.__cinit__ (src/pyfasttext.cpp:1800)      File &quot;src/pyfasttext.pyx&quot;, line 348, in pyfasttext.FastText.load_model (src/pyfasttext.cpp:5947)    RuntimeError: fastext tried to exit: 1Interruptible operations~~~~~~~~~~~~~~~~~~~~~~~~``pyfasttext`` uses ``cysignals`` to make all the computationallyintensive operations (*e.g.* training) interruptible.To easily interrupt such an operation, just type ``Ctrl-C`` in yourPython shell... code:: python    &gt;&gt;&gt; model.skipgram(input='/path/to/input.txt', output='/path/to/mymodel')    Read 12M words    Number of words:  60237    Number of labels: 0    ... # type Ctrl-C during training    Traceback (most recent call last):      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;      File &quot;src/pyfasttext.pyx&quot;, line 680, in pyfasttext.FastText.skipgram (src/pyfasttext.cpp:11125)      File &quot;src/pyfasttext.pyx&quot;, line 674, in pyfasttext.FastText.train (src/pyfasttext.cpp:11009)      File &quot;src/pyfasttext.pyx&quot;, line 668, in pyfasttext.FastText.train (src/pyfasttext.cpp:10926)      File &quot;src/cysignals/signals.pyx&quot;, line 94, in cysignals.signals.sig_raise_exception (build/src/cysignals/signals.c:1328)    KeyboardInterrupt    &gt;&gt;&gt; # you can have your shell back!</longdescription>
</pkgmetadata>