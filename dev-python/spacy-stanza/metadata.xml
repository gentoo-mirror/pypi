<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;a href=&quot;https://explosion.ai&quot;&gt;&lt;img src=&quot;https://explosion.ai/assets/img/logo.svg&quot; width=&quot;125&quot; height=&quot;125&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;# spaCy + Stanza (formerly StanfordNLP)This package wraps the [Stanza](https://github.com/stanfordnlp/stanza)(formerly StanfordNLP) library, so you can use Stanford's models in a[spaCy](https://spacy.io) pipeline. The Stanford models achieved top accuracyin the CoNLL 2017 and 2018 shared task, which involves tokenization,part-of-speech tagging, morphological analysis, lemmatization and labeleddependency parsing in 68 languages. As of v1.0, Stanza also supports namedentity recognition for selected languages.&gt; ‚ö†Ô∏è Previous version of this package were available as&gt; [`spacy-stanfordnlp`](https://pypi.python.org/pypi/spacy-stanfordnlp).[![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/17/master.svg?logo=azure-pipelines&amp;style=flat-square)](https://dev.azure.com/explosion-ai/public/_build?definitionId=17)[![PyPi](https://img.shields.io/pypi/v/spacy-stanza.svg?style=flat-square)](https://pypi.python.org/pypi/spacy-stanza)[![GitHub](https://img.shields.io/github/release/explosion/spacy-stanza/all.svg?style=flat-square)](https://github.com/explosion/spacy-stanza)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)Using this wrapper, you'll be able to use the following annotations, computed byyour pretrained `stanza` model:- Statistical tokenization (reflected in the `Doc` and its tokens)- Lemmatization (`token.lemma` and `token.lemma_`)- Part-of-speech tagging (`token.tag`, `token.tag_`, `token.pos`, `token.pos_`)- Morphological analysis (`token.morph`)- Dependency parsing (`token.dep`, `token.dep_`, `token.head`)- Named entity recognition (`doc.ents`, `token.ent_type`, `token.ent_type_`, `token.ent_iob`, `token.ent_iob_`)- Sentence segmentation (`doc.sents`)## Ô∏èÔ∏èÔ∏è‚åõÔ∏è InstallationAs of v1.0.0 `spacy-stanza` is only compatible with **spaCy v3.x**. To installthe most recent version:```bashpip install spacy-stanza```For spaCy v2, install v0.2.x and refer to the [v0.2.x usagedocumentation](https://github.com/explosion/spacy-stanza/tree/v0.2.x#-usage--examples):```bashpip install &quot;spacy-stanza&lt;0.3.0&quot;```Make sure to also[download](https://stanfordnlp.github.io/stanza/download_models.html) one ofthe [pre-trained Stanzamodels](https://stanfordnlp.github.io/stanza/models.html).## üìñ Usage &amp; Examples&gt; ‚ö†Ô∏è **Important note:** This package has been refactored to take advantage of&gt; [spaCy v3.0](https://spacy.io). Previous versions that were built for [spaCy&gt; v2.x](https://v2.spacy.io) worked considerably differently. Please see&gt; previous tagged versions of this README for documentation on prior versions.Use `spacy_stanza.load_pipeline()` to create an `nlp` object that you can use toprocess a text with a Stanza pipeline and create a spaCy [`Doc`object](https://spacy.io/api/doc). By default, both the spaCy pipeline and theStanza pipeline will be initialized with the same `lang`, e.g. &quot;en&quot;:```pythonimport stanzaimport spacy_stanza# Download the stanza model if necessarystanza.download(&quot;en&quot;)# Initialize the pipelinenlp = spacy_stanza.load_pipeline(&quot;en&quot;)doc = nlp(&quot;Barack Obama was born in Hawaii. He was elected president in 2008.&quot;)for token in doc:    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)print(doc.ents)```If language data for the given language is available in spaCy, the respectivelanguage class can be used as the base for the `nlp` object ‚Äì for example,`English()`. This lets you use spaCy's lexical attributes like `is_stop` or`like_num`. The `nlp` object follows the same API as any other spaCy `Language`class ‚Äì so you can visualize the `Doc` objects with displaCy, add customcomponents to the pipeline, use the rule-based matcher and do pretty muchanything else you'd normally do in spaCy.```python# Access spaCy's lexical attributesprint([token.is_stop for token in doc])print([token.like_num for token in doc])# Visualize dependenciesfrom spacy import displacydisplacy.serve(doc)  # or displacy.render if you're in a Jupyter notebook# Process texts with nlp.pipefor doc in nlp.pipe([&quot;Lots of texts&quot;, &quot;Even more texts&quot;, &quot;...&quot;]):    print(doc.text)# Combine with your own custom pipeline componentsfrom spacy import Language@Language.component(&quot;custom_component&quot;)def custom_component(doc):    # Do something to the doc here    print(f&quot;Custom component called: {doc.text}&quot;)    return docnlp.add_pipe(&quot;custom_component&quot;)doc = nlp(&quot;Some text&quot;)# Serialize attributes to a numpy arraynp_array = doc.to_array(['ORTH', 'LEMMA', 'POS'])```### Stanza Pipeline optionsAdditional options for the Stanza[`Pipeline`](https://stanfordnlp.github.io/stanza/pipeline.html#pipeline) can beprovided as keyword arguments following the `Pipeline` API:- Provide the Stanza language as `lang`. For Stanza languages without spaCy  support, use &quot;xx&quot; for the spaCy language setting:  ```python  # Initialize a pipeline for Coptic  nlp = spacy_stanza.load_pipeline(&quot;xx&quot;, lang=&quot;cop&quot;)  ```- Provide Stanza pipeline settings following the `Pipeline` API:  ```python  # Initialize a German pipeline with the `hdt` package  nlp = spacy_stanza.load_pipeline(&quot;de&quot;, package=&quot;hdt&quot;)  ```- Tokenize with spaCy rather than the statistical tokenizer (only for English):  ```python  nlp = spacy_stanza.load_pipeline(&quot;en&quot;, processors= {&quot;tokenize&quot;: &quot;spacy&quot;})  ```- Provide any additional processor settings as additional keyword arguments:  ```python  # Provide pretokenized texts (whitespace tokenization)  nlp = spacy_stanza.load_pipeline(&quot;de&quot;, tokenize_pretokenized=True)  ```The spaCy config specifies all `Pipeline` options in the `[nlp.tokenizer]`block. For example, the config for the last example above, a German pipelinewith pretokenized texts:```ini[nlp.tokenizer]@tokenizers = &quot;spacy_stanza.PipelineAsTokenizer.v1&quot;lang = &quot;de&quot;dir = nullpackage = &quot;default&quot;logging_level = nullverbose = nulluse_gpu = true[nlp.tokenizer.kwargs]tokenize_pretokenized = true[nlp.tokenizer.processors]```### SerializationThe full Stanza pipeline configuration is stored in the spaCy pipeline[config](https://spacy.io/usage/training#config), so you can save and load thepipeline just like any other `nlp` pipeline:```python# Save to a local directorynlp.to_disk(&quot;./stanza-spacy-model&quot;)# Reload the pipelinenlp = spacy.load(&quot;./stanza-spacy-model&quot;)```Note that this **does not save any Stanza model data by default**. The Stanzamodels are very large, so for now, this package expects you to download themodels separately with `stanza.download()` and have them available either inthe default model directory or in the path specified under`[nlp.tokenizer.dir]` in the config.### Adding additional spaCy pipeline componentsBy default, the spaCy pipeline in the `nlp` object returned by`spacy_stanza.load_pipeline()` will be empty, because all `stanza` attributesare computed and set within the custom tokenizer,[`StanzaTokenizer`](spacy_stanza/tokenizer.py). But since it's a regular `nlp`object, you can add your own components to the pipeline. For example, you couldadd [your own custom text classificationcomponent](https://spacy.io/usage/training) with `nlp.add_pipe(&quot;textcat&quot;,source=source_nlp)`, or augment the named entities with your own rule-basedpatterns using the [`EntityRuler`component](https://spacy.io/usage/rule-based-matching#entityruler).</longdescription>
</pkgmetadata>