<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;!---Copyright 2022 The HuggingFace Team. All rights reserved.Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);you may not use this file except in compliance with the License.You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an &quot;AS IS&quot; BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.--&gt;![](https://github.com/huggingface/optimum-habana/blob/main/readme_logo.png)# Optimum HabanaðŸ¤— Optimum Habana is the interface between the ðŸ¤— Transformers and Diffusers libraries and [Habana's Gaudi processor (HPU)](https://docs.habana.ai/en/latest/index.html).It provides a set of tools enabling easy model loading, training and inference on single- and multi-HPU settings for different downstream tasks.The list of officially validated models and tasks is available [here](https://github.com/huggingface/optimum-habana#validated-models). Users can try other models and tasks with only few changes.## What is a Habana Processing Unit (HPU)?HPUs offer fast model training and inference as well as a great price-performance ratio.Check out [this blog post about BERT pre-training](https://huggingface.co/blog/pretraining-bert) and [this article benchmarking Habana Gaudi2 versus Nvidia A100 GPUs](https://huggingface.co/blog/habana-gaudi-2-benchmark) for concrete examples.If you are not familiar with HPUs and would like to know more about them, we recommend you take a look at [our conceptual guide](https://huggingface.co/docs/optimum/habana/concept_guides/hpu).## InstallTo install the latest release of this package:```bashpip install optimum[habana]```&gt; To use DeepSpeed on HPUs, you also need to run the following command:&gt;```bash&gt;pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.8.0&gt;```Optimum Habana is a fast-moving project, and you may want to install it from source:```bashpip install git+https://github.com/huggingface/optimum-habana.git```&gt; Alternatively, you can install the package without pip as follows:&gt; ```bash&gt; git clone https://github.com/huggingface/optimum-habana.git&gt; cd optimum-habana&gt; python setup.py install&gt; ```Last but not least, don't forget to install the requirements for every example:```bashcd &lt;example-folder&gt;pip install -r requirements.txt```## How to use it?### Quick StartðŸ¤— Optimum Habana was designed with one goal in mind: **to make training and evaluation straightforward for any ðŸ¤— Transformers and ðŸ¤— Diffusers user while leveraging the complete power of Gaudi processors**.#### Transformers InterfaceThere are two main classes one needs to know:- [GaudiTrainer](https://huggingface.co/docs/optimum/habana/package_reference/trainer): the trainer class that takes care of compiling (lazy or eager mode) and distributing the model to run on HPUs, and performing training and evaluation.- [GaudiConfig](https://huggingface.co/docs/optimum/habana/package_reference/gaudi_config): the class that enables to configure Habana Mixed Precision and to decide whether optimized operators and optimizers should be used or not.The [GaudiTrainer](https://huggingface.co/docs/optimum/habana/package_reference/trainer) is very similar to the [ðŸ¤— Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), and adapting a script using the Trainer to make it work with Gaudi will mostly consist in simply swapping the `Trainer` class for the `GaudiTrainer` one.That's how most of the [example scripts](https://github.com/huggingface/optimum-habana/tree/main/examples) were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).Original script:```pythonfrom transformers import Trainer, TrainingArgumentstraining_args = TrainingArguments(  # training arguments...)# A lot of code here# Initialize our Trainertrainer = Trainer(    model=model,    args=training_args,  # Original training arguments.    train_dataset=train_dataset if training_args.do_train else None,    eval_dataset=eval_dataset if training_args.do_eval else None,    compute_metrics=compute_metrics,    tokenizer=tokenizer,    data_collator=data_collator,)```Transformed version that can run on Gaudi:```pythonfrom optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArgumentstraining_args = GaudiTrainingArguments(  # same training arguments...  use_habana=True,  use_lazy_mode=True,  # whether to use lazy or eager mode  use_hpu_graphs=True,  # whether to use HPU graphs for inference  gaudi_config_name=path_to_gaudi_config,)# A lot of the same code as the original script here# Initialize our Trainertrainer = GaudiTrainer(    model=model,    # You can manually specify the Gaudi configuration to use with    # gaudi_config=my_gaudi_config    args=training_args,    train_dataset=train_dataset if training_args.do_train else None,    eval_dataset=eval_dataset if training_args.do_eval else None,    compute_metrics=compute_metrics,    tokenizer=tokenizer,    data_collator=data_collator,)```where `gaudi_config_name` is the name of a model from the [Hub](https://huggingface.co/Habana) (Gaudi configurations are stored in model repositories). You can also give the path to a custom Gaudi configuration written in a JSON file such as this one:```json{  &quot;use_habana_mixed_precision&quot;: true,  &quot;hmp_opt_level&quot;: &quot;O1&quot;,  &quot;hmp_is_verbose&quot;: false,  &quot;use_fused_adam&quot;: true,  &quot;use_fused_clip_norm&quot;: true,  &quot;hmp_bf16_ops&quot;: [    &quot;add&quot;,    &quot;addmm&quot;,    &quot;bmm&quot;,    &quot;div&quot;,    &quot;dropout&quot;,    &quot;gelu&quot;,    &quot;iadd&quot;,    &quot;linear&quot;,    &quot;layer_norm&quot;,    &quot;matmul&quot;,    &quot;mm&quot;,    &quot;rsub&quot;,    &quot;softmax&quot;,    &quot;truediv&quot;  ],  &quot;hmp_fp32_ops&quot;: [    &quot;embedding&quot;,    &quot;nll_loss&quot;,    &quot;log_softmax&quot;  ]}```If you prefer to instantiate a Gaudi configuration to work on it before giving it to the trainer, you can do it as follows:```pythongaudi_config = GaudiConfig.from_pretrained(    gaudi_config_name,    cache_dir=model_args.cache_dir,    revision=model_args.model_revision,    use_auth_token=True if model_args.use_auth_token else None,)```#### Diffusers InterfaceYou can generate images from prompts using Stable Diffusion on Gaudi using the [`GaudiStableDiffusionPipeline`](https://huggingface.co/docs/optimum/habana/package_reference/stable_diffusion_pipeline) class and the [`GaudiDDIMScheduler`] which have been both optimized for HPUs. Here is how to use them and the differences with the ðŸ¤— Diffusers library:```diff- from diffusers import DDIMScheduler, StableDiffusionPipeline+ from optimum.habana.diffusers import GaudiDDIMScheduler, GaudiStableDiffusionPipelinemodel_name = &quot;CompVis/stable-diffusion-v1-4&quot;- scheduler = DDIMScheduler.from_pretrained(model_name, subfolder=&quot;scheduler&quot;)+ scheduler = GaudiDDIMScheduler.from_pretrained(model_name, subfolder=&quot;scheduler&quot;)- pipeline = StableDiffusionPipeline.from_pretrained(+ pipeline = GaudiStableDiffusionPipeline.from_pretrained(    model_name,    scheduler=scheduler,+   use_habana=True,+   use_hpu_graphs=True,+   gaudi_config=&quot;Habana/stable-diffusion&quot;,)outputs = generator(    [&quot;An image of a squirrel in Picasso style&quot;],    num_images_per_prompt=16,+   batch_size=4,)```### DocumentationCheck out [the documentation of Optimum Habana](https://huggingface.co/docs/optimum/habana/index) for more advanced usage.## Validated ModelsThe following model architectures, tasks and device distributions have been validated for ðŸ¤— Optimum Habana:&lt;div align=&quot;center&quot;&gt;| Architecture | Single Card | Multi Card | DeepSpeed | &lt;center&gt;Tasks&lt;/center&gt; ||--------------|:-----------:|:----------:|:---------:|------------------------|| BERT         | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[text classification](https://github.com/huggingface/optimum-habana/tree/main/examples/text-classification)&lt;/li&gt;&lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || RoBERTa | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || ALBERT | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || DistilBERT |:heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering)&lt;/li&gt;&lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || GPT2             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling)&lt;/li&gt; || T5 | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[summarization](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization)&lt;/li&gt;&lt;li&gt;[translation](https://github.com/huggingface/optimum-habana/tree/main/examples/translation)&lt;/li&gt; || ViT | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[image classification](https://github.com/huggingface/optimum-habana/tree/main/examples/image-classification)&lt;/li&gt; || Swin | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[image classification](https://github.com/huggingface/optimum-habana/tree/main/examples/image-classification)&lt;/li&gt; || Wav2Vec2 | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | &lt;li&gt;[audio classification](https://github.com/huggingface/optimum-habana/tree/main/examples/audio-classification)&lt;/li&gt;&lt;li&gt;[speech recognition](https://github.com/huggingface/optimum-habana/tree/main/examples/speech-recognition)&lt;/li&gt; || Stable Diffusion | :heavy_check_mark: | âœ— | âœ— | &lt;li&gt;[text-to-image generation](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion)&lt;/li&gt; |&lt;/div&gt;Other models and tasks supported by the ðŸ¤— Transformers library may also work. You can refer to this [section](https://github.com/huggingface/optimum-habana#how-to-use-it) for using them with ðŸ¤— Optimum Habana. Besides, [this page](https://github.com/huggingface/optimum-habana/tree/main/examples) explains how to modify any [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch) from the ðŸ¤— Transformers library to make it work with ðŸ¤— Optimum Habana.If you find any issue while using those, please open an issue or a pull request.## Gaudi SetupPlease refer to Habana Gaudi's official [installation guide](https://docs.habana.ai/en/latest/Installation_Guide/index.html).&gt; Tests should be run in a Docker container based on Habana Docker images.&gt;&gt; The current version has been validated for SynapseAI 1.8.</longdescription>
</pkgmetadata>