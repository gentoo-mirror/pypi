<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>==========================celery-prometheus-exporter==========================.. image:: https://img.shields.io/docker/automated/zerok/celery-prometheus-exporter.svg?maxAge=2592000    :target: https://hub.docker.com/r/zerok/celery-prometheus-exporter/celery-prometheus-exporter is a little exporter for Celery related metrics inorder to get picked up by Prometheus. As with other exporters likemongodb\_exporter or node\_exporter this has been implemented as astandalone-service to make reuse easier across different frameworks.So far it provides access to the following metrics:* ``celery_tasks`` exposes the number of tasks currently known to the queue  grouped by ``state`` (RECEIVED, STARTED, ...).* ``celery_tasks_by_name`` exposes the number of tasks currently known to the queue  grouped by ``name`` and ``state``.* ``celery_workers`` exposes the number of currently probably alive workers* ``celery_task_latency`` exposes a histogram of task latency, i.e. the time until  tasks are picked up by a worker* ``celery_tasks_runtime_seconds`` tracks the number of seconds tasks take  until completed as histogramHow to use==========There are multiple ways to install this. The obvious one is using ``pip installcelery-prometheus-exporter`` and then using the ``celery-prometheus-exporter``command::  $ celery-prometheus-exporter  Starting HTTPD on 0.0.0.0:8888This package only depends on Celery directly, so you will have to installwhatever other dependencies you will need for it to speak with your broker ðŸ™‚Celery workers have to be configured to send task-related events:http://docs.celeryproject.org/en/latest/userguide/configuration.html#worker-send-task-events.Running ``celery-prometheus-exporter`` with the ``--enable-events`` argumentwill periodically enable events on the workers. This is useful because itallows running celery workers with events disabled, until``celery-prometheus-exporter`` is deployed, at which time events get enabledon the workers.Alternatively, you can use the bundle Makefile and Dockerfile to generate aDocker image.By default, the HTTPD will listen at ``0.0.0.0:8888``. If you want the HTTPDto listen to another port, use the ``--addr`` option or the environment variable``DEFAULT_ADDR``.By default, this will expect the broker to be available through``redis://redis:6379/0``, although you can change via environment variable``BROKER_URL``. If you're using AMQP or something else other thanRedis, take a look at the Celery documentation and install the additioinalrequirements ðŸ˜Š Also use the ``--broker`` option to specify a different brokerURL.If you need to pass additional options to your broker's transport use the``--transport-options``  option. It tries to read a dict from a JSON object.E.g. to set your master name when using Redis Sentinel for broker discovery:``--transport-options '{&quot;master_name&quot;: &quot;mymaster&quot;}'``Use ``--tz`` to specify the timezone the Celery app is using. Otherwise thesystems local time will be used.By default, buckets for histograms are the same as default ones in the prometheus client:https://github.com/prometheus/client_python#histogram.It means they are intended to cover typical web/rpc requests from milliseconds to seconds,so you may want to customize them.It can be done via environment variable ``RUNTIME_HISTOGRAM_BUCKETS`` for tasks runtime andvia environment variable ``LATENCY_HISTOGRAM_BUCKETS`` for tasks latency.Buckets should be passed as a list of float values separated by a comma.E.g. ``&quot;.005, .05, 0.1, 1.0, 2.5&quot;``.Use ``--queue-list`` to specify the list of queues that will have its lengthmonitored (Automatic Discovery of queues isn't supported right now, see limitations/caveats. You can use the `QUEUE_LIST` environment variable as well.If you then look at the exposed metrics, you should see something like this::  $ http get http://localhost:8888/metrics | grep celery_  # HELP celery_workers Number of alive workers  # TYPE celery_workers gauge  celery_workers 1.0  # HELP celery_tasks Number of tasks per state  # TYPE celery_tasks gauge  celery_tasks{state=&quot;RECEIVED&quot;} 3.0  celery_tasks{state=&quot;PENDING&quot;} 0.0  celery_tasks{state=&quot;STARTED&quot;} 1.0  celery_tasks{state=&quot;RETRY&quot;} 2.0  celery_tasks{state=&quot;FAILURE&quot;} 1.0  celery_tasks{state=&quot;REVOKED&quot;} 0.0  celery_tasks{state=&quot;SUCCESS&quot;} 8.0  # HELP celery_tasks_by_name Number of tasks per state  # TYPE celery_tasks_by_name gauge  celery_tasks_by_name{name=&quot;my_app.tasks.calculate_something&quot;,state=&quot;RECEIVED&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.calculate_something&quot;,state=&quot;PENDING&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.calculate_something&quot;,state=&quot;STARTED&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.calculate_something&quot;,state=&quot;RETRY&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.calculate_something&quot;,state=&quot;FAILURE&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.calculate_something&quot;,state=&quot;REVOKED&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.calculate_something&quot;,state=&quot;SUCCESS&quot;} 1.0  celery_tasks_by_name{name=&quot;my_app.tasks.fetch_some_data&quot;,state=&quot;RECEIVED&quot;} 3.0  celery_tasks_by_name{name=&quot;my_app.tasks.fetch_some_data&quot;,state=&quot;PENDING&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.fetch_some_data&quot;,state=&quot;STARTED&quot;} 1.0  celery_tasks_by_name{name=&quot;my_app.tasks.fetch_some_data&quot;,state=&quot;RETRY&quot;} 2.0  celery_tasks_by_name{name=&quot;my_app.tasks.fetch_some_data&quot;,state=&quot;FAILURE&quot;} 1.0  celery_tasks_by_name{name=&quot;my_app.tasks.fetch_some_data&quot;,state=&quot;REVOKED&quot;} 0.0  celery_tasks_by_name{name=&quot;my_app.tasks.fetch_some_data&quot;,state=&quot;SUCCESS&quot;} 7.0  # HELP celery_task_latency Seconds between a task is received and started.  # TYPE celery_task_latency histogram  celery_task_latency_bucket{le=&quot;0.005&quot;} 2.0  celery_task_latency_bucket{le=&quot;0.01&quot;} 3.0  celery_task_latency_bucket{le=&quot;0.025&quot;} 4.0  celery_task_latency_bucket{le=&quot;0.05&quot;} 4.0  celery_task_latency_bucket{le=&quot;0.075&quot;} 5.0  celery_task_latency_bucket{le=&quot;0.1&quot;} 5.0  celery_task_latency_bucket{le=&quot;0.25&quot;} 5.0  celery_task_latency_bucket{le=&quot;0.5&quot;} 5.0  celery_task_latency_bucket{le=&quot;0.75&quot;} 5.0  celery_task_latency_bucket{le=&quot;1.0&quot;} 5.0  celery_task_latency_bucket{le=&quot;2.5&quot;} 8.0  celery_task_latency_bucket{le=&quot;5.0&quot;} 11.0  celery_task_latency_bucket{le=&quot;7.5&quot;} 11.0  celery_task_latency_bucket{le=&quot;10.0&quot;} 11.0  celery_task_latency_bucket{le=&quot;+Inf&quot;} 11.0  celery_task_latency_count 11.0  celery_task_latency_sum 16.478713035583496  celery_queue_length{queue_name=&quot;queue1&quot;} 35.0  celery_queue_length{queue_name=&quot;queue2&quot;} 0.0Limitations===========* Among tons of other features celery-prometheus-exporter doesn't support stats  for multiple queues. As far as I can tell, only the routing key is exposed  through the events API which might be enough to figure out the final queue,  though.* This has only been tested with Redis so far.* At this point, you should specify the queues that will be monitored using an  environment variable or an arg (`--queue-list`).</longdescription>
</pkgmetadata>