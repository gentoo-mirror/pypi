<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># LLM-Client-SDK[![Test](https://github.com/uripeled2/llm-client-sdk/actions/workflows/test.yml/badge.svg)](https://github.com/uripeled2/llm-client-sdk/actions/workflows/test.yml)[![License: MIT](https://img.shields.io/github/license/uripeled2/llm-client-sdk.svg)](https://opensource.org/licenses/MIT)LLM-Client-SDK is an SDK for seamless integration with generative AI large language models(We currently support - OpenAI, Google, AI21, HuggingfaceHub, Aleph Alpha, Anthropic,Local models with transformers - and many more soon).Our vision is to provide async native and production ready SDK while creating a powerful and fast integration with different LLM without letting the user lose any flexibility (API params, endpoints etc.). *We also provide sync version, seemore details below in Usage section.## Base InterfaceThe package exposes two simple interfaces for communicating with LLMs (In the future, we will expand the interface to support more tasks like embeddings, list models, edits, etc.and we will add a standardized for LLMs param like max_tokens, temperature, etc.):```pythonfrom abc import ABC, abstractmethodfrom dataclasses import dataclass, fieldfrom typing import Any, Optionalfrom aiohttp import ClientSessionclass BaseLLMClient(ABC):    @abstractmethod    async def text_completion(self, prompt: str, **kwargs) -&gt; list[str]:        raise NotImplementedError()    async def get_tokens_count(self, text: str, **kwargs) -&gt; int:        raise NotImplementedError()@dataclassclass LLMAPIClientConfig:    api_key: str    session: ClientSession    base_url: Optional[str] = None    default_model: Optional[str] = None    headers: dict[str, Any] = field(default_factory=dict)class BaseLLMAPIClient(BaseLLMClient, ABC):    def __init__(self, config: LLMAPIClientConfig):        ...    @abstractmethod    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: int | None = None,                              temperature: Optional[float] = None, **kwargs) -&gt; list[str]:        raise NotImplementedError()    async def embedding(self, text: str, model: Optional[str] = None, **kwargs) -&gt; list[float]:        raise NotImplementedError()```## RequirementsPython 3.9+## InstallationIf you are worried about the size of the package you can install only the clients you need,by default we install none of the clients.For all current clients support```console$ pip install llm-client[all]```For only the base interface and some light LLMs clients (AI21 and Aleph Alpha)```console$ pip install llm-client```### Optional DependenciesFor all current api clients support```console$ pip install llm-client[api]```For only local client support```console$ pip install llm-client[local]```For sync support```console$ pip install llm-client[sync]```For only OpenAI support```console$ pip install llm-client[openai]```For only HuggingFace support```console$ pip install llm-client[huggingface]```## UsageUsing OpenAI directly through OpenAIClient - Maximum control and best practice in production```pythonimport osfrom aiohttp import ClientSessionfrom llm_client import ChatMessage, Role, OpenAIClient, LLMAPIClientConfigOPENAI_API_KEY = os.environ[&quot;API_KEY&quot;]OPENAI_ORG_ID = os.getenv(&quot;ORG_ID&quot;)async def main():    async with ClientSession() as session:        llm_client = OpenAIClient(LLMAPIClientConfig(OPENAI_API_KEY, session, default_model=&quot;text-davinci-003&quot;,                                                     headers={&quot;OpenAI-Organization&quot;: OPENAI_ORG_ID}))  # The headers are optional        text = &quot;This is indeed a test&quot;        print(&quot;number of tokens:&quot;, await llm_client.get_tokens_count(text))  # 5        print(&quot;generated chat:&quot;, await llm_client.chat_completion(              messages=[ChatMessage(role=Role.USER, content=&quot;Hello!&quot;)], model=&quot;gpt-3.5-turbo&quot;))  # ['Hi there! How can I assist you today?']        print(&quot;generated text:&quot;, await llm_client.text_completion(text))  # [' string\n\nYes, this is a test string. Test strings are used to']        print(&quot;generated embedding:&quot;, await llm_client.embedding(text))  # [0.0023064255, -0.009327292, ...]```Using LLMAPIClientFactory - Perfect if you want to move fast and to not handle the client session yourself```pythonimport osfrom llm_client import LLMAPIClientFactory, LLMAPIClientTypeOPENAI_API_KEY = os.environ[&quot;API_KEY&quot;]async def main():    async with LLMAPIClientFactory() as llm_api_client_factory:        llm_client = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.OPEN_AI,                                                               api_key=OPENAI_API_KEY,                                                               default_model=&quot;text-davinci-003&quot;)        await llm_client.text_completion(prompt=&quot;This is indeed a test&quot;)        await llm_client.text_completion(prompt=&quot;This is indeed a test&quot;, max_tokens=50)        # Or if you don't want to use asyncfrom llm_client import init_sync_llm_api_clientllm_client = init_sync_llm_api_client(LLMAPIClientType.OPEN_AI, api_key=OPENAI_API_KEY,                                      default_model=&quot;text-davinci-003&quot;)llm_client.text_completion(prompt=&quot;This is indeed a test&quot;)llm_client.text_completion(prompt=&quot;This is indeed a test&quot;, max_tokens=50)```Local model```pythonimport osfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizerfrom llm_client import LocalClientConfig, LocalClientasync def main():    try:        model = AutoModelForCausalLM.from_pretrained(os.environ[&quot;MODEL_NAME_OR_PATH&quot;])    except ValueError:        model = AutoModelForSeq2SeqLM.from_pretrained(os.environ[&quot;MODEL_NAME_OR_PATH&quot;])    tokenizer = AutoTokenizer.from_pretrained(os.environ[&quot;MODEL_NAME_OR_PATH&quot;])    llm_client = LocalClient(LocalClientConfig(model, tokenizer, os.environ[&quot;TENSORS_TYPE&quot;], os.environ[&quot;DEVICE&quot;]))    await llm_client.text_completion(prompt=&quot;This is indeed a test&quot;)    await llm_client.text_completion(prompt=&quot;This is indeed a test&quot;, max_tokens=50)# Or if you don't want to use asyncimport async_to_synctry:    model = AutoModelForCausalLM.from_pretrained(os.environ[&quot;MODEL_NAME_OR_PATH&quot;])except ValueError:    model = AutoModelForSeq2SeqLM.from_pretrained(os.environ[&quot;MODEL_NAME_OR_PATH&quot;])tokenizer = AutoTokenizer.from_pretrained(os.environ[&quot;MODEL_NAME_OR_PATH&quot;])llm_client = LocalClient(LocalClientConfig(model, tokenizer, os.environ[&quot;TENSORS_TYPE&quot;], os.environ[&quot;DEVICE&quot;]))llm_client = async_to_sync.methods(llm_client)llm_client.text_completion(prompt=&quot;This is indeed a test&quot;)llm_client.text_completion(prompt=&quot;This is indeed a test&quot;, max_tokens=50)```## ContributingContributions are welcome! Please check out the todos below, and feel free to open issue or a pull request.### Todo*The list is unordered*- [x] Add support for more LLMs  - [x] Anthropic  - [x] Google  - [ ] Cohere- [x] Add support for more functions via LLMs   - [x] embeddings  - [ ] chat  - [ ] list models  - [ ] edits  - [ ] more- [ ] Add contributing guidelines and linter- [ ] Create an easy way to run multiple LLMs in parallel with the same prompts- [x] Convert common models parameter  - [x] temperature   - [x] max_tokens  - [ ] more### DevelopmentTo install the package in development mode, run the following command:```console$ pip install -e &quot;.[all,test]&quot;```To run the tests, run the following command:```console$ pytest tests```If you want to add a new LLMClient you need to implement BaseLLMClient or BaseLLMAPIClient.If you are adding a BaseLLMAPIClient you also need to add him in LLMAPIClientFactory.You can add dependencies to your LLMClient in [pyproject.toml](pyproject.toml) also make sure you are adding amatrix.flavor in [test.yml](.github%2Fworkflows%2Ftest.yml). </longdescription>
</pkgmetadata>