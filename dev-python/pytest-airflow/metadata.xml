<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>pytest-airflow: pytest support for airflow==========================================.. image:: https://circleci.com/gh/Flowminder/pytest-airflow.svg?style=svg&amp;circle-token=7e32dee2ea47f7961e93b9016d44bda103b3bede    :target: https://circleci.com/gh/Flowminder/pytest-airflow``pytest-airflow`` is a plugin for ``pytest`` that allows tests to be runwithin an Airflow DAG.``pytest`` handles test discovery and function encapsulation, allowingtest declaration to operate in the usual way with the use ofparametrization, fixtures and marks. The generated test callables testsare eventually passed to ``PythonOperators`` that are run as separateAirflow tasks.Installation------------``pytest-airflow`` can be installed with ``pip``:.. code-block:: bash    pip install pytest-airflowNote~~~~``pytest-airflow`` depends on Apache Airflow, which requires``export SLUGIFY_USES_TEXT_UNIDECODE=yes`` to be specified before install. Seethe `Airflow install instructions &lt;https://airflow.apache.org/installation.html&gt;`_for background on this requirement.Usage-----When running pytest from the command line, the plugin will collect thetests and construct the DAG. It will output a DAG tree view in addition tothe requested output... code-block:: bash        $ pytest --airflowWhen invoking pytest from python code, ``pytest.main()`` willreturn a reference to the DAG... code-block:: python        import pytest        dag, source, sink  = pytest.main([&quot;--airflow&quot;, &quot;--dag-id&quot;, &quot;FOO&quot;])The plugin generates two tasks at the start and end of the workflow whichrepresent the source and sink for the tests. The source task isresponsible for branching and the sink task for reporting. The former andthe later are called ``__pytest_source`` and ``__pytest_sink`` by defaultrespectively. In case the user desire to change those defaults name it is possible to make use of the ``source`` and ``sink`` flags as below... code-block:: bash   $ pytest --airflow --source branch --sink reportIf the plugin is installed, ``pytest`` will automatically use it. Savingthe script above in one's DAG folder is enough to trigger the DAG. Notethat ``pytest`` will be evaluated from the path where the Airflowscheduler is invoked.Plugin------The plugin creates a DAG of the form ``source -&gt; tests -&gt; sink``,``source`` marks tests that will be executed and skipped, ``tests``executes the selected tests as separate tasks and ``sink`` reports testoutcome.Branching~~~~~~~~~Airflow requires that any DAG be completely defined before it is run. Soby the nature of Airflow, we cannot use ``pytest`` to collect tests on thefly based on the results of ``source``. Rather, ``pytest`` is used togenerate the set of all possible desired tests before ``source`` isevaluated. The user can use all of the available flags to ``pytest`` (eg.``-m``, ``-k``, paths) to narrow the set of initial desired tests down.The plugin makes a source task called ``__pytest_source`` by defaultavailable. This task allows skipping unwanted tests for a particular DAGrun using the following configuration keys:* ``marks``: a list of marks, it filters tests in the same way as the  ``-m`` flag operates when collecting tests with ``pytest``.* ``keywords``: a list of keywords, it filters tests in the same way as  the ``-k`` flag operates when collecting tests with ``pytest``.Fixtures~~~~~~~~The plugin defers test execution for the DAG run. That means when calling``pytest``, the tests will be collected and the associated callables willbe generated and passed to the ``PythonOperator``. If the DAG is compiledwithout any errors, ``pytest`` will return the DAG and will exitsucessfully. That means that it will report that all tests passed, whichonly means that the DAG was compiled without any problems.Fixture setup and teardown are executed at the moment of DAG compilation.That means that fixtures such as database connections will not beavailable at the moment of test execution during a DAG run.In order to get around this problem there are two alternatives. The firstalternative is to implement a fixture as a factory, and handling fixtureteardown on the test itself.Alternatively, the plugin allows deferred fixture setup and teardown. Inorder to achieve deferred execution, the name of the fixture must beprefixed with ``defer_`` or it must depend on the reserved fixture``task_ctx``. That means that the plugin defer the execution of suchfixtures until the DAG is run. Fixtures that depend on a deferred fixturewill also have its execution deferred for later.The reserved fixture ``task_ctx`` is always deferred. This fixtureevaluates the Airflow task context and is available to the user whenwritting tests. Using this fixture, the user has access to all the itemsthat would be available to ``kwargs`` when setting ``provide_context`` to``True`` when using the ``PythonOperator`` in Airflow.All in all, collection time fixture execution should be used for testparametrization, for generating expensive resources that can be madeavailable to tests as copies and for generating fixture factories. On theother hand, deferred fixtures are great for database connections and otherresources that need to be recycled at each test execution.Reporting~~~~~~~~~Finally, the sink task ``report`` can be used for reporting purposes and forcommunicating test results to other DAGs using the ``xcom`` channel.  The usercan supply its own ``dag_report`` fixture for customizing its reportingrequirements. The plugin expects the following fixture signature, scoped at the``session`` level... code-block:: python        @pytest.fixture(scope=&quot;session&quot;)        def dag_report(**kwargs):          ...DAG Configuration~~~~~~~~~~~~~~~~~The user can configure the DAG using two reserved fixtures for this. Thefixtures must be scoped at the ``session`` level and its location should coverall the collected test items. The most narrow fixture that covers all of thecollected items will be selected. Otherwise, the plugin uses default values forthose fixtures. Apart from that, fixture execution and discovery should operatein the usual way.The first fixture is ``dag_default_args``, which should returna dictionary with ``default_args`` that will be passed to the daginitialization. The default returns.. code-block:: python      { &quot;owner&quot;: &quot;airflow&quot;,        &quot;start_date&quot;: datetime.datetime(2018, 1, 1),        &quot;end_date&quot;: None,        &quot;depends_on_past&quot;: False,      }The second fixture is ``dag`` which should return an Airflow DAG that willbe used throughout the script.If the user desires only to modify the name of the DAG, it is possible tosimply pass the ``--dag-id`` flag to the ``pytest`` cmdline.If the user desires to integrate the DAG generated from this plugin inher/his own DAG. One option is to define the whole DAG inside the same``conftest.py`` file that is used by ``pytest`` to initialize the tests.If this is not possible and the DAG must be defined separately, it ispossible to create a custom ``pytest`` plugin in the same file where theDAG is created and pass such plugin to ``pytest.main`` as the examplebelow illustrates... code-block:: python        import pytest        from airflow import DAG        my_dag = DAG(dag_id=&quot;foo&quot;, start_date = &quot;2017-01-01&quot;)        class MyPlugin:          @pytest.fixture(scope=&quot;session&quot;)          def dag(self):            return my_dag        my_dag, source, sink = pytest.main([&quot;--airflow&quot;], plugins=[MyPlugin()])License-------This Source Code Form is subject to the terms of the Mozilla PublicLicense, v. 2.0. If a copy of the MPL was not distributed with thisfile, You can obtain one at http://mozilla.org/MPL/2.0/.</longdescription>
</pkgmetadata>