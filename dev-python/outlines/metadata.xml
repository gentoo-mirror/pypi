<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;./docs/source/_static/logo.png&quot; alt=&quot;Outlines Logo&quot; width=300&gt;&lt;/img&gt;# OutlinesBuild _reliable_ workflows based on interactions with generative models.[Prompting](#prompting) •[Controlled generation](#controlled-generation) •[Agents](#agents-example) •[Sampling](#sampling-uncertainty-simulation-based-inference) •[Parallel execution](#vectorization-and-parallel-execution) •[Examples](#examples)&lt;/div&gt;**Outlines** allows you to control and diagnose interactions with LLMs more effectively. Modern language models are powerful and versatile, but the way they interface with existing systems [can be very brittle](https://github.com/Significant-Gravitas/Auto-GPT/labels/invalid_json), their outputs [can be unreliable](https://arxiv.org/abs/2302.04023), and complex workflows (agents) can introduce a lot of error-prone code duplication. Outlines provides robust prompting primitives that separate the prompting from the execution logic and lead to simple implementations of few-shot generations, ReAct, meta-prompting, agents, etc. Outlines helps developers control text generation and produce predictable outputs that make the interaction with user code more robust. Its sampling-first approach allows one to diagnose issues with model-generated output more easily, and implement more robust generation methods such as [self-consistency](https://arxiv.org/abs/2203.11171) or [DiVeRSe](https://arxiv.org/abs/2206.02336).**Outlines** is designed as a library that integrates well with the broader Python environment. Generation can be interleaved with control flow or custom function calls, prompts can be imported from other modules or libraries.## Features- [x] Simple and powerful prompting primitives based on the [Jinja templating engine](https://jinja.palletsprojects.com/).- [x] Interleave completions with loops, conditionals, and custom Python functions- [x] Caching of generations- [x] Integration with OpenAI and HuggingFace models- [x] Controlled generation, including multiple choice, type constraints and dynamic stopping- [x] Sampling of multiple sequences- [x] Vectorized execution## Installation**Outlines** is available on PyPi:``` bashpip install outlines```## PromptingWriting prompts by concatenating strings in pure Python quickly becomescumbersome: the prompt building logic gets entangled with the rest of theprogram, and the structure of the rendered prompt is obfuscated.**Outlines**makes it easier to write and manage prompts by encapsulating templates inside&quot;template functions&quot;.These functions make it possible to neatly separate the prompt logic from thegeneral program logic; they can be imported from other modules and libraries.Template functions require no superfluous abstraction, they use the Jinja2templating engine to help build complex prompts in a concise manner:``` pythonimport outlines.text as textimport outlines.models as modelsexamples = [    (&quot;The food was digusting&quot;, &quot;Negative&quot;),    (&quot;We had a fantastic night&quot;, &quot;Positive&quot;),    (&quot;Recommended&quot;, &quot;Positive&quot;),    (&quot;The waiter was rude&quot;, &quot;Negative&quot;)]@text.promptdef labelling(to_label, examples):    &quot;&quot;&quot;You are a sentiment-labelling assistant.    {% for example in examples %}    {{ example[0] }} // {{ example[1] }}    {% endfor %}    {{ to_label }} //    &quot;&quot;&quot;complete = models.text_completion.openai(&quot;text-davinci-003&quot;)prompt = labelling(&quot;Just awesome&quot;, examples)answer = complete(prompt)```## Chaining with loops and conditionals ([example](https://github.com/normal-computing/outlines/blob/readme/examples/react.py))**Outlines** comes with very few abstractions, and is designed to blend into existing code and integrate with the rest of the ecosystem.``` pythonreviews = [&quot;Just awesome&quot;, &quot;Avoid&quot;, &quot;Will come back&quot;]def send_notification(review):    &quot;&quot;&quot;This function sends a notification with the review's content.&quot;&quot;&quot;    ...for review in reviews:    prompt = labelling(review, examples)    answer = model(prompt)    if answer == &quot;Positive&quot;:        send_notification(review)```## Agents ([example](https://github.com/normal-computing/outlines/blob/readme/examples/babyagi.py))**Outlines** makes building agents like [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT), [BabyAGI](https://github.com/yoheinakajima/babyagi), [ViperGPT](https://viper.cs.columbia.edu/) or [Transformers Agent](https://huggingface.co/docs/transformers/transformers_agents) easier by removing boilerplate prompting code.### ToolsWe can teach language models to call external functions to get additional informations or perform tasks, by encoding the functions' description in the prompt. To avoid duplicating information between the function definition and the description passed to the prompt, we define custom Jinja filters that can extract the function's name, description, signature and source:``` pythonfrom typing import Callable, Listimport outlines.text as textdef google_search(query: str):    &quot;&quot;&quot;Google Search&quot;&quot;&quot;    passdef wikipedia_search(query: str):    &quot;&quot;&quot;Wikipedia Search&quot;&quot;&quot;    pass@text.promptdef agent(tools: List[Callable]):    &quot;&quot;&quot;AVAILABLE COMMANDS:    {% for tool in tools %}    TOOL    {{ tool | name }}, {{ tool | description }}, args: {{ tool | signature }}    {{ tool | source }}    {% endfor %}    &quot;&quot;&quot;prompt = my_commands([google_search, wikipedia_search])```### Response modelsWe can instruct models to return their output in a pre-defined format, often JSON. To avoid duplicating information between the function definition and the description passed to the prompt we define a custom Jinja filter that can extract the expected response's schema:``` pythonfrom pydantic import BaseModelimport outlines.text as textclass Joke(BaseModel):    joke: str    explanation: str@text.promptdef joke_ppt(response_model):    &quot;&quot;&quot;Tell a joke and explain why the joke is funny.    RESPONSE FORMAT:    {{ response_model | schema }}    &quot;&quot;&quot;joke_ppt(Joke)# Tell a joke and explain why the joke is funny.## RESPONSE FORMAT:# {#    &quot;joke&quot;: &quot;The joke&quot;#    &quot;explanation&quot;: &quot;The explanation of why the joke is funny&quot;#  }```## Controlled generationThe first step towards reliability of systems that include large language models is to ensure that there is a well-defined interface between their output and user-defined code. **Outlines** provides ways to control the generation of language models to make their output more predictable.You can stop the generation after a given sequence has been found:``` pythonanswer = model(&quot;Tell me a one-sentence joke.&quot;, stop_at=[&quot;.&quot;])```You can reduce the completion to a choice between multiple possibilities:``` pythonprompt = labelling(&quot;Just awesome&quot;, examples)answer = model(prompt, is_in=[&quot;Positive&quot;, &quot;Negative&quot;])```You can require the generated sequence to be an int or a float:``` pythonimport outlines.models as modelsmodel = models.text_completion.hf(&quot;sshleifer/tiny-gpt2&quot;)answer = model(&quot;2 + 2 = &quot;, type=&quot;int&quot;)print(answer)# 4model = models.text_completion.hf(&quot;sshleifer/tiny-gpt2&quot;)answer = model(&quot;1.7 + 3.2 = &quot;, type=&quot;float&quot;)print(answer)# 4.9```## Sampling ([uncertainty](https://github.com/normal-computing/outlines/blob/readme/examples/sampling.ipynb), [simulation-based inference](https://github.com/normal-computing/outlines/blob/readme/examples/simulation_based_inference.ipynb))Outlines is strictly sampling based, and focused on using methods such as [self-consistency](https://arxiv.org/abs/2203.11171), [adaptive consistency](https://arxiv.org/abs/2305.11860), [DiVeRSe](https://arxiv.org/abs/2206.02336), [Tree of thoughts](https://arxiv.org/abs/2305.10601), [lattice sampling](https://arxiv.org/abs/2112.07660), etc. Several samples can be obtained using the `num_samples` keyword argument:``` pythonimport outlines.models as modelsmodel = models.text_completion.hf(&quot;sshleifer/tiny-gpt2&quot;)answer = model(&quot;2 + 2 = &quot;, num_samples=5)print(answer)# [4, 5, 4, 4, 4]```The focus on sampling allows us to explore different ideas, such as [using the diversity of answers to evaluate the model's uncertainty](https://github.com/normal-computing/outlines/blob/readme/examples/sampling.ipynb), or [simulation-based inference to optimize the prompt](https://github.com/normal-computing/outlines/blob/readme/examples/simulation_based_inference.ipynb).## Vectorization and parallel executionYou can pass prompts in a NumPy array to Outlines models:``` pythonimport numpy as npimport outlines.models as modelsmodel = models.text_completion.openai(&quot;text-davinci-003&quot;)prompts = [    [&quot;Translate 'Hello' in Italian&quot;, &quot;Translate 'Hello' in French&quot;],    [&quot;Translate 'Hello' in Spanish&quot;, &quot;Translate 'Hello' in German&quot;],]answers = model(prompts)print(answers.shape)# (2, 2)```Outlines also provide a `outlines.vectorize` decorator that will vectorize any function. If the function is async the requests will be run concurrently:``` pythonimport aiohttpimport numpy as npimport outlines@outlines.vectorizeasync def wikipedia_search(query):    url = f&quot;https://en.wikipedia.org/w/api.php?format=json&amp;action=query&amp;prop=extracts&amp;exintro&amp;explaintext&amp;redirects=1&amp;titles={query}&amp;origin=*&quot;    async with aiohttp.ClientSession() as session:        async with session.get(url) as response:            return await response.text()results = wikipedia_search([[&quot;Cat&quot;, &quot;Dog&quot;],[&quot;Bird&quot;, &quot;Horse&quot;]])print(results.shape)# (2, 2)```This feature allows you to run multiple workflows in parallel, for instance to avoid overfitting when iterating over a workflow or in production to run workflows over several different inputs.## Contributing### What contributions?We curently only accept bug fixes and documentation contributions. If you have a feature request, please start a new [discussions](https://github.com/normal-computing/outlines/discussions). The issue tracker is only intended for actionable items.### How to contribute?Run `pip install -e .[test]` or `conda env create -f environment.yml`. To build the documentation you will also need to run `pip install -r requirements-doc.txt`.Before pushing your code to repository please run `pre-commit run --all-files` and `pytest` to make sure that the code is formatted correctly and that the tests pass.Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback.## Examples- [Pick the odd one out](https://github.com/normal-computing/outlines/blob/main/examples/pick_odd_one_out.py)- [Meta prompting](https://github.com/normal-computing/outlines/blob/main/examples/meta_prompting.py)- [ReAct](https://github.com/normal-computing/outlines/blob/main/examples/meta_prompting.py)- [Generate code to solve math problems](https://github.com/normal-computing/outlines/blob/main/examples/dust/math-generate-code.py)- [BabyAGI](https://github.com/normal-computing/outlines/blob/main/examples/babyagi.py)- [Uncertainty](https://github.com/normal-computing/outlines/blob/main/examples/sampling.ipynb)- [Simulation-based inference](https://github.com/normal-computing/outlines/blob/main/examples/simulation_based_inference.ipynb)</longdescription>
</pkgmetadata>