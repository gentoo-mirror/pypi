<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Vltava[1]: https://nlp.fi.muni.cz/czech-morphology-analyser/[2]: https://ufal.mff.cuni.cz/morphodita[3]: https://github.com/petrpulc/python-majka[4]: https://pypi.org/project/ufal.morphodita/![PyPI](https://badge.fury.io/py/vltava.svg)![Test](https://github.com/jancervenka/vltava/actions/workflows/test.yml/badge.svg)![Publish](https://github.com/jancervenka/vltava/actions/workflows/publish.yml/badge.svg)Opinionated Czech language processing.The processor takes in raw documents and applies basic preprocessing(such as tags and accent striping) and lemmatization using either [Majka][1]or [MorphoDiTa][2].```pythonfrom vltava import DocumentProcessordoc = &quot;v televizi říkali, že zítra pršet nebude&quot;document_processor = DocumentProcessor()result = document_processor.process(doc)# result is ['televize', 'rikat', 'zitra', 'prset', 'byt']````DocumentProcessor` supports multiprocessing when dealing withlarge collections of documents.```pythonfrom vltava import DocumentProcessordocs = [&quot;Ahoj, jak se máš?&quot;] * 100result = DocumentProcessor().process_from_iterable(docs, n_jobs=2)```## Installation```bashpip install vltava```## BackendThe package is using two different backends for finding Czech lemmas:[Majka][1], [MorphoDiTa][2]. Check out the links for more information.The required binary files are contained directly in the package.- [Majka Python API][3]- [MorphoDiTa Python API][4]## Public API### `vltava.DocumentProcessor````pythonvltava.DocumentProcessor(backend: str = &quot;majka&quot;)```Initializes `DocumentProcessor` with the selected `backend`.__Methods:__```pythonDocumentProcessor.process(    self, doc: str, tokenize: bool = True) -&gt; Union[str, List[str]]```Processes the input `doc` and returns it as a processedstring or a list of processed tokens, if `tokenize` is `True`.```pythonDocumentProcessor.process_from_iterable(    self, docs: Iterable[str], tokenize: bool = True, n_jobs: int = 1) -&gt; Union[Iterable[str], Iterable[List[str]]]:```Processes the input `docs` collection of documents. Result is eitheran iterable of processed strings or an iterable of lists of processedtokens (if `tokenize` is `True`).If `n_jobs` is greater than one, multiple worker are launched toprocess the documents.</longdescription>
</pkgmetadata>