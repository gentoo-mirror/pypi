<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># PyTorch implementation of Normalizer-Free Networks and Adaptive Gradient Clipping![Python Package](https://github.com/vballoli/nfnets-pytorch/workflows/Upload%20Python%20Package/badge.svg)![Docs](https://readthedocs.org/projects/nfnets-pytorch/badge/?version=latest)Paper: https://arxiv.org/abs/2102.06171.pdfOriginal code: https://github.com/deepmind/deepmind-research/tree/master/nfnetsBlog post: https://tourdeml.github.io/blog/posts/2021-03-31-adaptive-gradient-clipping/. Feel free to subscribe to the newsletter, and leave a comment if you have anything to add/suggest publicly.Do star this repository if it helps your work, and [don't forget to cite](https://github.com/vballoli/nfnets-pytorch#cite-this-repository) if you use this code in your research!# InstallationInstall from PyPi:`pip3 install nfnets-pytorch`or install the latest code using:`pip3 install git+https://github.com/vballoli/nfnets-pytorch`# Usage## WSConv2dUse `WSConv1d, WSConv2d, ScaledStdConv2d(timm)` and `WSConvTranspose2d` like any other `torch.nn.Conv2d` or `torch.nn.ConvTranspose2d` modules.```pythonimport torchfrom torch import nnfrom nfnets import WSConv2d, WSConvTranspose2d, ScaledStdConv2dconv = nn.Conv2d(3,6,3)w_conv = WSConv2d(3,6,3)conv_t = nn.ConvTranspose2d(3,6,3)w_conv_t = WSConvTranspose2d(3,6,3)```## Generic AGC (recommended)```pythonimport torchfrom torch import nn, optimfrom torchvision.models import resnet18from nfnets import WSConv2dfrom nfnets.agc import AGC # Needs testingconv = nn.Conv2d(3,6,3)w_conv = WSConv2d(3,6,3)optim = optim.SGD(conv.parameters(), 1e-3)optim_agc = AGC(conv.parameters(), optim) # Needs testing# Ignore fc of a model while applying AGC.model = resnet18()optim = torch.optim.SGD(model.parameters(), 1e-3)optim = AGC(model.parameters(), optim, model=model, ignore_agc=['fc'])```## SGD - Adaptive Gradient ClippingSimilarly, use `SGD_AGC` like `torch.optim.SGD````python# The generic AGC is preferable since the paper recommends not applying AGC to the last fc layer.import torchfrom torch import nn, optimfrom nfnets import WSConv2d, SGD_AGCconv = nn.Conv2d(3,6,3)w_conv = WSConv2d(3,6,3)optim = optim.SGD(conv.parameters(), 1e-3)optim_agc = SGD_AGC(conv.parameters(), 1e-3)```## Using it within any PyTorch model`replace_conv` replaces the convolution in your model with the convolution class and replaces the batchnorm with identity. While the identity is not ideal, it shouldn't cause a major difference in the latency. ```pythonimport torchfrom torch import nnfrom torchvision.models import resnet18from nfnets import replace_conv, WSConv2d, ScaledStdConv2dmodel = resnet18()replace_conv(model, WSConv2d) # This repo's original implementationreplace_conv(model, ScaledStdConv2d) # From timm&quot;&quot;&quot;class YourCustomClass(nn.Conv2d):  ...replace_conv(model, YourCustomClass)&quot;&quot;&quot;```# DocsFind the docs at [readthedocs](https://nfnets-pytorch.readthedocs.io/en/latest/)# Cite Original WorkTo cite the original paper, use:```@article{brock2021high,  author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},  title={High-Performance Large-Scale Image Recognition Without Normalization},  journal={arXiv preprint arXiv:},  year={2021}}```# Cite this repositoryTo cite this repository, use:```@misc{nfnets2021pytorch,  author = {Vaibhav Balloli},  title = {A PyTorch implementation of NFNets and Adaptive Gradient Clipping},  year = {2021},  howpublished = {\url{https://github.com/vballoli/nfnets-pytorch}}}```</longdescription>
</pkgmetadata>