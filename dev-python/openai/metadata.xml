<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># OpenAI Python API library[![PyPI version](https://img.shields.io/pypi/v/openai.svg)](https://pypi.org/project/openai/)The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.7+application. The library includes type definitions for all request params and response fields,and offers both synchronous and asynchronous clients powered by [httpx](https://github.com/encode/httpx).It is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).## DocumentationThe API documentation can be found [here](https://platform.openai.com/docs).## Installation&gt; [!IMPORTANT]&gt; The SDK was rewritten in v1, which was released November 6th 2023. See the [v1 migration guide](https://github.com/openai/openai-python/discussions/742), which includes scripts to automatically update your code.```shpip install openai```## UsageThe full API of this library can be found in [api.md](https://www.github.com/openai/openai-python/blob/main/api.md).```pythonfrom openai import OpenAIclient = OpenAI(    # defaults to os.environ.get(&quot;OPENAI_API_KEY&quot;)    api_key=&quot;My API Key&quot;,)chat_completion = client.chat.completions.create(    messages=[        {            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;Say this is a test&quot;,        }    ],    model=&quot;gpt-3.5-turbo&quot;,)```While you can provide an `api_key` keyword argument,we recommend using [python-dotenv](https://pypi.org/project/python-dotenv/)to add `OPENAI_API_KEY=&quot;My API Key&quot;` to your `.env` fileso that your API Key is not stored in source control.## Async usageSimply import `AsyncOpenAI` instead of `OpenAI` and use `await` with each API call:```pythonimport asynciofrom openai import AsyncOpenAIclient = AsyncOpenAI(    # defaults to os.environ.get(&quot;OPENAI_API_KEY&quot;)    api_key=&quot;My API Key&quot;,)async def main() -&gt; None:    chat_completion = await client.chat.completions.create(        messages=[            {                &quot;role&quot;: &quot;user&quot;,                &quot;content&quot;: &quot;Say this is a test&quot;,            }        ],        model=&quot;gpt-3.5-turbo&quot;,    )asyncio.run(main())```Functionality between the synchronous and asynchronous clients is otherwise identical.## Streaming ResponsesWe provide support for streaming responses using Server Side Events (SSE).```pythonfrom openai import OpenAIclient = OpenAI()stream = client.chat.completions.create(    model=&quot;gpt-4&quot;,    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}],    stream=True,)for part in stream:    print(part.choices[0].delta.content or &quot;&quot;)```The async client uses the exact same interface.```pythonfrom openai import AsyncOpenAIclient = AsyncOpenAI()stream = await client.chat.completions.create(    prompt=&quot;Say this is a test&quot;,    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test&quot;}],    stream=True,)async for part in stream:    print(part.choices[0].delta.content or &quot;&quot;)```## Module-level client&gt; [!IMPORTANT]&gt; We highly recommend instantiating client instances instead of relying on the global client.We also expose a global client instance that is accessible in a similar fashion to versions prior to v1.```pyimport openai# optional; defaults to `os.environ['OPENAI_API_KEY']`openai.api_key = '...'# all client options can be configured just like the `OpenAI` instantiation counterpartopenai.base_url = &quot;https://...&quot;openai.default_headers = {&quot;x-foo&quot;: &quot;true&quot;}completion = openai.chat.completions.create(    model=&quot;gpt-4&quot;,    messages=[        {            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;How do I output all files in a directory using Python?&quot;,        },    ],)print(completion.choices[0].message.content)```The API is the exact same as the standard client instance based API.This is intended to be used within REPLs or notebooks for faster iteration, **not** in application code.We recommend that you always instantiate a client (e.g., with `client = OpenAI()`) in application code because:- It can be difficult to reason about where client options are configured- It's not possible to change certain client options without potentially causing race conditions- It's harder to mock for testing purposes- It's not possible to control cleanup of network connections## Using typesNested request parameters are [TypedDicts](https://docs.python.org/3/library/typing.html#typing.TypedDict). Responses are [Pydantic models](https://docs.pydantic.dev), which provide helper methods for things like:- Serializing back into JSON, `model.model_dump_json(indent=2, exclude_unset=True)`- Converting to a dictionary, `model.model_dump(exclude_unset=True)`Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set `python.analysis.typeCheckingMode` to `basic`.## PaginationList methods in the OpenAI API are paginated.This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:```pythonimport openaiclient = OpenAI()all_jobs = []# Automatically fetches more pages as needed.for job in client.fine_tuning.jobs.list(    limit=20,):    # Do something with job here    all_jobs.append(job)print(all_jobs)```Or, asynchronously:```pythonimport asyncioimport openaiclient = AsyncOpenAI()async def main() -&gt; None:    all_jobs = []    # Iterate through items across all pages, issuing requests as needed.    async for job in client.fine_tuning.jobs.list(        limit=20,    ):        all_jobs.append(job)    print(all_jobs)asyncio.run(main())```Alternatively, you can use the `.has_next_page()`, `.next_page_info()`, or `.get_next_page()` methods for more granular control working with pages:```pythonfirst_page = await client.fine_tuning.jobs.list(    limit=20,)if first_page.has_next_page():    print(f&quot;will fetch next page using these details: {first_page.next_page_info()}&quot;)    next_page = await first_page.get_next_page()    print(f&quot;number of items we just fetched: {len(next_page.data)}&quot;)# Remove `await` for non-async usage.```Or just work directly with the returned data:```pythonfirst_page = await client.fine_tuning.jobs.list(    limit=20,)print(f&quot;next page cursor: {first_page.after}&quot;)  # =&gt; &quot;next page cursor: ...&quot;for job in first_page.data:    print(job.id)# Remove `await` for non-async usage.```## Nested paramsNested parameters are dictionaries, typed using `TypedDict`, for example:```pythonfrom openai import OpenAIclient = OpenAI()completion = client.chat.completions.create(    messages=[        {            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;Can you generate an example json object describing a fruit?&quot;,        }    ],    model=&quot;gpt-3.5-turbo&quot;,    response_format={&quot;type&quot;: &quot;json_object&quot;},)```## File UploadsRequest parameters that correspond to file uploads can be passed as `bytes`, a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance or a tuple of `(filename, contents, media type)`.```pythonfrom pathlib import Pathfrom openai import OpenAIclient = OpenAI()client.files.create(    file=Path(&quot;input.jsonl&quot;),    purpose=&quot;fine-tune&quot;,)```The async client uses the exact same interface. If you pass a [`PathLike`](https://docs.python.org/3/library/os.html#os.PathLike) instance, the file contents will be read asynchronously automatically.## Handling errorsWhen the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of `openai.APIConnectionError` is raised.When the API returns a non-success status code (that is, 4xx or 5xxresponse), a subclass of `openai.APIStatusError` is raised, containing `status_code` and `response` properties.All errors inherit from `openai.APIError`.```pythonimport openaifrom openai import OpenAIclient = OpenAI()try:    client.fine_tunes.create(        training_file=&quot;file-XGinujblHPwGLSztz8cPS8XY&quot;,    )except openai.APIConnectionError as e:    print(&quot;The server could not be reached&quot;)    print(e.__cause__)  # an underlying Exception, likely raised within httpx.except openai.RateLimitError as e:    print(&quot;A 429 status code was received; we should back off a bit.&quot;)except openai.APIStatusError as e:    print(&quot;Another non-200-range status code was received&quot;)    print(e.status_code)    print(e.response)```Error codes are as followed:| Status Code | Error Type                 || ----------- | -------------------------- || 400         | `BadRequestError`          || 401         | `AuthenticationError`      || 403         | `PermissionDeniedError`    || 404         | `NotFoundError`            || 422         | `UnprocessableEntityError` || 429         | `RateLimitError`           || &gt;=500       | `InternalServerError`      || N/A         | `APIConnectionError`       |### RetriesCertain errors are automatically retried 2 times by default, with a short exponential backoff.Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict,429 Rate Limit, and &gt;=500 Internal errors are all retried by default.You can use the `max_retries` option to configure or disable retry settings:```pythonfrom openai import OpenAI# Configure the default for all requests:client = OpenAI(    # default is 2    max_retries=0,)# Or, configure per-request:client.with_options(max_retries=5).chat.completions.create(    messages=[        {            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;How can I get the name of the current day in Node.js?&quot;,        }    ],    model=&quot;gpt-3.5-turbo&quot;,)```### TimeoutsBy default requests time out after 10 minutes. You can configure this with a `timeout` option,which accepts a float or an [`httpx.Timeout`](https://www.python-httpx.org/advanced/#fine-tuning-the-configuration) object:```pythonfrom openai import OpenAI# Configure the default for all requests:client = OpenAI(    # default is 60s    timeout=20.0,)# More granular control:client = OpenAI(    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),)# Override per-request:client.with_options(timeout=5 * 1000).chat.completions.create(    messages=[        {            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;How can I list all files in a directory using Python?&quot;,        }    ],    model=&quot;gpt-3.5-turbo&quot;,)```On timeout, an `APITimeoutError` is thrown.Note that requests that time out are [retried twice by default](#retries).## Advanced### LoggingWe use the standard library [`logging`](https://docs.python.org/3/library/logging.html) module.You can enable logging by setting the environment variable `OPENAI_LOG` to `debug`.```shell$ export OPENAI_LOG=debug```### How to tell whether `None` means `null` or missingIn an API response, a field may be explicitly `null`, or missing entirely; in either case, its value is `None` in this library. You can differentiate the two cases with `.model_fields_set`:```pyif response.my_field is None:  if 'my_field' not in response.model_fields_set:    print('Got json like {}, without a &quot;my_field&quot; key present at all.')  else:    print('Got json like {&quot;my_field&quot;: null}.')```### Accessing raw response data (e.g. headers)The &quot;raw&quot; Response object can be accessed by prefixing `.with_raw_response.` to any HTTP method call.```pyfrom openai import OpenAIclient = OpenAI()response = client.chat.completions.with_raw_response.create(    messages=[{        &quot;role&quot;: &quot;user&quot;,        &quot;content&quot;: &quot;Say this is a test&quot;,    }],    model=&quot;gpt-3.5-turbo&quot;,)print(response.headers.get('X-My-Header'))completion = response.parse()  # get the object that `chat.completions.create()` would have returnedprint(completion)```These methods return an [`APIResponse`](https://github.com/openai/openai-python/tree/main/src/openai/_response.py) object.### Configuring the HTTP clientYou can directly override the [httpx client](https://www.python-httpx.org/api/#client) to customize it for your use case, including:- Support for proxies- Custom transports- Additional [advanced](https://www.python-httpx.org/advanced/#client-instances) functionality```pythonimport httpxfrom openai import OpenAIclient = OpenAI(    # Or use the `OPENAI_BASE_URL` env var    base_url=&quot;http://my.test.server.example.com:8083&quot;,    http_client=httpx.Client(        proxies=&quot;http://my.test.proxy.example.com&quot;,        transport=httpx.HTTPTransport(local_address=&quot;0.0.0.0&quot;),    ),)```### Managing HTTP resourcesBy default the library closes underlying HTTP connections whenever the client is [garbage collected](https://docs.python.org/3/reference/datamodel.html#object.__del__). You can manually close the client using the `.close()` method if desired, or with a context manager that closes when exiting.## Microsoft Azure OpenAITo use this library with [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview), use the `AzureOpenAI`class instead of the `OpenAI` class.&gt; [!IMPORTANT]&gt; The Azure API shape differs from the core API shape which means that the static types for responses / params&gt; won't always be correct.```pyfrom openai import AzureOpenAI# gets the API Key from environment variable AZURE_OPENAI_API_KEYclient = AzureOpenAI(    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning    api_version=&quot;2023-07-01-preview&quot;    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource    azure_endpoint=&quot;https://example-endpoint.openai.azure.com&quot;,)completion = client.chat.completions.create(    model=&quot;deployment-name&quot;,  # e.g. gpt-35-instant    messages=[        {            &quot;role&quot;: &quot;user&quot;,            &quot;content&quot;: &quot;How do I output all files in a directory using Python?&quot;,        },    ],)print(completion.model_dump_json(indent=2))```In addition to the options provided in the base `OpenAI` client, the following options are provided:- `azure_endpoint` (or the `AZURE_OPENAI_ENDPOINT` environment variable)- `azure_deployment`- `api_version` (or the `OPENAI_API_VERSION` environment variable)- `azure_ad_token` (or the `AZURE_OPENAI_AD_TOKEN` environment variable)- `azure_ad_token_provider`An example of using the client with Azure Active Directory can be found [here](https://github.com/openai/openai-python/blob/main/examples/azure_ad.py).## VersioningThis package generally follows [SemVer](https://semver.org/spec/v2.0.0.html) conventions, though certain backwards-incompatible changes may be released as minor versions:1. Changes that only affect static types, without breaking runtime behavior.2. Changes to library internals which are technically public but not intended or documented for external use. _(Please open a GitHub issue to let us know if you are relying on such internals)_.3. Changes that we do not expect to impact the vast majority of users in practice.We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.We are keen for your feedback; please open an [issue](https://www.github.com/openai/openai-python/issues) with questions, bugs, or suggestions.## RequirementsPython 3.7 or higher.</longdescription>
</pkgmetadata>