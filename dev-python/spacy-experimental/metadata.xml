<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;a href=&quot;https://explosion.ai&quot;&gt;&lt;img src=&quot;https://explosion.ai/assets/img/logo.svg&quot; width=&quot;125&quot; height=&quot;125&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;# spacy-experimental: Cutting-edge experimental spaCy components and featuresThis package includes experimental components and features for[spaCy](https://spacy.io) v3.x, for example model architectures, pipelinecomponents and utilities.[![Azure Pipelines](https://img.shields.io/azure-devops/build/explosion-ai/public/26/master.svg?logo=azure-pipelines&amp;style=flat-square&amp;label=build)](https://dev.azure.com/explosion-ai/public/_build?definitionId=26)[![pypi Version](https://img.shields.io/pypi/v/spacy-experimental.svg?style=flat-square&amp;logo=pypi&amp;logoColor=white)](https://pypi.org/project/spacy-experimental/)## InstallationInstall with `pip`:```bashpython -m pip install -U pip setuptools wheelpython -m pip install spacy-experimental```## Using spacy-experimentalComponents and features may be modified or removed in any release, so alwaysspecify the exact version as a package requirement if you're experimenting witha particular component, e.g.:```spacy-experimental==0.147.0```Then you can add the experimental components to your config or import from`spacy_experimental`:```ini[components.experimental_char_ner_tokenizer]factory = &quot;experimental_char_ner_tokenizer&quot;```## Components### Trainable character-based tokenizersTwo trainable tokenizers represent tokenization as a sequence tagging problemover individual characters and use the existing spaCy tagger and NERarchitectures to perform the tagging.In the spaCy pipeline, a simple &quot;pretokenizer&quot; is applied as the pipelinetokenizer to split each doc into individual characters and the trainabletokenizer is a pipeline component that retokenizes the doc. The pretokenizerneeds to be configured manually in the config or with `spacy.blank()`:```pythonnlp = spacy.blank(    &quot;en&quot;,    config={        &quot;nlp&quot;: {            &quot;tokenizer&quot;: {&quot;@tokenizers&quot;: &quot;spacy-experimental.char_pretokenizer.v1&quot;}        }    },)```The two tokenizers currently reset any existing tag or entity annotationrespectively in the process of retokenizing.#### Character-based tagger tokenizerIn the tagger version `experimental_char_tagger_tokenizer`, the tagging problemis represented internally with character-level tags for token start (`T`),token internal (`I`), and outside a token (`O`). This representation comes from[Elephant: Sequence Labeling for Word and SentenceSegmentation](https://aclanthology.org/D13-1146/) (Evang et al., 2013).```noneThis is a sentence.TIIIOTIOTOTIIIIIIIT```With the option `annotate_sents`, `S` replaces `T` for the first token in eachsentence and the component predicts both token and sentence boundaries.```noneThis is a sentence.SIIIOTIOTOTIIIIIIIT```A config excerpt for `experimental_char_tagger_tokenizer`:```ini[nlp]pipeline = [&quot;experimental_char_tagger_tokenizer&quot;]tokenizer = {&quot;@tokenizers&quot;:&quot;spacy-experimental.char_pretokenizer.v1&quot;}[components][components.experimental_char_tagger_tokenizer]factory = &quot;experimental_char_tagger_tokenizer&quot;annotate_sents = truescorer = {&quot;@scorers&quot;:&quot;spacy-experimental.tokenizer_senter_scorer.v1&quot;}[components.experimental_char_tagger_tokenizer.model]@architectures = &quot;spacy.Tagger.v1&quot;nO = null[components.experimental_char_tagger_tokenizer.model.tok2vec]@architectures = &quot;spacy.Tok2Vec.v2&quot;[components.experimental_char_tagger_tokenizer.model.tok2vec.embed]@architectures = &quot;spacy.MultiHashEmbed.v2&quot;width = 128attrs = [&quot;ORTH&quot;,&quot;LOWER&quot;,&quot;IS_DIGIT&quot;,&quot;IS_ALPHA&quot;,&quot;IS_SPACE&quot;,&quot;IS_PUNCT&quot;]rows = [1000,500,50,50,50,50]include_static_vectors = false[components.experimental_char_tagger_tokenizer.model.tok2vec.encode]@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;width = 128depth = 4window_size = 4maxout_pieces = 2```#### Character-based NER tokenizerIn the NER version, each character in a token is part of an entity:```noneTB-TOKENhI-TOKENiI-TOKENsI-TOKEN OiB-TOKENsI-TOKENOaB-TOKEN OsB-TOKENeI-TOKENnI-TOKENtI-TOKENeI-TOKENnI-TOKENcI-TOKENeI-TOKEN.B-TOKEN```A config excerpt for `experimental_char_ner_tokenizer`:```ini[nlp]pipeline = [&quot;experimental_char_ner_tokenizer&quot;]tokenizer = {&quot;@tokenizers&quot;:&quot;spacy-experimental.char_pretokenizer.v1&quot;}[components][components.experimental_char_ner_tokenizer]factory = &quot;experimental_char_ner_tokenizer&quot;scorer = {&quot;@scorers&quot;:&quot;spacy-experimental.tokenizer_scorer.v1&quot;}[components.experimental_char_ner_tokenizer.model]@architectures = &quot;spacy.TransitionBasedParser.v2&quot;state_type = &quot;ner&quot;extra_state_tokens = falsehidden_width = 64maxout_pieces = 2use_upper = truenO = null[components.experimental_char_ner_tokenizer.model.tok2vec]@architectures = &quot;spacy.Tok2Vec.v2&quot;[components.experimental_char_ner_tokenizer.model.tok2vec.embed]@architectures = &quot;spacy.MultiHashEmbed.v2&quot;width = 128attrs = [&quot;ORTH&quot;,&quot;LOWER&quot;,&quot;IS_DIGIT&quot;,&quot;IS_ALPHA&quot;,&quot;IS_SPACE&quot;,&quot;IS_PUNCT&quot;]rows = [1000,500,50,50,50,50]include_static_vectors = false[components.experimental_char_ner_tokenizer.model.tok2vec.encode]@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;width = 128depth = 4window_size = 4maxout_pieces = 2```The NER version does not currently support sentence boundaries, but it would beeasy to extend using a `B-SENT` entity type.### Biaffine parserA biaffine dependency parser, similar to that proposed in [Deep BiaffineAttention for Neural Dependency Parsing](Deep Biaffine Attention for NeuralDependency Parsing) (Dozat &amp; Manning, 2016). The parser consists of two parts:an edge predicter and an edge labeler. For example:```ini[components.experimental_arc_predicter]factory = &quot;experimental_arc_predicter&quot;[components.experimental_arc_labeler]factory = &quot;experimental_arc_labeler&quot;```The arc predicter requires that a previous component (such as `senter`) setssentence boundaries during training. Therefore, such a component must beadded to `annotating_components`:```ini[training]annotating_components = [&quot;senter&quot;]```The [biaffine parser sample project](projects/biaffine_parser) provides anexample biaffine parser pipeline.### Span FinderThe SpanFinder is a new experimental component that identifies span boundariesby tagging potential start and end tokens. It's an ML approach to suggestcandidate spans with higher precision.`SpanFinder` uses the following parameters:- `threshold`: Probability threshold for predicted spans.- `predicted_key`: Name of the [SpanGroup](https://spacy.io/api/spangroup) the predicted spans are saved to.- `training_key`: Name of the [SpanGroup](https://spacy.io/api/spangroup) the training spans are read from.- `max_length`: Max length of the predicted spans. No limit when set to `0`. Defaults to `0`.- `min_length`: Min length of the predicted spans. No limit when set to `0`. Defaults to `0`.Here is a config excerpt for the `SpanFinder` together with a `SpanCategorizer`:```ini[nlp]lang = &quot;en&quot;pipeline = [&quot;tok2vec&quot;,&quot;span_finder&quot;,&quot;spancat&quot;]batch_size = 128disabled = []before_creation = nullafter_creation = nullafter_pipeline_creation = nulltokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}[components][components.tok2vec]factory = &quot;tok2vec&quot;[components.tok2vec.model]@architectures = &quot;spacy.Tok2Vec.v1&quot;[components.tok2vec.model.embed]@architectures = &quot;spacy.MultiHashEmbed.v2&quot;width = ${components.tok2vec.model.encode.width}attrs = [&quot;ORTH&quot;, &quot;SHAPE&quot;]rows = [5000, 2500]include_static_vectors = false[components.tok2vec.model.encode]@architectures = &quot;spacy.MaxoutWindowEncoder.v2&quot;width = 96depth = 4window_size = 1maxout_pieces = 3[components.span_finder]factory = &quot;experimental_span_finder&quot;threshold = 0.35predicted_key = &quot;span_candidates&quot;training_key = ${vars.spans_key}min_length = 0max_length = 0[components.span_finder.scorer]@scorers = &quot;spacy-experimental.span_finder_scorer.v1&quot;predicted_key = ${components.span_finder.predicted_key}training_key = ${vars.spans_key}[components.span_finder.model]@architectures = &quot;spacy-experimental.SpanFinder.v1&quot;[components.span_finder.model.scorer]@layers = &quot;spacy.LinearLogistic.v1&quot;nO=2[components.span_finder.model.tok2vec]@architectures = &quot;spacy.Tok2VecListener.v1&quot;width = ${components.tok2vec.model.encode.width}[components.spancat]factory = &quot;spancat&quot;max_positive = nullspans_key = ${vars.spans_key}threshold = 0.5[components.spancat.model]@architectures = &quot;spacy.SpanCategorizer.v1&quot;[components.spancat.model.reducer]@layers = &quot;spacy.mean_max_reducer.v1&quot;hidden_size = 128[components.spancat.model.scorer]@layers = &quot;spacy.LinearLogistic.v1&quot;nO = nullnI = null[components.spancat.model.tok2vec]@architectures = &quot;spacy.Tok2VecListener.v1&quot;width = ${components.tok2vec.model.encode.width}[components.spancat.suggester]@misc = &quot;spacy-experimental.span_finder_suggester.v1&quot;predicted_key = ${components.span_finder.predicted_key}```This package includes a [spaCy project](./projects/span_finder) which shows how to train and use the `SpanFinder` together with `SpanCategorizer`.### Coreference ComponentsThe [CoreferenceResolver](https://spacy.io/api/coref) and [SpanResolver](https://spacy.io/api/span-resolver) are designed to be used together to build a corerefence pipeline, which allows you to identify which spans in a document refer to the same thing. Each component also includes an architecture and scorer. For more details, see their pages in the main spaCy docs.For an example of how to build a pipeline with the components, see the [example coref project](https://github.com/explosion/projects/tree/v3/experimental/coref).## ArchitecturesNone currently.## Other### Tokenizers- `spacy-experimental.char_pretokenizer.v1`: Tokenize a text into individual  characters.### Scorers- `spacy-experimental.tokenizer_scorer.v1`: Score tokenization.- `spacy-experimental.tokenizer_senter_scorer.v1`: Score tokenization and  sentence segmentation.### MiscSuggester functions for spancat:**Subtree suggester**: Uses dependency annotation to suggest tokens with their syntactic descendants.- `spacy-experimental.subtree_suggester.v1`- `spacy-experimental.ngram_subtree_suggester.v1`**Chunk suggester**: Suggests noun chunks using the noun chunk iterator, which requires POS and dependency annotation.- `spacy-experimental.chunk_suggester.v1`- `spacy-experimental.ngram_chunk_suggester.v1`**Sentence suggester**: Uses sentence boundaries to suggest sentence spans.- `spacy-experimental.sentence_suggester.v1`- `spacy-experimental.ngram_sentence_suggester.v1`The package also contains a [`merge_suggesters`](spacy_experimental/span_suggesters/merge_suggesters.py) function which can be used to combine suggestions from multiple suggesters.Here are two config excerpts for using the `subtree suggester` with and without the ngram functionality:```[components.spancat.suggester]@misc = &quot;spacy-experimental.subtree_suggester.v1&quot;``````[components.spancat.suggester]@misc = &quot;spacy-experimental.ngram_subtree_suggester.v1&quot;sizes = [1, 2, 3]```Note that all the suggester functions are registered in `@misc`.## Bug reports and issuesPlease report bugs in the [spaCy issuetracker](https://github.com/explosion/spaCy/issues) or open a new thread on the[discussion board](https://github.com/explosion/spaCy/discussions) for otherissues.## Older documentationSee the READMEs in earlier [taggedversions](https://github.com/explosion/spacy-experimental/tags) for detailsabout components in earlier releases.</longdescription>
</pkgmetadata>