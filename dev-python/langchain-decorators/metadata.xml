<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># LangChain Decorators ‚ú®lanchchain decorators is a layer on top of LangChain that provides syntactic sugar üç≠ for writing custom langchain prompts and chains&gt; **Note:** This is an unofficial addon to the langchain library. It's not trying to compete, just to make using it easier. Lot's of ideas here are totally opinionatedHere is a simple example of a code written with **LangChain Decorators ‚ú®**``` python@llm_promptdef write_me_short_post(topic:str, platform:str=&quot;twitter&quot;, audience:str = &quot;developers&quot;)-&gt;str:    &quot;&quot;&quot;    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    &quot;&quot;&quot;    return# run it naturalywrite_me_short_post(topic=&quot;starwars&quot;)# orwrite_me_short_post(topic=&quot;starwars&quot;, platform=&quot;redit&quot;)```**Main principles and benefits:**- more `pythonic` way of writing code- write multiline prompts that won't break your code flow with indentation- making use of IDE in-built support for **hinting**, **type checking** and **popup with docs** to quickly peek in the function to see the prompt, parameters it consumes etc.- leverage all the power of ü¶úüîó LangChain ecosystem- adding support for **optional parameters**- easily share parameters between the prompts by binding them to one class[Quick start](#quick-start)- [Installation](#installation)- [Examples](#examples)[Prompt declarations](#prompt-declarations)- [Documenting your prompt](#documenting-your-prompt)- [Chat messages prompt](#chat-messages-prompt)- [Optional sections](#optional-sections)- [Output parsers](#output-parsers)[LLM functions (OpenAI functions)](#llm-functions)- [Functions provider](#functions-provider)- [Dynamic function schemas](#dynamic-function-schemas)[Simplified streaming](#simplified-streaming)[Automatic LLM selection](#automatic-llm-selection)[More complex structures](#more-complex-structures)[Binding the prompt to an object](#binding-the-prompt-to-an-object)[Defining custom settings](#defining-custom-settings)[Debugging](#debugging)[Passing a memory, callback, stop etc.](#passing-a-memory-callback-stop-etc)[Other](#other)- [More examples](#more-examples)- [Contributing](#contributing)## Quick start### Installation```bashpip install langchain_decorators```### ExamplesGood idea on how to start is to review the examples here:- [jupyter notebook](example_notebook.ipynb)- [colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)## Prompt declarationsBy default the prompt is the whole function docs, unless you mark your prompt### Documenting your promptWe can specify what part of our docs is the prompt definition, by specifying a code block with **&lt;prompt&gt;** language tag``` python@llm_promptdef write_me_short_post(topic:str, platform:str=&quot;twitter&quot;, audience:str = &quot;developers&quot;):    &quot;&quot;&quot;    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.    It needs to be a code block, marked as a `&lt;prompt&gt;` language    ```&lt;prompt&gt;    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    ```    Now only the code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It also has a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    &quot;&quot;&quot;    return ```### Chat messages promptFor chat models is very useful to define prompt as a set of message templates... here is how to do it:``` python@llm_promptdef simulate_conversation(human_input:str, agent_role:str=&quot;a pirate&quot;):    &quot;&quot;&quot;    ## System message     - note the `:system` sufix inside the &lt;prompt:_role_&gt; tag         ```&lt;prompt:system&gt;    You are a {agent_role} hacker. You must act like one.    You reply always in code, using python or javascript code block...    for example:        ... do not reply with anything else.. just with code - respecting your role.    ```    # human message     (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)    ``` &lt;prompt:user&gt;    Helo, who are you    ```    a reply:        ``` &lt;prompt:assistant&gt;    \``` python &lt;&lt;- escaping inner code block with \ that should be part of the prompt    def hello():        print(&quot;Argh... hello you pesky pirate&quot;)    \```    ```        we can also add some history using placeholder    ```&lt;prompt:placeholder&gt;    {history}    ```    ```&lt;prompt:user&gt;    {human_input}    ```    Now only the code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.    (It also has a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))    &quot;&quot;&quot;    pass```the roles here are model native roles (assistant, user, system for chatGPT)## Optional sections- you can define a whole section of your prompt that should be optional- if any input in the section is missing, the whole section won't be renderedthe syntax for this is as follows:``` python@llm_promptdef prompt_with_optional_partials():    &quot;&quot;&quot;    this text will be rendered always, but    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | &quot;&quot;)   ?}    you can also place it in between the words    this too will be rendered{? , but        this  block will be rendered only if {this_value} and {this_value}        are not empty?} !    &quot;&quot;&quot;```## Output parsers- llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)- list, dict and pydantic outputs are also supported natively (automatically)``` python# this code example is complete and should run as it isfrom langchain_decorators import llm_prompt@llm_promptdef write_name_suggestions(company_business:str, count:int)-&gt;list:    &quot;&quot;&quot; Write me {count} good name suggestions for company that {company_business}    &quot;&quot;&quot;    passwrite_name_suggestions(company_business=&quot;sells cookies&quot;, count=5)```## LLM functions- currently supported only for the latest OpenAI chat models- all you need to do is annotate your function with the @llm_function decorator.- This will parse the description for LLM (first coherent paragraph is considered as function description)- and aso parameter descriptions (Google, Numpy and Spihnx notations are supported for now)- by default the docstring format is automatically resolved, but setting the format of the docstring can speed things up a bit.        -  `auto` (default): the format is automatically inferred from the docstring        -  `google`: the docstring is parsed as markdown (see [Google docstring format](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html))        -  `numpy`: the docstring is parsed as markdown (see [Numpy docstring format](https://numpydoc.readthedocs.io/en/latest/format.html))        -  `sphinx`: the docstring is parsed as sphinx format (see [Sphinx docstring format](https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html))To annotate an *&quot;enum&quot;* like argument, you can use this &quot;typescript&quot; like format: `[&quot;value_a&quot; | &quot;value_b&quot;]` ... if will be parsed out.This text will be a part of a description too... if you dont want it, you can use this notation as a type notation.Example:```Args:    message_type ([&quot;email&quot; | &quot;sms&quot;]): type of a message  / channel how to send the message        ```Then you pass these functions as arguments to and  `@llm_prompt` (the argument must be named `functions` ‚ÄºÔ∏è)here you can pass any @llm_function there or a native LangChain toolhere is how to use it:```pythonfrom langchain.agents import load_toolsfrom langchian_decorators import llm_function, llm_prompt, GlobalSettings@llm_functiondef send_message(message:str, addressee:str=None, message_type:str=&quot;email&quot;):    &quot;&quot;&quot; Use this if user asks to send some message    Args:        message (str): message text to send        addressee (str): email of the adressese... in format firstName.lastName@company.com        message_type (str, optional): enum: [&quot;email&quot;|&quot;whatsapp&quot;]    &quot;&quot;&quot;    if message_type==&quot;email&quot;:        send_email(addressee, message)    elif message_type==&quot;whatsapp&quot;:        send_whatsapp(addressee, message)        # load some other tools from langchainlist_of_other_tools = load_tools(    tool_names=[...],     llm=GlobalSettings.get_current_settings().default_llm)@llm_promptdef do_what_user_asks_for(user_input:str, functions:List[Union[Callable,BaseTool]]):    &quot;&quot;&quot;     ``` &lt;prompt:system&gt;    Your role is to be a helpful asistant.    ```    ``` &lt;prompt:user&gt;    {user_input}    ```    &quot;&quot;&quot;user_input=&quot;Yo, send an email to John Smith that I will be late for the meeting&quot;result = do_what_user_asks_for(        user_input=user_input,         functions=[send_message, *list_of_other_tools]    )if result.is_function_call:    result.execute()else:    print(result.output_text)```&gt; Additionally you can also add a `function_call` argument to your LLM prompt to control GPT behavior.&gt; - if you set the value to &quot;none&quot; - it will disable the function call for the moment, but it can still see them (useful do to some reasoning/planning before calling the function)&gt; - if you set the value to &quot;auto&quot; - GPT will choose to use or to to use the functions&gt; - if you set the value to a name of function / or the function it self (decorators will handle resolving the same name as used in schema) it will force GPT to use that functionIf you use functions argument, the output will be always `OutputWithFunctionCall```` pythonclass OutputWithFunctionCall(BaseModel):    output_text:str    output:T    function_name:str =None    function_arguments:Union[Dict[str,Any],str,None]    function:Callable = None    function_async:Callable = None        @property    def is_function_call(self):        ...        @property    def support_async(self):        ...        @property    def support_sync(self):        ...    async def execute_async(self):       &quot;&quot;&quot;Executes the function asynchronously.&quot;&quot;&quot;       ...            def execute(self):        &quot;&quot;&quot; Executes the function synchronously.         If the function is async, it will be executed in a event loop.        &quot;&quot;&quot;        ...     def to_function_message(self, result=None):        &quot;&quot;&quot;        Converts the result to a FunctionMessage...         you can override the result collected via execute with your own        &quot;&quot;&quot;        ...```If you want to see how the schema has been build, you can use `get_function_schema` method that is added to the function by the decorator:```pythonfrom langchain_decorators import get_function_schema@llm_functiondef my_func(arg1:str):    ...f_schema = get_function_schema(my_func.get_function_schema) print(f_schema)```In order to add the result to memory / agent_scratchpad you can use `to_function_message` to generate a FunctionMessage that LLM will interpret as a Tool/Function result### Functions providerFunctions provider enables you to provide set of llm functions more dynamically, for example list of functions - based on the input.It also enables you to give a unique name to each function for this LLM run. This might be useful for two reasons:- avoid naming conflicts, if you are combining multiple general purpose functions- further guidance/hinting of LLM model### Dynamic function schemasFunction schemas (and especially their descriptions) are crucial tools to guide LLM. If you enable dynamic function declaration, you can (re)use the same prompt attributes for the main prompt also in the llm_function scheme:```python@llm_function(dynamic_schema=True)def db_search(query_input:str):    &quot;&quot;&quot;    This function is useful to search in our database.    {?Here are some examples of data available:    {closest_examples}?}    &quot;&quot;&quot;@llm_promptdef run_agent(query_input:str, closest_examples:str, functions):    &quot;&quot;&quot;    Help user. Use a function when appropriate    &quot;&quot;&quot;closest_examples = get_closest_examples()run_agent(query_input, closest_examples, functions=[db_search, ...])```this is just for illustration, fully executable example is available [here, in code examples](code_examples/dynamic_function_schema.py)## Simplified streamingIf we want to leverage streaming:- we need to define prompt as async function- turn on the streaming on the decorator, or we can define PromptType with streaming on- capture the stream using StreamingContextThis way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type...The streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream``` python# this code example is complete and should run as it isfrom langchain_decorators import StreamingContext, llm_prompt# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)# note that only async functions can be streamed (will get an error if it's not)@llm_prompt(capture_stream=True) async def write_me_short_post(topic:str, platform:str=&quot;twitter&quot;, audience:str = &quot;developers&quot;):    &quot;&quot;&quot;    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    &quot;&quot;&quot;    pass# just an arbitrary  function to demonstrate the streaming... wil be some websockets code in the real worldtokens=[]def capture_stream_func(new_token:str):    tokens.append(new_token)# if we want to capture the stream, we need to wrap the execution into StreamingContext... # this will allow us to capture the stream even if the prompt call is hidden inside higher level method# only the prompts marked with capture_stream will be captured herewith StreamingContext(stream_to_stdout=True, callback=capture_stream_func):    result = await run_prompt()    print(&quot;Stream finished ... we can distinguish tokens thanks to alternating colors&quot;)print(&quot;\nWe've captured&quot;,len(tokens),&quot;tokensüéâ\n&quot;)print(&quot;Here is the result:&quot;)print(result)```### Automatic LLM selectionIn real life there might be situations, where the context would grow over the window of the base model you're using (for example long chat history)...But since this might happen only some times, it would be great if only in this scenario the (usually more expensive) model with bigger context window would be used, and otherwise we'd use the cheaper one.Now you can do it with LlmSelector```pythonfrom langchain_decorators import  LlmSelectormy_llm_selector = LlmSelector(            generation_min_tokens=0, # how much token at min. I for generation I want to have as a buffer            prompt_to_generation_ratio=1/3 # what percentage of the prompt length should be used for generation buffer         )\        .with_llm_rule(ChatGooglePalm(),max_tokens=512)\  # ... if you want to use LLM whose window is not defined in langchain_decorators.common.MODEL_LIMITS (only OpenAI and Anthropic are there)        .with_llm(ChatOpenAI(model = &quot;gpt-3.5-turbo&quot;))\   # these models are known, therefore we can just pass them and the max window will be resolved        .with_llm(ChatOpenAI(model = &quot;gpt-3.5-turbo-16k-0613&quot;))\         .with_llm(ChatOpenAI(model = &quot;claude-v1.3-100k&quot;))```This class allows you to define a sequence of LLMs with a rule based on the length of the prompt, and expected generation length... and only after the threshold will be passed, the more expensive model will be used automatically.You can define it into GlobalSettings:``` pythonlangchain_decorators.GlobalSettings.define_settings(        llm_selector = my_llm_selector # pass the selector into global settings    )```&gt; **Note:** as of version v0.0.10 you there the LlmSelector is in the default settings predefined.&gt; You can override it by providing you own, or setting up the default LLM or default streaming LLMOr into specific prompt type:```pythonfrom langchain_decorators import PromptTypesclass MyCustomPromptTypes(PromptTypes):    MY_TUBO_PROMPT=PromptTypeSettings(llm_selector = my_llm_selector)```### More complex structuresFor dict / pydantic you need to specify the formatting instructions...this can be tedious, that's why you can let the output parser generate you the instructions based on the model (pydantic)``` pythonfrom langchain_decorators import llm_promptfrom pydantic import BaseModel, Fieldclass TheOutputStructureWeExpect(BaseModel):    name:str = Field (description=&quot;The name of the company&quot;)    headline:str = Field( description=&quot;The description of the company (for landing page)&quot;)    employees:list[str] = Field(description=&quot;5-8 fake employee names with their positions&quot;)@llm_prompt()def fake_company_generator(company_business:str)-&gt;TheOutputStructureWeExpect:    &quot;&quot;&quot; Generate a fake company that {company_business}    {FORMAT_INSTRUCTIONS}    &quot;&quot;&quot;    returncompany = fake_company_generator(company_business=&quot;sells cookies&quot;)# print the result nicely formattedprint(&quot;Company name: &quot;,company.name)print(&quot;company headline: &quot;,company.headline)print(&quot;company employees: &quot;,company.employees)```## Binding the prompt to an object``` pythonfrom pydantic import BaseModelfrom langchain_decorators import llm_promptclass AssistantPersonality(BaseModel):    assistant_name:str    assistant_role:str    field:str    @property    def a_property(self):        return &quot;whatever&quot;    def hello_world(self, function_kwarg:str=None):        &quot;&quot;&quot;        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method        &quot;&quot;&quot;        @llm_prompt    def introduce_your_self(self)-&gt;str:        &quot;&quot;&quot;        ```¬†&lt;prompt:system&gt;        You are an assistant named {assistant_name}.         Your role is to act as {assistant_role}        ```        ```&lt;prompt:user&gt;        Introduce your self (in less than 20 words)        ```        &quot;&quot;&quot;    personality = AssistantPersonality(assistant_name=&quot;John&quot;, assistant_role=&quot;a pirate&quot;)print(personality.introduce_your_self(personality))```## Defining custom settingsHere we are just marking a function as a prompt with `llm_prompt` decorator, turning it effectively into a LLMChain. Instead of running itStandard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator.Here is how it works:1. Using **Global settings**:    ``` python    # define global settings for all prompty (if not set - chatGPT is the current default)    from langchain_decorators import GlobalSettings    GlobalSettings.define_settings(        default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally        default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming    )    ```2. Using predefined **prompt types**    ``` python    #You can change the default prompt types    from langchain_decorators import PromptTypes, PromptTypeSettings    PromptTypes.AGENT_REASONING.llm = ChatOpenAI()    # Or you can just define your own ones:    class MyCustomPromptTypes(PromptTypes):        GPT4=PromptTypeSettings(llm=ChatOpenAI(model=&quot;gpt-4&quot;))    @llm_prompt(prompt_type=MyCustomPromptTypes.GPT4)     def write_a_complicated_code(app_idea:str)-&gt;str:        ...    ```3. Define the settings **directly in the decorator**    ``` python    from langchain.llms import OpenAI    @llm_prompt(        llm=OpenAI(temperature=0.7),        stop_tokens=[&quot;\nObservation&quot;],        ...        )    def creative_writer(book_title:str)-&gt;str:        ...    ```### Passing a memory, callback, stop, etcTo pass any of these, just declare them in the function (or use kwargs to pass anything)(They do not necessarily need to be declared, but it is a good practice if you are going to use them)```python@llm_prompt()async def write_me_short_post(topic:str, platform:str=&quot;twitter&quot;, memory:SimpleMemory = None):    &quot;&quot;&quot;    {history_key}    Write me a short header for my post about {topic} for {platform} platform.     It should be for {audience} audience.    (Max 15 words)    &quot;&quot;&quot;    passawait write_me_short_post(topic=&quot;old movies&quot;)```## Debugging### Logging to consoleThere are several options how to control the outputs logged into console.The easiest way is to define ENV variable: `LANGCHAIN_DECORATORS_VERBOSE` and set it to &quot;true&quot;You can also control this programmatically by defining your global settings as shown [here](#defining-custom-settings)The last option is to control it per each case, simply by turing on verbose mode on prompt:```@llm_prompt(verbose=True)def your_prompt(param1):  ...```### Using PromptWatch.ioPromptWatch io is a platform to track and trace details about everything that is going on in langchain executions.It allows a single line drop in integration, just by wrapping your entry point code in```with PromptWatch():    run_your_code()```Learn more about PromptWatch here: [www.promptwatch.io](https://www.promptwatch.io)## Other- this project is dependant on [langchain](https://github.com/hwchase17/langchain) (obviously)- as well as on [promptwatch](https://github.com/blip-solutions/promptwatch-client), which make it easy to track and store to logs, track changes in prompts and compare them by running unit tests over the prompts...### More examples- these and few more examples are also available in the [examples notebook here](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)- including the [ReAct Agent re-implementation](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp) using purely langchain decorators## Contributingfeedback, contributions and PR are welcomed üôè</longdescription>
</pkgmetadata>