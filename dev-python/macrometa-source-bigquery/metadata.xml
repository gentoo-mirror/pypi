<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># macrometa-source-bigqueryExtract data from BigQuery tables.This is a macrometa source bigquery connector that produces JSON-formatted datafollowing the [Singer spec](https://github.com/singer-io/getting-started/blob/master/SPEC.md).This tap:- Pulls data from Google BigQuery tables/views.- Infers the schema for each resource and produce catalog file.## Installation### Step 1: Activate the Google BigQuery API (originally found in the [Google API docs](https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/usage.html)) 1. Use [this wizard](https://console.developers.google.com/start/api?id=bigquery-json.googleapis.com) to create or select a project in the Google Developers Console and activate the BigQuery API. Click Continue, then Go to credentials. 2. On the **Add credentials to your project** page, click the **Cancel** button. 3. At the top of the page, select the **OAuth consent screen** tab. Select an **Email address**, enter a **Product name** if not already set, and click the **Save** button. 4. Select the **Credentials** tab, click the **Create credentials** button and select **OAuth client ID**. 5. Select the application type **Other**, enter the name &quot;Macrometa Source BigQuery&quot;, and click the **Create** button. 6. Click **OK** to dismiss the resulting dialog. 7. Click the Download button to the right of the client ID. 8. Move this file to your working directory and rename it *client_secrets.json*.Export the location of the secret file:```export GOOGLE_APPLICATION_CREDENTIALS=&quot;./client_secret.json&quot;```For other authentication method, please see Authentication section.### Step 2: InstallFirst, make sure Python 3 is installed on your system or follow these installation instructions for Mac or Ubuntu.```pip install -U macrometa-source-bigquery```## Run### Step 1: ConfigureCreate a file called tap_config.json in your working directory, following config.sample.json:```{  &quot;streams&quot;: [      {&quot;name&quot;: &quot;&lt;some_schema_name&gt;&quot;,       &quot;table&quot;: &quot;`&lt;project&gt;.&lt;dataset&gt;.&lt;table&gt;`&quot;,       &quot;columns&quot;: [&quot;&lt;col_name_0&gt;&quot;, &quot;&lt;col_name_1&gt;&quot;, &quot;&lt;col_name_2&gt;&quot;],       &quot;datetime_key&quot;: &quot;&lt;your_key&gt;&quot;,       &quot;filters&quot;: [&quot;country='us'&quot;, &quot;state='CA'&quot;,                   &quot;registered_on&gt;=DATE_ADD(current_date, INTERVAL -7 day)&quot;                  ] // also optional: these are parsed in 'WHERE' clause      }    ],  &quot;start_datetime&quot;: &quot;2017-01-01T00:00:00Z&quot;, // This can be set at the command line argument  &quot;end_datetime&quot;: &quot;2017-02-01T00:00:00Z&quot;, // end_datetime is optional  &quot;limit&quot;: 100,  &quot;start_always_inclusive&quot;: false // default is false, optional}```- The required parameters is at least one stream (one bigquery table/view) to copy.  - It is not a recommended BigQuery practice to use `*` to specify the columns    as it may blow up the cost for a table with a large number of columns.  - `filters` are optional but we strongly recommend using this over a large    partitioned table to control the cost. LIMIT  (The authors of tap-bigquery is not    responsible for the cost incurred by running this program. Always test    thoroughly with small data set first.)- `start_datetime` must also be set in the config file or as the command line  argument (See the next step).- `limit` will limit the number of results, but it does not result in reduce  the query cost.The table/view is expected to have a column to indicate the creation orupdate date and time so the tap sends the query with `ORDER BY` and usethe column to record the bookmark (See State section).### Step 2: Create catalogRun tap-bigquery in discovery mode to let it create json schema file and thenrun them together, piping the output of macrometa-source-bigquery to target-csv:```macrometa-source-bigquery -c tap_config.json -d &gt; catalog.json```### Step 3: Runmacrometa-source-bigquery can be run with any Target. As example, let use[target-csv](https://github.com/singer-io/target-csv).```pip install target-csv```Run:```macrometa-source-bigquery -c tap_config.json \    --catalog catalog.json --start_datetime '2020-08-01T00:00:00Z' \    --end_datetime '2020-08-02T01:00:00Z' | target-csv --config target_config.json \    &gt; state.json```This example should create a csv file in the same directory.`state.json` should contain a state (bookmark) after the run. (See State section).Notes:- start and end datetimes accept ISO 8601 format, can be date only. start datetime  is inclusive, end datetime is not.- It is recommended to inspect the catalog file and fix the auto-type assignment  if necessary.- target-csv's target_config.json is optinal.- macrometa-source-bigquery can produce nested records but it's up to target if the data  writing will be successful. In this example with target-csv, the table is  expected to be flat.## AuthenticationIt is recommended to use `macrometa-source-bigquery` with a service account.- Download the client_secrets.json file for your service account, and place it  on the machine where `macrometa-source-bigquery` will be executed.- Set a `GOOGLE_APPLICATION_CREDENTIALS` environment variable on the machine,  where the value is the fully qualified path to client_secrets.jsonIn the testing environment, you can also manually authenticate before runnigthe tap. In this case you do not need `GOOGLE_APPLICATION_CREDENTIALS` defined:```gcloud auth application-default login```You may also have to set the project:```gcloud config set project &lt;project-id&gt;```Though not tested, it should also be possible to use the OAuth flow toauthenticate to GCP as well:- `macrometa-source-bigquery` will attempt to open a new window or tab in your default  browser. If this fails, copy the URL from the console and manually open it  in your browser.- If you are not already logged into your Google account, you will be prompted  to log in.- If you are logged into multiple Google accounts, you will be asked to select  one account to use for the authorization.- Click the **Accept** button to allow `macrometa-source-bigquery` to access your Google BigQuery  table.- You can close the tab after the signup flow is complete.## StateThis source connector emits [state](https://github.com/singer-io/getting-started/blob/master/docs/CONFIG_AND_STATE.md#state-file).The command also takes a state file input with `--state &lt;file-name&gt;` option.If the state is set, start_datetime config and command line argument areignored and the datetime value from last_update key is used as the resumingpoint.To avoid the data duplication, start datetime is exclusive`start_datetime &lt; datetime_column` when the source connector runs with state option. Ifyou fear a data loss because of this, just use the `--start_datetime` optioninstead of state. Or set `start_always_inclusive: true` in configuration.The source connector itself does not output a state file. It anticipate the target programor a downstream process to fianlize the state safetly and produce a state file.## Original repohttps://github.com/Macrometacorp/macrometa-source-bigquery</longdescription>
</pkgmetadata>