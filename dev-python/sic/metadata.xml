<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># sic[![pypi][pypi-img]][pypi-url][pypi-img]: https://img.shields.io/pypi/v/sic?style=plastic[pypi-url]: https://pypi.org/project/sic/###### _(Latin)_ so, thus, such, in such a way, in this way###### _(English)_ spelling is correct`sic` is a module for string normalization. Given a string, it separatessequences of alphabetical, numeric, and punctuation characters, as wellas performs more complex transformation (i.e. separates or replaces specificwords or individual symbols). It comes with set of default normalization rulesto transliterate and tokenize greek letters and replace accented characterswith their base form. It also allows using custom normalization configurations.Basic usage:```pythonimport sicbuilder = sic.Builder()machine = builder.build_normalizer()x = machine.normalize('abc123xyzalphabetagammag')print(x)```The output will be:```bashabc 123 xyz alpha beta gamma g```## Installation- `sic` is designed to work in Python 3 environment.- `sic` only needs Python Standard Library (no other packages).To get wheel for Windows (Python &gt;= 3.6) or source code package for Linux:```bashpip install sic```To get source code package regardless the OS:```bashpip install sic --no-binary sic```Wheels and .tar.gz can also be downloaded from the project's repository.Wheels contain binaries compiled from cythonized code. Source code package ispure Python. Cythonized version performs better on short strings, whilenon-cythonized version performs better on long strings, so one may be preferredover another depending on usage scenario. The benchmark is below.| STRING LENGTH | REPEATS | VERSION | MEAN TIME (s) ||:-------------:|:-------:|:-------:|:-------------:|| 71            | 10000   | .tar.gz | 1.8           || 71            | 10000   | wheel   | 0.5           || 710000        | 1       | .tar.gz | 2.7           || 710000        | 1       | wheel   | 15.9          ||||||||||||||||||||||||||||||||||||||||||||||||||||||## Tokenization configs`sic` implements tokenization, i.e. it splits a given string into tokens andprocesses those tokens according to the rules specified in a configurationfile. Basic tokenization includes separating groups of alphabetical, numerical,and punctuation characters within a string, thus turning them into separatewords (for future reference, we'll call such words `tokens`). For instance,`abc-123` will be transformed into `abc - 123`, having tokens `abc`, `-`, and`123`.What happens next to initially tokenized string must be defined using XML inconfiguration file(s). Entry point to default tokenizer applied to a string is`sic/tokenizer.standard.xml`.Below is the template and description for tokenizer config.```xml&lt;!-- tokenizer.config.xml --&gt;&lt;!--  This is the description of config file for tokenizer.  General structure:  &lt;tokenizer&gt;  +-&lt;import&gt;  +-...  +-&lt;import&gt;  +-&lt;setting&gt;  +-...  +-&lt;setting&gt;  +-&lt;split&gt;  +-...  +-&lt;split&gt;  +-&lt;token&gt;  +-...  +-&lt;token&gt;  +-&lt;character&gt;  +-...  +-&lt;character&gt;--&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!-- There must be single root element, and it must be &lt;tokenizer&gt;: --&gt;&lt;tokenizer name=&quot;$name&quot;&gt;&lt;!-- $name: string label for this tokenizer --&gt;  &lt;!--    Direct children of &lt;tokenizer&gt; are &lt;import&gt;, &lt;setting&gt;, &lt;split&gt;,      &lt;token&gt;, and/or &lt;character&gt; elements (there can be zero to many      declarations of any of those)  --&gt;  &lt;!-- &lt;import&gt; elements point at other tokenizer configs to merge with --&gt;  &lt;import file=&quot;$file&quot; /&gt;  &lt;!-- $file: path to file with another tokenizer config --&gt;  &lt;!-- &lt;setting&gt; elements define high-level tokenizer settings --&gt;  &lt;setting name=&quot;$name&quot; value=&quot;$value&quot; /&gt;  &lt;!--    Names and value requirements for /tokenizer/setting elements:    $name=&quot;cs&quot;: $value=&quot;0&quot;|&quot;1&quot; (if &quot;1&quot;, this tokenizer will be case-sensitive)    $name=&quot;bypass&quot;: $value=&quot;0&quot;|&quot;1&quot; (if &quot;1&quot;, this tokenizer will do nothing,      regardless the rest content of this file)  --&gt;  &lt;!--    &lt;split&gt; elements define substrings that should be separated from text as      tokens  --&gt;  &lt;split where=&quot;$where&quot; value=&quot;$value&quot; /&gt;  &lt;!--    $where=&quot;l&quot;|&quot;r&quot;|&quot;m&quot; (&quot;l&quot; for left, &quot;r&quot; for right, &quot;m&quot; for middle)    $value: string that will be handled as token when it's either in the      beginning of word ($where=&quot;l&quot;), at the end of word ($where=&quot;r&quot;), or in      the middle ($where=&quot;m&quot;)  --&gt;  &lt;!--    &lt;token&gt; elements define tokens that should be replaced with other tokens      (or with nothing =&gt; removed)  --&gt;  &lt;token to=&quot;$to&quot; from=&quot;$from&quot; /&gt;  &lt;!--    $to: string that should replace the token specified in $from    $from: string that is the token to be replaced by another string specified      in $to  --&gt;  &lt;!--    &lt;character&gt; elements define single characters that should be replaced with      other single characters  --&gt;  &lt;character to=&quot;$to&quot; from=&quot;$from&quot; /&gt;  &lt;!--    $to: character that should replace the another character specified in $from    $from: character that should be replaced by another character specified in      $to  --&gt;&lt;/tokenizer&gt;```Below are descriptions and examples of tokenizer config elements.|    ELEMENT    |            ATTRIBUTES             |                                                              DESCRIPTION                                                              |                               EXAMPLE                                ||:-------------:|:---------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------:|| `&lt;import&gt;`    | file=&quot;path/to/another/config.xml&quot; | Import tokenization rules from another tokenizer config.                                                                              |                                                                      || `&lt;setting&gt;`   | name=&quot;bypass&quot; value=&quot;?&quot;           | If present and value=&quot;1&quot;, all tokenization rules are ignored, as if there was no tokenization at all (left for debug purposes).       |                                                                      || `&lt;setting&gt;`   | name=&quot;cs&quot; value=&quot;?&quot;               | If value=&quot;1&quot;, string is processed case-sensitively; if value=&quot;0&quot; - case-insensitively; if not present, tokenizer is case-insensitive. |                                                                      || `&lt;split&gt;`     | where=&quot;l&quot; value=&quot;?&quot;               | Separates token specified in `value` from **left** part of a bigger token.                                                            | where=&quot;l&quot; value=&quot;kappa&quot;: `nf kappab` --&gt; `nf kappa b`                || `&lt;split&gt;`     | where=&quot;m&quot; value=&quot;?&quot;               | Separates token specified in `value` when it is found in the **middle** of a bigger token.                                            | where=&quot;m&quot; value=&quot;kappa&quot;: `nfkappab` --&gt; `nf kappa b`                 || `&lt;split&gt;`     | where=&quot;r&quot; value=&quot;?&quot;               | Separates token specified in `value` from **right** part of a bigger token.                                                           | where=&quot;r&quot; value=&quot;gamma&quot;: `ifngamma` --&gt; `ifn gamma`                  || `&lt;token&gt;`     | to=&quot;?&quot; from=&quot;?&quot;                   | Replaces token specified in `from` with another token specified in `to`.                                                              | to=&quot;protein&quot; from=&quot;gene&quot;: `nf kappa b gene` --&gt; `nf kappa b protein` || `&lt;character&gt;` | to=&quot;?&quot; from=&quot;?&quot;                   | Replaces character specified in `from` with another character specified in `to`.                                                      | to=&quot;e&quot; from=&quot;ë&quot;: `citroën` --&gt; `citroen`                             |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||Attribute `where` of `&lt;split&gt;` element may have any combination of `l`, `m`, or`r` literals if the specified substring is required to be separated in differentplaces of a bigger string. So, instead of three different elements```xml&lt;split where=&quot;l&quot; value=&quot;word&quot; /&gt;&lt;split where=&quot;m&quot; value=&quot;word&quot; /&gt;&lt;split where=&quot;r&quot; value=&quot;word&quot; /&gt;```using the following single one```xml&lt;split where=&quot;lmr&quot; value=&quot;word&quot; /&gt;```will achieve the same result.Transformation is applied in the following order:1. Replacing characters2. Splitting tokens3. Replacing tokensWhen splitting tokens, longer ones shadow shorter ones. Token replacementinstructions may contradict each other locally, but in entire set they mustconverge so that each token has only one replacement option (otherwiseValueError exception will be thrown).## Usage```pythonimport sic```For detailed description of all function and methods, see comments in thesource code.### Class `sic.Model`This class is designed to instanly create tokenization rules directly inPython. It is neither convenient nor recommended for complex normalizationtasks, but can be handy for small ones where using external XML config mightseem an overkill.```python# instantiate Modelmodel = sic.Model()# model is case-sensitivemodel.case_sensitive = True# model will do nothingmodel.bypass = True```**Method** `sic.Model.add_rule` adds single tokenization instruction to theModel instance:```python# equivalent to XML &lt;split where=&quot;lmr&quot; value=&quot;beta&quot; /&gt;model.add_rule(sic.SplitToken('beta', 'lmr'))# equivalent to XML &lt;token to=&quot;good&quot; from=&quot;bad&quot; /&gt;model.add_rule(sic.ReplaceToken('bad', 'good'))# equivalent to XML &lt;character to=&quot;z&quot; from=&quot;a&quot; /&gt;model.add_rule(sic.ReplaceCharacter('a', 'z'))```&gt; **NB**: in case new `sic.ReplaceToken` or `sic.ReplaceChar` instruction&gt; contradicts something that is already in the model, the newer instruction&gt; overrides older instruction:&gt;&gt; ```python&gt; model.add_rule(sic.ReplaceToken('bad', 'good'))&gt; model.add_rule(sic.ReplaceToken('bad', 'better'))&gt; ```&gt;&gt; &quot;bad&quot; --&gt; &quot;good&quot; will not be used; &quot;bad&quot; --&gt; &quot;better&quot; will be used instead**Method** `sic.Model.remove_rule` removes single tokenization instruction fromModel instance if it is there:```pythonmodel.remove_rule(sic.ReplaceToken('bad', 'good'))# tokenization rule that fits definition above will be removed from model```### Class `sic.Builder`**Function** `sic.Builder.build_normalizer()` reads tokenization config,instantiates `sic.Normalizer` class that would perform tokenization accordingto rules specified in a given config, and returns this `sic.Normalizer` classinstance.| ARGUMENT |    TYPE     | DEFAULT |              DESCRIPTION              ||:--------:|:-----------:|:-------:|:-------------------------------------:|| endpoint | str, Model  | None    | Path to tokenizer configuration file. |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||```python# create Builder objectbuilder = sic.Builder()# create Normalizer object with default set of rulesmachine = builder.build_normalizer()# create Normalizer object with custom set of rulesmachine = builder.build_normalizer('/path/to/config.xml')# create Normalizer object using ad hoc modelmodel = sic.Model()model.add_rule(sic.SplitToken('beta', 'lmr'))machine = builder.build_normalizer(model)```### Class `sic.Normalizer`**Method** `sic.Normalizer.save()` saves data structure from instance of`sic.Normalizer` class to a specified file (pickle).| ARGUMENT | TYPE | DEFAULT |           DESCRIPTION           ||:--------:|:----:|:-------:|:-------------------------------:|| filename | str  |   n/a   | Path and name of file to write. ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||**Function** `sic.Normalizer.load()` reads specified file (pickle) and placesdata structure in `sic.Normalizer` instance.| ARGUMENT | TYPE | DEFAULT |          DESCRIPTION           ||:--------:|:----:|:-------:|:------------------------------:|| filename | str  |   n/a   | Path and name of file to read. |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||**Function** `sic.Normalizer.normalize()` performs string normalizationaccording to the rules ingested at the time of class initialization, andreturns normalized string.|     ARGUMENT      | TYPE | DEFAULT |            DESCRIPTION                              ||:-----------------:|:----:|:-------:|:---------------------------------------------------:|| source_string     | str  |   n/a   | String to normalize.                                || word_separator    | str  |   ' '   | Word delimiter (single character).                  || normalizer_option | int  |    0    | Mode of post-processing.                            || control_character | str  | '\x00'  | Character masking word delimiter (single character) |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||`word_separator`: Specified character will be considered a boundary betweentokens. The default value is `' '` (space) which seems reasonable choice fornatural language. However any character can be specified, which might be moreuseful in certain context.`normalizer_option`: The value can be either one of `0`, `1`, `2`, or `3` andcontrols the way tokenized string is post-processed:| VALUE |                             MODE                              ||:-----:|:-------------------------------------------------------------:||   0   | No post-processing.                                           ||   1   | Rearrange tokens in alphabetical order.                       ||   2   | Rearrange tokens in alphabetical order and remove duplicates. ||   3   | Remove all added word separators.                             ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||`control_character`: Implementation detail - character that used as worddelimiter inserted in a parsed string at the run time. If parsed stringinitially included this character somewhere, normalization will return error.The value is set to `\x00` by default.**Property** `sic.Normalizer.result` retains the result of last call for`sic.Normalizer.normalize` function as dict object with the following keys:|     KEY      |   VALUE TYPE    |                 DESCRIPTION                          ||:------------:|:---------------:|:----------------------------------------------------:|| 'original'   | str             | Original string value that was processed.            || 'normalized' | str             | Returned normalized string value.                    || 'map'        | list(int)       | Map between original and normalized strings.         || 'r_map'      | list(list(int)) | Reverse map between original and normalized strings. ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||`sic.Normalizer.result['map']`: Not only `sic.Normalizer.normalize()` generatesnormalized string out of originally provided, it also tries to map characterindexes in normalized string back on those in the original one. This map isrepresented as list of integers where item index is character position innormalized string and item value is character position in original string. Thisis only valid when `normalizer_option` argument for `sic.Normalizer.normalize()`call has been set to 0.`sic.Normalizer.result['r_map']`: Reverse map between character locations inoriginal string and its normalized reflection (item index is character positionin original string; item value is list [`x`, `y`] where `x` and `y` arerespectively lowest and highest indexes of mapped character in normalizedstring).### Method `sic.build_normalizer()``sic.build_normalizer()` implicitly creates single instance of `sic.Normalizer`class accessible globally from `sic` namespace. Arguments are same as for`sic.Builder.build_normalizer()` function.### Method `sic.save()``sic.save()` saves data structure stored in global instance of `sic.Normalizer`class to a specified file (pickle). Arguments are same as for`sic.Normalizer.save()` method.### Function `sic.load()``sic.load()` reads specified file (pickle) and places data structure in globalinstance of `sic.Normalizer` class stored in that file. Arguments are same asfor `sic.Normalizer.load()` function.### Function `sic.normalize()``sic.normalize(*args, **kwargs)` either uses global class `sic.Normalizer` orinstantly creates new local `sic.Normalizer` class, and uses it to performrequested string normalization.|     ARGUMENT      | TYPE | DEFAULT |            DESCRIPTION                              ||:-----------------:|:----:|:-------:|:---------------------------------------------------:|| source_string     | str  |   n/a   | String to normalize.                                || word_separator    | str  |   ' '   | Word delimiter (single character).                  || normalizer_option | int  |    0    | Mode of post-processing.                            || control_character | str  | '\x00'  | Character masking word delimiter (single character) || tokenizer_config  | str  |  None   | Path to tokenizer configuration file.               |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||If `tokenizer_config` argument is not provided, the function will use globalinstance of `sic.Normalizer` class (will create it if it is not initialized).### Method `sic.reset()``sic.reset()` resets global `sic.Normalizer` instance to `None`, forcingsubsequently called `sic.normalize()` to create new global instance again if itneeds it.### Attribute `sic.result`, function `sic.result()``sic.result` attribute retains the value of `sic.Normalizer.result` propertythat belonged to most recently used `sic.Normalizer` instance accessed from`sic.normalize()` function (either global or local).Python 3.6 does not support [PEP-562](https://www.python.org/dev/peps/pep-0562/)(module attributes). So in Python 3.6, use function `sic.result()` rather thanattribute `sic.result`:```pythonsic.result() # will work in Python &gt;= 3.6sic.result   # will work in Python &gt;= 3.7```## Examples### Basic usage```pythonimport sic# create Builder objectbuilder = sic.Builder()# create Normalizer object with default set of rulesmachine = builder.build_normalizer()# using default word_separator and normalizer_optionx = machine.normalize('alpha-2-macroglobulin-p')print(x) # 'alpha - 2 - macroglobulin - p'print(machine.result)&quot;&quot;&quot;{  'original': 'alpha-2-macroglobulin-p',  'normalized': 'alpha - 2 - macroglobulin - p',  'map': [    0, 1, 2, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 21, 22, 22  ],  'r_map: [    [0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 13], [14, 14], [15, 15], [16, 16], [17, 17], [18, 18], [19, 19], [20, 20], [21, 21], [22, 22], [23, 23], [24, 24], [25, 26], [27, 28]  ]}&quot;&quot;&quot;```### Custom word separator```pythonx = machine.normalize('alpha-2-macroglobulin-p', word_separator='|')print(x) # 'alpha|-|2|-|macroglobulin|-|p'```### Post-processing options```python# using normalizer_option=1x = machine.normalize('alpha-2-macroglobulin-p', normalizer_option=1)print(x) # '- - - 2 alpha macroglobulin p'``````python# using normalizer_option=2x = machine.normalize('alpha-2-macroglobulin-p', normalizer_option=2)print(x) # '- 2 alpha macroglobulin p'``````python# using normalizer_option=3# assuming normalization config includes the following:# &lt;setting name=&quot;cs&quot; value=&quot;0&quot; /&gt;# &lt;split value=&quot;mis&quot; where=&quot;l&quot; /&gt;# &lt;token to=&quot;spelling&quot; from=&quot;speling&quot; /&gt;x = machine.normalize('Misspeling')print(x) # 'Misspelling'```### Using implicitly instantiated classes```python# normalize() with default instancex = sic.normalize('alpha-2-macroglobulin-p', word_separator='|')print(x) # 'alpha|-|2|-|macroglobulin|-|p'# custom configuration for implicitly instantiated normalizersic.build_normalizer('/path/to/config.xml')x = sic.normalize('some string')print(x) # will be normalized according to config at /path/to/config.xml# custom config and normalization in one linex = sic.normalize('some string', tokenizer_config='/path/to/another/config.xml')print(x) # will be normalized according to config at /path/to/another/config.xml```### Saving and loading compiled normalizer to/from disk```pythonmachine.save('/path/to/file') # will write /path/to/fileanother_machine = sic.Normalizer()another_machine.load('/path/to/file') # will read /path/to/file```### Adding normalization rules to already compiled model```python# (assuming `machine` is sic.Normalizer instance armed with tokenization ruleset)new_ruleset = [sic.ReplaceToken('from', 'to'), sic.SplitToken('token', 'r')]new_ruleset_string = ''.join([rule.decode() for rule in new_ruleset])machine.make_tokenizer(new_ruleset_string, update=True) # rules from `new_ruleset` will be added to the normalizer```</longdescription>
</pkgmetadata>