<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Spark acceleration for Scikit-LearnThis project is a major re-write of the [spark-sklearn](https://github.com/databricks/spark-sklearn) project, which seems to no longer be under development. It focuses specifically on the acceleration of Scikit-Learn's cross validation functionality using PySpark.### Improvements over spark-sklearn`scikit-spark` supports `scikit-learn` versions past 0.19, `spark-sklearn` [have stated that they are probably not going to support newer versions](https://github.com/databricks/spark-sklearn/issues/113).The functionality in `scikit-spark` is based on `sklearn.model_selection` module rather than the deprecated and soon to be removed `sklearn.grid_search`. The new `model_selection` versions contain several nicer features and `scikit-spark` maintains full compatibility.## InstallationThe package can be installed through pip:```bashpip install scikit-spark```It has so far only been tested with Spark 2.2.0 and up, but may work with older versions. ### Supported scikit-learn versions- 0.18 untested, likely doesn't work- 0.19 supported- 0.20 supported- 0.21 supported (Python 3 only)- 0.22 supported (Python 3 only)## UsageThe functionality here is meant to as closely resemble using Scikit-Learn as possible. By default (with `spark=True`) the `SparkSession` is obtainedinternally by calling `SparkSession.builder.getOrCreate()`, so the instantiationand calling of the functions is the same (You will preferably have already created a `SparkSession`). This example is adapted from the Scikit-Learn documentation. It instantiatesa local `SparkSession`, and distributes the cross validation folds and iterations using this. In actual use, to get the benefit of this package it should be used distributed across several machines with Spark as running it locally is slower than the `Scikit-Learn` parallelisation implementation.```pythonfrom sklearn import svm, datasetsfrom pyspark.sql import SparkSessioniris = datasets.load_iris()parameters = {'kernel':('linear', 'rbf'), 'C':[0.01, 0.1, 1, 10, 100]}svc = svm.SVC()spark = SparkSession.builder\    .master(&quot;local[*]&quot;)\    .appName(&quot;skspark-grid-search-doctests&quot;)\    .getOrCreate()# How to run grid searchfrom skspark.model_selection import GridSearchCVgs = GridSearchCV(svc, parameters)gs.fit(iris.data, iris.target)# How to run random searchfrom skspark.model_selection import RandomizedSearchCVrs = RandomizedSearchCV(spark, svc, parameters)rs.fit(iris.data, iris.target)```## Current and upcoming functionality- Current    - model_selection.RandomizedSearchCV    - model_selection.GridSearchCV- Upcoming    - model_selection.cross_val_predict    - model_selection.cross_val_score*The docstrings are modifications of the Scikit-Learn ones and are still beingconverted to specifically refer to this project.* ## Performance optimisations ### Reducing RAM usage *Coming soon*</longdescription>
</pkgmetadata>