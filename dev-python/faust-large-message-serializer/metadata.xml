<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![GitHub license](https://img.shields.io/github/license/bakdata/faust-s3-backed-serializer)](https://github.com/bakdata/faust-large-message-serializer/blob/master/LICENSE)[![Python Version](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue.svg)](https://img.shields.io/badge/python-3.6%20%7C%203.7-blue.svg)[![Build Status](https://dev.azure.com/bakdata/public/_apis/build/status/bakdata.faust-large-message-serializer?branchName=master)](https://dev.azure.com/bakdata/public/_build/latest?definitionId=22&amp;branchName=master)[![PyPI version](https://badge.fury.io/py/faust-large-message-serializer.svg)](https://badge.fury.io/py/faust-large-message-serializer)# faust-large-message-serializerA Faust Serializer that reads and writes records from and to S3 or Azure Blob Storage transparently.This serializer is compatible with our [Kafka large-message-serializer SerDe](https://github.com/bakdata/kafka-large-message-serde) for Java.Read more about it on our [blog](https://medium.com/bakdata/processing-large-messages-with-kafka-streams-167a166ca38b).# Getting Started#### PyPi```pip install faust-large-message-serializer```##### UsageThe serializer was build to be used with other serializers. The idea is to use the [&quot;concatenation&quot; feature](https://faust.readthedocs.io/en/latest/userguide/models.html#codec-registry) that comes with Faust```pythonimport faustfrom faust import Recordimport loggingfrom faust_large_message_serializer import LargeMessageSerializer, LargeMessageSerializerConfigfrom faust.serializers import codecs# model.userclass UserModel(Record, serializer=&quot;s3_json&quot;):    first_name: str    last_name: strconfig = LargeMessageSerializerConfig(base_path=&quot;s3://your-bucket-name/&quot;,                                      max_size=0,                                      large_message_s3_region=&quot;eu-central-1&quot;,                                      large_message_s3_access_key=&quot;access_key&quot;,                                      large_message_s3_secret_key=&quot;secret_key&quot;)topic_name = &quot;users_s3&quot;s3_backed_serializer = LargeMessageSerializer(topic_name, config, is_key=False)json_serializer = codecs.get_codec(&quot;json&quot;)# Here we use json as the first serializer and# then we can upload everything to the S3 buckets3_json_serializer = json_serializer | s3_backed_serializer# configlogger = logging.getLogger(__name__)codecs.register(&quot;s3_json&quot;, s3_json_serializer)app = faust.App(&quot;app_id&quot;, broker=&quot;kafka://localhost:9092&quot;)users_topic = app.topic(topic_name, value_type=UserModel)@app.agent(users_topic)async def users(users):    async for user in users:        logger.info(&quot;Event received in topic&quot;)        logger.info(f&quot;The user is : {user}&quot;)@app.timer(5.0, on_leader=True)async def send_users():    data_user = {&quot;first_name&quot;: &quot;bar&quot;, &quot;last_name&quot;: &quot;foo&quot;}    user = UserModel(**data_user)    await users.send(value=user, value_serializer=s3_json_serializer)app.main()````## ContributingWe are happy if you want to contribute to this project.If you find any bugs or have suggestions for improvements, please open an issue.We are also happy to accept your PRs.Just open an issue beforehand and let us know what you want to do and why.## LicenseThis project is licensed under the MIT license.Have a look at the [LICENSE](https://github.com/bakdata/faust-s3-backed-serializer/blob/master/LICENSE) for more details.</longdescription>
</pkgmetadata>