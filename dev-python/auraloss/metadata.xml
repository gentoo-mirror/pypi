<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div  align=&quot;center&quot;&gt;# auraloss&lt;img width=&quot;200px&quot; src=&quot;docs/auraloss-logo.svg&quot;&gt;A collection of audio-focused loss functions in PyTorch. [[PDF](https://www.christiansteinmetz.com/s/DMRN15__auraloss__Audio_focused_loss_functions_in_PyTorch.pdf)]&lt;/div&gt;## Setup```pip install auraloss```If you want to use `MelSTFTLoss()` or `FIRFilter()` you will need to specify the extra install (librosa and scipy).```pip install auraloss[all]```## Usage```pythonimport torchimport auralossmrstft = auraloss.freq.MultiResolutionSTFTLoss()input = torch.rand(8,1,44100)target = torch.rand(8,1,44100)loss = mrstft(input, target)```# Loss functionsWe categorize the loss functions as either time-domain or frequency-domain approaches. Additionally, we include perceptual transforms.&lt;table&gt;    &lt;tr&gt;        &lt;th&gt;Loss function&lt;/th&gt;        &lt;th&gt;Interface&lt;/th&gt;        &lt;th&gt;Reference&lt;/th&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td colspan=&quot;3&quot; align=&quot;center&quot;&gt;&lt;b&gt;Time domain&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Error-to-signal ratio (ESR)&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.time.ESRLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/1911.08922&gt;Wright &amp; Välimäki, 2019&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;DC error (DC)&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.time.DCLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/1911.08922&gt;Wright &amp; Välimäki, 2019&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Log hyperbolic cosine (Log-cosh)&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.time.LogCoshLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://openreview.net/forum?id=rkglvsC9Ym&gt;Chen et al., 2019&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Signal-to-noise ratio (SNR)&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.time.SNRLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Scale-invariant signal-to-distortion &lt;br&gt;  ratio (SI-SDR)&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.time.SISDRLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/1811.02508&gt;Le Roux et al., 2018&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Scale-dependent signal-to-distortion &lt;br&gt;  ratio (SD-SDR)&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.time.SDSDRLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/1811.02508&gt;Le Roux et al., 2018&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td colspan=&quot;3&quot; align=&quot;center&quot;&gt;&lt;b&gt;Frequency domain&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Aggregate STFT&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.freq.STFTLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/1808.06719&gt;Arik et al., 2018&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Aggregate Mel-scaled STFT&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.freq.MelSTFTLoss(sample_rate)&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Multi-resolution STFT&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.freq.MultiResolutionSTFTLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/1910.11480&gt;Yamamoto et al., 2019*&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Random-resolution STFT&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.freq.RandomResolutionSTFTLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://www.christiansteinmetz.com/s/DMRN15__auraloss__Audio_focused_loss_functions_in_PyTorch.pdf&gt;Steinmetz &amp; Reiss, 2020&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Sum and difference STFT loss&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.freq.SumAndDifferenceSTFTLoss()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/2010.10291&gt;Steinmetz et al., 2020&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td colspan=&quot;3&quot; align=&quot;center&quot;&gt;&lt;b&gt;Perceptual transforms&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;Sum and difference signal transform&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.perceptual.SumAndDifference()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=#&gt;&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td&gt;FIR pre-emphasis filters&lt;/td&gt;        &lt;td&gt;&lt;code&gt;auraloss.perceptual.FIRFilter()&lt;/code&gt;&lt;/td&gt;        &lt;td&gt;&lt;a href=https://arxiv.org/abs/1911.08922&gt;Wright &amp; Välimäki, 2019&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;&lt;/table&gt;\* [Wang et al., 2019](https://arxiv.org/abs/1904.12088) also propose a multi-resolution spectral loss (that [Engel et al., 2020](https://arxiv.org/abs/2001.04643) follow), but they do not include both the log magnitude (L1 distance) and spectral convergence terms, introduced in [Arik et al., 2018](https://arxiv.org/abs/1808.0671), and then extended for the multi-resolution case in [Yamamoto et al., 2019](https://arxiv.org/abs/1910.11480).## ExamplesCurrently we include an example using a set of the loss functions to train a TCN for modeling an analog dynamic range compressor. For details please refer to the details in [`examples/compressor`](examples/compressor). We provide pre-trained models, evaluation scripts to compute the metrics in the [paper](https://www.christiansteinmetz.com/s/DMRN15__auraloss__Audio_focused_loss_functions_in_PyTorch.pdf), as well as scripts to retrain models. There are some more advanced things you can do based upon the `STFTLoss` class. For example, you can compute both linear and log scaled STFT errors as in [Engel et al., 2020](https://arxiv.org/abs/2001.04643).In this case we do not include the spectral convergence term. ```pythonstft_loss = auraloss.freq.STFTLoss(w_log_mag=1.0,                                    w_lin_mag=1.0,                                    w_sc=0.0, )```There is also a Mel-scaled STFT loss, which has some special requirements. This loss requires you set the sample rate as well as specify the correct device. ```pythonsample_rate = 44100melstft_loss = auraloss.freq.MelSTFTLoss(sample_rate, device=&quot;cuda&quot;)```You can also build a multi-resolution Mel-scaled STFT loss with 64 bins easily. Make sure you pass the correct device where the tensors you are comparing will be. ```pythonmrmelstft_loss = auraloss.freq.MultiResolutionSTFTLoss(scale=&quot;mel&quot;,                                                        n_bins=64,                                                       sample_rate=sample_rate,                                                       device=&quot;cuda&quot;)```# DevelopmentWe currently have no tests, but those will also be coming soon, so use caution at the moment. Future loss functions to be included will target neural network based perceptual losses, which tend to be a bit more sophisticated than those we have included so far. If you are interested in adding a loss function please make a pull request. ## Loss functions to be added- [Spectral Energy Distance](https://arxiv.org/abs/2008.01160)- [TFGAN Losses](https://arxiv.org/abs/2011.12206)# CiteIf you use this code in your work please consider citing us.```@inproceedings{steinmetz2020auraloss,    title={auraloss: {A}udio focused loss functions in {PyTorch}},    author={Steinmetz, Christian J. and Reiss, Joshua D.},    booktitle={Digital Music Research Network One-day Workshop (DMRN+15)},    year={2020}}```</longdescription>
</pkgmetadata>