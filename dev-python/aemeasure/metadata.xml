<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># AeMeasure - A macro-benchmarking tool with a serverless databaseThis module has been developed to save (macro-)benchmarks of algorithms in a simple anddynamic way. The primary features are:* Saving metadata such as Git-Revision, hostname, etc.* No requirement of a data scheme. This is important as often you add additional feature in later iterations but still want to compare it to the original data.* Compatibility with distributed execution, e.g., via slurm. If you want to use multi-processing, use separate `MeasurementSeries`.* Easy data rescue in case of data corruption (or non-availability of this library) as well as compression.  * Data is saved in multiple json files (coherent json is not efficient) and compressed as zip.* Optional capturing of stdin and stdout.You can also consider this tool as a simple serverless but NFS-compatible object database with helpers for benchmarks.The motivation for this tool came from the need to **quickly compare different optimization models (MIP, CP, SAT, ...)**and analyze their performance.  Here it is more important to save the context (parameters, revision,...) than tobe precise to a millisecond. If you need very precise measurements, you need to look for a micro-benchmarking tool.This is a **macro-benchmarking tool** with a **file-based database**.## When to use AeMeasure?&gt; *&quot;They say the workman is only as good as his tools; in experimental algorithmics the workman must often build his tools.&quot;* - Catherine McGeoch, A Guide to Experimental AlgorithmicsAeMeasure is designed for flexibility and simplicity. If you don't have changingrequirements every few weeks, you may be better off withusing a proper database. If you are somewhere in between, you could take a look at, e.g.,[MongoDB](https://www.mongodb.com/), which is more flexible regarding the schema butstill provides a proper database. If you want a very simple&amp;flexible solution and the datain the repository (compressed of course, but still human-readable),AeMeasure may be the right tool for you.## InstallationThe easiest installation is via pip```shellpip install -U aemeasure```## UsageA simple application that runs an algorithm for a set of instances and saves the results to `./results` could look like this:```pythonfrom aemeasure import MeasurementSerieswith MeasurementSeries(&quot;./results&quot;) as ms:    # By default, stdout, stdin, and metadata (git revision, hostname, etc) will    # automatically be added to each measurement.    for instance in instances:        with ms.measurement() as m:            m[&quot;instance&quot;] = str(instance)            m[&quot;size&quot;] = len(instance)            m[&quot;algorithm&quot;] = &quot;fancy_algorithm&quot;            m[&quot;parameters&quot;] = &quot;asdgfdfgsdgf&quot;            solution = run_algorithm(instance)            m[&quot;solution&quot;] = solution.as_json()            m[&quot;objective&quot;] = 42            m[&quot;lower_bound&quot;] = 13```You can then parse the database as pandas table via```pythonfrom aemeasure import read_as_pandas_tabletable = read_as_pandas_table(&quot;./results&quot;, defaults={&quot;uses_later_added_special_feature&quot;: False})```## Metadata and stdout/stderrIf you are using MeasurementSeries, all possible information is automaticallyadded to the measurements by default. You can deactivate this easily inthe constructor. However, often it is useful to have this data (especially stderrand git revision) at hand, when you notice some oddities in your results. Thiscan take up a lot of space, but the compression option of the database shouldhelp.The following data is saved:* Runtime (enter and exit of Measurement)* stdout/stderr* Git Revision* Timestamp of start* Hostname* Arguments* Python-File* Current working directoryYou can also activate individual metadata by just calling the corresponding memberfunction of the measurement.## Usage with SlurminadeThis tool is excellent in combination with [Slurminade](https://github.com/d-krupke/slurminade) to automatically distributeyour experiments to Slurm nodes. This also allows you to schedule the missing instances.An example could look like this:```pythonimport slurminadefrom aemeasure import MeasurementSeries, read_as_pandas_table, Database# your supervisor/admin will tell you the necessary configuration.slurminade.update_default_configuration(partition=&quot;alg&quot;, constraint=&quot;alggen03&quot;)slurminade.set_dispatch_limit(200)  # just a safety measure in case you messed up# Experiment parametersresult_folder = &quot;./results&quot;timelimit = 300# The part to be distributed@slurminade.slurmify()def run_for_instance(instance_name, timelimit):    &quot;&quot;&quot;    Solve instance with different solvers.    &quot;&quot;&quot;    instances = load_instances()    instance = instances[instance_name]    with MeasurementSeries(result_folder) as ms:        models = (Model1(instance), Model2(instance))        for model in models:            with ms.measurement() as m:                ub, lb = model.optimize(timelimit)                m[&quot;instance&quot;] = instance_name                m[&quot;timelimit&quot;] = timelimit                m[&quot;ub&quot;] = ub                m[&quot;lb&quot;] = lb                m[&quot;n&quot;] = len(instance)                m[&quot;Method&quot;] = str(model)                m.save_seconds()if __name__ == &quot;__main__&quot;:    # Read data    instances = load_instances()    Database(result_folder).compress()  # compress prior results to make space    t = read_as_pandas_table(result_folder)  # read prior results to check which instances are still missing    # find missing instances (skip already solved instances)    finished_instances = t[&quot;instance&quot;].to_list() if not t.empty else []    print(&quot;Already finished instances:&quot;, finished_instances)    missing_instances = [i for i in instances if i not in finished_instances]    if finished_instances and missing_instances:        assert isinstance(missing_instances[0], type(finished_instances[0]))    print(&quot;Still missing instances:&quot;, missing_instances)    # distribute    for instance in missing_instances:        run_for_instance.distribute(instance, timelimit)```If you have a lot of instances, you may want to use `slurminade.AutoBatch` to automaticallybatch multiple instances into a single task.Important points of this example:* Extract the parameters as variables and put them at the top so you can easily copy and adapt such a template.* The `run_for_instance` function will read the instance itself as this is more efficient than to distribute it via slurm as an argument.* We compress the results at the beginning. As this is executed before distribution, it is threadsafe.* We quickly check, which instances are already solved and only distribute the missing ones.* To compress the final results, simply run this script again (it will also check if you may have missed some instances due to an error).I have often seen scripts that are simply started on each node that either use a complicatedmanual instance distribution, require an additional server, or need a lot of additionaldata collection in the end. The above's approach seems to be much more elegant to me, ifyou already have Slurm and an NFS.## Serverless DatabaseThe serverless database allows to dump unstructured JSONs in a relatively threadsafe way (focus on Surm-node with NFS).```pythonfrom aemeasure import Database# Writingdb = Database(&quot;./db_folder&quot;)  # We use a folder, not a file, to make it NFS-safe.db.add({&quot;key&quot;: &quot;value&quot;}, flush=False)  # save simple dict, do not write directly.db.flush()  # save to diskdb.compress()  # compress data on disk via zip# Readingdb2 = Database(&quot;./db_folder&quot;)data = db2.load()  # load all entries as a list of dicts.# Cleardb2.clear()db2.dump([e for e in data if e[&quot;feature_x&quot;]])  # write back only entries with 'feature_x'```The primary points of the database are:* No server is needed, synchronization possible via NFS.* We are using a folder instead of a single file. Otherwise, the synchronization of different nodes via NFS would be difficult.* Every node uses a separate, unique file to prevent conflicts.* Every entry is a new line in JSON format appended to the current database file of the node. As this allows simply appending, this is much more efficient that keeping the whole structure in JSON. If something goes wrong, you can still easily repair it with a text editor and some basic JSON-skills.* The database has a very simple format, such that it can also be read without this tool.* As the nativ JSON format can need a signficant amount of disk, a compression option allows to significantly reduce the size via ZIP-compression.**This database is made for frequent writing, infrequent reading. Currently, there are no query options aside of list comprehensions. Use `clear` and `dump` for selective deletion.**## Changelog* 0.2.9: Added pyproject.toml for PEP compliance.* 0.2.8: Saving Python-environment, too.* 0.2.7: Robust JSON serialization. It will save the data but print an error if the data is not JSON-serializable.* 0.2.6: Extended logging and exception if data could not be written.* 0.2.5: Skipping on zero size (probably not yet written, can be a problem with NFS)* 0.2.4: Added some logging.* 0.2.3: Setting LZMA as compression standard.* For some reason, the default keys for 'stdin' and 'stdout' were wrong. Fixed.</longdescription>
</pkgmetadata>