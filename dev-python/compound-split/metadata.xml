<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># CharSplit - An *ngram*-based compound splitter for GermanSplits a German compound into its body and head, e.g.&gt; Autobahnraststätte -&gt; Autobahn - RaststätteImplementation of the method described in the appendix of the thesis:Tuggener, Don (2016). *Incremental Coreference Resolution for German.* University of Zurich, Faculty of Arts.### TL;DRThe method calculates probabilities of ngrams occurring at the beginning, end and in the middle of words and identifies the most likely position for a split.The method achieves ~95% accuracy for head detection on the [Germanet compound test set](http://www.sfs.uni-tuebingen.de/lsd/compounds.shtml).A model is provided, trained on 1 Mio. German nouns from Wikipedia.### Usage ### Train a new model:```$ python char_split_train.py &lt;your_train_file&gt;```where `&lt;your_train_file&gt;` contains one word (noun) per line.### Compound splittingFrom command line:```$ python char_split.py &lt;word&gt;```Outputs all possible splits, ranked by their score, e.g.```$ python char_split.py Autobahnraststätte0.84096566854AutobahnRaststätte-0.54568851959AutoBahnraststätte-0.719082070993AutobahnrastStätte...```As a module:```$ python&gt;&gt;&gt; from compound_split import char_split&gt;&gt;&gt; char_split.split_compound('Autobahnraststätte')[[0.7945872450631273, 'Autobahn', 'Raststätte'], [-0.7143290887876655, 'Auto', 'Bahnraststätte'],   [-1.1132332878581173, 'Autobahnrast', 'Stätte'],   [-1.4010051533086552, 'Aut', 'Obahnraststätte'],   [-2.3447843979244944, 'Autobahnrasts', 'Tätte'],   [-2.4761904761904763, 'Autobahnra', 'Ststätte'],   [-2.4761904761904763, 'Autobahnr', 'Aststätte'],   [-2.5733333333333333, 'Autob', 'Ahnraststätte'],   [-2.604651162790698, 'Autobahnras', 'Tstätte'],   [-2.7142857142857144, 'Autobah', 'Nraststätte'],   [-2.730248306997743, 'Autobahnrastst', 'Ätte'],   [-2.8033113109925973, 'Autobahnraststä', 'Tte'],   [-3.0, 'Autoba', 'Hnraststätte']]```### Document splittingFrom command line:```$ python doc_split.py &lt;dict&gt;```Reads everything from standard inputand writes out the same, with the best splitsseparated by the middle dot character `·`.Each word is split as many times as possible basedon the file &lt;dict&gt;, which contains German wordsone per line (comment lines beginning with # are allowed).The name of the default dictionary is in the file `doc_config.py`.Note that the `doc_split` module retains a cache of words already split,so long documents will typically be processed proportionately fasterthan short ones.The cache is discarded when the program ends.```$ python sentence1.txtUm die in jeder Hinsicht zufriedenzustellen, tüftelt er einen Weg aus,sinnlose Bürokratie wie Ladenschlußgesetz und Nachtbackverbot auszutricksen.  $ python doc_split.py &lt;sentence1.txt  Um die in jeder Hinsicht zufriedenzustellen, tüftelt er einen Weg aus,sinnlose Bürokratie wie Laden·schluß·gesetz und Nacht·back·verbot auszutricksen.  ```As a module:```$ python&gt;&gt;&gt; from compound_split import doc_split&gt;&gt;&gt; # Constant containing a middle dot&gt;&gt;&gt; doc_split.MIDDLE_DOT'·'&gt;&gt;&gt; # Split a word as much as possible, return a list&gt;&gt;&gt; doc_split.maximal_split('Verfassungsschutzpräsident')['Verfassungs', 'Schutz', 'Präsident']&gt;&gt;&gt; # Split a word as much as possible, return a word with middle dots'Verfassungs·schutz·präsident'&gt;&gt;&gt; # Split all splittable words in a sentence&gt;&gt;&gt; doc_split.doc_split('Der Marquis schlug mit dem Handteller auf sein Regiepult.')Der Marquis schlug mit dem Hand·teller auf sein Regie·pult.```### Document splitting serverBecause of the startup time, you can run the document splitteras a simple server, and the responses will be quicker.```$ python doc_server [ -d ] &lt;dict&gt; &lt;port&gt;```The server will load `&lt;dict&gt;` and listen on `&lt;port&gt;`.The client mustsend the raw data in UTF-8 encoding to the portand close the write side of the port, and theserver will return the split data.The option `-d` causes the server to return a sorted dictionaryof split words instead.  Each word is on a single line,with the original word followed by a tab character followed by the split word.Because of Python restrictions, the server is single-threaded.The default dictionary and port are in the file `doc_config.py`.A trivial client is provided:```$ python doc_client &lt;port&gt; &lt;host&gt;```Reads a document from standard input,send it to the server running on `&lt;host&gt;` and `&lt;port&gt;`,and send the server's output to standard output.Thus it has the same interface as `doc_split`(except that the dictionary cannot be specified),but should run somewhat faster.The default host and port are in the file `doc_config.py`.## Downloading dictionariesTo download German and Dutch dictionaries for `doc_split` and `doc_server`:```$ cd dicts$ sh getdicts```This will download the spelling plugins from the LibreOffice site,extract the wordlists, and write five files into the current directory.It leaves a good many files in `/tmp`, which are not needed further.  * The dictionaries `de-DE.dic`, `de-AT.dic`, and `de-CH.dic` are    fairly extensive (about 250,000 words each)    and provide current German, Austrian, and Swiss spelling.  * The file `de-1901.dic` provides the spelling used between 1901 and 1996.  * The file `misc.dic` is a collection of nouns that are mis-split and    are therefore included in the dictionary so that they won't be split.  * The file `legal.dic` contains legal terms.  Remove it before running    getdicts if you don't want it to be included.  * The file `de-mixed.dic` is a merger of all of the other files.  * The file `nl-NL.dic` is from OpenOffice and provides Dutch spelling    (not currently used).You can add your own wordlists before running `getdicts` if you want.They must be plain UTF-8 text with one word per lineand begin with the correct language code (`de` for German).If the program is not splitting hard enough for your purposes,you may want to find and use a smaller dictionary.Since it is only checked if the exact word is in these dictionariesthe following problem can arise:&quot;Beschwerden&quot; is not split because the dictionaries only contain &quot;Beschwerde&quot;!A solution to this problem would be to do this compound splitting only on the lemmatized text with dictionaries containing lemmatized words.=&gt; TODO: implement this OR make it possible to run it on a list of tokens!TODO: Write more documentation</longdescription>
</pkgmetadata>