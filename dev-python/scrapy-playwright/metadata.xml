<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># scrapy-playwright: Playwright integration for Scrapy[![version](https://img.shields.io/pypi/v/scrapy-playwright.svg)](https://pypi.python.org/pypi/scrapy-playwright)[![pyversions](https://img.shields.io/pypi/pyversions/scrapy-playwright.svg)](https://pypi.python.org/pypi/scrapy-playwright)[![Tests](https://github.com/scrapy-plugins/scrapy-playwright/actions/workflows/tests.yml/badge.svg)](https://github.com/scrapy-plugins/scrapy-playwright/actions/workflows/tests.yml)[![codecov](https://codecov.io/gh/scrapy-plugins/scrapy-playwright/branch/master/graph/badge.svg)](https://codecov.io/gh/scrapy-plugins/scrapy-playwright)A [Scrapy](https://github.com/scrapy/scrapy) Download Handler which performs requests using[Playwright for Python](https://github.com/microsoft/playwright-python).It can be used to handle pages that require JavaScript (among other things),while adhering to the regular Scrapy workflow (i.e. without interferingwith request scheduling, item processing, etc).## RequirementsAfter the release of [version 2.0](https://docs.scrapy.org/en/latest/news.html#scrapy-2-0-0-2020-03-03),which includes [coroutine syntax support](https://docs.scrapy.org/en/2.0/topics/coroutines.html)and [asyncio support](https://docs.scrapy.org/en/2.0/topics/asyncio.html), Scrapy allowsto integrate `asyncio`-based projects such as `Playwright`.### Minimum required versions* Python &gt;= 3.7* Scrapy &gt;= 2.0 (!= 2.4.0)* Playwright &gt;= 1.15## Installation`scrapy-playwright` is available on PyPI and can be installed with `pip`:```pip install scrapy-playwright````playwright` is defined as a dependency so it gets installed automatically,however it might be necessary to install the specific browser(s) that will beused:```playwright install```It's also possible to install only a subset of the available browsers:```playwright install firefox chromium```## ChangelogSee the [changelog](docs/changelog.md) document.## ActivationReplace the default `http` and/or `https` Download Handlers through[`DOWNLOAD_HANDLERS`](https://docs.scrapy.org/en/latest/topics/settings.html):```pythonDOWNLOAD_HANDLERS = {    &quot;http&quot;: &quot;scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler&quot;,    &quot;https&quot;: &quot;scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler&quot;,}```Note that the `ScrapyPlaywrightDownloadHandler` class inherits from the default`http/https` handler. Unless explicitly marked (see [Basic usage](#basic-usage)),requests will be processed by the regular Scrapy download handler.Also, be sure to [install the `asyncio`-based Twisted reactor](https://docs.scrapy.org/en/latest/topics/asyncio.html#installing-the-asyncio-reactor):```pythonTWISTED_REACTOR = &quot;twisted.internet.asyncioreactor.AsyncioSelectorReactor&quot;```## Basic usageSet the [`playwright`](#playwright) [Request.meta](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta)key to download a request using Playwright:```pythonimport scrapyclass AwesomeSpider(scrapy.Spider):    name = &quot;awesome&quot;    def start_requests(self):        # GET request        yield scrapy.Request(&quot;https://httpbin.org/get&quot;, meta={&quot;playwright&quot;: True})        # POST request        yield scrapy.FormRequest(            url=&quot;https://httpbin.org/post&quot;,            formdata={&quot;foo&quot;: &quot;bar&quot;},            meta={&quot;playwright&quot;: True},        )    def parse(self, response):        # 'response' contains the page as seen by the browser        return {&quot;url&quot;: response.url}```### Notes about the User-Agent headerBy default, outgoing requests include the `User-Agent` set by Scrapy (either with the`USER_AGENT` or `DEFAULT_REQUEST_HEADERS` settings or via the `Request.headers` attribute).This could cause some sites to react in unexpected ways, for instance if the user agentdoes not match the running Browser. If you prefer the `User-Agent` sent bydefault by the specific browser you're using, set the Scrapy user agent to `None`.## Supported [settings](https://docs.scrapy.org/en/latest/topics/settings.html)### `PLAYWRIGHT_BROWSER_TYPE`Type `str`, default `&quot;chromium&quot;`.The browser type to be launched, e.g. `chromium`, `firefox`, `webkit`.```pythonPLAYWRIGHT_BROWSER_TYPE = &quot;firefox&quot;```### `PLAYWRIGHT_LAUNCH_OPTIONS`Type `dict`, default `{}`A dictionary with options to be passed as keyword arguments when launching theBrowser. See the docs for[`BrowserType.launch`](https://playwright.dev/python/docs/api/class-browsertype#browser-type-launch)for a list of supported keyword arguments.```pythonPLAYWRIGHT_LAUNCH_OPTIONS = {    &quot;headless&quot;: False,    &quot;timeout&quot;: 20 * 1000,  # 20 seconds}```### `PLAYWRIGHT_CONTEXTS`Type `dict[str, dict]`, default `{}`A dictionary which defines Browser contexts to be created on startup.It should be a mapping of (name, keyword arguments).```pythonPLAYWRIGHT_CONTEXTS = {    &quot;foobar&quot;: {        &quot;context_arg1&quot;: &quot;value&quot;,        &quot;context_arg2&quot;: &quot;value&quot;,    },    &quot;default&quot;: {        &quot;context_arg1&quot;: &quot;value&quot;,        &quot;context_arg2&quot;: &quot;value&quot;,    },    &quot;persistent&quot;: {        &quot;user_data_dir&quot;: &quot;/path/to/dir&quot;,  # will be a persistent context        &quot;context_arg1&quot;: &quot;value&quot;,    },}```See the section on [browser contexts](#browser-contexts) for more information.See also the docs for [`Browser.new_context`](https://playwright.dev/python/docs/api/class-browser#browser-new-context).### `PLAYWRIGHT_MAX_CONTEXTS`Type `Optional[int]`, default `None`Maximum amount of allowed concurrent Playwright contexts. If unset or `None`,no limit is enforced. See the [Maximum concurrent context count](#maximum-concurrent-context-count)section for more information.```pythonPLAYWRIGHT_MAX_CONTEXTS = 8```### `PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT`Type `Optional[float]`, default `None`Timeout to be used when requesting pages by Playwright, in milliseconds. If`None` or unset, the default value will be used (30000 ms at the time of writing).See the docs for [BrowserContext.set_default_navigation_timeout](https://playwright.dev/python/docs/api/class-browsercontext#browser-context-set-default-navigation-timeout).```pythonPLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT = 10 * 1000  # 10 seconds```### `PLAYWRIGHT_PROCESS_REQUEST_HEADERS`Type `Optional[Union[Callable, str]]`, default `scrapy_playwright.headers.use_scrapy_headers`A function (or the path to a function) that processes headers for a given requestand returns a dictionary with the headers to be used (note that, depending on the browser,additional default headers could be sent as well). Coroutine functions (`async def`) aresupported.This will be called at least once for each Scrapy request (receiving said request and thecorresponding Playwright request), but it could be called additional times if the givenresource generates more requests (e.g. to retrieve assets like images or scripts).The function must return a `dict` object, and receives the following positional arguments:```python- browser_type: str- playwright_request: playwright.async_api.Request- scrapy_headers: scrapy.http.headers.Headers```The default function (`scrapy_playwright.headers.use_scrapy_headers`) tries toemulate Scrapy's behaviour for navigation requests, i.e. overriding headerswith their values from the Scrapy request. For non-navigation requests (e.g.images, stylesheets, scripts, etc), only the `User-Agent` header is overriden,for consistency.Setting `PLAYWRIGHT_PROCESS_REQUEST_HEADERS=None` will give complete control toPlaywright, i.e. headers from Scrapy requests will be ignored and only headersset by Playwright will be sent. Keep in mind that in this case, headers passedvia the `Request.headers` attribute or set by Scrapy components are ignored(including cookies set via the `Request.cookies` attribute).```pythondef custom_headers(    browser_type: str,    playwright_request: playwright.async_api.Request,    scrapy_headers: scrapy.http.headers.Headers,) -&gt; dict:    if browser_type == &quot;firefox&quot;:        return {&quot;User-Agent&quot;: &quot;foo&quot;}    return {&quot;User-Agent&quot;: &quot;bar&quot;}PLAYWRIGHT_PROCESS_REQUEST_HEADERS = custom_headers```### `PLAYWRIGHT_MAX_PAGES_PER_CONTEXT`Type `int`, defaults to the value of Scrapy's `CONCURRENT_REQUESTS` settingMaximum amount of allowed concurrent Playwright pages for each context.See the [notes about leaving unclosed pages](#receiving-page-objects-in-callbacks).```pythonPLAYWRIGHT_MAX_PAGES_PER_CONTEXT = 4```### `PLAYWRIGHT_ABORT_REQUEST`Type `Optional[Union[Callable, str]]`, default `None`A predicate function (or the path to a function) that receives a[`playwright.async_api.Request`](https://playwright.dev/python/docs/api/class-request)object and must return `True` if the request should be aborted, `False` otherwise.Coroutine functions (`async def`) are supported.Note that all requests will appear in the DEBUG level logs, however there willbe no corresponding response log lines for aborted requests. Aborted requestsare counted in the `playwright/request_count/aborted` job stats item.```pythondef should_abort_request(request):    return (        request.resource_type == &quot;image&quot;        or &quot;.jpg&quot; in request.url    )PLAYWRIGHT_ABORT_REQUEST = should_abort_request```### General note about settingsFor settings that accept object paths as strings, passing callable objects isonly supported when using Scrapy&gt;=2.4. With prior versions, only strings aresupported.## Supported [`Request.meta`](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta) keys### `playwright`Type `bool`, default `False`If set to a value that evaluates to `True` the request will be processed by Playwright.```pythonreturn scrapy.Request(&quot;https://example.org&quot;, meta={&quot;playwright&quot;: True})```### `playwright_context`Type `str`, default `&quot;default&quot;`Name of the context to be used to download the request.See the section on [browser contexts](#browser-contexts) for more information.```pythonreturn scrapy.Request(    url=&quot;https://example.org&quot;,    meta={        &quot;playwright&quot;: True,        &quot;playwright_context&quot;: &quot;awesome_context&quot;,    },)```### `playwright_context_kwargs`Type `dict`, default `{}`A dictionary with keyword arguments to be used when creating a new context, if a contextwith the name specified in the `playwright_context` meta key does not exist already.See the section on [browser contexts](#browser-contexts) for more information.```pythonreturn scrapy.Request(    url=&quot;https://example.org&quot;,    meta={        &quot;playwright&quot;: True,        &quot;playwright_context&quot;: &quot;awesome_context&quot;,        &quot;playwright_context_kwargs&quot;: {            &quot;ignore_https_errors&quot;: True,        },    },)```### `playwright_include_page`Type `bool`, default `False`If `True`, the [Playwright page](https://playwright.dev/python/docs/api/class-page)that was used to download the request will be available in the callback at`response.meta['playwright_page']`. If `False` (or unset) the page will beclosed immediately after processing the request.**Important!**This meta key is entirely optional, it's NOT necessary for the page to load or for anyasynchronous operation to be performed (specifically, it's NOT necessary for `PageMethod`objects to be applied). Use it only if you need access to the Page object in the callbackthat handles the response.For more information and important notes see[Receiving Page objects in callbacks](#receiving-page-objects-in-callbacks).```pythonreturn scrapy.Request(    url=&quot;https://example.org&quot;,    meta={&quot;playwright&quot;: True, &quot;playwright_include_page&quot;: True},)```### `playwright_page_event_handlers`Type `Dict[Str, Callable]`, default `{}`A dictionary of handlers to be attached to page events.See [Handling page events](#handling-page-events).### `playwright_page_init_callback`Type `Optional[Union[Callable, str]]`, default `None`A coroutine function (`async def`) to be invoked for newly created pages.Called after attaching page event handlers &amp; setting up internal routehandling, before making any request. It receives the Playwright page and theScrapy request as positional arguments. Useful for initialization code.Ignored if the page for the request already exists (e.g. by passing`playwright_page`).```pythonasync def init_page(page, request):    await page.add_init_script(path=&quot;./custom_script.js&quot;)class AwesomeSpider(scrapy.Spider):    def start_requests(self):        yield scrapy.Request(            url=&quot;https://httpbin.org/headers&quot;,            meta={                &quot;playwright&quot;: True,                &quot;playwright_page_init_callback&quot;: init_page,            },        )```**Important!**`scrapy-playwright` uses `Page.route` &amp; `Page.unroute` internally, avoid usingthese methods unless you know exactly what you're doing.### `playwright_page_methods`Type `Iterable[PageMethod]`, default `()`An iterable of [`scrapy_playwright.page.PageMethod`](#pagemethod-class)objects to indicate actions to be performed on the page before returning thefinal response. See [Executing actions on pages](#executing-actions-on-pages).### `playwright_page`Type `Optional[playwright.async_api.Page]`, default `None`A [Playwright page](https://playwright.dev/python/docs/api/class-page) to be used todownload the request. If unspecified, a new page is created for each request.This key could be used in conjunction with `playwright_include_page` to make a chain ofrequests using the same page. For instance:```pythondef start_requests(self):    yield scrapy.Request(        url=&quot;https://httpbin.org/get&quot;,        meta={&quot;playwright&quot;: True, &quot;playwright_include_page&quot;: True},    )def parse(self, response):    page = response.meta[&quot;playwright_page&quot;]    yield scrapy.Request(        url=&quot;https://httpbin.org/headers&quot;,        callback=self.parse_headers,        meta={&quot;playwright&quot;: True, &quot;playwright_page&quot;: page},    )```### `playwright_page_goto_kwargs`Type `dict`, default `{}`A dictionary with keyword arguments to be passed to the page's[`goto` method](https://playwright.dev/python/docs/api/class-page#page-goto)when navigating to an URL. The `url` key is ignored if present, the requestURL is used instead.```pythonreturn scrapy.Request(    url=&quot;https://example.org&quot;,    meta={        &quot;playwright&quot;: True,        &quot;playwright_page_goto_kwargs&quot;: {            &quot;wait_until&quot;: &quot;networkidle&quot;,        },    },)```### `playwright_security_details`Type `Optional[dict]`, read onlyA dictionary with [security information](https://playwright.dev/python/docs/api/class-response#response-security-details)about the give response. Only available for HTTPS requests. Could be accessedin the callback via `response.meta['playwright_security_details']````pythondef parse(self, response):    print(response.meta[&quot;playwright_security_details&quot;])    # {'issuer': 'DigiCert TLS RSA SHA256 2020 CA1', 'protocol': 'TLS 1.3', 'subjectName': 'www.example.org', 'validFrom': 1647216000, 'validTo': 1678838399}```## Receiving Page objects in callbacksSpecifying a value that evaluates to `True` in the[`playwright_include_page`](#playwright_include_page) meta key for arequest will result in the corresponding `playwright.async_api.Page` objectbeing available in the `playwright_page` meta key in the request callback.In order to be able to `await` coroutines on the provided `Page` object,the callback needs to be defined as a coroutine function (`async def`).**Caution**Use this carefully, and only if you really need to do things with the Pageobject in the callback. If pages are not properly closed after they are no longernecessary the spider job could get stuck because of the limit set by the`PLAYWRIGHT_MAX_PAGES_PER_CONTEXT` setting.```pythonimport scrapyclass AwesomeSpiderWithPage(scrapy.Spider):    name = &quot;page_spider&quot;    def start_requests(self):        yield scrapy.Request(            url=&quot;https://example.org&quot;,            callback=self.parse_first,            meta={&quot;playwright&quot;: True, &quot;playwright_include_page&quot;: True},            errback=self.errback_close_page,        )    def parse_first(self, response):        page = response.meta[&quot;playwright_page&quot;]        return scrapy.Request(            url=&quot;https://example.com&quot;,            callback=self.parse_second,            meta={&quot;playwright&quot;: True, &quot;playwright_include_page&quot;: True, &quot;playwright_page&quot;: page},            errback=self.errback_close_page,        )    async def parse_second(self, response):        page = response.meta[&quot;playwright_page&quot;]        title = await page.title()  # &quot;Example Domain&quot;        await page.close()        return {&quot;title&quot;: title}    async def errback_close_page(self, failure):        page = failure.request.meta[&quot;playwright_page&quot;]        await page.close()```**Notes:*** When passing `playwright_include_page=True`, make sure pages are always closed  when they are no longer used. It's recommended to set a Request errback to make  sure pages are closed even if a request fails (if `playwright_include_page=False`  pages are automatically closed upon encountering an exception).  This is important, as open pages count towards the limit set by  `PLAYWRIGHT_MAX_PAGES_PER_CONTEXT` and crawls could freeze if the limit is reached  and pages remain open indefinitely.* Defining callbacks as `async def` is only necessary if you need to `await` things,  it's NOT necessary if you just need to pass over the Page object from one callback  to another (see the example above).* Any network operations resulting from awaiting a coroutine on a Page object  (`goto`, `go_back`, etc) will be executed directly by Playwright, bypassing the  Scrapy request workflow (Scheduler, Middlewares, etc).## Browser contextsMultiple [browser contexts](https://playwright.dev/python/docs/browser-contexts)to be launched at startup can be defined via the[`PLAYWRIGHT_CONTEXTS`](#playwright_contexts) setting.### Choosing a specific context for a requestPass the name of the desired context in the `playwright_context` meta key:```pythonyield scrapy.Request(    url=&quot;https://example.org&quot;,    meta={&quot;playwright&quot;: True, &quot;playwright_context&quot;: &quot;first&quot;},)```### Default contextIf a request does not explicitly indicate a context via the `playwright_context`meta key, it falls back to using a general context called `default`. This `default`context can also be customized on startup via the `PLAYWRIGHT_CONTEXTS` setting.### Persistent contextsPass a value for the `user_data_dir` keyword argument to launch a context aspersistent. See also [`BrowserType.launch_persistent_context`](https://playwright.dev/python/docs/api/class-browsertype#browser-type-launch-persistent-context).Note that persistent contexts are launched independently from the main browserinstance, hence keyword arguments passed in the[`PLAYWRIGHT_LAUNCH_OPTIONS`](#playwright_launch_options)setting do not apply.### Creating contexts while crawlingIf the context specified in the `playwright_context` meta key does not exist, it will be created.You can specify keyword arguments to be passed to[`Browser.new_context`](https://playwright.dev/python/docs/api/class-browser#browser-new-context)in the `playwright_context_kwargs` meta key:```pythonyield scrapy.Request(    url=&quot;https://example.org&quot;,    meta={        &quot;playwright&quot;: True,        &quot;playwright_context&quot;: &quot;new&quot;,        &quot;playwright_context_kwargs&quot;: {            &quot;java_script_enabled&quot;: False,            &quot;ignore_https_errors&quot;: True,            &quot;proxy&quot;: {                &quot;server&quot;: &quot;http://myproxy.com:3128&quot;,                &quot;username&quot;: &quot;user&quot;,                &quot;password&quot;: &quot;pass&quot;,            },        },    },)```Please note that if a context with the specified name already exists,that context is used and `playwright_context_kwargs` are ignored.### Closing contexts while crawlingAfter [receiving the Page object in your callback](#receiving-page-objects-in-callbacks),you can access a context though the corresponding [`Page.context`](https://playwright.dev/python/docs/api/class-page#page-context)attribute, and await [`close`](https://playwright.dev/python/docs/api/class-browsercontext#browser-context-close) on it.```pythondef parse(self, response):    yield scrapy.Request(        url=&quot;https://example.org&quot;,        callback=self.parse_in_new_context,        errback=self.close_context_on_error,        meta={            &quot;playwright&quot;: True,            &quot;playwright_context&quot;: &quot;awesome_context&quot;,            &quot;playwright_include_page&quot;: True,        },    )async def parse_in_new_context(self, response):    page = response.meta[&quot;playwright_page&quot;]    title = await page.title()    await page.close()    await page.context.close()    return {&quot;title&quot;: title}async def close_context_on_error(self, failure):    page = failure.request.meta[&quot;playwright_page&quot;]    await page.close()    await page.context.close()```### Avoid race conditions &amp; memory leaks when closing contextsMake sure to close the page before closing the context. See[this comment](https://github.com/scrapy-plugins/scrapy-playwright/issues/191#issuecomment-1548097114)in [#191](https://github.com/scrapy-plugins/scrapy-playwright/issues/191)for more information.### Maximum concurrent context countSpecify a value for the `PLAYWRIGHT_MAX_CONTEXTS` setting to limit the amountof concurent contexts. Use with caution: it's possible to block the whole crawlif contexts are not closed after they are no longer used (refer to[this section](#closing-contexts-while-crawling) to dinamically close contexts).Make sure to define an errback to still close contexts even if there are errors.## Proxy supportProxies are supported at the Browser level by specifying the `proxy` key inthe `PLAYWRIGHT_LAUNCH_OPTIONS` setting:```pythonfrom scrapy import Spider, Requestclass ProxySpider(Spider):    name = &quot;proxy&quot;    custom_settings = {        &quot;PLAYWRIGHT_LAUNCH_OPTIONS&quot;: {            &quot;proxy&quot;: {                &quot;server&quot;: &quot;http://myproxy.com:3128&quot;,                &quot;username&quot;: &quot;user&quot;,                &quot;password&quot;: &quot;pass&quot;,            },        }    }    def start_requests(self):        yield Request(&quot;http://httpbin.org/get&quot;, meta={&quot;playwright&quot;: True})    def parse(self, response):        print(response.text)```Proxies can also be set at the context level with the `PLAYWRIGHT_CONTEXTS` setting:```pythonPLAYWRIGHT_CONTEXTS = {    &quot;default&quot;: {        &quot;proxy&quot;: {            &quot;server&quot;: &quot;http://default-proxy.com:3128&quot;,            &quot;username&quot;: &quot;user1&quot;,            &quot;password&quot;: &quot;pass1&quot;,        },    },    &quot;alternative&quot;: {        &quot;proxy&quot;: {            &quot;server&quot;: &quot;http://alternative-proxy.com:3128&quot;,            &quot;username&quot;: &quot;user2&quot;,            &quot;password&quot;: &quot;pass2&quot;,        },    },}```Or passing a `proxy` key when [creating contexts while crawling](#creating-contexts-while-crawling).See also:* [`zyte-smartproxy-playwright`](https://github.com/zytedata/zyte-smartproxy-playwright):  seamless support for [Zyte Smart Proxy Manager](https://www.zyte.com/smart-proxy-manager/)  in the Node.js version of Playwright.* the [upstream Playwright for Python section](https://playwright.dev/python/docs/network#http-proxy)  on HTTP Proxies.## Executing actions on pagesA sorted iterable (e.g. `list`, `tuple`, `dict`) of `PageMethod` objectscould be passed in the `playwright_page_methods`[Request.meta](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta)key to request methods to be invoked on the `Page` object before returning the final`Response` to the callback.This is useful when you need to perform certain actions on a page (like scrollingdown or clicking links) and you want to handle only the final result in your callback.### `PageMethod` class#### `scrapy_playwright.page.PageMethod(method: str, *args, **kwargs)`:Represents a method to be called (and awaited if necessary) on a`playwright.page.Page` object (e.g. &quot;click&quot;, &quot;screenshot&quot;, &quot;evaluate&quot;, etc).`method` is the name of the method, `*args` and `**kwargs`are passed when calling such method. The return valuewill be stored in the `PageMethod.result` attribute.For instance:```pythondef start_requests(self):    yield Request(        url=&quot;https://example.org&quot;,        meta={            &quot;playwright&quot;: True,            &quot;playwright_page_methods&quot;: [                PageMethod(&quot;screenshot&quot;, path=&quot;example.png&quot;, full_page=True),            ],        },    )def parse(self, response):    screenshot = response.meta[&quot;playwright_page_methods&quot;][0]    # screenshot.result contains the image's bytes```produces the same effect as:```pythondef start_requests(self):    yield Request(        url=&quot;https://example.org&quot;,        meta={&quot;playwright&quot;: True, &quot;playwright_include_page&quot;: True},    )async def parse(self, response):    page = response.meta[&quot;playwright_page&quot;]    screenshot = await page.screenshot(path=&quot;example.png&quot;, full_page=True)    # screenshot contains the image's bytes    await page.close()```### Supported methodsRefer to the [upstream docs for the `Page` class](https://playwright.dev/python/docs/api/class-page)to see available methods.### Impact on Response objectsCertain `Response` attributes (e.g. `url`, `ip_address`) reflect the state after the lastaction performed on a page. If you issue a `PageMethod` with an action that results ina navigation (e.g. a `click` on a link), the `Response.url` attribute will point to thenew URL, which might be different from the request's URL.## Handling page eventsA dictionary of Page event handlers can be specified in the `playwright_page_event_handlers`[Request.meta](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta) key.Keys are the name of the event to be handled (e.g. `dialog`, `download`, etc).Values can be either callables or strings (in which case a spider method with the name will be looked up).Example:```pythonfrom playwright.async_api import Dialogasync def handle_dialog(dialog: Dialog) -&gt; None:    logging.info(f&quot;Handled dialog with message: {dialog.message}&quot;)    await dialog.dismiss()class EventSpider(scrapy.Spider):    name = &quot;event&quot;    def start_requests(self):        yield scrapy.Request(            url=&quot;https://example.org&quot;,            meta={                &quot;playwright&quot;: True,                &quot;playwright_page_event_handlers&quot;: {                    &quot;dialog&quot;: handle_dialog,                    &quot;response&quot;: &quot;handle_response&quot;,                },            },        )    async def handle_response(self, response: PlaywrightResponse) -&gt; None:        logging.info(f&quot;Received response with URL {response.url}&quot;)```See the [upstream `Page` docs](https://playwright.dev/python/docs/api/class-page)for a list of the accepted events and the arguments passed to their handlers.### Notes about page event handlers* Event handlers will remain attached to the page and will be called for  subsequent downloads using the same page unless they are  [removed later](https://playwright.dev/python/docs/events#addingremoving-event-listener).  This is usually not a problem, since by default requests are performed in  single-use pages.* Event handlers will process Playwright objects, not Scrapy ones. For example,  for each Scrapy request/response there will be a matching Playwright  request/response, but not the other way: background requests/responses to get  images, scripts, stylesheets, etc are not seen by Scrapy.## Examples**Click on a link, save the resulting page as PDF**```pythonclass ClickAndSavePdfSpider(scrapy.Spider):    name = &quot;pdf&quot;    def start_requests(self):        yield scrapy.Request(            url=&quot;https://example.org&quot;,            meta=dict(                playwright=True,                playwright_page_methods={                    &quot;click&quot;: PageMethod(&quot;click&quot;, selector=&quot;a&quot;),                    &quot;pdf&quot;: PageMethod(&quot;pdf&quot;, path=&quot;/tmp/file.pdf&quot;),                },            ),        )    def parse(self, response):        pdf_bytes = response.meta[&quot;playwright_page_methods&quot;][&quot;pdf&quot;].result        with open(&quot;iana.pdf&quot;, &quot;wb&quot;) as fp:            fp.write(pdf_bytes)        yield {&quot;url&quot;: response.url}  # response.url is &quot;https://www.iana.org/domains/reserved&quot;```**Scroll down on an infinite scroll page, take a screenshot of the full page**```pythonclass ScrollSpider(scrapy.Spider):    name = &quot;scroll&quot;    def start_requests(self):        yield scrapy.Request(            url=&quot;http://quotes.toscrape.com/scroll&quot;,            meta=dict(                playwright=True,                playwright_include_page=True,                playwright_page_methods=[                    PageMethod(&quot;wait_for_selector&quot;, &quot;div.quote&quot;),                    PageMethod(&quot;evaluate&quot;, &quot;window.scrollBy(0, document.body.scrollHeight)&quot;),                    PageMethod(&quot;wait_for_selector&quot;, &quot;div.quote:nth-child(11)&quot;),  # 10 per page                ],            ),        )    async def parse(self, response):        page = response.meta[&quot;playwright_page&quot;]        await page.screenshot(path=&quot;quotes.png&quot;, full_page=True)        await page.close()        return {&quot;quote_count&quot;: len(response.css(&quot;div.quote&quot;))}  # quotes from several pages```See the [examples](examples) directory for more.## Known issues### Lack of native support for WindowsThis package does not work natively on Windows. This is because:* Playwright runs the driver in a subprocess. Source:[Playwright repository](https://github.com/microsoft/playwright-python/blob/v1.28.0/playwright/_impl/_transport.py#L120-L129).* &quot;On Windows, the default event loop `ProactorEventLoop` supports subprocesses,whereas `SelectorEventLoop` does not&quot;. Source:[Python docs](https://docs.python.org/3/library/asyncio-platforms.html#asyncio-windows-subprocess).* Twisted's `asyncio` reactor requires the `SelectorEventLoop`. Source:[Twisted repository](https://github.com/twisted/twisted/blob/twisted-22.4.0/src/twisted/internet/asyncioreactor.py#L31).Some users have reported having success[running under WSL](https://github.com/scrapy-plugins/scrapy-playwright/issues/7#issuecomment-817394494).See also [#78](https://github.com/scrapy-plugins/scrapy-playwright/issues/78)for information about working in headful mode under WSL.### No per-request proxy supportSpecifying a proxy via the `proxy` Request meta key is not supported.Refer to the [Proxy support](#proxy-support) section for more information.### Unsopported signalsThe `headers_received` and `bytes_received` signals are not fired by thescrapy-playwright download handler.## Reporting issuesBefore opening an issue please make sure the unexpected behavior can only beobserved by using this package and not with standalone Playwright. To do this,translate your spider code to a reasonably close Playwright script: if theissue also occurs this way, you should instead report it[upstream](https://github.com/microsoft/playwright-python).For instance:```pythonimport scrapyclass ExampleSpider(scrapy.Spider):    name = &quot;example&quot;    def start_requests(self):        yield scrapy.Request(            url=&quot;https://example.org&quot;,            meta=dict(                playwright=True,                playwright_page_methods=[                    PageMethod(&quot;screenshot&quot;, path=&quot;example.png&quot;, full_page=True),                ],            ),        )```translates roughly to:```pythonimport asynciofrom playwright.async_api import async_playwrightasync def main():    async with async_playwright() as pw:        browser = await pw.chromium.launch()        page = await browser.new_page()        await page.goto(&quot;https://example.org&quot;)        await page.screenshot(path=&quot;example.png&quot;, full_page=True)        await browser.close()asyncio.run(main())```## Frequently Asked QuestionsSee the [FAQ](docs/faq.md) document.## Deprecation policyDeprecated features will be supported for at least six monthsfollowing the release that deprecated them. After that, theymay be removed at any time. See the [changelog](docs/changelog.md)for more information about deprecations and removals.</longdescription>
</pkgmetadata>