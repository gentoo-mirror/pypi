<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># SKT Package[![Actions Status](https://github.com/sktaiflow/skt/workflows/release/badge.svg)](https://github.com/sktaiflow/skt/actions)This is highly site dependent package.Resources are abstracted into package structure.## UsageHive metastore```pythonfrom skt.ye import get_hmsc = get_hms()c.get_partition_names(&quot;db&quot;, &quot;table&quot;)c.close()```Hash and unhash```pythonfrom skt.lake import hash_sfrom skt.lake import unhash_sunhashed_list = ['0000000000']hashed_list = hash_s(unhashed_list)unhash_s(hashed_list)```Execute hive query without fetch result```pythonfrom skt.ye import hive_executehive_execute(ddl_or_ctas_query)```Fetch resultset from hive query```pythonfrom skt.ye import hive_get_resultresult_set = hive_get_result(select_query)```Get pandas dataframe from hive qeruy resultset```pythonfrom skt.ye import hive_to_pandaspandas_df = hive_to_pandas(hive_query)```Get pandas dataframe from parquet file in hdfs```pythonfrom skt.ye import parquet_to_pandaspandas_df = parquet_to_pandas(hdfs_path)```Save pandas dataframe as parquet in hdfs```pythonfrom skt.ye import get_sparkfrom skt.ye import pandas_to_parquetspark = get_spark()pandas_to_parquet(pandas_df, hdfs_path, spark) # we need spark for this operationspark.stop()```Work with spark```pythonfrom skt.ye import get_sparkspark = get_spark()# do with spark sessionspark.stop()```Work with spark-bigquery-connector```python# SELECTfrom skt.gcp import bq_table_to_pandas pandas_df = bq_table_to_pandas(&quot;dataset&quot;, &quot;table_name&quot;, [&quot;col_1&quot;, &quot;col_2&quot;], &quot;2020-01-01&quot;, &quot;svc_mgmt_num is not null&quot;)# INSERT from skt.gcp import pandas_to_bq_tablepandas_to_bq_table(pandas_df, &quot;dataset&quot;, &quot;table_name&quot;, &quot;2020-03-01&quot;)```Send slack message```pythonfrom skt.ye import slack_sendtext = 'Hello'username = 'airflow'channel = '#leavemealone'slack_send(text=text, username=username, channel=channel)# Send dataframe as textdf = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})slack_send(text=df, username=username, channel=channel, dataframe=True)```Get bigquery client```pythonfrom skt.gcp import get_bigquery_clientbq = get_bigquery_client()bq.query(query)```IPython BigQuery Magic```pythonfrom skt.gcp import import_bigquery_ipython_magicimport_bigquery_ipython_magic()query_params = {    &quot;p_1&quot;: &quot;v_1&quot;,    &quot;dataset&quot;: &quot;mnoai&quot;,}``````python%% bq --params $query_paramsSELECT c_1 FROM {dataset}.user_logsWHERE c_1 = @p_1```Access MLS```pythonfrom skt.mls import set_model_namefrom skt.mls import get_recent_model_pathfrom skt.ye import get_pkl_from_hdfsset_model_name(COMM_DB, params)path = get_recent_model_path(COMM_DB, model_key)model = get_pkl_from_hdfs(f'{path})```MLS Model Registry (Upload model_binary(model.tar.gz) / model_meta(model.json) to AWS S3 from YE)```pythonfrom skt.mls import save_model# model object generated by LightGBM or XGBoostmodel# model namemodel_name = 'sample_model'# model versionmodel_version = 'v1'# AWS ENV in 'stg / prd / dev' (default is 'stg')aws_env = 'stg'# List of features used in ML Model in string type (only for XGBoost model_type)feature_list = ['feature_1', 'feature_2', 'feature_3']# Force to overwrite model files on S3 if exists (default is False)force = False save_model(model, model_name, model_version, aws_env, force)```MLS *meta_table* &amp; *meta_table_item* related methods```pythonfrom skt.mls import get_meta_tablefrom skt.mls import create_meta_table_itemfrom skt.mls import update_meta_table_itemfrom skt.mls import get_meta_table_itemfrom skt.mls import meta_table_to_pandasfrom skt.mls import pandas_to_meta_table# Get a meta_table infoget_meta_table(meta_table_name, aws_env, edd)# Create a meta_itemcreate_meta_table_item(meta_table_name, item_name, item_dict, aws_env, edd)# Update a meta_itemupdate_meta_table_item(meta_table_name, item_name, item_dict, aws_env, edd)# Get a meta_itemget_meta_table_item(meta_table_name, item_name, aws_env, edd)# Get a meta_table as pandas dataframemeta_table_to_pandas(meta_table_name, aws_env, edd)# Update pandas dataframe to meta_tablepandas_to_meta_table(method, meta_table_name, dataframe, key, values, aws_env, edd)# For the detal, use ?{method} to get detailed info (ex. ?get_meta_table)# For the user of EDD, must set edd=True```MLS *model_meta* related methods  (*Need to set *user* for the ml_model)```pythonfrom skt.mls import get_ml_modelfrom skt.mls import create_meta_table_itemfrom skt.mls import update_meta_table_item# Get a ml_modelget_ml_model(user, model_name, model_version, aws_env, edd)# Get a model_meta of ml_modelget_ml_model_meta(user, model_name, model_version, aws_env, edd)# Update or Create meta_item(s)update_ml_model_meta(user, model_name, model_version, model_meta_dict, aws_env, edd)# For the detal, use ?{method} to get detailed info (ex. ?get_ml_model)# For the user of EDD, must set edd=True```Use NES CLI```basnes input_notebook_url -p k1 v1 -p k2 v2 -p k3 v3```Use github util```pythonfrom skt.ye import get_github_utilg = get_github_util# query graphqlres = g.query_gql(graph_ql)# get file in github repositorybyte_object = g.download_from_git(github_url_path)```## Installation```sh$ pip install skt --upgrade```If you would like to install submodules for AIR```sh$ pip install skt[air] --upgrade```## DevelopCreate issue first and follow the GitHub flowhttps://help.github.com/en/github/collaborating-with-issues-and-pull-requests/github-flow# AIPS EDA tools## OVERVIEW- **Modeling EDA** 시 활용할 수 있는 기능의 공통 module- **Modules**    - 1) EDA (Nuemric / Categorical variable)&lt;br&gt;&lt;br&gt;## 1) EDA#### 1. Numeric variable EDA- **def** *numeric_eda_plot*```    Numeric feature에 대한 EDA Plot function    Args. :        - df           :   Pandas DataFrame 형태의 EDA대상 데이터        - feature_list :   EDA 대상 feature list (df의 columns)        - label_col    :   Label(or Hue) column        - cols         :   Multi-plot 시 grid column 개수 (row 개수는 feature_list에 따라 자동으로 결정 됨)        - n_samples    :   Label 별 sampling 할 개수 (default = -1(전수 데이터로 EDA할 경우))        - plot_type    :   density or box (default = 'density')        - stat_yn      :   기초 통계량 출력여부 (mean / min / max / 1q / 3q) (default : False)        - figsize      :   (default : (7,4))    Returns :         matplotlib.pyplot object    Example :         fig = numeric_eda_plot(df, ['age'], 'answer', cols = 1, n_samples = 10000, plot_type='density', stat_yn=True, figsize = (7,4))        fig        if want to Save the EDA images,        fig.savefig('filename')```#### 2. Categorical variable EDA- **def** *categorical_eda_plot*```    Categorical feature에 대한 EDA Plot function    Args. :        - df           :   Pandas DataFrame 형태의 EDA대상 데이터        - feature_list :   EDA 대상 feature list (df의 columns)        - label_col    :   Label(or Hue) column        - cols         :   Multi-plot 시 grid column 개수 (row 개수는 feature_list에 따라 자동으로 결정 됨)        - n_samples    :   Label 별 sampling 할 개수 (default = -1(전수 데이터로 EDA할 경우))        - figsize      :   (default : (7,4))    Returns :         matplotlib.pyplot object    Example :         Example :         fig = categorical_eda_plot(df, ['sex_cd'], 'answer', cols = 1, n_samples = 10000, figsize = (7,4))        fig        if want to Save the EDA images,        fig.savefig('filename')```</longdescription>
</pkgmetadata>