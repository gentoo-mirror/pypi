<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/setfit/main/assets/setfit.png&quot;&gt;&lt;p align=&quot;center&quot;&gt;    ü§ó &lt;a href=&quot;https://huggingface.co/setfit&quot; target=&quot;_blank&quot;&gt;Models &amp; Datasets&lt;/a&gt; | üìñ &lt;a href=&quot;https://huggingface.co/blog/setfit&quot; target=&quot;_blank&quot;&gt;Blog&lt;/a&gt; | üìÉ &lt;a href=&quot;https://arxiv.org/abs/2209.11055&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;# SetFit - Efficient Few-shot Learning with Sentence TransformersSetFit is an efficient and prompt-free framework for few-shot fine-tuning of [Sentence Transformers](https://sbert.net/). It achieves high accuracy with little labeled data - for instance, with only 8 labeled examples per class on the Customer Reviews sentiment dataset, SetFit is competitive with fine-tuning RoBERTa Large on the full training set of 3k examples ü§Ø!Compared to other few-shot learning methods, SetFit has several unique features:* üó£ **No prompts or verbalisers:** Current techniques for few-shot fine-tuning require handcrafted prompts or verbalisers to convert examples into a format that's suitable for the underlying language model. SetFit dispenses with prompts altogether by generating rich embeddings directly from text examples.* üèé **Fast to train:** SetFit doesn't require large-scale models like T0 or GPT-3 to achieve high accuracy. As a result, it is typically an order of magnitude (or more) faster to train and run inference with.* üåé **Multilingual support**: SetFit can be used with any [Sentence Transformer](https://huggingface.co/models?library=sentence-transformers&amp;sort=downloads) on the Hub, which means you can classify text in multiple languages by simply fine-tuning a multilingual checkpoint.## InstallationDownload and install `setfit` by running:```bashpython -m pip install setfit```If you want the bleeding-edge version, install from source by running:```bashpython -m pip install git+https://github.com/huggingface/setfit.git```## UsageThe examples below provide a quick overview on the various features supported in `setfit`. For more examples, check out the [`notebooks`](https://github.com/huggingface/setfit/tree/main/notebooks) folder.### Training a SetFit model`setfit` is integrated with the [Hugging Face Hub](https://huggingface.co/) and provides two main classes:* `SetFitModel`: a wrapper that combines a pretrained body from `sentence_transformers` and a classification head from either [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) or [`SetFitHead`](https://github.com/huggingface/setfit/blob/main/src/setfit/modeling.py) (a differentiable head built upon `PyTorch` with similar APIs to `sentence_transformers`).* `SetFitTrainer`: a helper class that wraps the fine-tuning process of SetFit.Here is an end-to-end example using a classification head from `scikit-learn`:```pythonfrom datasets import load_datasetfrom sentence_transformers.losses import CosineSimilarityLossfrom setfit import SetFitModel, SetFitTrainer, sample_dataset# Load a dataset from the Hugging Face Hubdataset = load_dataset(&quot;sst2&quot;)# Simulate the few-shot regime by sampling 8 examples per classtrain_dataset = sample_dataset(dataset[&quot;train&quot;], label_column=&quot;label&quot;, num_samples=8)eval_dataset = dataset[&quot;validation&quot;]# Load a SetFit model from Hubmodel = SetFitModel.from_pretrained(&quot;sentence-transformers/paraphrase-mpnet-base-v2&quot;)# Create trainertrainer = SetFitTrainer(    model=model,    train_dataset=train_dataset,    eval_dataset=eval_dataset,    loss_class=CosineSimilarityLoss,    metric=&quot;accuracy&quot;,    batch_size=16,    num_iterations=20, # The number of text pairs to generate for contrastive learning    num_epochs=1, # The number of epochs to use for contrastive learning    column_mapping={&quot;sentence&quot;: &quot;text&quot;, &quot;label&quot;: &quot;label&quot;} # Map dataset columns to text/label expected by trainer)# Train and evaluatetrainer.train()metrics = trainer.evaluate()# Push model to the Hubtrainer.push_to_hub(&quot;my-awesome-setfit-model&quot;)# Download from Hub and run inferencemodel = SetFitModel.from_pretrained(&quot;lewtun/my-awesome-setfit-model&quot;)# Run inferencepreds = model([&quot;i loved the spiderman movie!&quot;, &quot;pineapple on pizza is the worst ü§Æ&quot;])```Here is an end-to-end example using `SetFitHead`:```pythonfrom datasets import load_datasetfrom sentence_transformers.losses import CosineSimilarityLossfrom setfit import SetFitModel, SetFitTrainer, sample_dataset# Load a dataset from the Hugging Face Hubdataset = load_dataset(&quot;sst2&quot;)# Simulate the few-shot regime by sampling 8 examples per classtrain_dataset = sample_dataset(dataset[&quot;train&quot;], label_column=&quot;label&quot;, num_samples=8)eval_dataset = dataset[&quot;validation&quot;]# Load a SetFit model from Hubmodel = SetFitModel.from_pretrained(    &quot;sentence-transformers/paraphrase-mpnet-base-v2&quot;,    use_differentiable_head=True,    head_params={&quot;out_features&quot;: num_classes},)# Create trainertrainer = SetFitTrainer(    model=model,    train_dataset=train_dataset,    eval_dataset=eval_dataset,    loss_class=CosineSimilarityLoss,    metric=&quot;accuracy&quot;,    batch_size=16,    num_iterations=20, # The number of text pairs to generate for contrastive learning    num_epochs=1, # The number of epochs to use for contrastive learning    column_mapping={&quot;sentence&quot;: &quot;text&quot;, &quot;label&quot;: &quot;label&quot;} # Map dataset columns to text/label expected by trainer)# Train and evaluatetrainer.freeze() # Freeze the headtrainer.train() # Train only the body# Unfreeze the head and freeze the body -&gt; head-only trainingtrainer.unfreeze(keep_body_frozen=True)# or# Unfreeze the head and unfreeze the body -&gt; end-to-end trainingtrainer.unfreeze(keep_body_frozen=False)trainer.train(    num_epochs=25, # The number of epochs to train the head or the whole model (body and head)    batch_size=16,    body_learning_rate=1e-5, # The body's learning rate    learning_rate=1e-2, # The head's learning rate    l2_weight=0.0, # Weight decay on **both** the body and head. If `None`, will use 0.01.)metrics = trainer.evaluate()# Push model to the Hubtrainer.push_to_hub(&quot;my-awesome-setfit-model&quot;)# Download from Hub and run inferencemodel = SetFitModel.from_pretrained(&quot;lewtun/my-awesome-setfit-model&quot;)# Run inferencepreds = model([&quot;i loved the spiderman movie!&quot;, &quot;pineapple on pizza is the worst ü§Æ&quot;])```Based on our experiments, `SetFitHead` can achieve similar performance as using a `scikit-learn` head. We use `AdamW` as the optimizer and scale down learning rates by 0.5 every 5 epochs. For more details about the experiments, please check out [here](https://github.com/huggingface/setfit/pull/112#issuecomment-1295773537). We recommend using a large learning rate (e.g. `1e-2`) for `SetFitHead` and a small learning rate (e.g. `1e-5`) for the body in your first attempt.### Training on multilabel datasetsTo train SetFit models on multilabel datasets, specify the `multi_target_strategy` argument when loading the pretrained model:#### Example using a classification head from `scikit-learn`:```pythonfrom setfit import SetFitModelmodel = SetFitModel.from_pretrained(    model_id,    multi_target_strategy=&quot;one-vs-rest&quot;,)```This will initialise a multilabel classification head from `sklearn` - the following options are available for `multi_target_strategy`:* `one-vs-rest`: uses a `OneVsRestClassifier` head.* `multi-output`: uses a `MultiOutputClassifier` head.* `classifier-chain`: uses a `ClassifierChain` head.From here, you can instantiate a `SetFitTrainer` using the same example above, and train it as usual.#### Example using the differentiable `SetFitHead`:```pythonfrom setfit import SetFitModelmodel = SetFitModel.from_pretrained(    model_id,    multi_target_strategy=&quot;one-vs-rest&quot;    use_differentiable_head=True,    head_params={&quot;out_features&quot;: num_classes},)```**Note:** If you use the differentiable `SetFitHead` classifier head, it will automatically use `BCEWithLogitsLoss` for training. The prediction involves a `sigmoid` after which probabilities are rounded to 1 or 0. Furthermore, the `&quot;one-vs-rest&quot;` and `&quot;multi-output&quot;` multi-target strategies are equivalent for the differentiable `SetFitHead`.### Zero-shot text classificationSetFit can also be applied to scenarios where no labels are available. To do so, create a synthetic dataset of training examples:```pythonfrom datasets import Datasetfrom setfit import get_templated_datasetcandidate_labels = [&quot;negative&quot;, &quot;positive&quot;]train_dataset = get_templated_dataset(candidate_labels=candidate_labels, sample_size=8)```This will create examples of the form `&quot;This sentence is {}&quot;`, where the `{}` is filled in with one of the candidate labels. From here you can train a SetFit model as usual:```pythonfrom setfit import SetFitModel, SetFitTrainermodel = SetFitModel.from_pretrained(&quot;sentence-transformers/paraphrase-mpnet-base-v2&quot;)trainer = SetFitTrainer(    model=model,    train_dataset=train_dataset)trainer.train()```We find this approach typically outperforms the [zero-shot pipeline](https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline) in ü§ó Transformers (based on MNLI with Bart), while being 5x faster to generate predictions with.### Running hyperparameter search`SetFitTrainer` provides a `hyperparameter_search()` method that you can use to find good hyperparameters for your data. To use this feature, first install the `optuna` backend:```bashpython -m pip install setfit[optuna]```To use this method, you need to define two functions:* `model_init()`: A function that instantiates the model to be used. If provided, each call to `train()` will start from a new instance of the model as given by this function.* `hp_space()`: A function that defines the hyperparameter search space.Here is an example of a `model_init()` function that we'll use to scan over the hyperparameters associated with the classification head in `SetFitModel`:```pythonfrom setfit import SetFitModeldef model_init(params):    params = params or {}    max_iter = params.get(&quot;max_iter&quot;, 100)    solver = params.get(&quot;solver&quot;, &quot;liblinear&quot;)    params = {        &quot;head_params&quot;: {            &quot;max_iter&quot;: max_iter,            &quot;solver&quot;: solver,        }    }    return SetFitModel.from_pretrained(&quot;sentence-transformers/paraphrase-albert-small-v2&quot;, **params)```Similarly, to scan over hyperparameters associated with the SetFit training process, we can define a `hp_space()` function as follows:```pythondef hp_space(trial):  # Training parameters    return {        &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 1e-6, 1e-4, log=True),        &quot;num_epochs&quot;: trial.suggest_int(&quot;num_epochs&quot;, 1, 5),        &quot;batch_size&quot;: trial.suggest_categorical(&quot;batch_size&quot;, [4, 8, 16, 32, 64]),        &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 1, 40),        &quot;num_iterations&quot;: trial.suggest_categorical(&quot;num_iterations&quot;, [5, 10, 20]),        &quot;max_iter&quot;: trial.suggest_int(&quot;max_iter&quot;, 50, 300),        &quot;solver&quot;: trial.suggest_categorical(&quot;solver&quot;, [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;liblinear&quot;]),    }```**Note:** In practice, we found `num_iterations` to be the most important hyperparameter for the contrastive learning process.The next step is to instantiate a `SetFitTrainer` and call `hyperparameter_search()`:```pythonfrom datasets import Datasetfrom setfit import SetFitTrainerdataset = Dataset.from_dict(            {&quot;text_new&quot;: [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], &quot;label_new&quot;: [0, 1, 2], &quot;extra_column&quot;: [&quot;d&quot;, &quot;e&quot;, &quot;f&quot;]}        )trainer = SetFitTrainer(    train_dataset=dataset,    eval_dataset=dataset,    model_init=model_init,    column_mapping={&quot;text_new&quot;: &quot;text&quot;, &quot;label_new&quot;: &quot;label&quot;},)best_run = trainer.hyperparameter_search(direction=&quot;maximize&quot;, hp_space=hp_space, n_trials=20)```Finally, you can apply the hyperparameters you found to the trainer, and lock in the optimal model, before training fora final time.```pythontrainer.apply_hyperparameters(best_run.hyperparameters, final_model=True)trainer.train()```## Compressing a SetFit model with knowledge distillationIf you have access to unlabeled data, you can use knowledge distillation to compress a trained SetFit model into a smaller version. The result is a model that can run inference much faster, with little to no drop in accuracy. Here's an end-to-end example (see our paper for more details):```pythonfrom datasets import load_datasetfrom sentence_transformers.losses import CosineSimilarityLossfrom setfit import SetFitModel, SetFitTrainer, DistillationSetFitTrainer, sample_dataset# Load a dataset from the Hugging Face Hubdataset = load_dataset(&quot;ag_news&quot;)# Create a sample few-shot dataset to train the teacher modeltrain_dataset_teacher = sample_dataset(dataset[&quot;train&quot;], label_column=&quot;label&quot;, num_samples=16)# Create a dataset of unlabeled examples to train the studenttrain_dataset_student = dataset[&quot;train&quot;].shuffle(seed=0).select(range(500))# Dataset for evaluationeval_dataset = dataset[&quot;test&quot;]# Load teacher modelteacher_model = SetFitModel.from_pretrained(    &quot;sentence-transformers/paraphrase-mpnet-base-v2&quot;)# Create trainer for teacher modelteacher_trainer = SetFitTrainer(    model=teacher_model,    train_dataset=train_dataset_teacher,    eval_dataset=eval_dataset,    loss_class=CosineSimilarityLoss,)# Train teacher modelteacher_trainer.train()# Load small student modelstudent_model = SetFitModel.from_pretrained(&quot;paraphrase-MiniLM-L3-v2&quot;)# Create trainer for knowledge distillationstudent_trainer = DistillationSetFitTrainer(    teacher_model=teacher_model,    train_dataset=train_dataset_student,    student_model=student_model,    eval_dataset=eval_dataset,    loss_class=CosineSimilarityLoss,    metric=&quot;accuracy&quot;,    batch_size=16,    num_iterations=20,    num_epochs=1,)# Train student with knowledge distillationstudent_trainer.train()```## Reproducing the results from the paperWe provide scripts to reproduce the results for SetFit and various baselines presented in Table 2 of our paper. Check out the setup and training instructions in the `scripts/` directory.## Developer installationTo run the code in this project, first create a Python virtual environment using e.g. Conda:```bashconda create -n setfit python=3.9 &amp;&amp; conda activate setfit```Then install the base requirements with:```bashpython -m pip install -e '.[dev]'```This will install `datasets` and packages like `black` and `isort` that we use to ensure consistent code formatting.### Formatting your codeWe use `black` and `isort` to ensure consistent code formatting. After following the installation steps, you can check your code locally by running:```make style &amp;&amp; make quality```## Project structure```‚îú‚îÄ‚îÄ LICENSE‚îú‚îÄ‚îÄ Makefile        &lt;- Makefile with commands like `make style` or `make tests`‚îú‚îÄ‚îÄ README.md       &lt;- The top-level README for developers using this project.‚îú‚îÄ‚îÄ notebooks       &lt;- Jupyter notebooks.‚îú‚îÄ‚îÄ final_results   &lt;- Model predictions from the paper‚îú‚îÄ‚îÄ scripts         &lt;- Scripts for training and inference‚îú‚îÄ‚îÄ setup.cfg       &lt;- Configuration file to define package metadata‚îú‚îÄ‚îÄ setup.py        &lt;- Make this project pip installable with `pip install -e`‚îú‚îÄ‚îÄ src             &lt;- Source code for SetFit‚îî‚îÄ‚îÄ tests           &lt;- Unit tests```## Related work* [jxpress/setfit-pytorch-lightning](https://github.com/jxpress/setfit-pytorch-lightning) - A PyTorch Lightning implementation of SetFit.## Citation```@misc{https://doi.org/10.48550/arxiv.2209.11055,  doi = {10.48550/ARXIV.2209.11055},  url = {https://arxiv.org/abs/2209.11055},  author = {Tunstall, Lewis and Reimers, Nils and Jo, Unso Eun Seo and Bates, Luke and Korat, Daniel and Wasserblat, Moshe and Pereg, Oren},  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},  title = {Efficient Few-Shot Learning Without Prompts},  publisher = {arXiv},  year = {2022},  copyright = {Creative Commons Attribution 4.0 International}}```</longdescription>
</pkgmetadata>