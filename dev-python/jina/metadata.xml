<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;&lt;!-- survey banner start --&gt;&lt;a href=&quot;https://10sw1tcpld4.typeform.com/to/EGAEReM7?utm_source=readme&amp;utm_medium=github&amp;utm_campaign=user%20experience&amp;utm_term=feb2023&amp;utm_content=survey&quot;&gt;  &lt;img src=&quot;./.github/banner.svg?raw=true&quot;&gt;&lt;/a&gt;&lt;!-- survey banner start --&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.jina.ai&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/jina/blob/master/docs/_static/logo-light.svg?raw=true&quot; alt=&quot;Jina logo: Build multimodal AI services via cloud native technologies · Neural Search · Generative AI · Cloud Native&quot; width=&quot;150px&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;&lt;b&gt;Build multimodal AI services with cloud native technologies&lt;/b&gt;&lt;/p&gt;&lt;p align=center&gt;&lt;a href=&quot;https://pypi.org/project/jina/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/jina?label=Release&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;!--&lt;a href=&quot;https://codecov.io/gh/jina-ai/jina&quot;&gt;&lt;img alt=&quot;Codecov branch&quot; src=&quot;https://img.shields.io/codecov/c/github/jina-ai/jina/master?&amp;logo=Codecov&amp;logoColor=white&amp;style=flat-square&quot;&gt;&lt;/a&gt;--&gt;&lt;a href=&quot;https://discord.jina.ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1106542220112302130?logo=discord&amp;logoColor=white&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://pypistats.org/packages/jina&quot;&gt;&lt;img alt=&quot;PyPI - Downloads from official pypistats&quot; src=&quot;https://img.shields.io/pypi/dm/jina?style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/jina-ai/jina/actions/workflows/cd.yml&quot;&gt;&lt;img alt=&quot;Github CD status&quot; src=&quot;https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;!-- start jina-description --&gt;Jina lets you build multimodal [**AI services**](#build-ai-services) and [**pipelines**](#build-a-pipeline) that communicate via gRPC, HTTP and WebSockets, then scale them up and deploy to production. You can focus on your logic and algorithms, without worrying about the infrastructure complexity.![](./.github/images/build-deploy.png)Jina provides a smooth Pythonic experience transitioning from local deployment to advanced orchestration frameworks like Docker-Compose, Kubernetes, or Jina AI Cloud. Jina makes advanced solution engineering and cloud-native technologies accessible to every developer.- Build applications for any [data type](https://docs.docarray.org/data_types/first_steps/), any mainstream [deep learning framework](), and any [protocol](https://docs.jina.ai/concepts/serving/gateway/#set-protocol-in-python).- Design high-performance microservices, with [easy scaling](https://docs.jina.ai/concepts/orchestration/scale-out/), duplex client-server streaming, and async/non-blocking data processing over dynamic flows.- Docker container integration via [Executor Hub](https://cloud.jina.ai), OpenTelemetry/Prometheus observability, and fast Kubernetes/Docker-Compose deployment.- CPU/GPU hosting via [Jina AI Cloud](https://cloud.jina.ai).&lt;details&gt;    &lt;summary&gt;&lt;strong&gt;Wait, how is Jina different from FastAPI?&lt;/strong&gt;&lt;/summary&gt;Jina's value proposition may seem quite similar to that of FastAPI. However, there are several fundamental differences: **Data structure and communication protocols**  - FastAPI communication relies on Pydantic and Jina relies on [DocArray](https://github.com/docarray/docarray) allowing Jina to support multiple protocols  to expose its services. **Advanced orchestration and scaling capabilities**  - Jina lets you deploy applications formed from multiple microservices that can be containerized and scaled independently.  - Jina allows you to easily containerize and orchestrate your services, providing concurrency and scalability. **Journey to the cloud**  - Jina provides a smooth transition from local development (using [DocArray](https://github.com/docarray/docarray)) to local serving using (Jina's orchestration layer)  to having production-ready services by using Kubernetes capacity to orchestrate the lifetime of containers.  - By using [Jina AI Cloud](https://cloud.jina.ai) you have access to scalable and serverless deployments of your applications in one command.&lt;/details&gt;&lt;!-- end jina-description --&gt;## [Documentation](https://docs.jina.ai)## Install ```bashpip install jina```Find more install options on [Apple Silicon](https://docs.jina.ai/get-started/install/apple-silicon-m1-m2/)/[Windows](https://docs.jina.ai/get-started/install/windows/).## Get Started### Basic ConceptsJina has four fundamental concepts:- A [**Document**](https://docarray.jina.ai/) (from [DocArray](https://github.com/docarray/docarray)) is the input/output format in Jina.- An [**Executor**](https://docs.jina.ai/concepts/serving/executor/) is a Python class that transforms and processes Documents.- A [**Deployment**](https://docs.jina.ai/concepts/orchestration/deployment) serves a single Executor, while a [**Flow**](https://docs.jina.ai/concepts/orchestration/flow/) serves Executors chained into a pipeline.[The full glossary is explained here](https://docs.jina.ai/concepts/preliminaries/#).### Build AI Services&lt;!-- start build-ai-services --&gt;Let's build a fast, reliable and scalable gRPC-based AI service. In Jina we call this an **[Executor](https://docs.jina.ai/concepts/executor/)**. Our simple Executor will wrap the [StableLM](https://huggingface.co/stabilityai/stablelm-base-alpha-3b) LLM from Stability AI. We'll then use a **Deployment** to serve it.![](./.github/images/deployment-diagram.png)&gt; **Note**&gt; A Deployment serves just one Executor. To combine multiple Executors into a pipeline and serve that, use a [Flow](#build-a-pipeline).Let's implement the service's logic:&lt;table&gt;&lt;tr&gt;&lt;th&gt;&lt;code&gt;executor.py&lt;/code&gt;&lt;/th&gt; &lt;tr&gt;&lt;td&gt;```pythonfrom jina import Executor, requestsfrom docarray import DocumentArrayfrom transformers import pipelineclass StableLM(Executor):    def __init__(self, **kwargs):        super().__init__(**kwargs)        self.generator = pipeline('text-generation', model='stablelm-3b')    @requests    def generate(self, docs: DocumentArray, **kwargs):        generated_text = self.generator(docs.texts)        docs.texts = [gen[0]['generated_text'] for gen in generated_text]```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;Then we deploy it with either the Python API or YAML:&lt;div class=&quot;table-wrapper&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt; Python API: &lt;code&gt;deployment.py&lt;/code&gt; &lt;/th&gt; &lt;th&gt; YAML: &lt;code&gt;deployment.yml&lt;/code&gt; &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```pythonfrom jina import Deploymentfrom executor import StableLMdep = Deployment(uses=StableLM, timeout_ready=-1, port=12345)with dep:    dep.block()```&lt;/td&gt;&lt;td&gt;```yamljtype: Deploymentwith:  uses: StableLM  py_modules:    - executor.py  timeout_ready: -1  port: 12345```And run the YAML Deployment with the CLI: `jina deployment --uses deployment.yml`&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;Use [Jina Client](https://docs.jina.ai/concepts/client/) to make requests to the service:```pythonfrom docarray import Documentfrom jina import Clientprompt = Document(    tags = {'prompt': 'suggest an interesting image generation prompt for a mona lisa variant'})client = Client(port=12345)  # use port from output aboveresponse = client.post(on='/', inputs=[prompt])print(response[0].text)``````texta steampunk version of the Mona Lisa, incorporating mechanical gears, brass elements, and Victorian era clothing details```&lt;!-- end build-ai-services --&gt;&gt; **Note**&gt; In a notebook, you can't use `deployment.block()` and then make requests to the client. Please refer to the Colab link above for reproducible Jupyter Notebook code snippets.### Build a pipeline&lt;!-- start build-pipelines --&gt;Sometimes you want to chain microservices together into a pipeline. That's where a [Flow](https://docs.jina.ai/concepts/flow/) comes in.A Flow is a [DAG](https://de.wikipedia.org/wiki/DAG) pipeline, composed of a set of steps, It orchestrates a set of [Executors](https://docs.jina.ai/concepts/executor/) and a [Gateway](https://docs.jina.ai/concepts/gateway/) to offer an end-to-end service.&gt; **Note**&gt; If you just want to serve a single Executor, you can use a [Deployment](#build-ai--ml-services).For instance, let's combine [our StableLM language model](#build-ai--ml-services) with a Stable Diffusion image generation service from Jina AI's [Executor Hub](https://cloud.jina.ai/executors). Chaining these services together into a [Flow](https://docs.jina.ai/concepts/flow/) will give us a service that will generate images based on a prompt generated by the LLM.![](./.github/images/flow-diagram.png)Build the Flow with either Python or YAML:&lt;div class=&quot;table-wrapper&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt; Python API: &lt;code&gt;flow.py&lt;/code&gt; &lt;/th&gt; &lt;th&gt; YAML: &lt;code&gt;flow.yml&lt;/code&gt; &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```pythonfrom jina import Flowfrom executor import StableLMflow = (    Flow()    .add(uses=StableLM, timeout_ready=-1, port=12345)    .add(        uses='jinaai://jina-ai/TextToImage',        timeout_ready=-1,        install_requirements=True,    ))  # use the Executor from Jina's Executor hubwith flow:    flow.block()```&lt;/td&gt;&lt;td&gt;```yamljtype: Flowwith:    port: 12345executors:  - uses: StableLM    timeout_ready: -1    py_modules:      - executor.py  - uses: jinaai://jina-ai/TextToImage    timeout_ready: -1    install_requirements: true```Then run the YAML Flow with the CLI: `jina flow --uses flow.yml`&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;Then, use [Jina Client](https://docs.jina.ai/concepts/client/) to make requests to the Flow:```pythonfrom jina import Client, Documentclient = Client(port=12345)prompt = Document(    tags = {'prompt': 'suggest an interesting image generation prompt for a mona lisa variant'})response = client.post(on='/', inputs=[prompt])response[0].display()```![](./.github/images/mona-lisa.png)## Deploy to the cloudYou can also deploy a Flow to JCloud.First, turn the `flow.yml` file into a [JCloud-compatible YAML](https://docs.jina.ai/concepts/jcloud/yaml-spec/) by specifying resource requirements and using containerized Hub Executors.Then, use `jina cloud deploy` command to deploy to the cloud:```shellwget https://raw.githubusercontent.com/jina-ai/jina/master/.github/getting-started/jcloud-flow.ymljina cloud deploy jcloud-flow.yml```&gt; **Warning**&gt;&gt; Make sure to delete/clean up the Flow once you are done with this tutorial to save resources and credits.Read more about [deploying Flows to JCloud](https://docs.jina.ai/concepts/jcloud/#deploy).&lt;!-- end build-pipelines --&gt;Check [the getting-started project source code](https://github.com/jina-ai/jina/tree/master/.github/getting-started).### Easy scalability and concurrencyWhy not just use standard Python to build that microservice and pipeline? Jina accelerates time to market of your application by making it more scalable and cloud-native. Jina also handles the infrastructure complexity in production and other Day-2 operations so that you can focus on the data application itself.Increase your application's throughput with scalability features out of the box, like [replicas](https://docs.jina.ai/concepts/orchestration/scale-out/#replicate-executors), [shards](https://docs.jina.ai/concepts/orchestration/scale-out/#customize-polling-behaviors) and [dynamic batching](https://docs.jina.ai/concepts/serving/executor/dynamic-batching/).Let's scale a Stable Diffusion Executor deployment with replicas and dynamic batching:![](./.github/images/scaled-deployment.png)* Create two replicas, with [a GPU assigned for each](https://docs.jina.ai/concepts/flow/scale-out/#replicate-on-multiple-gpus).* Enable dynamic batching to process incoming parallel requests together with the same model inference.&lt;div class=&quot;table-wrapper&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt; Normal Deployment &lt;/th&gt; &lt;th&gt; Scaled Deployment &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```yamljtype: Deploymentwith:  timeout_ready: -1  uses: jinaai://jina-ai/TextToImage  install_requirements: true```&lt;/td&gt;&lt;td&gt;```yamljtype: Deploymentwith:  timeout_ready: -1  uses: jinaai://jina-ai/TextToImage  install_requirements: true  env:   CUDA_VISIBLE_DEVICES: RR  replicas: 2  uses_dynamic_batching: # configure dynamic batching    /default:      preferred_batch_size: 10      timeout: 200```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;Assuming your machine has two GPUs, using the scaled deployment YAML will give better throughput compared to the normal deployment.These features apply to both [Deployment YAML](https://docs.jina.ai/concepts/executor/deployment-yaml-spec/#deployment-yaml-spec) and [Flow YAML](https://docs.jina.ai/concepts/flow/yaml-spec/). Thanks to the YAML syntax, you can inject deployment configurations regardless of Executor code.### Get on the fast lane to cloud-nativeUsing Kubernetes with Jina is easy:```bashjina export kubernetes flow.yml ./my-k8skubectl apply -R -f my-k8s```And so is Docker Compose:```bashjina export docker-compose flow.yml docker-compose.ymldocker-compose up```&gt; **Note**&gt; You can also export Deployment YAML to [Kubernetes](https://docs.jina.ai/concepts/executor/serve/#serve-via-kubernetes) and [Docker Compose](https://docs.jina.ai/concepts/executor/serve/#serve-via-docker-compose).That's not all. We also support [OpenTelemetry, Prometheus, and Jaeger](https://docs.jina.ai/cloud-nativeness/opentelemetry/).What cloud-native technology is still challenging to you? [Tell us](https://github.com/jina-ai/jina/issues) and we'll handle the complexity and make it easy for you.&lt;!-- start support-pitch --&gt;## Support- Join our [Discord community](https://discord.jina.ai) and chat with other community members about ideas.- Subscribe to the latest video tutorials on our [YouTube channel](https://youtube.com/c/jina-ai)## Join UsJina is backed by [Jina AI](https://jina.ai) and licensed under [Apache-2.0](./LICENSE).&lt;!-- end support-pitch --&gt;</longdescription>
</pkgmetadata>