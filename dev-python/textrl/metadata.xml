<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># TextRL&lt;p&gt;    &lt;a href=&quot;https://pypi.org/project/textrl/&quot;&gt;        &lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/textrl&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/voidful/tfkit&quot;&gt;        &lt;img alt=&quot;Download&quot; src=&quot;https://img.shields.io/pypi/dm/textrl&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/voidful/tfkit&quot;&gt;        &lt;img alt=&quot;Last Commit&quot; src=&quot;https://img.shields.io/github/last-commit/voidful/textrl&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://www.codefactor.io/repository/github/voidful/textrl&quot;&gt;        &lt;img src=&quot;https://www.codefactor.io/repository/github/voidful/textrl/badge&quot; alt=&quot;CodeFactor&quot; /&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/voidful/textrl&quot;&gt;        &lt;img src=&quot;https://visitor-badge.glitch.me/badge?page_id=voidful.textrl&quot; alt=&quot;Visitor&quot; /&gt;    &lt;/a&gt;&lt;/p&gt;Text generation with reinforcement learning using huggingface's transformer.  RLHF (Reinforcement Learning with Human Feedback)Implementation of ChatGPT for human interaction to improve generation model with reinforcement learning.![example](https://i.imgur.com/pqJn9lJ.png)## IntroductionThis project is trying to use reinforcement learning to adjust text generation results. It is based on anytext-generation model on huggingaface's [transformer](https://github.com/huggingface/transformers)with [PFRL](https://github.com/pfnet/pfrl) and [OpenAI GYM](https://gym.openai.com).## Key parameter for RL trainingTo finetune language model using RL, you basically need to modify the reward function:```pythonfrom textrl import TextRLEnvclass MyRLEnv(TextRLEnv):    def get_reward(self, input_item, predicted_list, finish):        # input_item is the prompt input for the model, it will be one of your observation        # an observation will be a list of sentence of eg: ['inputted sentence','xxx','yyy']        # only the first input will feed to the model 'inputted sentence', and         # the remaining can be the reference for reward calculation        # predicted_list is the list of predicted sentences of RL model generated,        # it will be used for ranking reward calculation        # finish is the end of sentences flags, get_reward will be called during generating each word, and         # when finish is True, it means the sentence is finished, it will use for sentence level reward calculation.        # reward should be the list equal to the length of predicted_list        return reward```parameters for sampling diverse example:```python actor = TextRLActor(env, model, tokenizer,                    act_deterministically=False,  # select the max probability token for each step or not                    temperature=1,                # temperature for sampling                    compare_sample=2,             # num of sample to rank                    top_k=0,                      # top k sampling                    top_p=1.0,                    # top p sampling                    repetition_penalty=2)         # repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)```## Example 1 - `gpt2`&lt;details&gt;&lt;summary&gt;CLICK ME&lt;/summary&gt;&lt;p&gt;#### GPT2 Example```pythonimport pfrlfrom textrl import TextRLEnv, TextRLActor, train_agent_with_evaluationfrom transformers import AutoModelForCausalLM, AutoTokenizercheckpoint = &quot;gpt2&quot;tokenizer = AutoTokenizer.from_pretrained(checkpoint)model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;)model = model.cuda()class MyRLEnv(TextRLEnv):    def get_reward(self, input_item, predicted_list, finish):  # predicted will be the list of predicted token        reward = [0]        if finish:            reward = [1]  # calculate reward score base on predicted_list        return rewardobservaton_list = [{&quot;input&quot;:&quot;explain how attention work in seq2seq model&quot;}]env = TextRLEnv(model, tokenizer, observation_input=observaton_list, max_length=20, compare_sample=2)actor = TextRLActor(env, model, tokenizer,                    act_deterministically=False,                    temperature=1.0,                    top_k=10,                    top_p=1.0,                    repetition_penalty=2)agent = actor.agent_ppo(update_interval=2, minibatch_size=2, epochs=10)print(actor.predict(observaton_list[0]))train_agent_with_evaluation(    agent,    env,    steps=100,    eval_n_steps=None,    eval_n_episodes=1,    eval_interval=2,    outdir='bloom—test',)print(actor.predict(observaton_list[0]))```&lt;/p&gt;&lt;/details&gt;## Example 2 - `bigscience/bloomz-7b1-mt`&lt;details&gt;&lt;summary&gt;CLICK ME&lt;/summary&gt;&lt;p&gt;#### bloomz-7b1-mt Example```pythonimport pfrlfrom textrl import TextRLEnv, TextRLActor, train_agent_with_evaluationfrom transformers import AutoModelForCausalLM, AutoTokenizercheckpoint = &quot;bigscience/bloomz-7b1-mt&quot;tokenizer = AutoTokenizer.from_pretrained(checkpoint)model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;)model = model.cuda()class MyRLEnv(TextRLEnv):    def get_reward(self, input_item, predicted_list, finish):  # predicted will be the list of predicted token        reward = [0]        if finish:            reward = [1]  # calculate reward score base on predicted_list        return rewardobservaton_list = [{&quot;input&quot;:&quot;explain how attention work in seq2seq model&quot;}]env = TextRLEnv(model, tokenizer, observation_input=observaton_list, max_length=20, compare_sample=2)actor = TextRLActor(env, model, tokenizer,                    act_deterministically=False,                    temperature=1.0,                    top_k=10,                    top_p=1.0,                    repetition_penalty=2)agent = actor.agent_ppo(update_interval=2, minibatch_size=2, epochs=10)print(actor.predict(observaton_list[0]))train_agent_with_evaluation(    agent,    env,    steps=100,    eval_n_steps=None,    eval_n_episodes=1,    eval_interval=2,    outdir='bloom—test',)print(actor.predict(observaton_list[0]))```&lt;/p&gt;&lt;/details&gt;## Example 3 - 176B BLOOMStrongly recommend contribute on public swarm to increase petals capacityhttps://github.com/bigscience-workshop/petalsinstall `pip install petals -U` first&lt;details&gt;&lt;summary&gt;CLICK ME&lt;/summary&gt;&lt;p&gt;#### bloomz-7b1-mt Example```pythonimport pfrlfrom textrl import TextRLEnv, TextRLActor, train_agent_with_evaluationfrom transformers import BloomTokenizerFastfrom petals import DistributedBloomForCausalLMMODEL_NAME = &quot;bigscience/bloom-petals&quot;tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME)model = model.cuda()class MyRLEnv(TextRLEnv):    def get_reward(self, input_item, predicted_list, finish):  # predicted will be the list of predicted token        reward = [0]        if finish:            reward = [1]  # calculate reward score base on predicted_list        return rewardobservaton_list = [{&quot;input&quot;:&quot;explain how attention work in seq2seq model&quot;}]env = TextRLEnv(model, tokenizer, observation_input=observaton_list, max_length=20, compare_sample=2)actor = TextRLActor(env, model, tokenizer,                    act_deterministically=False,                    temperature=1.0,                    top_k=10,                    top_p=1.0,                    repetition_penalty=2)agent = actor.agent_ppo(update_interval=2, minibatch_size=2, epochs=10)print(actor.predict(observaton_list[0]))train_agent_with_evaluation(    agent,    env,    steps=100,    eval_n_steps=None,    eval_n_episodes=1,    eval_interval=2,    outdir='bloom—test',)print(actor.predict(observaton_list[0]))```&lt;/p&gt;&lt;/details&gt;## Example 4[Controllable generation via RL to let Elon Musk speak ill of DOGE](https://github.com/voidful/TextRL/blob/main/example/2022-12-10-textrl-elon-musk.ipynb)colabexample: [bigscience/bloom-560m](https://colab.research.google.com/drive/1ThSHtkfzC2dDc6JOdeCTthuDovTCheRf?usp=sharing)colabexmaple: [huggingtweets/elonmusk](https://colab.research.google.com/drive/149MG6uxu7CjMU1pXnYXfSvJ6HEdwcOFt?usp=sharing)before: `i think dogecoin is a great idea.`    after: `i think dogecoin is a great idea, but I think it is a little overused.`## Installation### pip install```bashpip install pfrl@git+https://github.com/voidful/pfrl.gitpip install textrl```### Build from sourcegit clone and cd into this project.```bashpip install -e .```## Usage### init agent and environment```pythonimport torchfrom textrl import TextRLEnv, TextRLActor, train_agent_with_evaluationfrom transformers import AutoModelForCausalLM, AutoTokenizercheckpoint = &quot;bigscience/bloomz-7b1-mt&quot;tokenizer = AutoTokenizer.from_pretrained(checkpoint)model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;)model = model.cuda()```### setup reward function for environment* predicted(list[str]): will be the list of predicted token* finish(bool): it met the end of sentence or not```pythonclass MyRLEnv(TextRLEnv):    def get_reward(self, input_item, predicted_list, finish):  # predicted will be the list of predicted token        if finish:            reward = [0]  # calculate reward score base on predicted_list        return reward```### prepare for training* observaton_list should be a list of all possible input string for model training  eg: `observaton_list = [{&quot;input&quot;:'testing sent 1'},{&quot;input&quot;:'testing sent 2'}]````pythonenv = MyRLEnv(model, tokenizer, observation_input=observaton_list)actor = TextRLActor(env, model, tokenizer)agent = actor.agent_ppo(update_interval=10, minibatch_size=2000, epochs=20)```### Train```pythonn_episodes = 1000max_episode_len = 200  # max sentence lengthfor i in range(1, n_episodes + 1):    obs = env.reset()    R = 0    t = 0    while True:        action = agent.act(obs)        obs, reward, done, pred = env.step(action)        R += reward        t += 1        reset = t == max_episode_len        agent.observe(obs, reward, done, reset)        if done or reset:            break    if i % 10 == 0:        print('episode:', i, 'R:', R)    if i % 50 == 0:        print('statistics:', agent.get_statistics())print('Finished.')```another way to train```pythonimport loggingimport syslogging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')train_agent_with_evaluation(    agent,    env,    steps=1000,    eval_n_steps=None,    eval_n_episodes=1500,    train_max_episode_len=50,    eval_interval=10000,    outdir='somewhere',)```### prediction```pythonagent.load(&quot;somewhere/best&quot;)  # loading the best modelactor.predict(&quot;input text&quot;)```## dump trained model to huggingface's model```shelltextrl-dump --model ./model_path_before_rl --rl ./rl_path --dump ./output_dir```</longdescription>
</pkgmetadata>