<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Dependencies:=============* numpy *(for optimizations in FilesCacheClient)** python-memcached *(for MemcachedClient)** pyzmq *(for coherence support)** dnspython  *(for dynamic dns-based load-balancing of MemcachedClient)** cython *(for optimizations in InprocCacheClient)*Optional features=================When declaring dependencies in your project, you can reference the following optional features:* mq *(everything under chorde.mq, mostly pulls pyzmq)** shmem *(shared memory utils, needed for optimized files cache clients)** memcache* elasticacheBasic Usage:============The simplest way to use is to create one of the supported cache clients,and use it directly, like.. code:: python    from chorde.clients.inproc import InprocCacheClient    from chorde import CacheMissError    c = InprocCacheClient(200)    c.put(3, 10, 300) # put value 10 on key 3, TTL 5min    assert 10 == c.get(3)    try:        c.get(5)    except CacheMissError:        print &quot;miss&quot;This creates an In-process LRU cache. The in-process part indicates that itis process-private, and not shared with other processes.There are two implementations of the LRU, with different performancecharacteristics. The `InprocCacheClient` can take alternative storeimplementations as an argument, see the module for details.The default LRUCache, accessible as `chorde.clients.inproc.Cache`,is a regular LRU implemented witha priority queue and a hash table in tandem, so it has *O(log n)* writesand *O(1)* reads, but by default all reads entail a write (to update theLRU). That can be disabled by specifying custom options, see the module'sdocumentation for more details.There's an alternative approximate LRU, accessible in`chorde.clients.inproc.CuckooCache`, that implements a lazy version ofa cuckoo hash table, and has *O(1)* reads and amortized *O(1)* writes.It's also quite more space-efficient than the regular LRU, so it's bettersuited for very large caches, but its eviction strategy will be approximate,and thus not guaranteed to always evict the actual least-recently-used item.Shared Caches=============The most straightforward way to get a shared cache, is to use a memcache:.. code:: python    from chorde.clients.memcached import MemcachedClient    from chorde import CacheMissError    c = MemcachedClient([&quot;localhost:11211&quot;], checksum_key = &quot;testing&quot;)    c.put(3, 10, 300)    assert 10 == c.get(3)    try:        c.get(5)    except CacheMissError:        print &quot;miss&quot;The MemcachedClient is used just like any other client, only it talks to, in thisexample, a local memcached listening on localhost port 11211. Multiple clientscan be given, and a consistent hash on the key will be used to spread the loadamong them.If a hostname is given, and the hostname points to multiple IP addresses, thesame effect will be obtained, and the distribution will be dynamically updatedaccording to the TTL specified on the DNS entry. This, for example, makes theclient work seamlessly with Amazon ElastiCache's &quot;configuration endpoint&quot;,which is a DNS entry that points to one of the cache nodes. But it only workslike that with single-node clusters. For multi-node clusters, use``chorde.clients.elasticache.ElastiCacheClient``, which goes a step furtherand queries this configuration endpoint for all the other nodes.Do beware that the key used against the memcached isn't the key given, sincethe client supports any hashable object as key, whereas memcached only supportsa subset of string keys. MemcachedClient has none of memcached's limitationswith regards to key format and/or length and value size. It works aroundmemcaches limitations, by constructing a derived key that is suitable formemcached. However, there is overhead in providing huge keys or values, so it's generallygood advise to avoid them anyway.Values in the MemcachedClient are plickled, compressed and signed with thechecksum key, so it is relatively safe from both malicious code injection throughpickle, and transmission errors causing cPickle to dump core (which does happen whenit is fed unchecked data).In case the compression becomes a bottleneck, which shouldn't be a problem unlessit is a high-traffic cache with rarely compressible values, one can disable it.Check MemcachedClient's documentation for more details.To reduce roundtrips, all clients (in particular MemcachedClients) supportgetMulti and getTtlMulti, to fetch multiple keys at once:.. code:: python    from chorde.clients.memcached import MemcachedClient    from chorde import CacheMissError    c = MemcachedClient([&quot;localhost:11211&quot;], checksum_key = &quot;testing&quot;)    c.put(3, 10, 300)    c.put(4, 20, 300)    assert {3:10, 4:20, 5:None} == dict(c.getMulti([3,4,5], None))See the documentation on clients.base for more details.Multilevel caches=================A common approach when dealing with remote caches, like the above example usingmemcached, is to have at least two levels: the memcached itself, and an in-processsmall cache to avoid having to talk to the memcached all the time.This can be done straightforwardly with the tiered clients:.. code:: python    from chorde.clients.memcached import MemcachedClient    from chorde.clients.inproc import InprocCacheClient    from chorde.clients.tiered import TieredInclusiveClient    from chorde import CacheMissError    l1 = InprocCacheClient(10)    l2 = MemcachedClient([&quot;localhost:11211&quot;], checksum_key=&quot;test&quot;)    c = TieredInclusiveClient(l1,l2)    c.put(3, 10, 300)    assert 10 == c.get(3)    try:        c.get(5)    except CacheMissError:        print &quot;miss&quot;Here we build an *inclusive* tiered client, in which elements on higher levels arepromoted into the lower levels by copying, rather than swapping. This means thereis duplication among them, but this is usually best in cases like these, where theupper levels are shared among processes.An exclusive client isn't provided at this moment, since there is seldom any usefor the exclusive pattern on these types of caches.Decorators==========A more natural way to think about caching, is in that it's a decorator of plain functions.Rather than explicitly putting and getting from caches, one can simply considercaching as an optimization on an otherwise expensive function.Decorators in chorde.decorators provide a huge amount of functionality and flexibility,these examples cover only the most basic usage:Assuming *c* is the client we want to use for caching,.. code:: python    from chorde.decorators import cached    import random    @cached(c, ttl=300, async_ttl=-60)    def expensive_func(x):        return x * random.random()    print expensive_func(1)    print expensive_func(1) # Should return the same    print expensive_func.bg()(1) # will refresh asynchronously every minute    print expensive_func.future()(1).result() # same as before, but using the futures interface    print expensive_func.peek(1) # just check the cache    print expensive_func.put(1, _cache_put=5) # write an explicit value    print expensive_func.bg().lazy(1) # don't wait, raise CacheMissError if not available, compute in background    print expensive_func.future().lazy(1).result() # same as before, but using the futures interfaceThere, the async_ttl means the minimum TTL that triggersan asynchronous recomputation (you can use it to avoid ever having to wait on a recomputation).The negative value makes it relative to the total TTL, so -60 always means recomputeevery minute (60 seconds). The plain ttl is an absolute limit, no result older thanthat will ever be returned.The documentation on chorde.decorators.cached will have more to say about the ways ofinvoking cached functions.In general, the terms are:  * lazy: don't wait for computation, return a cached result or raise CacheMissError.    When combined with bg, it will compute in the background.  * peek: don't compute. Similar to lazy, but it will never trigger a computation  * bg: expensive things (computation) happen on a background threadpool.  * future: return futures rather than results, use the future to get notified of    results when they're available. Actual cache access happens on a threadpool.    A non-blocking way of calling.  * refresh: immediately recompute the value.Integration with other libraries--------------------------------The decorators' future() interface is especially suited for integration with other libraries that can talk tofutures. Chorde's futures, however, are not directly compatible with other libraries', but they can easily bewrapped like so:.. code:: python    import tornado.web    import tornado.gen    from chorde.clients.asyncache import makeFutureWrapper    WF = makeFutureWrapper(tornado.web.Future)    ...    @tornado.gen.coroutine    def get(self):        some_result = yield WF(some_func.future()(some_args))There is a better way to integrate with tornado &gt;= 4.0.. code:: python    from chorde.external_integration import monkey_patch_tornado    monkey_patch_tornado()    import tornado.web    import tornado.gen    ...    @tornado.gen.coroutine    def get(self):        some_result = yield some_func.future()(some_args)Additional documentation========================- `In Depth Look into Caching (Part 1) &lt;https://geeks.jampp.com/python/in-depth-look-into-caching-1/&gt;`__ (English)- `In Depth Look into Caching (Part 2) &lt;https://geeks.jampp.com/python/in-depth-look-into-caching-2/&gt;`__ (English)- `Caching para hordas y estampidas por Claudio Freire - PyCon 2013 &lt;https://www.youtube.com/watch?v=ZlK4rBlrJlY&gt;`__ (Spanish)</longdescription>
</pkgmetadata>