<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Stars](https://img.shields.io/github/stars/laminlabs/lamindb?logo=GitHub&amp;color=yellow)](https://github.com/laminlabs/lamindb)[![codecov](https://codecov.io/gh/laminlabs/lamindb/branch/main/graph/badge.svg?token=VKMRJ7OWR3)](https://codecov.io/gh/laminlabs/lamindb)[![pypi](https://img.shields.io/pypi/v/lamindb?color=blue&amp;label=pypi%20package)](https://pypi.org/project/lamindb)[![Documentation](https://img.shields.io/badge/Documentation-green)](https://lamin.ai/docs/guide/)# LaminDBOpen-source data lake to manage your existing data in your existing infrastructure.```{warning}Public beta: Currently only recommended for collaborators as we still make breaking changes.Update 2023-06-14:- We completed a major migration from SQLAlchemy/SQLModel to Django, available in 0.42.0.- The last version before the migration is 0.41.2.```## FeaturesFree:- Track [data lineage](https://lamin.ai/docs/guide/data-lineage) across notebooks, pipelines &amp; apps.- Manage [biological registries, ontologies &amp; features](https://lamin.ai/docs/biology/registries).- [Query, search &amp; look up anything](https://lamin.ai/docs/guide/select), [manage &amp; migrate custom schemas](https://lamin.ai/docs/setup/migrate).- [Persist, load](https://lamin.ai/docs/guide/files-records#in-memory-objects) &amp; [stream data objects](https://lamin.ai/docs/guide/stream) with a single line of code.- [Idempotent](https://lamin.ai/docs/faq/idempotency) and [ACID](https://lamin.ai/docs/faq/acid) operations.- Use a mesh of LaminDB instances and [share them in a hub](https://lamin.ai/laminlabs) akin to GitHub.Enterprise:- Explore, share data &amp; submit samples with LaminApp (deployable in your infrastructure).- Receive support, code templates &amp; services for a BioTech data &amp; analytics platform.## Usage overviewImport `lamindb` and initialize a data lake instance with local or cloud default storage:```pythonimport lamindb as lnln.setup.init(storage=&quot;./mydata&quot;)  # or s3://my-bucket, gs://my-bucket, etc.```### Store, query, search &amp; load data objectsStore a `DataFrame` in default storage:```pythondf = pd.DataFrame({&quot;feat1&quot;: [1, 2], &quot;feat2&quot;: [3, 4]})  # AnnData works, tooln.File(df, name=&quot;My dataset1&quot;).save()  # create a File object and save/upload it```You have the full power of SQL to query for metadata, but the simplest query for a file is:```pythonfile = ln.File.select(name=&quot;My dataset1&quot;).one()  # get exactly one result```If you don't have specific metadata in mind, run a search:```pythonln.File.search(&quot;dataset1&quot;)```Once you queried or searched it, load a file back into memory:```pythondf = file.load()```Or get a backed accessor to stream its content from the cloud:```pythonbacked = file.backed()  # currently works for AnnData, zarr, HDF5, not yet for DataFrame```### Store, query &amp; search filesThe same API works for any file:```pythonfile = ln.File(&quot;s3://my-bucket/images/image001.jpg&quot;)  # or a local pathfile.save()  # register the file```Query by `key` (the relative path within your storage):```pythonfile.select(key_startswith=&quot;images/&quot;).df()  # all files in folder &quot;images/&quot; in default storage```### Auto-complete categoricalsWhen you're unsure about spellings, use a lookup object:```pythonusers = ln.User.lookup()ln.File.select(created_by=users.lizlemon)```### Track &amp; query data lineageIn addition to basic provenance information (`created_by`, `created_at`,`created_by`), you can track which notebooks, pipelines &amp; appstransformed files.#### NotebooksTrack a Jupyter Notebook:```pythonln.track()  # auto-detect &amp; save notebook metadataln.File(&quot;my_artifact.parquet&quot;).save()  # this file is now aware that it was saved in this notebook```When you query the file, later on, you'll know from which notebook it came:```pythonfile = ln.File.select(name=&quot;my_artifact.parquet&quot;).one()  # query for a filefile.transform  # the notebook with id, title, filename, version, etc.file.run  # the specific run of the notebook that created the file```Alternatively, you can query for notebooks and find the files written by them:```pythontransforms = ln.Transform.select(  # all notebooks with 'T cell' in the title created in 2022    name__contains=&quot;T cell&quot;, type=&quot;notebook&quot;, created_at__year=2022).all()ln.File.select(transform__in=transforms).df()  # the files created by these notebooks```#### PipelinesThis works like for notebooks just that you need to provide pipeline metadata yourself.To save a pipeline to the `Transform` registry, call```pythonln.Transform(name=&quot;Awesom-O&quot;, version=&quot;0.41.2&quot;).save()  # save a pipeline, optionally with metadata```Track a pipeline run:```pythontransform = ln.Transform.select(name=&quot;Awesom-O&quot;, version=&quot;0.41.2&quot;).one()  # select pipeline from the registryln.track(transform)  # create a new global run contextln.File(&quot;s3://my_samples01/my_artifact.fastq.gz&quot;).save()  # file gets auto-linked against run &amp; transform```Now, you can query for the latest pipeline runs:```pythonln.Run.select(transform=transform).order_by(&quot;-created_at&quot;).df()  # get the latest pipeline runs```#### Run inputsTo track run inputs, pass `is_run_input` to any `File` accessor: `.stage()`, `.load()` or `.backed()`. For instance,```pythonfile.load(is_run_input=True)```You can also track inputs by default by setting `ln.settings.track_run_inputs = True`.### Load your data lake from anywhereIf provided with access, others can load your data lake via a single line:```$ lamin load myaccount/myartifacts```### Manage biological registries```shelllamin init --storage ./bioartifacts --schema bionty```...### Track biological features...### Track biological samples...### Manage custom schemas1. Create a GitHub repository with Django ORMs similar to [github.com/laminlabs/lnschema-lamin1](https://github.com/laminlabs/lnschema-lamin1)2. Create &amp; deploy migrations via `lamin migrate create` and `lamin migrate deploy`It's fastest if we do this for you based on our templates within an enterprise plan, but you can fully manage the process yourself.## Installation![pyversions](https://img.shields.io/pypi/pyversions/lamindb)```shellpip install lamindb  # basic data lakepip install 'lamindb[jupyter]'  # Jupyter notebook trackingpip install 'lamindb[bionty]'  # basic biological entitiespip install 'lamindb[fcs]'  # .fcs files (flow cytometry)pip install 'lamindb[zarr]'  # zarr storage (streaming arrays)pip install 'lamindb[aws]'  # AWS (s3fs, etc.)pip install 'lamindb[gcp]'  # Google Cloud (gcfs, etc.)```## Sign upWhy do I have to sign up?- Data lineage requires a user identity (who modified which data when?).- Collaboration requires a user identity (who shares this with me?).Signing up takes 1 min.We do _not_ store any of your data, but only basic metadata about you (email address, etc.) &amp; your LaminDB instances (S3 bucket names, etc.).- Sign up: `lamin signup &lt;email&gt;`- Log in: `lamin login &lt;handle&gt;`## How does it work?LaminDB builds semantics of R&amp;D and biology onto well-established tools:- SQLite &amp; Postgres for SQL databases using Django ORM (previously: SQLModel)- S3, GCP &amp; local storage for object storage using fsspec- Configurable storage formats: pyarrow, anndata, zarr, etc.- Biological knowledge sources &amp; ontologies: see [Bionty](https://lamin.ai/docs/bionty)LaminDB is open source.## ArchitectureLaminDB consists of the `lamindb` Python package (repository [here](https://github.com/laminlabs/lamindb)) with its components:- [bionty](https://github.com/laminlabs/bionty): Basic biological entities (usable standalone).- [lamindb-setup](https://github.com/laminlabs/lamindb-setup): Setup &amp; configure LaminDB, client for Lamin Hub.- [lnschema-core](https://github.com/laminlabs/lnschema-core): Core schema, ORMs to model data objects &amp; data lineage.- [lnschema-bionty](https://github.com/laminlabs/lnschema-bionty): Bionty schema, ORMs that are coupled to Bionty's entities.- [lnschema-lamin1](https://github.com/laminlabs/lnschema-lamin1): Exemplary configured schema to track samples, treatments, etc.- [nbproject](https://github.com/laminlabs/nbproject): Parse metadata from Jupyter notebooks.LaminHub &amp; LaminApp are not open-sourced, and neither are templates that model lab operations.Lamin's packages build on the infrastructure listed [above](#how-does-it-work).## Notebooks- Find all guide notebooks [here](https://github.com/laminlabs/lamindb/tree/main/docs/guide).- You can run these notebooks in hosted versions of JupyterLab, e.g., [Saturn Cloud](https://github.com/laminlabs/run-lamin-on-saturn), Google Vertex AI, Google Colab, and others.- Jupyter Lab &amp; Notebook offer a fully interactive experience, VS Code &amp; others require using the CLI (`lamin track my-notebook.ipynb`)## DocumentationRead the [docs](https://lamin.ai/docs/guide/).</longdescription>
</pkgmetadata>