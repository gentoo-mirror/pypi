<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Tensorslow![Tensorslow Logo](imgs/tensorslow.jpg)Have you ever wanted to to use the power of neural network modeling with absolutely **NONE** of the rush given by competing neural network frameworks? Well if you answered yes to the previous question then Tensorslow is the machine learning framework to use. Using state of the art [**Python3**](https://www.python.org/doc/humor/#the-zen-of-python) lists rather than any complicated and vectorized backends from languages of yester-year like  [C++](https://en.wikipedia.org/wiki/C%2B%2B), we seek to use the Zen of Python to make your machine learning models learn at a pace more compatible with that of your three remaining brain-cells (or at least mine). So, get ready to sit back, take a sip of that green-tea, and enjoy the magic of machine learning with absolutely **NONE** of the stress provided by a machine that works way too quickly for its own good.# InstallTo download the code from the repository```shellgit clone git@github.com:oortega20/tensorslow.git```To install the current release of Tensorslow```shellpip install tensorslow```# Try some simple Tensorslow ProgramsSome fun matrix manipulations with tensorslow's linear-algebra package ```python&gt;&gt;&gt; from tensorslow.linalg import Tensor&gt;&gt;&gt; x = Tensor(list(range(6)), (2,3))&gt;&gt;&gt; xTensor([[0.000 1.000 2.000]        [3.000 4.000 5.000]])&gt;&gt;&gt; x @ x.TTensor([[5.000 14.000]        [14.000 50.000]])&gt;&gt;&gt; x.T @ xTensor([[9.000 12.000 15.000]        [12.000 17.000 22.000]        [15.000 22.000 29.000]])&gt;&gt;&gt; x - 3Tensor([[-3.000 -2.000 -1.000]        [0.000 1.000 2.000]])&gt;&gt;&gt; 0 * xTensor([[0.000 0.000 0.000]        [0.000 0.000 0.000]])```A simple demonstration of forward propagation with Tensorslow's available layers and activations```python&gt;&gt;&gt; from tensorslow.linalg import Tensor&gt;&gt;&gt; from tensorslow.activations import Relu&gt;&gt;&gt; from tensorslow.layers import Dense&gt;&gt;&gt;&gt;&gt;&gt; x = Tensor(list(range(6)), (2,3))&gt;&gt;&gt; xTensor([[0.000 1.000 2.000]        [3.000 4.000 5.000]])&gt;&gt;&gt; act = Sigmoid()&gt;&gt;&gt; f = Dense('f', in_dim=3, out_dim=3)&gt;&gt;&gt; f.weights['w']Tensor([[0.057 0.051 0.021]        [0.047 -0.031 0.003]        [0.015 -0.052 0.058]])&gt;&gt;&gt; f.weights['b']Tensor([0.333 0.333 0.333])&gt;&gt;&gt; out = act(f(x))&gt;&gt;&gt; outTensor([[0.601 0.549 0.611]        [0.682 0.525 0.667]])&gt;&gt;&gt;```Inference using Tensorslow MNIST Classifier```pythonfrom tensorslow.datasets import MNISTfrom tensorslow.models import ts_mnist_classifiermodel = ts_mnist_classifier(from_ts=True)data = MNIST(load_train=False, load_test=True, batch_size=128)x_test, y_test = data.get_test_data()for x, y in zip(x_test, y_test):    probs = model.forward(x) # if we only want the class probabilities    loss, grad = model.forward(x, y) # if we want to compute losses and gradients```Example of simple model training using tensorslow```pythonfrom tensorslow.datasets import MNISTfrom tensorslow.models import ts_mnist_classifierfrom tensorslow.optimizers import SGDdata = MNIST(batch_size=128)x_train, y_train = data.get_train_data()epochs = 10model = ts_mnist_classifier(from_ts=False)opt = SGD(model, learning_rate=5e-4)train_loss, test_loss = [], []for epoch in range(epochs):    for data in zip(x_train, y_train):        x, y = data        batch_loss, grad = model(x, y)        model.backward(grad)        opt.update()```Details of the Tensorslow MNIST Classifier can be found in a link to our [tensorslow-experimentation](https://github.com/oortega20/tensorslow-experimentation) repository. Improving the accuracy of the current MNIST classifier is something being researched! </longdescription>
</pkgmetadata>