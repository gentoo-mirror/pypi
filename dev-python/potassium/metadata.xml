<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Potassium![Potassium (1)](https://user-images.githubusercontent.com/44653944/222016748-ca2c6905-8fd5-4ee5-a68e-7aed48f23436.png)[Potassium](https://github.com/bananaml/potassium) is an open source web framework, built to tackle the unique challenges of serving custom models in production.The goal of this project is to:- Provide a familiar web framework similar to Flask/FastAPI- Bake in best practices for handling large, GPU-bound ML models- Provide a set of primitives common in ML serving, such as:    - POST request handlers    - Websocket / streaming connections    - Async handlers w/ webhooks- Maintain a standard interface, to allow the code and models to compile to specialized hardware (ideally on [Banana Serverless GPUs](https://banana.dev) ðŸ˜‰)### Stability Notes:- This is a v0 release using SemVer, and is not stable; the interface may change at any time. Be sure to lock your versions!- If deploying to Banana, Potassium apps currently won't receive [fast boot optimizations](https://docs.banana.dev/banana-docs/core-concepts/build-system). This will be added within a few days.---## Quickstart: Serving a Huggingface BERT modelThe fastest way to get up and running is to use the [Banana CLI](https://github.com/bananaml/banana-cli), which downloads and runs your first model.[Here's a demo video](https://www.loom.com/share/86d4e7b0801549b9ab2f7a1acce772aa)1. Install the CLI with pip```bashpip3 install banana-cli```This downloads boilerplate for your potassium app, and automatically installs potassium into the venv.2. Create a new project directory with ```bashbanana init my-appcd my-app```3. Start the hot-reload dev server```bashbanana dev```4. Call your API (from a separate terminal)```bashcurl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;prompt&quot;: &quot;Hello I am a [MASK] model.&quot;}' http://localhost:8000/``` ---## Or do it yourself:1. Install the potassium package```bashpip3 install potassium```Create a python file called `app.py` containing:```pythonfrom potassium import Potassium, Request, Responsefrom transformers import pipelineimport torchimport timeapp = Potassium(&quot;my_app&quot;)# @app.init runs at startup, and initializes the app's context@app.initdef init():    device = 0 if torch.cuda.is_available() else -1    model = pipeline('fill-mask', model='bert-base-uncased', device=device)       context = {        &quot;model&quot;: model,        &quot;hello&quot;: &quot;world&quot;    }    return context# @app.handler is an http post handler running for every call@app.handler()def handler(context: dict, request: Request) -&gt; Response:        prompt = request.json.get(&quot;prompt&quot;)    model = context.get(&quot;model&quot;)    outputs = model(prompt)    return Response(        json = {&quot;outputs&quot;: outputs},         status=200    )if __name__ == &quot;__main__&quot;:    app.serve()```This runs a Huggingface BERT model.For this example, you'll also need to install transformers and torch.```pip3 install transformers torch```Start the server with:```bashpython3 app.py```Test the running server with:```bashcurl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;prompt&quot;: &quot;Hello I am a [MASK] model.&quot;}' http://localhost:8000```---# Documentation## potassium.Potassium```pythonfrom potassium import Potassiumapp = Potassium(&quot;server&quot;)```This instantiates your HTTP app, similar to popular frameworks like [Flask](https://flask.palletsprojects.com/en/2.2.x/_)---## @app.init```python@app.initdef init():    device = 0 if torch.cuda.is_available() else -1    model = pipeline('fill-mask', model='bert-base-uncased', device=device)    return {        &quot;model&quot;: model    }```The `@app.init` decorated function runs once on server startup, and is used to load any reuseable, heavy objects such as:- Your AI model, loaded to GPU- Tokenizers- Precalculated embeddingsThe return value is a dictionary which saves to the app's `context`, and is used later in the handler functions.There may only be one `@app.init` function.---## @app.handler()```python@app.handler(&quot;/&quot;)def handler(context: dict, request: Request) -&gt; Response:        prompt = request.json.get(&quot;prompt&quot;)    model = context.get(&quot;model&quot;)    outputs = model(prompt)    return Response(        json = {&quot;outputs&quot;: outputs},         status=200    )```The `@app.handler` decorated function runs for every http call, and is used to run inference or training workloads against your model(s).You may configure as many `@app.handler` functions as you'd like, with unique API routes.Note: Banana serverless currently only supports handlers at the root &quot;/&quot;---## @app.async_handler(path=&quot;/async&quot;)```python@app.async_handler(&quot;/async&quot;)def handler(context: dict, request: Request) -&gt; Response:    prompt = request.json.get(&quot;prompt&quot;)    model = context.get(&quot;model&quot;)    outputs = model(prompt)    send_webhook(url=&quot;http://localhost:8001&quot;, json={&quot;outputs&quot;: outputs})    return```The `@app.async_handler()` decorated function runs a nonblocking job in the background, for tasks where results aren't expected to return clientside. It's on you to forward the data to wherever you please. Potassium supplies a `send_webhook()` helper function for POSTing data onward to a url, or you may add your own custom upload/pipeline code.When invoked, the client immediately returns a `{&quot;success&quot;: true}` message.You may configure as many `@app.async_handler` functions as you'd like, with unique API routes.Note: Banana serverless isn't perfectly stable running async_handler. You can use it, but concurrency may be weird.---## app.serve()`app.serve` runs the server, and is a blocking operation.</longdescription>
</pkgmetadata>