<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Installation from pip3```shellpip3 install --verbose subtitle_analyzerpython -m spacy download en_core_web_trfpython -m spacy download es_dep_news_trfpython -m spacy download de_dep_news_trf```# UsagePlease refer to [api docs](https://qishe-nlp.github.io/subtitle-analyzer/).### Excutable usageSince the x2cdict needs environment variables `DICT_DB_HOST` and `DEEPL_AUTH_KEY`, so **Don't forget!!!**.* Write ass file with vocabulary information```shellsta_vocab --srtfile movie.srt --lang en --assfile en_vocab.ass --external False``` * Write ass file with phrase information ```shellsta_phrase --srtfile movie.srt --lang en --assfile en_phrase.ass --external False```### Package usage```from subtitlecore import Subtitlefrom subtitle_analyzer import VocabAnalyzer, PhraseAnalyzerfrom subtitle_analyzer import VocabASSWriter, PhraseASSWriterimport jsondef subtitle_vocab(srtfile, lang, assfile, external):  phase = {&quot;step&quot;: 1, &quot;msg&quot;: &quot;Start sentenizing&quot;}  print(json.dumps(phase), flush=True)  sf = Subtitle(srtfile, lang)  sens = sf.sentenize()  for e in sens:    print(e)  phase = {&quot;step&quot;: 2, &quot;msg&quot;: &quot;Finish sentenizing&quot;}  print(json.dumps(phase), flush=True)  analyzer = VocabAnalyzer(lang)  exs = analyzer.get_line_vocabs(sens, external)  shown = exs[:20]  phase = {&quot;step&quot;: 3, &quot;msg&quot;: &quot;Finish vocabs dictionary lookup&quot;, &quot;vocabs&quot;: shown}  print(json.dumps(phase), flush=True)  if assfile:    ass_writer = VocabASSWriter(srtfile)    ass_writer.write(exs, assfile, {&quot;animation&quot;: False})        phase = {&quot;step&quot;: 4, &quot;msg&quot;: &quot;Finish ass saving&quot;}     print(json.dumps(phase), flush=True)def subtitle_phrase(srtfile, lang, assfile, external):  phase = {&quot;step&quot;: 1, &quot;msg&quot;: &quot;Start sentenizing&quot;}  print(json.dumps(phase), flush=True)  sf = Subtitle(srtfile, lang)  sens = sf.sentenize()  for e in sens:    print(e)  phase = {&quot;step&quot;: 2, &quot;msg&quot;: &quot;Finish sentenizing&quot;}  print(json.dumps(phase), flush=True)  analyzer = PhraseAnalyzer(lang)  exs = analyzer.get_line_phrases(sens, external)  phase = {&quot;step&quot;: 3, &quot;msg&quot;: &quot;Finish phrases dictionary lookup&quot;, &quot;vocabs&quot;: exs[:10]}  print(json.dumps(phase), flush=True)  if assfile:    ass_writer = PhraseASSWriter(srtfile)    ass_writer.write(exs, assfile, {&quot;animation&quot;: False})        phase = {&quot;step&quot;: 4, &quot;msg&quot;: &quot;Finish ass saving&quot;}     print(json.dumps(phase), flush=True)```# Development### Clone project```git clone https://github.com/qishe-nlp/subtitle-analyzer.git```### Install [poetry](https://python-poetry.org/docs/)### Install dependencies```poetry update```### Test```poetry run pytest -rP```which run tests under `tests/*`### Execute```poetry run sta_vocab --helppoetry run sta_phrase --help```### Create sphinx docs```poetry shellcd apidocssphinx-apidoc -f -o source ../subtitle_analyzermake htmlpython -m http.server -d build/html```### Host docs on github pages```cp -rf apidocs/build/html/* docs/```### Build* Change `version` in `pyproject.toml` and `subtitle_analyzer/__init__.py`* Build python package by `poetry build`### Git commit and push### Publish from local dev env* Set pypi test environment variables in poetry, refer to [poetry doc](https://python-poetry.org/docs/repositories/)* Publish to pypi test by `poetry publish -r test`### Publish through CI * Github action build and publish package to [test pypi repo](https://test.pypi.org/)```git tag [x.x.x]git push origin master```* Manually publish to [pypi repo](https://pypi.org/) through [github action](https://github.com/qishe-nlp/subtitle-analyzer/actions/workflows/pypi.yml)</longdescription>
</pkgmetadata>