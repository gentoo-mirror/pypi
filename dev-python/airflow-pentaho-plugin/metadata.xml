<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Pentaho Airflow plugin[![Build Status](https://api.travis-ci.com/damavis/airflow-pentaho-plugin.svg?branch=master)](https://app.travis-ci.com/damavis/airflow-pentaho-plugin)[![codecov](https://codecov.io/gh/damavis/airflow-pentaho-plugin/branch/master/graph/badge.svg)](https://codecov.io/gh/damavis/airflow-pentaho-plugin)[![PyPI](https://img.shields.io/pypi/v/airflow-pentaho-plugin)](https://pypi.org/project/airflow-pentaho-plugin/)[![PyPI - Downloads](https://img.shields.io/pypi/dm/airflow-pentaho-plugin)](https://pypi.org/project/airflow-pentaho-plugin/)This plugins runs Jobs and Transformations through Carte servers.It allows to orchestrate a massive number of trans/jobs taking careof the dependencies between them, even between different instances.This is done by using `CarteJobOperator` and `CarteTransOperator`It also runs Pan (transformations) and Kitchen (Jobs) in local mode,both from repository and local XML files. For this approach, use`KitchenOperator` and `PanOperator`## Requirements1. A Apache Airflow system deployed.2. One or many working PDI CE installations.3. A Carte server for Carte Operators.## SetupThe same setup process must be performed on webserver, schedulerand workers (that runs this tasks) to get it working. If you want todeploy specific workers to run this kind of tasks, see[Queues](https://airflow.apache.org/docs/stable/concepts.html#queues),in **Airflow** *Concepts* section.### Pip packageFirst of all, the package should be installed via `pip install` command.```bashpip install airflow-pentaho-plugin```### Airflow connectionThen, a new connection needs to be added to Airflow Connections, to do this,go to Airflow web UI, and click on `Admin -&gt; Connections` on the top menu.Now, click on `Create` tab.Use HTTP connection type. Enter the **Conn Id**, this plugin uses `pdi_default`by default, the username and the password for your Pentaho Repository.At the bottom of the form, fill the **Extra** field with `pentaho_home`, thepath where your pdi-ce is placed, and `rep`, the repository name for thisconnection, using a json formatted string like it follows.```json{    &quot;pentaho_home&quot;: &quot;/opt/pentaho&quot;,    &quot;rep&quot;: &quot;Default&quot;}```### CarteIn order to use `CarteJobOperator`, the connection should be set different. Fill`host` (including `http://` or `https://`) and `port` for Carte hostname and port,`username` and `password` for PDI repository, and `extra` as it follows.```json{    &quot;rep&quot;: &quot;Default&quot;,    &quot;carte_username&quot;: &quot;cluster&quot;,    &quot;carte_password&quot;: &quot;cluster&quot;}```## Usage### CarteJobOperatorCarteJobOperator is responsible for running jobs in remote slave servers. Hereit is an example of `CarteJobOperator` usage.```python# For versions before 2.0# from airflow.operators.airflow_pentaho import CarteJobOperatorfrom airflow_pentaho.operators.carte import CarteJobOperator# ... ## Define the task using the CarteJobOperatoravg_spent = CarteJobOperator(    conn_id='pdi_default',    task_id=&quot;average_spent&quot;,    job=&quot;/home/bi/average_spent&quot;,    params={&quot;date&quot;: &quot;{{ ds }}&quot;},  # Date in yyyy-mm-dd format    dag=dag)# ... #some_task &gt;&gt; avg_spent &gt;&gt; another_task```### KitchenOperatorKitchen operator is responsible for running Jobs. Lets suppose that we havea defined *Job* saved on `/home/bi/average_spent` in our repository withthe argument `date` as input parameter. Lets define the task using the`KitchenOperator`.```python# For versions before 2.0# from airflow.operators.airflow_pentaho import KitchenOperatorfrom airflow_pentaho.operators.kettle import KitchenOperator# ... ## Define the task using the KitchenOperatoravg_spent = KitchenOperator(    conn_id='pdi_default',    queue=&quot;pdi&quot;,    task_id=&quot;average_spent&quot;,    directory=&quot;/home/bi&quot;,    job=&quot;average_spent&quot;,    params={&quot;date&quot;: &quot;{{ ds }}&quot;},  # Date in yyyy-mm-dd format    dag=dag)# ... #some_task &gt;&gt; avg_spent &gt;&gt; another_task```### CarteTransOperatorCarteTransOperator is responsible for running transformations in remote slaveservers. Here it is an example of `CarteTransOperator` usage.```python# For versions before 2.0# from airflow.operators.airflow_pentaho import CarteTransOperatorfrom airflow_pentaho.operators.carte import CarteTransOperator# ... ## Define the task using the CarteJobOperatorenriche_customers = CarteTransOperator(    conn_id='pdi_default',    task_id=&quot;enrich_customer_data&quot;,    job=&quot;/home/bi/enrich_customer_data&quot;,    params={&quot;date&quot;: &quot;{{ ds }}&quot;},  # Date in yyyy-mm-dd format    dag=dag)# ... #some_task &gt;&gt; enrich_customers &gt;&gt; another_task```### PanOperatorPan operator is responsible for running transformations. Lets suppose thatwe have one saved on `/home/bi/clean_somedata`. Lets define the task using the`PanOperator`. In this case, the transformation receives a parameter thatdetermines the file to be cleaned.```python# For versions before 2.0# from airflow.operators.airflow_pentaho import PanOperatorfrom airflow_pentaho.operators.kettle import PanOperator# ... ## Define the task using the PanOperatorclean_input = PanOperator(    conn_id='pdi_default',    queue=&quot;pdi&quot;,    task_id=&quot;cleanup&quot;,    directory=&quot;/home/bi&quot;,    trans=&quot;clean_somedata&quot;,    params={&quot;file&quot;: &quot;/tmp/input_data/{{ ds }}/sells.csv&quot;},    dag=dag)# ... #some_task &gt;&gt; clean_input &gt;&gt; another_task```For more information, please see `sample_dags/pdi_flow.py`</longdescription>
</pkgmetadata>