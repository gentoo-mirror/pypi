<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># AdaBound[![PyPI - Version](https://img.shields.io/pypi/v/adabound.svg?style=flat)](https://pypi.org/project/adabound/)[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/adabound.svg)](https://pypi.org/project/adabound/)[![PyPI - Wheel](https://img.shields.io/pypi/wheel/adabound.svg?style=flat)](https://pypi.org/project/adabound/)[![GitHub - LICENSE](https://img.shields.io/github/license/Luolc/AdaBound.svg?style=flat)](./LICENSE)An optimizer that trains as fast as Adam and as good as SGD, for developing state-of-the-art deep learning models on a wide variety of pupolar tasks in the field of CV, NLP, and etc.Based on Luo et al. (2019). [Adaptive Gradient Methods with Dynamic Bound of Learning Rate](https://openreview.net/forum?id=Bkg3g2R9FX).In *Proc. of ICLR 2019*.&lt;p align='center'&gt;  &lt;img src='https://www.luolc.com/assets/research/adabound/adabound-banner.png' width=&quot;60%&quot;/&gt;&lt;/p&gt;## Quick Links- [Website](https://www.luolc.com/publications/adabound/)- [Demos](./demos)## InstallationAdaBound requires Python 3.6.0 or later.We currently provide PyTorch version and AdaBound for TensorFlow is coming soon.### Installing via pipThe preferred way to install AdaBound is via `pip` with a virtual environment.Just run ```bashpip install adabound```in your Python environment and you are ready to go!### Using source codeAs AdaBound is a Python class with only 100+ lines, an alternative way is directly downloading[adabound.py](./adabound/adabound.py) and copying it to your project.## UsageYou can use AdaBound just like any other PyTorch optimizers.```python3optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)```As described in the paper, AdaBound is an optimizer that behaves like Adam at the beginning oftraining, and gradually transforms to SGD at the end.The `final_lr` parameter indicates AdaBound would transforms to an SGD with this learning rate.In common cases, a default final learning rate of `0.1` can achieve relatively good and stableresults on unseen data.It is not very sensitive to its hyperparameters.See Appendix G of the paper for more details.Despite of its robust performance, we still have to state that, **there is no silver bullet**.It does not mean that you will be free from tuning hyperparameters once using AdaBound.The performance of a model depends on so many things including the task, the model structure,the distribution of data, and etc.**You still need to decide what hyperparameters to use based on your specific situation,but you may probably use much less time than before!**## DemosThanks to the awesome work by the GitHub team and the Jupyter team, the Jupyter notebook (`.ipynb`)files can render directly on GitHub.We provide several notebooks (like [this one](./demos/cifar10/visualization.ipynb)) for bettervisualization.We hope to illustrate the robust performance of AdaBound through these examples.For the full list of demos, please refer to [this page](./demos).## CitingIf you use AdaBound in your research, please cite [Adaptive Gradient Methods with Dynamic Bound of Learning Rate](https://openreview.net/forum?id=Bkg3g2R9FX).```text@inproceedings{Luo2019AdaBound,  author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},  title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},  booktitle = {Proceedings of the 7th International Conference on Learning Representations},  month = {May},  year = {2019},  address = {New Orleans, Louisiana}}```## License[Apache 2.0](./LICENSE)</longdescription>
</pkgmetadata>