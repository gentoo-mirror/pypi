<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Cloudmesh SbatchA general purpose HPC Template and Experiment management system## BackgroundHyper Performance Computation Clusters (HPCs) are designed around atimesharing principle and are powered by queue-based executionecosystems such as SchedMD's[SLURM](https://slurm.schedmd.com/overview.html) and IBM's PlatformLoad Sharing Facility([LSF](https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=overview-lsf-introduction)).While these ecosystems provide a great deal of control and extensionfor planning, scheduling, and batching jobs, they are limited in theirability to support parameterization in a scheduled task.  While thereare facilities in place to execute jobs on an Array, the ability to dopermutation based experments are limited to what you integrate intoyour own batch script.  Even then, parameterization of values are onlymade availabile as environment variables, which can be limiteddepending on your OS or selected programming language.  In many caseslimitations set by the deployment trhough the compute center alsohinder optimal use while restrictions are placed on duration andnumber of parallel accessible resources. In some cases theserestrictions are soo established that removing them is impractical andtakes weks to implement on temporary basis.Cloudmesh Sbatch is a framework that wraps the SLURM batch processorinto a templated framework such that experiments can be generatedbased on configuration files focusing on the livecycle of generatingmany permutations of experiments with standard tooling, so that youcan focus more on modeling your experiments than how to orchestratethem with tools.  A number of batch scripts can be generated that thancan be executed according to center policies.## DependenciesWhen you install cloudmesh-sbatch, you will also be installing aminimum baseline of the `cms` command (as part of the Cloudmeshecosystem).  For more details on Cloudmesh, see its documentation on[read the docs](https://cloudmesh.github.io/cloudmesh-manual/). Howeverall instalation can be done thorugh pip. After instalation, you willneed to initialize cloudmesh with the command```bash$ cms help```While SLURM is not needed to run the `cloudmesh sbatch` command, thegenerated output will not exectue unless your system has slurm installedand you are able to run jobs via the `slurm sbatch` command.## Documentation### Running Cloudmesh SBatchThe `cloudmesh sbatch` command takes one of two forms of execution.  It is started with ```bash$ cms sbatch &lt;command&gt; &lt;parameters&gt;```Where the command invokes a partiuclar action and parameters include anumber of parameters for the command These commands allow you toinspect the generated output to confirm your parameterizationfunctions as expected and as intended.In general, configuration arguments that appear in multiple locations areprioritized in the following order (highest priority first)1. CLI Arguments with `cms sbatch`2. Configuration Files3. Preset values### Generating Experiments with the CLIThe `generate` command is used to generate your experiments based upon either a passedconfiguration file, or via CLI arguments.  You can issue the command usingeither of the below forms:```textcms sbatch generate SOURCE --name=NAME [--verbose] [--mode=MODE] [--config=CONFIG] [--attributes=PARAMS] [--out=DESTINATION] [--dryrun] [--noos] [--nocm] [--dir=DIR] [--experiment=EXPERIMENT]cms sbatch generate --setup=FILE [SOURCE] [--verbose] [--mode=MODE]  [--config=CONFIG] [--attributes=PARAMS] [--out=DESTINATION] [--dryrun] [--noos] [--nocm] [--dir=DIR] [--experiment=EXPERIMENT] [--name=NAME]```If you have prepared a configuration file that conforms to the schemadefined in [Setup Config](#setup-config), then you can use the secondform which overrides the default values.* `--name=NAME` - Supplies a name for this experiment.  Note that the name  must not match any existing files or directories where you are currently  executing the command* `--verbose` - Enables additional logging useful when troubleshooting the  program.* `--mode=MODE` - specifies how the output should be generated.  One of: f,h,d.  * `f` or `flat` - specifies a &quot;flat&quot; mode, where slurm scripts are generated in a flattened structure, all in one directory.  * `h` or `hierarchical` - specifies a &quot;hierarchical&quot; mode, where experiments are nested into unique directories from each other.  * `d` or `debug` - instructs the command to not generate any output.* `--config=CONFIG` - specifies key-value pairs to be used across all files for substitution.  This can be a python, yaml, or json file.* `--attributes=PARAMS` - specifies key-value pairs that can be listed at the command line and used as substitution across all experiments.  Note this command leverages [cloudmesh's parameter expansion specification](https://cloudmesh.github.io/cloudmesh-manual/autoapi/cloudmeshcommon/cloudmesh/common/parameter/index.html) for different types of expansion rules.* `--out=DESTINATION` - specifies the directory to write the generated scripts out to.* `--dryrun` - Runs the command without performing any operations* `--noos` - Prevents the interleaving of OS environemnt variables into the subsitution logic* `--dir=DIR` - specifies the directory to write the generated scripts out to.* `--experiment=EXPERIMENT` - specifies a listing of key-value parameters that establish a unique experiment for each combination of values (a cartisian product across all values for each key).* `--setup=FILE` - provides all the above configuration options within a configuration  file to simplify executions.### Form 2 - Generating Submission Scripts```textsbatch generate submit --name=NAME [--verbose]```This command uses the output of the[generate command](#command-1---generating-experiments) and generatesa shell script that can be used to submit your previously generatedoutputs to SLURM as a sequence of sbatch commands.* `--name=NAME` - specifies the name used in the  [generate command](#command-1---generating-experiments).  The generate command will inspect the `&lt;NAME&gt;.json` file and build the  necessary commands to run all permutations that the cloudmesh sbatch  command generated.Note that this command only generates the script, and you must run theoutputted file in your shell for the commands to be issued to SLURM andrun your jobs.**Sample YAML File**This command requires a YAML file which is configured for the host and gpu.The YAML file also points to the desired slurm template.```pythonslurm_template: 'slurm_template.slurm'sbatch_setup:  &lt;hostname&gt;-&lt;gpu&gt;:    - card_name: &quot;a100&quot;    - time: &quot;05:00:00&quot;    - num_cpus: 6    - num_gpus: 1  rivanna-v100:    - card_name: &quot;v100&quot;    - time: &quot;06:00:00&quot;    - num_cpus: 6    - num_gpus: 1```example:```cms sbatch slurm.in.sh --config=a.py,b.json,c.yaml --attributes=a=1,b=4  --noos --dir=example --experiment=\&quot;epoch=[1-3] x=[1,4] y=[10,11]\&quot;sbatch slurm.in.sh --config=a.py,b.json,c.yaml --attributes=a=1,b=4 --noos --dir=example --experiment=&quot;epoch=[1-3] x=[1,4] y=[10,11]&quot;# ERROR: Importing python not yet implementedepoch=1 x=1 y=10  sbatch example/slurm.shepoch=1 x=1 y=11  sbatch example/slurm.shepoch=1 x=4 y=10  sbatch example/slurm.shepoch=1 x=4 y=11  sbatch example/slurm.shepoch=2 x=1 y=10  sbatch example/slurm.shepoch=2 x=1 y=11  sbatch example/slurm.shepoch=2 x=4 y=10  sbatch example/slurm.shepoch=2 x=4 y=11  sbatch example/slurm.shepoch=3 x=1 y=10  sbatch example/slurm.shepoch=3 x=1 y=11  sbatch example/slurm.shepoch=3 x=4 y=10  sbatch example/slurm.shepoch=3 x=4 y=11  sbatch example/slurm.shTimer: 0.0022s Load: 0.0013s sbatch slurm.in.sh --config=a.py,b.json,c.yaml --attributes=a=1,b=4 --noos --dir=example --experiment=&quot;epoch=[1-3] x=[1,4] y=[10,11]&quot;```## Slurm on a single computer ubuntu 20.04### Install see https://drtailor.medium.com/how-to-setup-slurm-on-ubuntu-20-04-for-single-node-work-scheduling-6cc90957436532 Processors (threads)```bashsudo apt update -ysudo apt install slurmd slurmctld -ysudo chmod 777 /etc/slurm-llnl# make sure to use the HOSTNAMEsudo cat &lt;&lt; EOF &gt; /etc/slurm-llnl/slurm.conf# slurm.conf file generated by configurator.html.# Put this file on all nodes of your cluster.# See the slurm.conf man page for more information.#ClusterName=localclusterSlurmctldHost=$HOSTNAMEMpiDefault=noneProctrackType=proctrack/linuxprocReturnToService=2SlurmctldPidFile=/var/run/slurmctld.pidSlurmctldPort=6817SlurmdPidFile=/var/run/slurmd.pidSlurmdPort=6818SlurmdSpoolDir=/var/lib/slurm-llnl/slurmdSlurmUser=slurmStateSaveLocation=/var/lib/slurm-llnl/slurmctldSwitchType=switch/noneTaskPlugin=task/none## TIMERSInactiveLimit=0KillWait=30MinJobAge=300SlurmctldTimeout=120SlurmdTimeout=300Waittime=0# SCHEDULINGSchedulerType=sched/backfillSelectType=select/cons_tresSelectTypeParameters=CR_Core##AccountingStoragePort=AccountingStorageType=accounting_storage/noneJobCompType=jobcomp/noneJobAcctGatherFrequency=30JobAcctGatherType=jobacct_gather/noneSlurmctldDebug=infoSlurmctldLogFile=/var/log/slurm-llnl/slurmctld.logSlurmdDebug=infoSlurmdLogFile=/var/log/slurm-llnl/slurmd.log## COMPUTE NODES # THis machine has 128GB main memoryNodeName=$HOSTNAME CPUs=32 RealMemory==128762 State=UNKNOWNPartitionName=local Nodes=ALL Default=YES MaxTime=INFINITE State=UPEOFsudo chmod 755 /etc/slurm-llnl/```### Start```sudo systemctl start slurmctldsudo systemctl start slurmd# sudo scontrol update nodename=$HOSTNAME state=idlesudo scontrol update nodename=$HOSTNAME state=resume```### Stop```sudo systemctl stop slurmdsudo systemctl stop slurmctld```### Info```sinfosinfo -Rsinfo -a```### Jobsave into gregor.slurm```#!/bin/bash#SBATCH --job-name=gregors_test          # Job name#SBATCH --mail-type=END,FAIL             # Mail events (NONE, BEGIN, END, FAIL, ALL)#SBATCH --mail-user=laszewski@gmail.com  # Where to send mail#SBATCH --ntasks=1                       # Run on a single CPU####  XBATCH --mem=1gb                        # Job memory request#SBATCH --time=00:05:00                  # Time limit hrs:min:sec#SBATCH --output=sgregors_test_%j.log    # Standard output and error logpwd; hostname; dateecho &quot;Gregors Test&quot;datesleep(30)date```Run with ```sbatch gregor.slurmwatch -n 1 squeue```BUG```JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)                 2    LocalQ gregors_    green PD       0:00      1 (Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions)```### sbatch slurm manageement commands for localhoststart slurm deamons```bashcms sbatch slurm start```stop surm deamons```bashcms sbatch slurm stop```BUG:```bashsrun gregor.slurmsrun: Required node not available (down, drained or reserved)srun: job 7 queued and waiting for resources``````sudo scontrol update nodename=localhost state=POWER_UPValid states are: NoResp DRAIN FAIL FUTURE RESUME POWER_DOWN POWER_UP UNDRAIN```### Cheatsheet* &lt;https://slurm.schedmd.com/pdfs/summary.pdf&gt;## AcknowledgementsContinued work was in part funded by the NSFCyberTraining: CIC: CyberTraining for Students and Technologiesfrom Generation Z with the awadrd numbers 1829704 and 2200409.</longdescription>
</pkgmetadata>