<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Pytest JSON Report[![CI](https://github.com/numirias/pytest-json-report/actions/workflows/main.yml/badge.svg)](https://github.com/numirias/pytest-json-report/actions/workflows/main.yml)[![PyPI Version](https://img.shields.io/pypi/v/pytest-json-report.svg)](https://pypi.python.org/pypi/pytest-json-report)[![Python Versions](https://img.shields.io/pypi/pyversions/pytest-json-report.svg)](https://pypi.python.org/pypi/pytest-json-report)This pytest plugin creates test reports as JSON. This makes it easy to process test results in other applications.It can report a summary, test details, captured output, logs, exception tracebacks and more. Additionally, you can use the available fixtures and hooks to [add metadata](#metadata) and [customize](#modifying-the-report) the report as you like.## Table of contents* [Installation](#installation)* [Options](#options)* [Usage](#usage)   * [Metadata](#metadata)   * [Modifying the report](#modifying-the-report)   * [Direct invocation](#direct-invocation)* [Format](#format)   * [Summary](#summary)   * [Environment](#environment)   * [Collectors](#collectors)   * [Tests](#tests)   * [Test stage](#test-stage)   * [Log](#log)   * [Warnings](#warnings)* [Related tools](#related-tools)## Installation```pip install pytest-json-report --upgrade ```## Options| Option | Description || --- | --- || `--json-report` | Create JSON report || `--json-report-file=PATH` | Target path to save JSON report (use &quot;none&quot; to not save the report) || `--json-report-summary` | Just create a summary without per-test details || `--json-report-omit=FIELD_LIST` | List of fields to omit in the report (choose from: `collectors`, `log`, `traceback`, `streams`, `warnings`, `keywords`) || `--json-report-indent=LEVEL` | Pretty-print JSON with specified indentation level || `--json-report-verbosity=LEVEL` | Set verbosity (default is value of `--verbosity`) |## UsageJust run pytest with `--json-report`. The report is saved in `.report.json` by default.```bash$ pytest --json-report -v tests/$ cat .report.json{&quot;created&quot;: 1518371686.7981803, ... &quot;tests&quot;:[{&quot;nodeid&quot;: &quot;test_foo.py&quot;, &quot;outcome&quot;: &quot;passed&quot;, ...}, ...]}```If you just need to know how many tests passed or failed and don't care about details, you can produce a summary only:```bash$ pytest --json-report --json-report-summary```Many fields can be omitted to keep the report size small. E.g., this will leave out keywords and stdout/stderr output:```bash$ pytest --json-report --json-report-omit keywords streams```If you don't like to have the report saved, you can specify `none` as the target file name:```bash$ pytest --json-report --json-report-file none```## Advanced usage### MetadataThe easiest way to add your own metadata to a test item is by using the `json_metadata` [test fixture](https://docs.pytest.org/en/stable/fixture.html):```pythondef test_something(json_metadata):    json_metadata['foo'] = {&quot;some&quot;: &quot;thing&quot;}    json_metadata['bar'] = 123```Or use the `pytest_json_runtest_metadata` [hook](https://docs.pytest.org/en/stable/reference.html#hooks) (in your `conftest.py`) to add metadata based on the current test run. The dict returned will automatically be merged with any existing metadata. E.g., this adds the start and stop time of each test's `call` stage:```pythondef pytest_json_runtest_metadata(item, call):    if call.when != 'call':        return {}    return {'start': call.start, 'stop': call.stop}```Also, you could add metadata using [pytest-metadata's `--metadata` switch](https://github.com/pytest-dev/pytest-metadata#additional-metadata) which will add metadata to the report's `environment` section, but not to a specific test item. You need to make sure all your metadata is JSON-serializable.### A note on hooksIf you're using a `pytest_json_*` hook although the plugin is not installed or not active (not using `--json-report`), pytest doesn't recognize it and may fail with an internal error like this:```INTERNALERROR&gt; pluggy.manager.PluginValidationError: unknown hook 'pytest_json_runtest_metadata' in plugin &lt;module 'conftest' from 'conftest.py'&gt;```You can avoid this by declaring the hook implementation optional:```pythonimport pytest@pytest.hookimpl(optionalhook=True)def pytest_json_runtest_metadata(item, call):    ...```### Modifying the reportYou can modify the entire report before it's saved by using the `pytest_json_modifyreport` hook.Just implement the hook in your `conftest.py`, e.g.:```pythondef pytest_json_modifyreport(json_report):    # Add a key to the report    json_report['foo'] = 'bar'    # Delete the summary from the report    del json_report['summary']```After `pytest_sessionfinish`, the report object is also directly available to script via `config._json_report.report`. So you can access it using some built-in hook:```pythondef pytest_sessionfinish(session):    report = session.config._json_report.report    print('exited with', report['exitcode'])```If you *really* want to change how the result of a test stage run is turned into JSON, you can use the `pytest_json_runtest_stage` hook. It takes a [`TestReport`](https://docs.pytest.org/en/latest/reference.html#_pytest.runner.TestReport) and returns a JSON-serializable dict:```pythondef pytest_json_runtest_stage(report):    return {'outcome': report.outcome}```### Direct invocationYou can use the plugin when invoking `pytest.main()` directly from code:```pythonimport pytestfrom pytest_jsonreport.plugin import JSONReportplugin = JSONReport()pytest.main(['--json-report-file=none', 'test_foo.py'], plugins=[plugin])```You can then access the `report` object:```pythonprint(plugin.report)```And save the report manually:```pythonplugin.save_report('/tmp/my_report.json')```## FormatThe JSON report contains metadata of the session, a summary, collectors, tests and warnings. You can find a sample report in [`sample_report.json`](sample_report.json).| Key | Description || --- | --- || `created` | Report creation date. (Unix time) || `duration` | Session duration in seconds. || `exitcode` | Process exit code as listed [in the pytest docs](https://docs.pytest.org/en/latest/usage.html#possible-exit-codes). The exit code is a quick way to tell if any tests failed, an internal error occurred, etc. || `root` | Absolute root path from which the session was started. || `environment` | [Environment](#environment) entry. || `summary` | [Summary](#summary) entry. || `collectors` | [Collectors](#collectors) entry. (absent if `--json-report-summary` or if no collectors)  || `tests` | [Tests](#tests) entry. (absent if `--json-report-summary`)  || `warnings` | [Warnings](#warnings) entry. (absent if `--json-report-summary` or if no warnings)  |#### Example```python{    &quot;created&quot;: 1518371686.7981803,    &quot;duration&quot;: 0.1235666275024414,    &quot;exitcode&quot;: 1,    &quot;root&quot;: &quot;/path/to/tests&quot;,    &quot;environment&quot;: ENVIRONMENT,    &quot;summary&quot;: SUMMARY,    &quot;collectors&quot;: COLLECTORS,    &quot;tests&quot;: TESTS,    &quot;warnings&quot;: WARNINGS,}```### SummaryNumber of outcomes per category and the total number of test items.| Key | Description || --- | --- ||  `collected` | Total number of tests collected. ||  `total` | Total number of tests run. ||  `deselected` | Total number of tests deselected. (absent if number is 0) || `&lt;outcome&gt;` | Number of tests with that outcome. (absent if number is 0) |#### Example```python{    &quot;collected&quot;: 10,    &quot;passed&quot;: 2,    &quot;failed&quot;: 3,    &quot;xfailed&quot;: 1,    &quot;xpassed&quot;: 1,    &quot;error&quot;: 2,    &quot;skipped&quot;: 1,    &quot;total&quot;: 10}```### EnvironmentThe environment section is provided by [pytest-metadata](https://github.com/pytest-dev/pytest-metadata). All metadata given by that plugin will be added here, so you need to make sure it is JSON-serializable.#### Example```python{    &quot;Python&quot;: &quot;3.6.4&quot;,    &quot;Platform&quot;: &quot;Linux-4.56.78-9-ARCH-x86_64-with-arch&quot;,    &quot;Packages&quot;: {        &quot;pytest&quot;: &quot;3.4.0&quot;,        &quot;py&quot;: &quot;1.5.2&quot;,        &quot;pluggy&quot;: &quot;0.6.0&quot;    },    &quot;Plugins&quot;: {        &quot;json-report&quot;: &quot;0.4.1&quot;,        &quot;xdist&quot;: &quot;1.22.0&quot;,        &quot;metadata&quot;: &quot;1.5.1&quot;,        &quot;forked&quot;: &quot;0.2&quot;,        &quot;cov&quot;: &quot;2.5.1&quot;    },    &quot;foo&quot;: &quot;bar&quot;, # Custom metadata entry passed via pytest-metadata}```### CollectorsA list of collector nodes. These are useful to check what tests are available without running them, or to debug an error during test discovery.| Key | Description || --- | --- || `nodeid` | ID of the collector node. ([See docs](https://docs.pytest.org/en/latest/example/markers.html#node-id)) The root node has an empty node ID. || `outcome` | Outcome of the collection. (Not the test outcome!) || `result` | Nodes collected by the collector. || `longrepr` | Representation of the collection error. (absent if no error occurred) |The `result` is a list of the collected nodes:| Key | Description || --- | --- || `nodeid` | ID of the node. || `type` | Type of the collected node. || `lineno` | Line number. (absent if not applicable) || `deselected` | `true` if the test is deselected. (absent if not deselected) |#### Example```python[    {        &quot;nodeid&quot;: &quot;&quot;,        &quot;outcome&quot;: &quot;passed&quot;,        &quot;result&quot;: [            {                &quot;nodeid&quot;: &quot;test_foo.py&quot;,                &quot;type&quot;: &quot;Module&quot;            }        ]    },    {        &quot;nodeid&quot;: &quot;test_foo.py&quot;,        &quot;outcome&quot;: &quot;passed&quot;,        &quot;result&quot;: [            {                &quot;nodeid&quot;: &quot;test_foo.py::test_pass&quot;,                &quot;type&quot;: &quot;Function&quot;,                &quot;lineno&quot;: 24,                &quot;deselected&quot;: true            },            ...        ]    },    {        &quot;nodeid&quot;: &quot;test_bar.py&quot;,        &quot;outcome&quot;: &quot;failed&quot;,        &quot;result&quot;: [],        &quot;longrepr&quot;: &quot;/usr/lib/python3.6 ... invalid syntax&quot;    },    ...]```### TestsA list of test nodes. Each completed test stage produces a stage object (`setup`, `call`, `teardown`) with its own `outcome`.| Key | Description || --- | --- || `nodeid` | ID of the test node. || `lineno` | Line number where the test starts. || `keywords` | List of keywords and markers associated with the test. || `outcome` | Outcome of the test run. || `{setup, call, teardown}` | [Test stage](#test-stage) entry. To find the error in a failed test you need to check all stages. (absent if stage didn't run) || `metadata` | [Metadata](#metadata) item. (absent if no metadata) |#### Example```python[    {        &quot;nodeid&quot;: &quot;test_foo.py::test_fail&quot;,        &quot;lineno&quot;: 50,        &quot;keywords&quot;: [            &quot;test_fail&quot;,            &quot;test_foo.py&quot;,            &quot;test_foo0&quot;        ],        &quot;outcome&quot;: &quot;failed&quot;,        &quot;setup&quot;: TEST_STAGE,        &quot;call&quot;: TEST_STAGE,        &quot;teardown&quot;: TEST_STAGE,        &quot;metadata&quot;: {            &quot;foo&quot;: &quot;bar&quot;,        }    },    ...]```### Test stageA test stage item.| Key | Description || --- | --- || `duration` | Duration of the test stage in seconds. || `outcome` | Outcome of the test stage. (can be different from the overall test outcome) || `crash` | Crash entry. (absent if no error occurred) || `traceback` | List of traceback entries. (absent if no error occurred; affected by `--tb` option) || `stdout` | Standard output. (absent if none available) || `stderr` | Standard error. (absent if none available) || `log` | [Log](#log) entry. (absent if none available) || `longrepr` | Representation of the error. (absent if no error occurred; format affected by `--tb` option) |#### Example```python{    &quot;duration&quot;: 0.00018835067749023438,    &quot;outcome&quot;: &quot;failed&quot;,    &quot;crash&quot;: {        &quot;path&quot;: &quot;/path/to/tests/test_foo.py&quot;,        &quot;lineno&quot;: 54,        &quot;message&quot;: &quot;TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'&quot;    },    &quot;traceback&quot;: [        {            &quot;path&quot;: &quot;test_foo.py&quot;,            &quot;lineno&quot;: 65,            &quot;message&quot;: &quot;&quot;        },        {            &quot;path&quot;: &quot;test_foo.py&quot;,            &quot;lineno&quot;: 63,            &quot;message&quot;: &quot;in foo&quot;        },        {            &quot;path&quot;: &quot;test_foo.py&quot;,            &quot;lineno&quot;: 63,            &quot;message&quot;: &quot;in &lt;listcomp&gt;&quot;        },        {            &quot;path&quot;: &quot;test_foo.py&quot;,            &quot;lineno&quot;: 54,            &quot;message&quot;: &quot;TypeError&quot;        }    ],    &quot;stdout&quot;: &quot;foo\nbar\n&quot;,    &quot;stderr&quot;: &quot;baz\n&quot;,    &quot;log&quot;: LOG,    &quot;longrepr&quot;: &quot;def test_fail_nested():\n ...&quot;}```### LogA list of log records. The fields of a log record are the [`logging.LogRecord` attributes](https://docs.python.org/3/library/logging.html#logrecord-attributes), with the exception that the fields `exc_info` and `args` are always empty and `msg` contains the formatted log message.You can apply [`logging.makeLogRecord()`](https://docs.python.org/3/library/logging.html#logging.makeLogRecord)  on a log record to convert it back to a `logging.LogRecord` object.#### Example```python[    {        &quot;name&quot;: &quot;root&quot;,        &quot;msg&quot;: &quot;This is a warning.&quot;,        &quot;args&quot;: null,        &quot;levelname&quot;: &quot;WARNING&quot;,        &quot;levelno&quot;: 30,        &quot;pathname&quot;: &quot;/path/to/tests/test_foo.py&quot;,        &quot;filename&quot;: &quot;test_foo.py&quot;,        &quot;module&quot;: &quot;test_foo&quot;,        &quot;exc_info&quot;: null,        &quot;exc_text&quot;: null,        &quot;stack_info&quot;: null,        &quot;lineno&quot;: 8,        &quot;funcName&quot;: &quot;foo&quot;,        &quot;created&quot;: 1519772464.291738,        &quot;msecs&quot;: 291.73803329467773,        &quot;relativeCreated&quot;: 332.90839195251465,        &quot;thread&quot;: 140671803118912,        &quot;threadName&quot;: &quot;MainThread&quot;,        &quot;processName&quot;: &quot;MainProcess&quot;,        &quot;process&quot;: 31481    },    ...]```### WarningsA list of warnings that occurred during the session. (See the [pytest docs on warnings](https://docs.pytest.org/en/latest/warnings.html).)| Key | Description || --- | --- || `filename` | File name. || `lineno` | Line number. || `message` | Warning message. || `when` | When the warning was captured. (`&quot;config&quot;`, `&quot;collect&quot;` or `&quot;runtest&quot;` as listed [here](https://docs.pytest.org/en/latest/reference.html#_pytest.hookspec.pytest_warning_captured)) |#### Example```python[    {        &quot;code&quot;: &quot;C1&quot;,        &quot;path&quot;: &quot;/path/to/tests/test_foo.py&quot;,        &quot;nodeid&quot;: &quot;test_foo.py::TestFoo&quot;,        &quot;message&quot;: &quot;cannot collect test class 'TestFoo' because it has a __init__ constructor&quot;    }]```## Related tools- [pytest-json](https://github.com/mattcl/pytest-json) has some great features but appears to be unmaintained. I borrowed some ideas and test cases from there.- [tox has a switch](http://tox.readthedocs.io/en/latest/example/result.html) to create a JSON report including a test result summary. However, it just provides the overall outcome without any per-test details.</longdescription>
</pkgmetadata>