<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># jupyterhub_moss: JupyterHub MOdular Slurm Spawner**jupyterhub_moss** is a Python package that provides:- A [JupyterHub](https://jupyterhub.readthedocs.io/)  [Slurm](https://slurm.schedmd.com/) Spawner that can be configured by  [setting the available partitions](#partition-settings). It is an extension of  [`batchspawner.SlurmSpawner`](https://github.com/jupyterhub/batchspawner).- An associated [spawn page](#spawn-page) that changes according to the  partitions set in the Spawner and allows the user to select Slurm resources to  use.&lt;img style=&quot;margin:auto&quot; src=https://user-images.githubusercontent.com/9449698/215526389-2ef5ac32-5d50-49de-aa5f-46972feaccf1.png width=&quot;50%&quot;&gt;## Install`pip install jupyterhub_moss`## Usage### Partition settingsTo use **jupyterhub_moss**, you need first a working[JupyterHub](https://jupyterhub.readthedocs.io/) instance. **jupyterhub_moss**needs then to be imported in[your JupyterHub configuration file](https://jupyterhub.readthedocs.io/en/stable/getting-started/config-basics.html)(usually named `jupyterhub_conf.py`):```pythonimport batchspawnerimport jupyterhub_mossc = get_config()# ...your config# Init JupyterHub configuration to use this spawnerjupyterhub_moss.set_config(c)```Once **jupyterhub_moss** is set up, you can define the partitions available onSlurm by setting `c.MOSlurmSpawner.partitions` in the same file:```python# ...# Partition descriptionsc.MOSlurmSpawner.partitions = {    &quot;partition_1&quot;: {  # Partition name     # (See description of fields below for more info)        &quot;architecture&quot;: &quot;x86_86&quot;,          # Nodes architecture        &quot;description&quot;: &quot;Partition 1&quot;,      # Displayed description        &quot;gpu&quot;: None,                       # --gres= template to use for requesting GPUs        &quot;max_ngpus&quot;: 0,                    # Maximum number of GPUs per node        &quot;max_nprocs&quot;: 28,                  # Maximum number of CPUs per node        &quot;max_runtime&quot;: 12*3600,            # Maximum time limit in seconds (Must be at least 1hour)        &quot;simple&quot;: True,                    # True to show in Simple tab        &quot;jupyter_environments&quot;: {            &quot;default&quot;: {                   # Jupyter environment identifier, at least &quot;path&quot; or &quot;modules&quot; is mandatory                &quot;description&quot;: &quot;Default&quot;,  # Text displayed for this environment select option                &quot;path&quot;: &quot;/env/path/bin/&quot;,  # Path to Python environment bin/ used to start Jupyter server on the Slurm nodes                &quot;modules&quot;: &quot;&quot;,             # Space separated list of environment modules to load before starting Jupyter server                &quot;add_to_path&quot;: True,       # Toggle adding the environment to shell PATH (optional, default: True)                &quot;prologue&quot;: &quot;&quot;,            # Shell commands to execute before starting the Jupyter server (optional, default: &quot;&quot;)            },        },    },    &quot;partition_2&quot;: {        &quot;architecture&quot;: &quot;ppc64le&quot;,        &quot;description&quot;: &quot;Partition 2&quot;,        &quot;gpu&quot;: &quot;gpu:V100-SXM2-32GB:{}&quot;,        &quot;max_ngpus&quot;: 2,        &quot;max_nprocs&quot;: 128,        &quot;max_runtime&quot;: 1*3600,        &quot;simple&quot;: True,        &quot;jupyter_environments&quot;: {            &quot;default&quot;: {                &quot;description&quot;: &quot;Default&quot;,                &quot;path&quot;: &quot;&quot;,                &quot;modules&quot;: &quot;JupyterLab/3.6.0&quot;,                &quot;add_to_path&quot;: True,                &quot;prologue&quot;: &quot;echo 'Starting default environment'&quot;,            },        },    },    &quot;partition_3&quot;: {        &quot;architecture&quot;: &quot;x86_86&quot;,        &quot;description&quot;: &quot;Partition 3&quot;,        &quot;gpu&quot;: None,        &quot;max_ngpus&quot;: 0,        &quot;max_nprocs&quot;: 28,        &quot;max_runtime&quot;: 12*3600,        &quot;simple&quot;: False,        &quot;jupyter_environments&quot;: {            &quot;default&quot;: {                &quot;description&quot;: &quot;Partition 3 default&quot;,                &quot;path&quot;: &quot;/path/to/jupyter/env/for/partition_3/bin/&quot;,                &quot;modules&quot;: &quot;JupyterLab/3.6.0&quot;,                &quot;add_to_path&quot;: True,                &quot;prologue&quot;: &quot;echo 'Starting default environment'&quot;,        },    },}```For a minimalistic working demo, check the[`demo/jupyterhub_conf.py`](demo/jupyterhub_conf.py) config file.### Field descriptions- `architecture`: The architecture of the partition. This is only cosmetic and  will be used to generate subtitles in the spawn page.- `description`: The description of the partition. This is only cosmetic and  will be used to generate subtitles in the spawn page.- `gpu`: [Optional] A template string that will be used to request GPU resources  through `--gres`. The template should therefore include a `{}` that will be  replaced by the number of requested GPU **and** follow the format expected by  `--gres`. If no GPU is available for this partition, set to `&quot;&quot;`. It is  retrieved from SLURM if not provided.- `max_ngpus`: [Optional] The maximum number of GPU that can be requested for  this partition. The spawn page will use this to generate appropriate bounds  for the user inputs. If no GPU is available for this partition, set to `0`. It  is retrieved from SLURM if not provided.- `max_nprocs`: [Optional] The maximum number of processors that can be  requested for this partition. The spawn page will use this to generate  appropriate bounds for the user inputs. It is retrieved from SLURM if not  provided.- `max_runtime`: [Optional] The maximum job runtime for this partition in  seconds. It should be of minimum 1 hour as the _Simple_ tab only display  buttons for runtimes greater than 1 hour. It is retrieved from SLURM if not  provided.- `simple`: Whether the partition should be available in the _Simple_ tab. The  spawn page that will be generated is organized in a two tabs: a _Simple_ tab  with minimal settings that will be enough for most users and an _Advanced_ tab  where almost all Slurm job settings can be set. Some partitions can be hidden  from the _Simple_ tab with setting `simple` to `False`.- `jupyter_environments`: Mapping of identifer name to information about Python  environment used to run Jupyter on the Slurm nodes. Either `path` or `modules`  (or both) should be defined. This information is a mapping containing:  - `description`: Text used for display in the selection options.  - `path`: The path to a Python environment bin/ used to start jupyter on the    Slurm nodes. **jupyterhub_moss** needs that a virtual (or conda) environment    is used to start Jupyter. This path can be changed according to the    partitions.  - `modules`: Space separated list of environment modules to load before    starting the Jupyter server. Environment modules will be loaded with the    `module` command.  - `add_to_path`: Whether or not to prepend the environment `path` to shell    `PATH`.  - `prologue`: Shell commands to execute on the Slurm node before starting the    Jupyter single-user server. This can be used to run, e.g.,    `module load &lt;module&gt;`. By default no command is run.### Spawn pageThe spawn page (available at `/hub/spawn`) will be generated according to thepartition settings. For example, this is the spawn page generated for thepartition settings above:&lt;img style=&quot;margin:1rem auto&quot; src=https://user-images.githubusercontent.com/9449698/215526389-2ef5ac32-5d50-49de-aa5f-46972feaccf1.png width=&quot;50%&quot;&gt;This spawn page is separated in two tabs: a _Simple_ and an _Advanced_ tab. Onthe _Simple_ tab, the user can choose between the partitions set though`simple: True` (`partition_1` and `partition_2` in this case), choose to take aminimum, a half or a maximum number of cores and choose the job duration. Theavailable resources are checked using `sinfo` and displayed on the table below.Clicking on the **Start** button will request the job.The spawn page adapts to the chosen partition. This is the page when selectingthe `partition_2`:&lt;img style=&quot;margin:1rem auto&quot; src=https://user-images.githubusercontent.com/9449698/215526553-4ba57510-efac-4a28-a576-ef81ff9ec2f5.png width=&quot;50%&quot;&gt;As the maximum number of cores is different, the CPUs row change accordingly.Also, as `gpu` was set for `partition_2`, a new button row appears to enable GPUrequests.The _Advanced_ tab allows finer control on the requested resources.&lt;img style=&quot;margin:1rem auto&quot; src=https://user-images.githubusercontent.com/9449698/262627623-91bd63de-6374-47d4-9064-d1a6e3d56411.png width=&quot;50%&quot;&gt;The user can select any partition (`partition_3` is added in this case) and thetable of available resources reflects this. The user can also choose any numberof nodes (with the max given by `max_nprocs`), of GPUs (max: `max_gpus`) andhave more control on the job duration (max: `max_runtime`).### Spawn through URLIt is also possible to pass the spawning options as query arguments to the spawnURL: `https://&lt;server:port&gt;/hub/spawn`. For example,`https://&lt;server:port&gt;/hub/spawn?partition=partition_1&amp;nprocs=4` will directlyspawn a Jupyter server on `partition_1` with 4 cores allocated.The following query argument is required:- `partition`: The name of the SLURM partition to use.The following optional query arguments are available:- SLURM configuration:  - `memory`: Total amount of memory per node    ([`--mem`](https://slurm.schedmd.com/sbatch.html#OPT_mem))  - `ngpus`: Number of GPUs    ([`--gres:&lt;gpu&gt;:`](https://slurm.schedmd.com/sbatch.html#OPT_gres))  - `nprocs`: Number of CPUs per task    ([`--cpus-per-task`](https://slurm.schedmd.com/sbatch.html#OPT_cpus-per-task))  - `options`: Extra SLURM options  - `output`: Set to `true` to save logs to `slurm-*.out` files.  - `reservation`: SLURM reservation name    ([`--reservation`](https://slurm.schedmd.com/sbatch.html#OPT_reservation))  - `runtime`: Job duration as hh:mm:ss    ([`--time`](https://slurm.schedmd.com/sbatch.html#OPT_time))- Jupyter(Lab) configuration:  - `default_url`: The URL to open the Jupyter environment with: use `/lab` to    start [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/) or use    [JupyterLab URLs](https://jupyterlab.readthedocs.io/en/stable/user/urls.html)  - `environment_id`: Name of the Python environment defined in the    configuration used to start Jupyter  - `environment_path`: Path to the Python environment bin/ used to start    Jupyter  - `environment_modules`: Space-separated list of    [environment module](https://modules.sourceforge.net/) names to load before    starting Jupyter  - `root_dir`: The path of the &quot;root&quot; folder browsable from Jupyter(Lab)    (user's home directory if not provided)To use a Jupyter environment defined in the configuration, only provide its`environment_id`, for example:`https://&lt;server:port&gt;/hub/spawn?partition=partition_1&amp;environment_id=default`.To use a custom Jupyter environment, instead provide the corresponding`environment_path` and/or `environment_modules`, for example:- `https://&lt;server:port&gt;/hub/spawn?partition=partition_1&amp;environment_path=/path/to/jupyter/bin`,  or- `https://&lt;server:port&gt;/hub/spawn?partition=partition_1&amp;environment_modules=myjupytermodule`.## DevelopmentSee [CONTRIBUTING.md](CONTRIBUTING.md).## Credits:We would like acknowledge the following ressources that served as base for thisproject and thank their authors:- This [gist](https://gist.github.com/zonca/aaed55502c4b16535fe947791d02ac32)  for the initial spawner implementation.- The  [DESY JupyterHub Slurm service](https://confluence.desy.de/display/IS/JupyterHub+on+Maxwell)  for the table of available resources.- The  [TUDresden JupyterHub Slurm service](https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/JupyterHub)  for the spawn page design.</longdescription>
</pkgmetadata>