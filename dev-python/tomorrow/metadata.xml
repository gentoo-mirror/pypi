<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Tomorrow========Magic decorator syntax for asynchronous code in PythonInstallation------------Tomorrow is conveniently available via pip:::    pip install tomorrowor installable via ``git clone`` and ``setup.py``::    git clone git@github.com:madisonmay/Tomorrow.git    sudo python setup.py installUsage-----The tomorrow library enables you to utilize the benefits ofmulti-threading with minimal concern about the implementation details.Behind the scenes, the library is a thin wrapper around the ``Future``object in ``concurrent.futures`` that resolves the ``Future`` wheneveryou try to access any of its attributes.Enough of the implementation details, let's take a look at how simple itis to speed up an inefficient chunk of blocking code with minimaleffort.Naive Web Scraper-----------------You've collected a list of urls and are looking to download the HTML ofthe lot. The following is a perfectly reasonable first stab at solvingthe task.For the following examples, we'll be using the top sites from the Alexarankings... code:: python    urls = [        'http://google.com',        'http://facebook.com',        'http://youtube.com',        'http://baidu.com',        'http://yahoo.com',    ]Right then, let's get on to the code... code:: python    import time    import requests    def download(url):        return requests.get(url)    if __name__ == &quot;__main__&quot;:        start = time.time()        responses = [download(url) for url in urls]        html = [response.text for response in responses]        end = time.time()        print &quot;Time: %f seconds&quot; % (end - start)More Efficient Web Scraper--------------------------Using tomorrow's decorator syntax, we can define a function thatexecutes in multiple threads. Individual calls to ``download`` arenon-blocking, but we can largely ignore this fact and write codeidentically to how we would in a synchronous paradigm... code:: python    import time    import requests    from tomorrow import threads    @threads(5)    def download(url):        return requests.get(url)    if __name__ == &quot;__main__&quot;:        import time        start = time.time()        responses = [download(url) for url in urls]        html = [response.text for response in responses]        end = time.time()        print &quot;Time: %f seconds&quot; % (end - start)Awesome! With a single line of additional code (and no explicitthreading logic) we can now download websites ~10x as efficiently.You can also optionally pass in a timeout argument, to prevent hangingon a task that is not guaranteed to return... code:: python    import time    from tomorrow import threads    @threads(1, timeout=0.1)    def raises_timeout_error():        time.sleep(1)    if __name__ == &quot;__main__&quot;:        print raises_timeout_error()How Does it Work?-----------------Feel free to read the source for a peek behind the scenes -- it's lessthat 50 lines of code... |Codeship Status for madisonmay/Tomorrow| image:: https://codeship.com/projects/9a3b4c60-1b5b-0133-5ec7-7e346f2e432c/status?branch=master   :target: https://codeship.com/projects/94472</longdescription>
</pkgmetadata>