<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># iRODS Automated Ingest FrameworkThe automated ingest framework gives iRODS an enterprise solution that solves two major use cases: getting existing data under management and ingesting incoming data hitting a landing zone.Based on the Python iRODS Client and Celery, this framework can scale up to match the demands of data coming off instruments, satellites, or parallel filesystems.The example diagrams below show a filesystem scanner and a landing zone.![Automated Ingest: Filesystem Scanner Diagram](capability_automated_ingest_filesystem_scanner.jpg)![Automated Ingest: Landing Zone Diagram](capability_automated_ingest_landing_zone.jpg)## Supported/Tested Python versions- 3.7- 3.8- 3.9- 3.10## Usage options### Redis options| option | effect | default || ----   |  ----- |  ----- || redis_host | Domain or IP address of Redis host | localhost || redis_port | Port number for Redis | 6379 || redis_db | Redis DB number to use for ingest | 0 |### S3 optionsTo scan S3 bucket, minimally requires `--s3_keypair` and source path of the form `/bucket_name/path/to/root/folder`.| option | effect | default || ----   |  ----- |  ----- || s3_keypair | path to S3 keypair file | None || s3_endpoint_domain | S3 endpoint domain | s3.amazonaws.com || s3_region_name | S3 region name | us-east-1 || s3_proxy_url | URL to proxy for S3 access | None || s3_insecure_connection | Do not use SSL when connecting to S3 endpoint | False |### Logging/Profiling options| option | effect | default || ----   |  ----- |  ----- || log_filename | Path to output file for logs | None || log_level | Minimum level of message to log | None || log_interval | Time interval with which to rollover ingest log file | None || log_when | Type/units of log_interval (see TimedRotatingFileHandler) | None |`--profile` allows you to use vis to visualize a profile of Celery workers over time of ingest job.| option | effect | default || ----   |  ----- |  ----- || profile_filename | Specify name of profile filename (JSON output) | None || profile_level | Minimum level of message to log for profiling | None || profile_interval | Time interval with which to rollover ingest profile file | None || profile_when | Type/units of profile_interval (see TimedRotatingFileHandler) | None |### Ingest start optionsThese options are used at the &quot;start&quot; of an ingest job.| option | effect | default || ----   |  ----- |  ----- || job_name | Reference name for ingest job | a generated uuid || interval | Restart interval (in seconds). If absent, will only sync once. | None || file_queue | Name for the file queue. | file || path_queue | Name for the path queue. | path || restart_queue | Name for the restart queue. | restart || event_handler | Path to event handler file | None (see &quot;event_handler methods&quot; below) || synchronous | Block until sync job is completed | False || progress | Show progress bar and task counts (must have --synchronous flag) | False || ignore_cache | Ignore last sync time in cache - like starting a new sync | False |### Optimization options| option | effect | default || ----   |  ----- |  ----- || exclude_file_type | types of files to exclude: regular, directory, character, block, socket, pipe, link | None || exclude_file_name | a list of space-separated python regular expressions defining the file names to exclude such as &quot;(\S+)exclude&quot; &quot;(\S+)\.hidden&quot; | None || exclude_directory_name | a list of space-separated python regular expressions defining the directory names to exclude such as &quot;(\S+)exclude&quot; &quot;(\S+)\.hidden&quot; | None || files_per_task | Number of paths to process in a given task on the queue | 50 || initial_ingest | Use this flag on initial ingest to avoid check for data object paths already in iRODS | False || irods_idle_disconnect_seconds | Seconds to hold open iRODS connection while idle | 60 |## available `--event_handler` methods| method |  effect  | default || ----   |   ----- |  ----- || pre_data_obj_create   |   user-defined python  |  none || post_data_obj_create   | user-defined python  |  none || pre_data_obj_modify     |   user-defined python   |  none || post_data_obj_modify     | user-defined python  |  none || pre_coll_create    |   user-defined python |  none || post_coll_create    |  user-defined python   |  none || pre_coll_modify    |   user-defined python |  none || post_coll_modify    |  user-defined python   |  none || character_map | user-defined python | none || as_user |   takes action as this iRODS user |  authenticated user || target_path | set mount path on the irods server which can be different from client mount path | client mount path || to_resource | defines  target resource request of operation |  as provided by client environment || operation | defines the mode of operation |  `Operation.REGISTER_SYNC` || max_retries | defines max number of retries on failure | 0 || timeout | defines seconds until job times out | 3600 || delay | defines seconds between retries | 0 |Event handlers can use `logger` to write logs. See `structlog` for available logging methods and signatures.### Character Mapping optionIf an application should require that iRODS logical paths produced by the ingest process exclude subsets of therange of possible Unicode characters, we can add a character\_map method that returns a dict object. For example:```    class event_handler(Core):        @staticmethod        def character_map():            return {                re.compile('[^a-zA-Z0-9]'):'_'            }        # ...```The returned dictionary, in this case, indicates that the ingest process should replace all non-alphanumeric (aswell as non-ASCII) characters with an underscore wherever they may occur in an otherwise normally generated logical path.The substition process also applies to the intermediate (ie collection name) elements in a logical path, and a suffix isappended to affected path elements to avoid potential collisions with other remapped object names.Each key of the returned dictionary indicates a character or set of characters needing substitution.Possible key types include:   1. character   ```       # substitute backslashes with underscores       '\\': '_'   ```   2. tuple of characters   ```       # any character of the tuple is replaced by a Unicode small script x       ('\\','#','-'): '\u2093'   ```   3. regular expression   ```       # any character outside of range 0-256 becomes an underscore       re.compile('[\u0100-\U0010ffff]'): '_'   ```   4. callable accepting a character argument and returning a boolean   ```       # ASCII codes above 'z' become ':'       (lambda c: ord(c) in range(ord('z')+1,128)): ':'   ```In the event that the order-of-substitution is significant, the method may instead return a list of key-value tuples.### UnicodeEncodeErrorAny file whose path in the filesystem whose ingest results in a UnicodeEncodeError exception being raised (e.g. by theinclusion of an unencodable UTF8 sequence) will be automatically renamed using a base-64 sequence to represent the original,unmodified vault path.Additionally, data objects that have had their names remapped, whether pro forma or via a UnicodeEncodeError, will beannotated with an AVU of the form   Attribute:&quot;irods::automated_ingest::&quot; + ANNOTATION_REASON   Value:A PREFIX plus the base64-converted &quot;bad filepath&quot;   Units:&quot;python3.base64.b64encode(full_path_of_source_file)&quot;Where :   - ANNOTATION_REASON is either &quot;UnicodeEncodeError&quot; or &quot;character\_map&quot; depending on why the remapping occurred.   - PREFIX is either &quot;irods_UnicodeEncodeError\_&quot; or blank(&quot;&quot;), again depending on the re-mapping cause.Note that the UnicodeEncodeError type of remapping is unconditional, whereas the character remapping is contingent onan event handler's character_map method being defined.  Also, if a UnicodeEncodeError-style ingest is performed on agiven object, this precludes character mapping being done for the object.### operation mode ###| operation |  new files  | updated files  || ----   |   ----- | ----- || `Operation.REGISTER_SYNC` (default)   |  registers in catalog | updates size in catalog || `Operation.REGISTER_AS_REPLICA_SYNC`  |   registers first or additional replica | updates size in catalog || `Operation.PUT`  |   copies file to target vault, and registers in catalog | no action || `Operation.PUT_SYNC`  |   copies file to target vault, and registers in catalog | copies entire file again, and updates catalog || `Operation.PUT_APPEND`  |   copies file to target vault, and registers in catalog | copies only appended part of file, and updates catalog || `Operation.NO_OP` | no action | no action`--event_handler` usage examples can be found [in the examples directory](irods_capability_automated_ingest/examples).## Deployment### Basic: manual redis, Celery, pipRunning the sync job and Celery workers requires a valid iRODS environment file for an authenticated iRODS user on each node.#### Starting Redis ServerInstall redis-server:```sudo yum install redis-server``````sudo apt-get install redis-server```Or, build it yourself: https://redis.io/topics/quickstartStart redis:```redis-server```Or, dameonized:```sudo service redis-server start``````sudo systemctl start redis```The [Redis documentation](https://redis.io/topics/admin) also recommends an additional step:&gt; Make sure to set the Linux kernel overcommit memory setting to 1. Add vm.overcommit_memory = 1 to /etc/sysctl.conf and then reboot or run the command sysctl vm.overcommit_memory=1 for this to take effect immediately.This allows the Linux kernel to overcommit virtual memory even if this exceeds the physical memory on the host machine. See [kernel.org documentation](https://www.kernel.org/doc/Documentation/vm/overcommit-accounting) for more information.**Note:** If running in a distributed environment, make sure Redis server accepts connections by editing the `bind` line in /etc/redis/redis.conf or /etc/redis.conf.#### Setting up virtual environmentYou may need to upgrade pip:```pip install --upgrade pip```Install virtualenv:```pip install virtualenv```Create a virtualenv with python3:```virtualenv -p python3 rodssync```Activate virtual environment:```source rodssync/bin/activate```#### Install this package```pip install irods_capability_automated_ingest```Set up environment for Celery:```export CELERY_BROKER_URL=redis://&lt;redis host&gt;:&lt;redis port&gt;/&lt;redis db&gt; # e.g. redis://127.0.0.1:6379/0export PYTHONPATH=`pwd````Start celery worker(s):```celery -A irods_capability_automated_ingest.sync_task worker -l error -Q restart,path,file -c &lt;num workers&gt; ```**Note:** Make sure queue names match those of the ingest job (default queue names shown here).#### Run tests**Note:** The test suite requires Python version &gt;=3.7.**Note:** The tests should be run without running Celery workers or with an unused redis database.```python -m irods_capability_automated_ingest.test.test_irods_sync```See [docker/test/README.md](docker/test/README.md) for how to run in a dockerized environment.#### Start sync job```python -m irods_capability_automated_ingest.irods_sync start &lt;source dir&gt; &lt;destination collection&gt;```#### List jobs```python -m irods_capability_automated_ingest.irods_sync list```#### Stop jobs```python -m irods_capability_automated_ingest.irods_sync stop &lt;job name&gt;```#### Watch jobs (same as using `--progress`)```python -m irods_capability_automated_ingest.irods_sync watch &lt;job name&gt;```### Intermediate: dockerize, manually config (needs to be updated for Celery)See [docker/README.md](docker/README.md)### Advanced: kubernetes (needs to be updated for Celery)This does not assume that your iRODS installation is in kubernetes.#### `kubeadm`setup `Glusterfs` and `Heketi`create storage classcreate a persistent volume claim `data`#### install minikube and helmset memory to at least 8g and cpu to at least 4```minikube start --memory 8192 --cpus 4```#### enable ingress on minikube```minikube addons enable ingress```#### mount host dirsThis is where you data and event handler. In this setup, we assume that your event handler is under `/tmp/host/event_handler` and you data is under `/tmp/host/data`. We will mount `/tmp/host/data` into `/host/data` in minikube which will mount `/host/data` into `/data` in containers,`/tmp/host/data` -&gt; minikube `/host/data` -&gt; container `/data`.and similarly,`/tmp/host/event_handler` -&gt; minikube `/host/event_handler` -&gt; container `/event_handler`. Your setup may differ.```mkdir /tmp/host/event_handlermkdir /tmp/host/data````/tmp/host/event_handler/event_handler.py````pythonfrom irods_capability_automated_ingest.core import Corefrom irods_capability_automated_ingest.utils import Operationclass event_handler(Core):    @staticmethod    def target_path(session, meta, **options):        return path``````minikube mount /tmp/host:/host --gid 998 --uid 998 --9p-version=9p2000.L```#### enable incubator```helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/```#### build local docker images (optional)If you want to use local docker images, you can build the docker images into minikube as follows:`fish````eval (minikube docker-env)````bash````eval $(minikube docker-env)``````cd &lt;repo&gt;/docker/irods-postgresqldocker build . -t irods-provider-postgresql:4.2.2``````cd &lt;repo&gt;/docker/irods-cockroachdbdocker build . -t irods-provider-cockroachdb:4.3.0``````cd &lt;repo&gt;/dockerdocker build . -t irods_capability_automated_ingest:0.1.0``````cd &lt;repo&gt;/docker/rqdocker build . -t irods_rq:0.1.0``````cd &lt;repo&gt;/docker/rq-schedulerdocker build . -t irods_rq-scheduler:0.1.0```#### install irods##### `postgresql````cd &lt;repo&gt;/kubernetes/irods-provider-postgresqlhelm dependency update``````cd &lt;repo&gt;/kuberneteshelm install ./irods-provider-postgresql --name irods```##### `cockroachdb````cd &lt;repo&gt;/kubernetes/irods-provider-cockroachdbhelm dependency update``````cd &lt;repo&gt;/kuberneteshelm install ./irods-provider-cockroachdb --name irods```when reinstalling, run```kubectl delete --all pvkubectl delete --all pvc ```#### update irods configurationsSet configurations in `&lt;repo&gt;/kubernetes/chart/values.yaml` or `--set` command line argument.#### install chart```cd &lt;repo&gt;/kubernetes/chart```We call our release `icai`.```cd &lt;repo&gt;/kuberneteshelm install ./chart --name icai```#### scale rq workers```kubectl scale deployment.apps/icai-rq-deployment --replicas=&lt;n&gt;```#### access by REST API (recommended)##### submit job`submit.yaml````yamlroot: /datatarget: /tempZone/home/rods/datainterval: &lt;interval&gt;append_json: &lt;yaml&gt;timeout: &lt;timeout&gt;all: &lt;all&gt;event_handler: &lt;event_handler&gt;event_handler_data: |    from irods_capability_automated_ingest.core import Core    from irods_capability_automated_ingest.utils import Operation    class event_handler(Core):        @staticmethod        def target_path(session, meta, **options):            return path````fish````curl -XPUT &quot;http://&quot;(minikube ip)&quot;/job/&lt;job name&gt; -H &quot;Content-Type: application/x-yaml&quot; --data-binary=`submit.yaml`````bash````curl -XPUT &quot;http://$(minikube ip)/job/&lt;job name&gt;&quot; -H &quot;Content-Type: application/x-yaml&quot; --data-binary &quot;@submit.yaml&quot;````fish````curl -XPUT &quot;http://&quot;(minikube ip)&quot;/job&quot; -H &quot;Content-Type: application/x-yaml&quot; --data-binary &quot;@submit.yaml&quot;````bash````curl -XPUT &quot;http://$(minikube ip)/job&quot; -H &quot;Content-Type: application/x-yaml&quot; --data-binary &quot;@submit.yaml&quot;```##### list job`fish````curl -XGET &quot;http://&quot;(minikube ip)&quot;/job&quot;````bash````curl -XGET &quot;http://$(minikube ip)/job&quot;```##### delete job`fish````curl -XDELETE &quot;http://&quot;(minikube ip)&quot;/job/&lt;job name&gt;&quot;````bash````curl -XDELETE &quot;http://$(minikube ip)/job/&lt;job name&gt;&quot;```#### access by command line (not recommended)##### submit job```kubectl run --rm -i icai --image=irods_capability_automated_ingest:0.1.0 --restart=Never -- start /data /tempZone/home/rods/data -i &lt;interval&gt; --event_handler=event_handler --job_name=&lt;job name&gt; --redis_host icai-redis-master```##### list job```kubectl run --rm -i icai --image=irods_capability_automated_ingest:0.1.0 --restart=Never -- list --redis_host icai-redis-master```##### delete job```kubectl run --rm -i icai --image=irods_capability_automated_ingest:0.1.0 --restart=Never -- stop &lt;job name&gt; --redis_host icai-redis-master```#### install logging toolInstall chart with set `log_level` to `INFO`.```helm del --purge icai``````cd &lt;repo&gt;/kuberneteshelm install ./chart --set log_level=INFO --name icai```set parameters for elasticsearch```minikube ssh 'echo &quot;sysctl -w vm.max_map_count=262144&quot; | sudo tee -a /var/lib/boot2docker/bootlocal.sh'minikube stopminikube start``````cd &lt;repo&gt;/kuberneteshelm install ./elk --name icai-elk```##### Grafanalook for service port```kubectl get svc icai-elk-grafana```forward port```kubectl port-forward svc/icai-elk-grafana 8000:80```If `--set grafana.adminPassword=&quot;&quot;` system generates a random password, lookup admin password```kubectl get secret --namespace default icai-elk-grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode ; echo```open browser url `localhost:8000`login with username `admin` and password `admin`click on `icai dashboard`##### KibanaUncomment kibana sections in the yaml files under the `&lt;repo&gt;/kubernetes/elk` directorylook for service port```kubectl get svc icai-elk-kibana```forward port```kubectl port-forward svc/icai-elk-kibana 8000:443```open browser url `localhost:8000`</longdescription>
</pkgmetadata>