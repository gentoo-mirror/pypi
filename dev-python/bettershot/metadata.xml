<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># bettershot üí°üöÄ‚ö°Ô∏è A Python package for adding error monitoring to LLM Apps in a few minutes ‚ö°&gt;BetterShot by BerriAI let‚Äôs you add error (hallucinations/refusal to answer) monitoring to your LLM App in a few minutes. Like Bugsnag/Sentry for LLM apps!## Getting Started Install bettershot by running this command.:`pip install bettershot`## Using bettershotIt's just 1 line of code: `log(messages=messages, completion=result, user_email=&quot;YOUR_EMAIL&quot;, query=query)`![7f11c616-853a-4016-802c-ef705dea51c7](https://github.com/ClerkieAI/bettershot/assets/17561003/c16c7149-bf57-4fc1-8f50-76e16850a0a2)### Dashboard All your logs are available @ 'https://better-test.vercel.app/' + YOUR_EMAILe.g. `log(messages=messages, completion=result, user_email=&quot;krrish@berri.ai&quot;, query=query)`will have it's results logged @ `https://better-test.vercel.app/krrish@berri.ai`### ImplementationHere are all the items you can log: | Parameter | Type | Required/Optional | Description || --------- | ---- | ----------------- | ----------- || `messages` | List | Required | The list of messages sent to the OpenAI chat completions endpoint || `completion` | Dictionary | Required | The response received from the OpenAI chat completions endpoint || `user_email` | String | Required | Your user email || `query` | String | Required | The query being asked by your user || `customer_id` | String | Optional | A way to identify your customer |Here's 2 examples of using it: ### Calling the 'raw' OpenAI API```from bettershot import logimport openai def simple_openai_call(query):    messages = [{'role': 'user', 'content': query}]    completion = openai.ChatCompletion.create(                model=&quot;gpt-3.5-turbo&quot;,                messages=messages                )    log(messages=messages, completion=completion, user_email=&quot;YOUR_EMAIL&quot;, query=query, customer_id=&quot;fake_user@fake_accounts.xyz&quot;) #JUST 1 LINE OF CODE ü§Øsimple_openai_call(&quot;hey! how's it going?&quot;)```### Calling the Langchain OpenAI API ```import openaiimport langchain from bettershot import logdef simple_langchain_call(query):    chat = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;, temperature=0.7)    prompt = &quot;You are an extremely intelligent AI assistant for BerriAI, answer all questions resepectfully and in a warm tone&quot;    messages = [      SystemMessage(content=prompt),      HumanMessage(content=query)    ]    result = chat(messages)    log(messages=messages, completion=completion, user_email=&quot;YOUR_EMAIL&quot;, query=query, customer_id=&quot;fake_user@fake_accounts.xyz&quot;) #JUST 1 LINE OF CODE üéâ simple_langchain_call(&quot;hey! how's it going?&quot;)```bettershot automatically evaluates your OpenAI responses to determine if the model either invented new information (hallucination) or refused to answer (&quot;Sorry, as an AI language model...&quot;) a user's question. ## How does eval work?Reliable + Fast testing is hard, and that's what we want to tackle.Each question is evaluated 3 times. Each evaluation returns either True or False, along with the model's rationale for why it chose what it did. We pick the evaluation (True/False) that occurs most, along with the model rationale to explain reasoning. Each question is run in parallel and results are added to your dashboard in real-time. &gt;We will be sharing the prompts soon!## ContributingWe welcome contributions to bettershot! Feel free to create issues/PR's/or DM us (üëã Hi I'm Krrish - +17708783106)## Licensebettershot is released under the [MIT License](https://github.com/bettershot/readme/blob/master/LICENSE).</longdescription>
</pkgmetadata>