<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># sparkql ✨[![PyPI version](https://badge.fury.io/py/sparkql.svg)](https://badge.fury.io/py/sparkql)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)[![CI](https://circleci.com/gh/mattjw/sparkql.svg?style=svg)](https://circleci.com/gh/mattjw/sparkql)[![codecov](https://codecov.io/gh/mattjw/sparkql/branch/master/graph/badge.svg)](https://codecov.io/gh/mattjw/sparkql)Python Spark SQL DataFrame schema management for sensible humans, with no dependencies aside from pyspark.&gt; _Don't sweat it... sparkql it ✨_## Why use sparkql`sparkql` takes the pain out of working with DataFrame schemas in PySpark.It makes schema definition more Pythonic. And it'sparticularly useful you're dealing with structured data.In plain old PySpark, you might find that you write schemas[like this](https://github.com/mattjw/sparkql/tree/master/examples/conferences_comparison/plain_schema.py):```pythonCITY_SCHEMA = StructType()CITY_NAME_FIELD = &quot;name&quot;CITY_SCHEMA.add(StructField(CITY_NAME_FIELD, StringType(), False))CITY_LAT_FIELD = &quot;latitude&quot;CITY_SCHEMA.add(StructField(CITY_LAT_FIELD, FloatType()))CITY_LONG_FIELD = &quot;longitude&quot;CITY_SCHEMA.add(StructField(CITY_LONG_FIELD, FloatType()))CONFERENCE_SCHEMA = StructType()CONF_NAME_FIELD = &quot;name&quot;CONFERENCE_SCHEMA.add(StructField(CONF_NAME_FIELD, StringType(), False))CONF_CITY_FIELD = &quot;city&quot;CONFERENCE_SCHEMA.add(StructField(CONF_CITY_FIELD, CITY_SCHEMA))```And then plain old PySpark makes you deal with nested fields like this:```pythondframe.withColumn(&quot;city_name&quot;, df[CONF_CITY_FIELD][CITY_NAME_FIELD])```Instead, with `sparkql`, schemas become a lot[more literate](https://github.com/mattjw/sparkql/tree/master/examples/conferences_comparison/sparkql_schema.py):```pythonclass City(Struct):    name = String(nullable=False)    latitude = Float()    longitude = Float()class Conference(Struct):    name = String(nullable=False)    city = City()```As does dealing with nested fields:```pythondframe.withColumn(&quot;city_name&quot;, Conference.city.name.COL)```Here's a summary of `sparkql`'s features.- ORM-like class-based Spark schema definitions.- Automated field naming: The attribute name of a field as it appears  in its `Struct` is (by default) used as its field name. This name can  be optionally overridden.- Programatically reference nested fields in your structs with the  `PATH` and `COL` special properties. Avoid hand-constructing strings  (or `Column`s) to reference your nested fields.- Validate that a DataFrame matches a `sparkql` schema.- Reuse and build composite schemas with `inheritance`, `includes`, and  `implements`.- Get a human-readable Spark schema representation with `pretty_schema`.- Create an instance of a schema as a dictionary, with validation of  the input values.Read on for documentation on these features.## Defining a schemaEach Spark atomic type has a counterpart `sparkql` field:| PySpark type | `sparkql` field ||---|---|| `ByteType` | `Byte` || `IntegerType` | `Integer` || `LongType` | `Long` || `ShortType` | `Short` || `DecimalType` | `Decimal` || `DoubleType` | `Double` || `FloatType` | `Float` || `StringType` | `String` || `BinaryType` | `Binary` || `BooleanType` | `Boolean` || `DateType` | `Date` || `TimestampType` | `Timestamp` |`Array` (counterpart to `ArrayType` in PySpark) allows the definitionof arrays of objects. By creating a subclass of `Struct`, we candefine a custom class that will be converted to a `StructType`.For[example](https://github.com/mattjw/sparkql/tree/master/examples/arrays/arrays.py),given the `sparkql` schema definition:```pythonfrom sparkql import Struct, String, Arrayclass Article(Struct):    title = String(nullable=False)    tags = Array(String(), nullable=False)    comments = Array(String(nullable=False))```Then we can build the equivalent PySpark schema (a `StructType`)with:```pythonfrom sparkql import schemapyspark_struct = schema(Article)```Pretty printing the schema with the expression`sparkql.pretty_schema(pyspark_struct)` will give the following:```textStructType([    StructField('title', StringType(), False),    StructField('tags',        ArrayType(StringType(), True),        False),    StructField('comments',        ArrayType(StringType(), False),        True)])```## FeaturesMany examples of how to use `sparkql` can be found in[`examples`](https://github.com/mattjw/sparkql/tree/master/examples).### Automated field namingBy default, field names are inferred from the attribute name in thestruct they are declared.For example, given the struct```pythonclass Geolocation(Struct):    latitude = Float()    longitude = Float()```the concrete name of the `Geolocation.latitude` field is `latitude`.Names also be overridden by explicitly specifying the field name as anargument to the field```pythonclass Geolocation(Struct):    latitude = Float(name=&quot;lat&quot;)    longitude = Float(name=&quot;lon&quot;)```which would mean the concrete name of the `Geolocation.latitude` fieldis `lat`.### Field paths and nested objectsReferencing fields in nested data can be a chore. `sparkql` simplifies thiswith path referencing.[For example](https://github.com/mattjw/sparkql/tree/master/examples/nested_objects/sparkql_example.py), if we have aschema with nested objects:```pythonclass Address(Struct):    post_code = String()    city = String()class User(Struct):    username = String(nullable=False)    address = Address()class Comment(Struct):    message = String()    author = User(nullable=False)class Article(Struct):    title = String(nullable=False)    author = User(nullable=False)    comments = Array(Comment())```We can use the special `PATH` property to turn a path into aSpark-understandable string:```pythonauthor_city_str = Article.author.address.city.PATH&quot;author.address.city&quot;````COL` is a counterpart to `PATH` that returns a Spark `Column`object for the path, allowing it to be used in all places where Sparkrequires a column.Function equivalents `path_str`, `path_col`, and `name` are also available.This table demonstrates the equivalence of the property styles and the functionstyles:| Property style | Function style | Result (both styles are equivalent) || --- | --- | --- || `Article.author.address.city.PATH` | `sparkql.path_str(Article.author.address.city)` | `&quot;author.address.city&quot;` || `Article.author.address.city.COL` | `sparkql.path_col(Article.author.address.city)` | `Column` pointing to `author.address.city` || `Article.author.address.city.NAME` | `sparkql.name(Article.author.address.city)` | `&quot;city&quot;` |For paths that include an array, two approaches are provided:```pythoncomment_usernames_str = Article.comments.e.author.username.PATH&quot;comments.author.username&quot;comment_usernames_str = Article.comments.author.username.PATH&quot;comments.author.username&quot;```Both give the same result. However, the former (`e`) is moretype-oriented. The `e` attribute corresponds to the array's elementfield. Although this looks strange at first, it has the advantage ofbeing inspectable by IDEs and other tools, allowing goodness such asIDE auto-completion, automated refactoring, and identifying errorsbefore runtime.### Field metadataField [metadata](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StructField.html) can be specified with the `metadata` argument to a field, which accepts a dictionaryof key-value pairs.```pythonclass Article(Struct):    title = String(nullable=False,                   metadata={&quot;description&quot;: &quot;The title of the article&quot;, &quot;max_length&quot;: 100})```The metadata can be accessed with the `METADATA` property of the field:```pythonArticle.title.METADATA{&quot;description&quot;: &quot;The title of the article&quot;, &quot;max_length&quot;: 100}```### DataFrame validationStruct method `validate_data_frame` will verify if a given DataFrame'sschema matches the Struct.[For example](https://github.com/mattjw/sparkql/tree/master/examples/validation/test_validation.py),if we have our `Article`struct and a DataFrame we want to ensure adheres to the `Article`schema:```pythondframe = spark_session.createDataFrame([{&quot;title&quot;: &quot;abc&quot;}])class Article(Struct):    title = String()    body = String()```Then we can can validate with:```pythonvalidation_result = Article.validate_data_frame(dframe)````validation_result.is_valid` indicates whether the DataFrame is valid(`False` in this case), and `validation_result.report` is ahuman-readable string describing the differences:```textStruct schema...StructType([    StructField('title', StringType(), True),    StructField('body', StringType(), True)])DataFrame schema...StructType([    StructField('title', StringType(), True)])Diff of struct -&gt; data frame...  StructType([-     StructField('title', StringType(), True)])+     StructField('title', StringType(), True),+     StructField('body', StringType(), True)])```For convenience,```pythonArticle.validate_data_frame(dframe).raise_on_invalid()```will raise a `InvalidDataFrameError` (see `sparkql.exceptions`) if the  DataFrame is not valid.### Creating an instance of a schema`sparkql` simplifies the process of creating an instance of a struct.You might need to do this, for example, when creating test data, orwhen creating an object (a dict or a row) to return from a UDF.Use `Struct.make_dict(...)` to instantiate a struct as a dictionary.This has the advantage that the input values will be correctlyvalidated, and it will convert schema property names into theirunderlying field names.For[example](https://github.com/mattjw/sparkql/tree/master/examples/struct_instantiation/instantiate_as_dict.py),given some simple Structs:```pythonclass User(Struct):    id = Integer(name=&quot;user_id&quot;, nullable=False)    username = String()class Article(Struct):    id = Integer(name=&quot;article_id&quot;, nullable=False)    title = String()    author = User()    text = String(name=&quot;body&quot;)```Here are a few examples of creating dicts from `Article`:```pythonArticle.make_dict(    id=1001,    title=&quot;The article title&quot;,    author=User.make_dict(        id=440,        username=&quot;user&quot;    ),    text=&quot;Lorem ipsum article text lorem ipsum.&quot;)# generates...{    &quot;article_id&quot;: 1001,    &quot;author&quot;: {        &quot;user_id&quot;: 440,        &quot;username&quot;: &quot;user&quot;},    &quot;body&quot;: &quot;Lorem ipsum article text lorem ipsum.&quot;,    &quot;title&quot;: &quot;The article title&quot;}``````pythonArticle.make_dict(    id=1002)# generates...{    &quot;article_id&quot;: 1002,    &quot;author&quot;: None,    &quot;body&quot;: None,    &quot;title&quot;: None}```See[this example](https://github.com/mattjw/sparkql/tree/master/examples/conferences_extended/conferences.py)for an extended example of using `make_dict`.### Composite schemasIt is sometimes useful to be able to re-use the fields of one structin another struct. `sparkql` provides a few features to enable this:- _inheritance_: A subclass inherits the fields of a base struct class.- _includes_: Incorporate fields from another struct.- _implements_: Enforce that a struct must implement the fields of  another struct.See the following examples for a better explanation.#### Using inheritanceFor [example](https://github.com/mattjw/sparkql/tree/master/examples/composite_schemas/inheritance.py), the following:```pythonclass BaseEvent(Struct):    correlation_id = String(nullable=False)    event_time = Timestamp(nullable=False)class RegistrationEvent(BaseEvent):    user_id = String(nullable=False)```will produce the following `RegistrationEvent` schema:```textStructType([    StructField('correlation_id', StringType(), False),    StructField('event_time', TimestampType(), False),    StructField('user_id', StringType(), False)])```#### Using an `includes` declarationFor [example](https://github.com/mattjw/sparkql/tree/master/examples/composite_schemas/includes.py), the following:```pythonclass EventMetadata(Struct):    correlation_id = String(nullable=False)    event_time = Timestamp(nullable=False)class RegistrationEvent(Struct):    class Meta:        includes = [EventMetadata]    user_id = String(nullable=False)```will produce the `RegistrationEvent` schema:```textStructType(List(    StructField('user_id', StringType(), False),    StructField('correlation_id', StringType(), False),    StructField('event_time', TimestampType(), False)))```#### Using an `implements` declaration`implements` is similar to `includes`, but does not automaticallyincorporate the fields of specified structs. Instead, it is up tothe implementor to ensure that the required fields are declared inthe struct.Failing to implement a field from an `implements` struct will result ina `StructImplementationError` error.[For example](https://github.com/mattjw/sparkql/tree/master/examples/composite_schemas/implements.py):```class LogEntryMetadata(Struct):    logged_at = Timestamp(nullable=False)class PageViewLogEntry(Struct):    class Meta:        implements = [LogEntryMetadata]    page_id = String(nullable=False)# the above class declaration will fail with the following StructImplementationError error:#   Struct 'PageViewLogEntry' does not implement field 'logged_at' required by struct 'LogEntryMetadata'```### Prettified Spark schema stringsSpark's stringified schema representation isn't very user-friendly, particularly for large schemas:```textStructType([StructField('name', StringType(), False), StructField('city', StructType([StructField('name', StringType(), False), StructField('latitude', FloatType(), True), StructField('longitude', FloatType(), True)]), True)])```The function `pretty_schema` will return something more useful:```textStructType([    StructField('name', StringType(), False),    StructField('city',        StructType([            StructField('name', StringType(), False),            StructField('latitude', FloatType(), True),            StructField('longitude', FloatType(), True)]),        True)])```### Merge two Spark `StructType` typesIt can be useful to build a composite schema from two `StructType`s. sparkql provides a`merge_schemas` function to do this.[For example](https://github.com/mattjw/sparkql/tree/master/examples/merge_struct_types/merge_struct_types.py):```pythonschema_a = StructType([    StructField(&quot;message&quot;, StringType()),    StructField(&quot;author&quot;, ArrayType(        StructType([            StructField(&quot;name&quot;, StringType())        ])    ))])schema_b = StructType([    StructField(&quot;author&quot;, ArrayType(        StructType([            StructField(&quot;address&quot;, StringType())        ])    ))])merged_schema = merge_schemas(schema_a, schema_b) ```results in a `merged_schema` that looks like:```textStructType([    StructField('message', StringType(), True),    StructField('author',        ArrayType(StructType([            StructField('name', StringType(), True),            StructField('address', StringType(), True)]), True),        True)])```## ContributingContributions are very welcome. Developers who'd like to contribute tothis project should refer to [CONTRIBUTING.md](./CONTRIBUTING.md).</longdescription>
</pkgmetadata>