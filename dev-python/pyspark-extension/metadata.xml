<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Spark ExtensionThis project provides extensions to the [Apache Spark project](https://spark.apache.org/) in Scala and Python:**Diff:** A `diff` transformation and application for `Dataset`s that computes the differences betweentwo datasets, i.e. which rows to _add_, _delete_ or _change_ to get from one dataset to the other.**Global Row Number:** A `withRowNumbers` transformation that provides the global row number w.r.t.the current order of the Dataset, or any given order. In contrast to the existing SQL function `row_number`, whichrequires a window spec, this transformation provides the row number across the entire Dataset without scaling problems.**Inspect Parquet files:** The structure of Parquet files (the metadata, not the data stored in Parquet) can be inspected similar to [parquet-tools](https://pypi.org/project/parquet-tools/)or [parquet-cli](https://pypi.org/project/parquet-cli/) by reading from a simple Spark data source.This simplifies identifying why some Parquet files cannot be split by Spark into scalable partitions.**.Net DateTime.Ticks:** Convert .Net (C#, F#, Visual Basic) `DateTime.Ticks` into Spark timestamps, seconds and nanoseconds.&lt;details&gt;&lt;summary&gt;Available methods:&lt;/summary&gt;```scala// ScaladotNetTicksToTimestamp(Column): Column       // returns timestamp as TimestampTypedotNetTicksToUnixEpoch(Column): Column       // returns Unix epoch seconds as DecimalTypedotNetTicksToUnixEpochNanos(Column): Column  // returns Unix epoch nanoseconds as LongType```The reverse is provided by (all return `LongType` .Net ticks):```scala// ScalatimestampToDotNetTicks(Column): ColumnunixEpochToDotNetTicks(Column): ColumnunixEpochNanosToDotNetTicks(Column): Column```These methods are also available in Python:```python# Pythondotnet_ticks_to_timestamp(column_or_name)         # returns timestamp as TimestampTypedotnet_ticks_to_unix_epoch(column_or_name)        # returns Unix epoch seconds as DecimalTypedotnet_ticks_to_unix_epoch_nanos(column_or_name)  # returns Unix epoch nanoseconds as LongTypetimestamp_to_dotnet_ticks(column_or_name)unix_epoch_to_dotnet_ticks(column_or_name)unix_epoch_nanos_to_dotnet_ticks(column_or_name)```&lt;/details&gt;**Spark job description:** Set Spark job description for all Spark jobs within a context:```pythonfrom gresearch.spark import job_description, append_job_descriptionwith job_description(&quot;parquet file&quot;):    df = spark.read.parquet(&quot;data.parquet&quot;)    with append_job_description(&quot;count&quot;):        count = df.count    with append_job_description(&quot;write&quot;):        df.write.csv(&quot;data.csv&quot;)```For details, see the [README.md](https://github.com/G-Research/spark-extension#spark-extension) at the project homepage.## Using Spark Extension#### PyPi package (local Spark cluster only)You may want to install the `pyspark-extension` python package from PyPi into your development environment.This provides you code completion, typing and test capabilities during your development phase.Running your Python application on a Spark cluster will still require one of the ways belowto add the Scala package to the Spark environment.```shell scriptpip install pyspark-extension==2.8.0.3.3```Note: Pick the right Spark version (here 3.3) depending on your PySpark version.#### PySpark APIStart a PySpark session with the Spark Extension dependency (version ≥1.1.0) as follows:```pythonfrom pyspark.sql import SparkSessionspark = SparkSession \    .builder \    .config(&quot;spark.jars.packages&quot;, &quot;uk.co.gresearch.spark:spark-extension_2.12:2.8.0-3.3&quot;) \    .getOrCreate()```Note: Pick the right Scala version (here 2.12) and Spark version (here 3.3) depending on your PySpark version.#### PySpark REPLLaunch the Python Spark REPL with the Spark Extension dependency (version ≥1.1.0) as follows:```shell scriptpyspark --packages uk.co.gresearch.spark:spark-extension_2.12:2.8.0-3.3```Note: Pick the right Scala version (here 2.12) and Spark version (here 3.3) depending on your PySpark version.#### PySpark `spark-submit`Run your Python scripts that use PySpark via `spark-submit`:```shell scriptspark-submit --packages uk.co.gresearch.spark:spark-extension_2.12:2.8.0-3.3 [script.py]```Note: Pick the right Scala version (here 2.12) and Spark version (here 3.3) depending on your Spark version.### Your favorite Data Science notebookThere are plenty of [Data Science notebooks](https://datasciencenotebook.org/) around. To use this library,add **a jar dependency** to your notebook using these **Maven coordinates**:    uk.co.gresearch.spark:spark-extension_2.12:2.8.0-3.3Or [download the jar](https://mvnrepository.com/artifact/uk.co.gresearch.spark/spark-extension) and place iton a filesystem where it is accessible by the notebook, and reference that jar file directly.Check the documentation of your favorite notebook to learn how to add jars to your Spark environment.</longdescription>
</pkgmetadata>