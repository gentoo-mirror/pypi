<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;# LangPort&lt;a href=&quot;https://github.com/vtuber-plan/langport&quot;&gt;  &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/vtuber-plan/langport?style=social&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/vtuber-plan/langport/blob/main/LICENSE&quot;&gt;  &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/vtuber-plan/langport&quot;&gt;&lt;/a&gt;![architecture](assets/architecture.jpg)&lt;/div&gt;LangPort is a open-source large language model serving platform.Our goal is to build a super fast LLM inference service.This project is inspired by [lmsys/fastchat](https://github.com/lm-sys/FastChat), we hope that the serving platform is lightweight and fast, but fastchat includes other features such as training and evaluation make it complicated.The core features include:- Huggingface transformers support.- ggml (llama.cpp) support.- A distributed serving system for state-of-the-art models.- Streaming generation support with various decoding strategies.- Batch inference for higher throughput.- Support for encoder-only, decoder-only and encoder-decoder models.- OpenAI-compatible RESTful APIs.- FauxPilot-compatible RESTful APIs.- HuggingFace-compatible RESTful APIs.- Tabby-compatible RESTful APIs.## Support Model Architectures* LLaMa* GLM* Bloom* OPT* GPT2* GPT Neo* GPT Big Code## Tested Models* LLaMa* Vicuna* ChatGLM* ChatGLM2* Falcon* Starcoder* WizardLM* OpenBuddy## BenchmarkWe use single RTX3090 to run a finetuned 7B LLaMA model (OpenBuddy V0.9) under the bf16 setting.We create 32 threads to submit chat tasks to the server, and the following figure shows the Queries Per Second (QPS) and Tokens Per Second (TPS) of FastChat and LangPort with different max model concurrency settings.![benchmark_chat](assets/benchmark_chat.jpg)## News- [2023/06/18] Add ggml (llama.cpp gpt.cpp starcoder.cpp etc.) worker support.- [2023/06/09] Add LLama.cpp worker support.- [2023/06/01] Add HuggingFace Bert embedding worker support.- [2023/06/01] Add HuggingFace text generation API support.- [2023/06/01] Add tabby API support.- [2023/05/23] Add chat throughput test script.- [2023/05/22] New distributed architecture.- [2023/05/14] Batch inference supported.- [2023/05/10] Langport project started.## Install### Method 1: With pip```bashpip install langport```or:```bashpip install git+https://github.com/vtuber-plan/langport.git ```If you need ggml generation worker, use this command:```bashpip install langport[ggml]```If you wanna use GPU:```bashCT_CUBLAS=1 pip install langport[ggml]```### Method 2: From source1. Clone this repository```bashgit clone https://github.com/vtuber-plan/langport.gitcd langport```2. Install the Package```bashpip install --upgrade pippip install -e .```## Start the serverIt is simple to start a single node chat API service:``` bashpython -m langport.service.server.generation_worker --port 21001 --model-path &lt;your model path&gt;python -m langport.service.gateway.openai_api```If you need the embeddings API or other features, you can deploy a distributed inference cluster:``` bashpython -m langport.service.server.dummy_worker --port 21001python -m langport.service.server.generation_worker --model-path &lt;your model path&gt; --neighbors http://localhost:21001python -m langport.service.server.embedding_worker --model-path &lt;your model path&gt; --neighbors http://localhost:21001python -m langport.service.gateway.openai_api --controller-address http://localhost:21001```In practice, the gateway can connect to any node to distribute inference tasks:``` bashpython -m langport.service.server.dummy_worker --port 21001python -m langport.service.server.generation_worker --port 21002 --model-path &lt;your model path&gt; --neighbors http://localhost:21001python -m langport.service.server.generation_worker --port 21003 --model-path &lt;your model path&gt; --neighbors http://localhost:21001 http://localhost:21002python -m langport.service.server.generation_worker --port 21004 --model-path &lt;your model path&gt; --neighbors http://localhost:21001 http://localhost:21003python -m langport.service.server.generation_worker --port 21005 --model-path &lt;your model path&gt; --neighbors http://localhost:21001 http://localhost:21004python -m langport.service.gateway.openai_api --controller-address http://localhost:21003 # 21003 is OK!python -m langport.service.gateway.openai_api --controller-address http://localhost:21002 # Any worker is also OK!```Run text generation with multi GPUs:``` bashpython -m langport.service.server.generation_worker --port 21001 --model-path &lt;your model path&gt; --gpus 0,1 --num-gpus 2python -m langport.service.gateway.openai_api```Run text generation with ggml worker:```bashpython -m langport.service.server.ggml_generation_worker --port 21001 --model-path &lt;your model path&gt; --gpu-layers &lt;num layer to gpu (resize this for your VRAM)&gt;```## Licenselangport is released under the Apache Software License.## See also- [langport-docs](https://github.com/vtuber-plan/langport/tree/main/docs)- [langport-source](https://github.com/vtuber-plan/langport)## Star History[![Star History Chart](https://api.star-history.com/svg?repos=vtuber-plan/langport&amp;type=Date)](https://star-history.com/#vtuber-plan/langport&amp;Date)</longdescription>
</pkgmetadata>