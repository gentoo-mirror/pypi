<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Datera Python SDK## IntroductionThis is Python SDK version v1.2 for the **Datera** Fabric Services API.Download and use of this package implicitly accepts the terms in COPYINGUsers of this package are assumed to have familiarity with the **Datera** API.Details around the API itself are not necessarily covered through this SDK.## Features* Automatic session management and login* Automatic and configuable request retries* Object to REST request translation* Standard Logging Format (compatible with Datera SREQ log parsing)* Endpoint validation (toggleable)* Dot-notation access to response attributes* [UDC](#universal-datera-config) compliance## Installation### From Source```bash    apt-get install python-virtualenv (or yum install python-virtualenv for CentOS)    virtualenv sdk    source sdk/bin/activate    git clone https://github.com/Datera/python-sdk.git    cd python-sdk    pip install -r requirements.txt    python setup.py install```### From PYPI```bash    pip install -U dfs_sdk```## Universal Datera ConfigThe Universal Datera Config (UDC) is a config that can be specified in anumber of ways:* JSON file with any of the following names:    - .datera-config    - datera-config    - .datera-config.json    - datera-config.json* The JSON file has the following configuration:```json     {&quot;mgmt_ip&quot;: &quot;1.1.1.1&quot;,      &quot;username&quot;: &quot;admin&quot;,      &quot;password&quot;: &quot;password&quot;,      &quot;tenant&quot;: &quot;/root&quot;,      &quot;api_version&quot;: &quot;2.3&quot;,      &quot;ldap&quot;: &quot;&quot;}```* The file can be in any of the following places.  This is also the lookup  order for config files:    - current directory    - home directory    - home/config directory    - /etc/datera* If no datera config file is found and a cinder.conf file is present, the  config parser will try and pull connection credentials from the  cinder.conf* Tenant and API version and LDAP are always optional, but it's generally  suggested to include them in your UDC file for easy reference.* Instead of a JSON file, environment variables can be used.    - `DAT_MGMT`    - `DAT_USER`    - `DAT_PASS`    - `DAT_TENANT`    - `DAT_API`    - `DAT_LDAP`* Most tools built to use the Universal Datera Config will also allow  for providing/overriding any of the config values via command line flags.    - --hostname    - --username    - --password    - --tenant    - --api-version    - --ldap## Developing with Universal Datera ConfigTo use UDC in a new python tool is very simple just add the following toyour python script:```pythonfrom dfs_sdk import scaffoldparser = scaffold.get_argparser()parser.add_argument('my-new-arg')args = parser.parse_args()```If you want to use subparsers, or customize the help outptu of your parserthen use the following```pythonimport argparsefrom dfs_sdk import scaffoldtop_parser = scaffold.get_argparser(add_help=False)new_parser = argparse.ArgumentParser(parents=[top_parser])new_parser.add_argument('my-new-arg')args = new_parser.parse_args()```Inside a script the config can be recieved by calling```pythonfrom dfs_sdk import scaffoldscaffold.get_argparser()config = scaffold.get_config()```NOTE: It is no longer required to call ``scaffold.get_argparser()`` beforecalling ``scaffold.get_config()``.  This is only necessary if buildinga CLI tool that needs the cli parser.## LoggingTo set custom logging.json file```bash    export DSDK_LOG_CFG=your/log/location.json```Or the value can be set to a debug, info or error```bash    export DSDK_LOG_CFG=info```To set logging to stdout.  The value can be any logging level supported bythe python logging module (eg: debug, info, etc)```bash    export DSDK_LOG_STDOUT=debug```The debug logs generated by the python-sdk are quite large, and are on arotating file handler (provided that a custom logging.json file is not provided)## Managed ObjectsDatera provides an application-driven storage management model, whose goal is to closely align storagewith a corresponding application's requirements.The main storage objects are defined and differentiated as follows:### Application Instance (AppInstance)    -    Corresponds to an application, service, etc.    -    Contains Zero or more Storage Instances### Storage Instance    -    Corresponds to one set of storage requirements for a given AppInstance    -    ACL Policies, including IQN Initiators    -    Target IQN    -    Contains Zero or more Volumes### Volumes    -    Corresponds to a single allocated storage object    -    Size (default unit is GB)    -    Replication Factor    -    Performance Policies (QoS for Bandwidth and IOPS)    -    Protection Policies (Snapshot scheduling)Another way of viewing the managed object hierarchy is as follows:    app_instances:        - storage_instances:                 (1 or more per app_instance)            + acl_policy                     (1 or more host initiators )            + iqn                            (target IQN)            + ips                            (target IPs)            + volumes:                       (1 or more per storage_instance)                * name                * size                * replication                * performance_policy         (i.e. QoS)                * protection_policy          (i.e. Snapshot schedules)## EndpointsHTTP operations on URL endpoints is the only way to interact with the set of managed objects.URL's have the format:```bash      http://192.168.42.13:7717/v2.3/&lt;object_class&gt;/[&lt;instance&gt;]/...```where **7717** is the port used to access the API, and &quot;v2.3&quot; corresponds to an API version control.Briefly, the REST API supports 4 operations/methods **create (POST), modify (PUT), list (GET), delete (DELETE)**.Any input payload is in JSON format;  any return payload is in JSON format.Login session keys are required within the &quot;header&quot; of any HTTP request.Sessions keys have a 15 minute lifetime.For a full reference documentation of the REST API, please review the Datera REST API Guide.This Python SDK serves as a wrapper around the raw HTTP layer.## Using this SDKThe Datera module is named **dfs_sdk**, and the main entry point is called __DateraApi__.Obtaining an object handle can be done as follows:```python    from dfs_sdk import get_api    [...]    api = get_api(mgmt_ip, username, password, &quot;v2.3&quot; **kwargs)```You can also initialize the SDK using a Datera UDC file.  The following will read any validUDC file on the system or from the current environment variables.```python    from dfs_sdk.scaffold import get_api    [...]    api = get_api()```## Configurable OptionsThese options can be set on instantiation via the ``get_api`` constructorOption | Default | Description-------|-------- | -----------tenant | '/root' | Datera account tenant/subtenanttimeout | 300 (s) | Timeout for HTTP requestssecure | True | Whether to use HTTPS (False sets HTTP)strict | False | Whether to check if an endpoint is valid before sending requestcert | None | HTTPS verification certificatecert\_key | None | HTTPS verification certificate keythread\_local | {} | Used for passing values down to the connection layer, usually for logging## Common Objects, Examples and  Use CasesPlease see the **utils** directory for programming examples that cover the following:Common methods for all objects include **create(), set(), delete(), list()**+ To create an app\_instance with name **FOO**:```python        ai = api.app_instances.create(name=&quot;FOO&quot;)```+ Looping through objects can be done via **list()**:```python        for ai in api.app_instances.list():            print &quot;AppInstance: &quot;, ai```+ To set a given **app_instance** into an _offline_ state:```python        ai.set(admin_state=&quot;offline&quot;)```+ To delete a given app\_instance:```python        ai.delete()```## Building the PyPI packageRun the following to build the packages (if uploading, ensure the versionis incremented in constants.py)```bash        python setup.py sdist bdist_wheel```Then to upload the package to PyPI (this step requires valid PyPI credentials)```bash        twine upload dist/*```You can perform a test upload by running.  This requires credentials on thetest PyPI server```bash        twine upload --repository-url https://test.pypi.org/legacy/ dist/*```## Reporting ProblemsFor problems and feedback, please open an github issue. This project is community supported.</longdescription>
</pkgmetadata>