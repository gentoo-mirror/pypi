<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># :sauropod: Grounding DINO[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-mscoco)](https://paperswithcode.com/sota/zero-shot-object-detection-on-mscoco?p=grounding-dino-marrying-dino-with-grounded) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-odinw)](https://paperswithcode.com/sota/zero-shot-object-detection-on-odinw?p=grounding-dino-marrying-dino-with-grounded) \[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco-minival)](https://paperswithcode.com/sota/object-detection-on-coco-minival?p=grounding-dino-marrying-dino-with-grounded) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco)](https://paperswithcode.com/sota/object-detection-on-coco?p=grounding-dino-marrying-dino-with-grounded)[![image](https://img.shields.io/pypi/v/rf_groundingdino.svg)](https://pypi.python.org/pypi/rf_groundingdino)Official PyTorch implementation of [&quot;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&quot;](https://arxiv.org/abs/2303.05499): the SoTA open-set object detector.## :sun_with_face: Helpful Tutorial- :grapes: [[Read our arXiv Paper](https://arxiv.org/abs/2303.05499)]- :apple: [[Watch our simple introduction video on YouTube](https://youtu.be/wxWDt5UiwY8)]- :rose: &amp;nbsp;[[Try the Colab Demo](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb)]- :sunflower: [[Try our Official Huggingface Demo](https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo)]- :maple_leaf: [[Watch the Step by Step Tutorial about GroundingDINO by Roboflow AI](https://youtu.be/cMa77r3YrDk)]- :mushroom: [[GroundingDINO: Automated Dataset Annotation and Evaluation by Roboflow AI](https://youtu.be/C4NqaRBz_Kw)]- :hibiscus: [[Accelerate Image Annotation with SAM and GroundingDINO by Roboflow AI](https://youtu.be/oEQYStnF2l8)]&lt;!-- Grounding DINO Methods |[![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499)[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wxWDt5UiwY8) --&gt;&lt;!-- Grounding DINO Demos |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) --&gt;&lt;!-- [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/cMa77r3YrDk)[![HuggingFace space](https://img.shields.io/badge/ðŸ¤—-HuggingFace%20Space-cyan.svg)](https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo)[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/oEQYStnF2l8)[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/C4NqaRBz_Kw) --&gt;## :sparkles: Highlight Projects- [DetGPT: Detect What You Need via Reasoning](https://github.com/OptimalScale/DetGPT)- [Grounded-SAM: Marrying Grounding DINO with Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)- [Grounding DINO with Stable Diffusion](demo/image_editing_with_groundingdino_stablediffusion.ipynb)- [Grounding DINO with GLIGEN for Controllable Image Editing](demo/image_editing_with_groundingdino_gligen.ipynb)- [OpenSeeD: A Simple and Strong Openset Segmentation Model](https://github.com/IDEA-Research/OpenSeeD)- [SEEM: Segment Everything Everywhere All at Once](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)- [X-GPT: Conversational Visual Agent supported by X-Decoder](https://github.com/microsoft/X-Decoder/tree/xgpt)- [GLIGEN: Open-Set Grounded Text-to-Image Generation](https://github.com/gligen/GLIGEN)- [LLaVA: Large Language and Vision Assistant](https://github.com/haotian-liu/LLaVA)&lt;!-- Extensions | [Grounding DINO with Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything); [Grounding DINO with Stable Diffusion](demo/image_editing_with_groundingdino_stablediffusion.ipynb); [Grounding DINO with GLIGEN](demo/image_editing_with_groundingdino_gligen.ipynb)  --&gt;&lt;!-- Official PyTorch implementation of [Grounding DINO](https://arxiv.org/abs/2303.05499), a stronger open-set object detector. Code is available now! --&gt;## :bulb: Highlight- **Open-Set Detection.** Detect **everything** with language!- **High Performancce.** COCO zero-shot **52.5 AP** (training without COCO data!). COCO fine-tune **63.0 AP**.- **Flexible.** Collaboration with Stable Diffusion for Image Editting.## :fire: News- **`2023/04/15`**: Refer to [CV in the Wild Readings](https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings) for those who are interested in open-set recognition!- **`2023/04/08`**: We release [demos](demo/image_editing_with_groundingdino_gligen.ipynb) to combine [Grounding DINO](https://arxiv.org/abs/2303.05499) with [GLIGEN](https://github.com/gligen/GLIGEN) for more controllable image editings.- **`2023/04/08`**: We release [demos](demo/image_editing_with_groundingdino_stablediffusion.ipynb) to combine [Grounding DINO](https://arxiv.org/abs/2303.05499) with [Stable Diffusion](https://github.com/Stability-AI/StableDiffusion) for image editings.- **`2023/04/06`**: We build a new demo by marrying GroundingDINO with [Segment-Anything](https://github.com/facebookresearch/segment-anything) named **[Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)** aims to support segmentation in GroundingDINO.- **`2023/03/28`**: A YouTube [video](https://youtu.be/cMa77r3YrDk) about Grounding DINO and basic object detection prompt engineering. [[SkalskiP](https://github.com/SkalskiP)]- **`2023/03/28`**: Add a [demo](https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo) on Hugging Face Space!- **`2023/03/27`**: Support CPU-only mode. Now the model can run on machines without GPUs.- **`2023/03/25`**: A [demo](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) for Grounding DINO is available at Colab. [[SkalskiP](https://github.com/SkalskiP)]- **`2023/03/22`**: Code is available Now!&lt;details open&gt;&lt;summary&gt;&lt;font size=&quot;4&quot;&gt;Description&lt;/font&gt;&lt;/summary&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;Paper&lt;/a&gt; introduction.&lt;img src=&quot;.asset/hero_figure.png&quot; alt=&quot;ODinW&quot; width=&quot;100%&quot;&gt;Marrying &lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&quot;https://github.com/gligen/GLIGEN&quot;&gt;GLIGEN&lt;/a&gt;&lt;img src=&quot;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/GD_GLIGEN.png&quot; alt=&quot;gd_gligen&quot; width=&quot;100%&quot;&gt;&lt;/details&gt;## :star: Explanations/Tips for Grounding DINO Inputs and Outputs- Grounding DINO accepts an `(image, text)` pair as inputs.- It outputs `900` (by default) object boxes. Each box has similarity scores across all input words. (as shown in Figures below.)- We defaultly choose the boxes whose highest similarities are higher than a `box_threshold`.- We extract the words whose similarities are higher than the `text_threshold` as predicted labels.- If you want to obtain objects of specific phrases, like the `dogs` in the sentence `two dogs with a stick.`, you can select the boxes with highest text similarities with `dogs` as final outputs.- Note that each word can be split to **more than one** tokens with different tokenlizers. The number of words in a sentence may not equal to the number of text tokens.- We suggest separating different category names with `.` for Grounding DINO.  ![model_explain1](.asset/model_explan1.PNG)  ![model_explain2](.asset/model_explan2.PNG)## :label: TODO- [x] Release inference code and demo.- [x] Release checkpoints.- [x] Grounding DINO with Stable Diffusion and GLIGEN demos.- [ ] Release training codes.## :hammer_and_wrench: Install**Note:**If you have a CUDA environment, please make sure the environment variable `CUDA_HOME` is set. It will be compiled under CPU-only mode if no CUDA available.**Installation:**Clone the GroundingDINO repository from GitHub.```bashgit clone https://github.com/IDEA-Research/GroundingDINO.git```Change the current directory to the GroundingDINO folder.```bashcd GroundingDINO/```Install the required dependencies in the current directory.```bashpip3 install -q -e .```Create a new directory called &quot;weights&quot; to store the model weights.```bashmkdir weights```Change the current directory to the &quot;weights&quot; folder.```bashcd weights```Download the model weights file.```bashwget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth```## :arrow_forward: DemoCheck your GPU ID (only if you're using a GPU)```bashnvidia-smi```Replace `{GPU ID}`, `image_you_want_to_detect.jpg`, and `&quot;dir you want to save the output&quot;` with appropriate values in the following command```bashCUDA_VISIBLE_DEVICES={GPU ID} python demo/inference_on_a_image.py \-c /GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py \-p /GroundingDINO/weights/groundingdino_swint_ogc.pth \-i image_you_want_to_detect.jpg \-o &quot;dir you want to save the output&quot; \-t &quot;chair&quot; [--cpu-only] # open it for cpu mode```See the `demo/inference_on_a_image.py` for more details.**Running with Python:**```pythonfrom groundingdino.util.inference import load_model, load_image, predict, annotateimport cv2model = load_model(&quot;groundingdino/config/GroundingDINO_SwinT_OGC.py&quot;, &quot;weights/groundingdino_swint_ogc.pth&quot;)IMAGE_PATH = &quot;weights/dog-3.jpeg&quot;TEXT_PROMPT = &quot;chair . person . dog .&quot;BOX_TRESHOLD = 0.35TEXT_TRESHOLD = 0.25image_source, image = load_image(IMAGE_PATH)boxes, logits, phrases = predict(    model=model,    image=image,    caption=TEXT_PROMPT,    box_threshold=BOX_TRESHOLD,    text_threshold=TEXT_TRESHOLD)annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)cv2.imwrite(&quot;annotated_image.jpg&quot;, annotated_frame)```**Web UI**We also provide a demo code to integrate Grounding DINO with Gradio Web UI. See the file `demo/gradio_app.py` for more details.**Notebooks**- We release [demos](demo/image_editing_with_groundingdino_gligen.ipynb) to combine [Grounding DINO](https://arxiv.org/abs/2303.05499) with [GLIGEN](https://github.com/gligen/GLIGEN) for more controllable image editings.- We release [demos](demo/image_editing_with_groundingdino_stablediffusion.ipynb) to combine [Grounding DINO](https://arxiv.org/abs/2303.05499) with [Stable Diffusion](https://github.com/Stability-AI/StableDiffusion) for image editings.## :luggage: Checkpoints&lt;!-- insert a table --&gt;&lt;table&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;name&lt;/th&gt;      &lt;th&gt;backbone&lt;/th&gt;      &lt;th&gt;Data&lt;/th&gt;      &lt;th&gt;box AP on COCO&lt;/th&gt;      &lt;th&gt;Checkpoint&lt;/th&gt;      &lt;th&gt;Config&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;GroundingDINO-T&lt;/td&gt;      &lt;td&gt;Swin-T&lt;/td&gt;      &lt;td&gt;O365,GoldG,Cap4M&lt;/td&gt;      &lt;td&gt;48.4 (zero-shot) / 57.2 (fine-tune)&lt;/td&gt;      &lt;td&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&quot;&gt;GitHub link&lt;/a&gt; | &lt;a href=&quot;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swint_ogc.pth&quot;&gt;HF link&lt;/a&gt;&lt;/td&gt;      &lt;td&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO/blob/main/groundingdino/config/GroundingDINO_SwinT_OGC.py&quot;&gt;link&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2&lt;/th&gt;      &lt;td&gt;GroundingDINO-B&lt;/td&gt;      &lt;td&gt;Swin-B&lt;/td&gt;      &lt;td&gt;COCO,O365,GoldG,Cap4M,OpenImage,ODinW-35,RefCOCO&lt;/td&gt;      &lt;td&gt;56.7 &lt;/td&gt;      &lt;td&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth&quot;&gt;GitHub link&lt;/a&gt;  | &lt;a href=&quot;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swinb_cogcoor.pth&quot;&gt;HF link&lt;/a&gt;       &lt;td&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO/blob/main/groundingdino/config/GroundingDINO_SwinB.cfg.py&quot;&gt;link&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;## :medal_military: Results&lt;details open&gt;&lt;summary&gt;&lt;font size=&quot;4&quot;&gt;COCO Object Detection Results&lt;/font&gt;&lt;/summary&gt;&lt;img src=&quot;.asset/COCO.png&quot; alt=&quot;COCO&quot; width=&quot;100%&quot;&gt;&lt;/details&gt;&lt;details open&gt;&lt;summary&gt;&lt;font size=&quot;4&quot;&gt;ODinW Object Detection Results&lt;/font&gt;&lt;/summary&gt;&lt;img src=&quot;.asset/ODinW.png&quot; alt=&quot;ODinW&quot; width=&quot;100%&quot;&gt;&lt;/details&gt;&lt;details open&gt;&lt;summary&gt;&lt;font size=&quot;4&quot;&gt;Marrying Grounding DINO with &lt;a href=&quot;https://github.com/Stability-AI/StableDiffusion&quot;&gt;Stable Diffusion&lt;/a&gt; for Image Editing&lt;/font&gt;&lt;/summary&gt;See our example &lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO/blob/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&quot;&gt;notebook&lt;/a&gt; for more details.&lt;img src=&quot;.asset/GD_SD.png&quot; alt=&quot;GD_SD&quot; width=&quot;100%&quot;&gt;&lt;/details&gt;&lt;details open&gt;&lt;summary&gt;&lt;font size=&quot;4&quot;&gt;Marrying Grounding DINO with &lt;a href=&quot;https://github.com/gligen/GLIGEN&quot;&gt;GLIGEN&lt;/a&gt; for more Detailed Image Editing.&lt;/font&gt;&lt;/summary&gt;See our example &lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO/blob/main/demo/image_editing_with_groundingdino_gligen.ipynb&quot;&gt;notebook&lt;/a&gt; for more details.&lt;img src=&quot;.asset/GD_GLIGEN.png&quot; alt=&quot;GD_GLIGEN&quot; width=&quot;100%&quot;&gt;&lt;/details&gt;## :sauropod: Model: Grounding DINOIncludes: a text backbone, an image backbone, a feature enhancer, a language-guided query selection, and a cross-modality decoder.![arch](.asset/arch.png)## :hearts: AcknowledgementOur model is related to [DINO](https://github.com/IDEA-Research/DINO) and [GLIP](https://github.com/microsoft/GLIP). Thanks for their great work!We also thank great previous work including DETR, Deformable DETR, SMCA, Conditional DETR, Anchor DETR, Dynamic DETR, DAB-DETR, DN-DETR, etc. More related work are available at [Awesome Detection Transformer](https://github.com/IDEACVR/awesome-detection-transformer). A new toolbox [detrex](https://github.com/IDEA-Research/detrex) is available as well.Thanks [Stable Diffusion](https://github.com/Stability-AI/StableDiffusion) and [GLIGEN](https://github.com/gligen/GLIGEN) for their awesome models.## :black_nib: CitationIf you find our work helpful for your research, please consider citing the following BibTeX entry.```bibtex@article{liu2023grounding,  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},  journal={arXiv preprint arXiv:2303.05499},  year={2023}}```</longdescription>
</pkgmetadata>