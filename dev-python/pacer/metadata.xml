<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>pacer=====About-----``pacer`` is a lightweight Python package for implementing distributeddata processing workflows. Instead of defining a`DAG &lt;https://en.wikipedia.org/wiki/Directed_acyclic_graph&gt;`__ whichmodels the data flow from sources to a final result ``pacer`` uses apull model which is very similar to nesting function calls. Running sucha workflow starts on the result node and recursively delegates work tothe inputs.Originally we developed ``pacer`` for running analysis pipelines in`emzed &lt;http://emzed.ethz.ch&gt;`__, a framework for analyzing *LCMS* data.How does pacer work ?---------------------Under the hood ``pacer`` has two core components:-  one for managing distributed computations of chained computations:   Processing steps in ``pacer`` are just Python functions with some   additional annotations. ``pacer`` tries to compute as many processing   steps as possible in parallel, either because such a function has to   be applied to different data sets, or it has more than one input and   those are computed concurrently.-  a distributed cache which is retained on the file system   In case of partial modifications of the inputs a ``pacer`` workflow   does not determine needed update computations but uses a distributed   cache for mapping the input values of single processing steps to   their final result. So a repeated run of the workflow with unchanged   inputs will run the full workflow with all processing steps returning   already known results immediately. Running the workflow with unknown   or modified inputs will only execute the needed computations and   update the cache.These two components are independent and can be used seperately.Examples========We provide some simple examples which show how easy it is to use``pacer``. You find these examples which we extended to print morelogging information in the ``examples/`` folder in the git repository.In a real world *LCMS* workflow we would not use as simple functions asused below but longer running computation steps such as running a LCMSpeak picker and a subsequent peak aligner.How to declare a pipeline-------------------------In this case our input sources are a list of Python strings``[&quot;a&quot;, &quot;bc&quot;, &quot;def&quot;]`` and a tuple of numbers ``(1, 2)``. The verysimple example workflow computes the length of each string andmultiplies it with every number from the tuple. This very simple examplecould be implemented in pure Python as follows:::    import itertools    def main():        def length(what):            return len(what)        def multiply(a, b):            return a * b        words = [&quot;a&quot;, &quot;bc&quot;, &quot;def&quot;]        multipliers = (1, 2)        result = [multiply(length(w), v) for (w, v) in itertools.product(words, multipliers)]        assert result == [1, 2, 2, 4, 3, 6]    if __name__ == &quot;__main__&quot;:        main()In order to transform this computations to a smart parallel processingpipeline we use the ``apply`` and ``join`` function decorators from``pacer`` and declare the dependencies among the single steps usingfunction calls.::    from pacer import apply, output, join, Engine    def main():        @apply        def length(what):            return len(what)        @output        @join        def multiply(a, b):            return a * b        words = [&quot;a&quot;, &quot;bc&quot;, &quot;def&quot;]        multipliers = (1, 2)        # now we DECLARE the workflow (no execution at that time):        workflow = multiply(length(words), multipliers)    if __name__ == &quot;__main__&quot;:        main()Running this workflow on three CPU cores is easy now. In this case thecomputation steps are run in parallel:::    Engine.set_number_of_processes(3)    workflow.start_computations()    result = workflow.get_all_in_order()    assert result == [1, 2, 2, 4, 3, 6]pacers approach to compute needed updates in case of modified input data------------------------------------------------------------------------As already stated above, pacer does not determine needed updatecomputations in case of modified input data but uses a distributed cacheinstead. So running a workflow a second time will fetch the alreadyknown results of computations not affected by changes, and startcomputations with unknown input arguments.We use decorators again. Leveraging the example above only needs fewadjustments:::    from pacer import apply, join, output, Engine, CacheBuilder    cache = CacheBuiler(&quot;/tmp/cache_000&quot;)    @apply    @cache    def length(what):        return len(what)    @output    @join    @cache    def multiply(a, b):        return a * b    # inputs to workflow    words = [&quot;a&quot;, &quot;bc&quot;, &quot;def&quot;]    multipliers = (1, 2)    workflow = multiply(length(words), multipliers)    # run workflow    Engine.set_number_of_processes(3)    workflow.start_computations()    result = workflow.get_all_in_order()    assert result == [1, 2, 2, 4, 3, 6]If you run these examples from a command line you see logging resultsshowing the parallel execution of single steps and cache hits avoidingrecomputations.</longdescription>
</pkgmetadata>