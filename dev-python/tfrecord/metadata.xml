<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># TFRecord reader and writerThis library allows reading and writing tfrecord files efficiently in python. The library also provides an IterableDataset reader of tfrecord files for PyTorch. Currently uncompressed and compressed gzip TFRecords are supported.## Installation```pip3 install tfrecord```## UsageIt's recommended to create an index file for each TFRecord file. Index file must be provided when using multiple workers, otherwise the loader may return duplicate records.```python3 -m tfrecord.tools.tfrecord2idx &lt;tfrecord path&gt; &lt;index path&gt;```## Reading &amp; Writing tf.train.Example### Reading tf.Example records in PyTorchUse TFRecordDataset to read TFRecord files in PyTorch.```pythonimport torchfrom tfrecord.torch.dataset import TFRecordDatasettfrecord_path = &quot;/tmp/data.tfrecord&quot;index_path = Nonedescription = {&quot;image&quot;: &quot;byte&quot;, &quot;label&quot;: &quot;float&quot;}dataset = TFRecordDataset(tfrecord_path, index_path, description)loader = torch.utils.data.DataLoader(dataset, batch_size=32)data = next(iter(loader))print(data)```Use MultiTFRecordDataset to read multiple TFRecord files. This class samples from given tfrecord files with given probability.```pythonimport torchfrom tfrecord.torch.dataset import MultiTFRecordDatasettfrecord_pattern = &quot;/tmp/{}.tfrecord&quot;index_pattern = &quot;/tmp/{}.index&quot;splits = {    &quot;dataset1&quot;: 0.8,    &quot;dataset2&quot;: 0.2,}description = {&quot;image&quot;: &quot;byte&quot;, &quot;label&quot;: &quot;int&quot;}dataset = MultiTFRecordDataset(tfrecord_pattern, index_pattern, splits, description)loader = torch.utils.data.DataLoader(dataset, batch_size=32)data = next(iter(loader))print(data)```### Infinite and finite PyTorch datasetBy default, `MultiTFRecordDataset` is infinite, meaning that it samples the data forever. You can make it finite by providing the appropriate flag```dataset = MultiTFRecordDataset(..., infinite=False)```### Shuffling the dataBoth TFRecordDataset and MultiTFRecordDataset automatically shuffle the data when you provide a queue size.```dataset = TFRecordDataset(..., shuffle_queue_size=1024)```### Transforming input dataYou can optionally pass a function as `transform` argument to perform post processing of features before returning. This can for example be used to decode images or normalize colors to a certain range or pad variable length sequence. ```pythonimport tfrecordimport cv2def decode_image(features):    # get BGR image from bytes    features[&quot;image&quot;] = cv2.imdecode(features[&quot;image&quot;], -1)    return featuresdescription = {    &quot;image&quot;: &quot;bytes&quot;,}dataset = tfrecord.torch.TFRecordDataset(&quot;/tmp/data.tfrecord&quot;,                                         index_path=None,                                         description=description,                                         transform=decode_image)data = next(iter(dataset))print(data)```### Writing tf.Example records in Python```pythonimport tfrecordwriter = tfrecord.TFRecordWriter(&quot;/tmp/data.tfrecord&quot;)writer.write({    &quot;image&quot;: (image_bytes, &quot;byte&quot;),    &quot;label&quot;: (label, &quot;float&quot;),    &quot;index&quot;: (index, &quot;int&quot;)})writer.close()```### Reading tf.Example records in Python```pythonimport tfrecordloader = tfrecord.tfrecord_loader(&quot;/tmp/data.tfrecord&quot;, None, {    &quot;image&quot;: &quot;byte&quot;,    &quot;label&quot;: &quot;float&quot;,    &quot;index&quot;: &quot;int&quot;})for record in loader:    print(record[&quot;label&quot;])```## Reading &amp; Writing tf.train.SequenceExampleSequenceExamples can be read and written using the same methods shown above with an extra argument(`sequence_description` for reading and `sequence_datum` for writing) which cause the respectiveread/write functions to treat the data as a SequenceExample.### Writing SequenceExamples to file```pythonimport tfrecordwriter = tfrecord.TFRecordWriter(&quot;/tmp/data.tfrecord&quot;)writer.write({'length': (3, 'int'), 'label': (1, 'int')},             {'tokens': ([[0, 0, 1], [0, 1, 0], [1, 0, 0]], 'int'), 'seq_labels': ([0, 1, 1], 'int')})writer.write({'length': (3, 'int'), 'label': (1, 'int')},             {'tokens': ([[0, 0, 1], [1, 0, 0]], 'int'), 'seq_labels': ([0, 1], 'int')})writer.close()```### Reading SequenceExamples in pythonReading from a SequenceExample yeilds a tuple containing two elements.```pythonimport tfrecordcontext_description = {&quot;length&quot;: &quot;int&quot;, &quot;label&quot;: &quot;int&quot;}sequence_description = {&quot;tokens&quot;: &quot;int&quot;, &quot;seq_labels&quot;: &quot;int&quot;}loader = tfrecord.tfrecord_loader(&quot;/tmp/data.tfrecord&quot;, None,                                  context_description,                                  sequence_description=sequence_description)for context, sequence_feats in loader:    print(context[&quot;label&quot;])    print(sequence_feats[&quot;seq_labels&quot;])```### Read SequenceExamples in PyTorchAs described in the section on `Transforming Input`, one can pass a function as the `transform` argument toperform post processing of features. This should be used especially for the sequence features as these arevariable length sequence and need to be padded out before being batched.```pythonimport torchimport numpy as npfrom tfrecord.torch.dataset import TFRecordDatasetPAD_WIDTH = 5def pad_sequence_feats(data):    context, features = data    for k, v in features.items():        features[k] = np.pad(v, ((0, PAD_WIDTH - len(v)), (0, 0)), 'constant')    return (context, features)context_description = {&quot;length&quot;: &quot;int&quot;, &quot;label&quot;: &quot;int&quot;}sequence_description = {&quot;tokens&quot;: &quot;int &quot;, &quot;seq_labels&quot;: &quot;int&quot;}dataset = TFRecordDataset(&quot;/tmp/data.tfrecord&quot;,                          index_path=None,  description=context_description,  transform=pad_sequence_feats,  sequence_description=sequence_description)loader = torch.utils.data.DataLoader(dataset, batch_size=32)data = next(iter(loader))print(data)```Alternatively, you could choose to implement a custom `collate_fn` in order to assemble the batch,for example, to perform dynamic padding.```pythonimport torchimport numpy as npfrom tfrecord.torch.dataset import TFRecordDatasetdef collate_fn(batch):    from torch.utils.data._utils import collate    from torch.nn.utils import rnn    context, feats = zip(*batch)    feats_ = {k: [torch.Tensor(d[k]) for d in feats] for k in feats[0]}    return (collate.default_collate(context),            {k: rnn.pad_sequence(f, True) for (k, f) in feats_.items()})context_description = {&quot;length&quot;: &quot;int&quot;, &quot;label&quot;: &quot;int&quot;}sequence_description = {&quot;tokens&quot;: &quot;int &quot;, &quot;seq_labels&quot;: &quot;int&quot;}dataset = TFRecordDataset(&quot;/tmp/data.tfrecord&quot;,                          index_path=None,  description=context_description,  transform=pad_sequence_feats,  sequence_description=sequence_description)loader = torch.utils.data.DataLoader(dataset, batch_size=32, collate_fn=collate_fn)data = next(iter(loader))print(data)```</longdescription>
</pkgmetadata>