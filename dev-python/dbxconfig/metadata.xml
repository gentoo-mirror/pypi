<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># dbxconfigConfiguration framework for databricks pipelines.Define configuration and table dependencies in yaml config then get the table mappings config model:Define your tables.```yamllanding:  read:    landing_dbx_patterns:      customer_details_1: null      customer_details_2: nullraw:  delta_lake:    raw_dbx_patterns:      customers:        ids: id        depends_on:          - landing.landing_dbx_patterns.customer_details_1          - landing.landing_dbx_patterns.customer_details_2        warning_thresholds:          invalid_ratio: 0.1          invalid_rows: 0          max_rows: 100          min_rows: 5        exception_thresholds:          invalid_ratio: 0.2          invalid_rows: 2          max_rows: 1000          min_rows: 0        custom_properties:          process_group: 1base:  delta_lake:    # delta table properties can be set at stage level or table level    delta_properties:      delta.appendOnly: true      delta.autoOptimize.autoCompact: true          delta.autoOptimize.optimizeWrite: true        delta.enableChangeDataFeed: false    base_dbx_patterns:      customer_details_1:        ids: id        depends_on:          - raw.raw_dbx_patterns.customers        # delta table properties can be set at stage level or table level        # table level properties will overwride stage level properties        delta_properties:            delta.enableChangeDataFeed: true      customer_details_2:        ids: id        depends_on:          - raw.raw_dbx_patterns.customers```Define you load configuration:```yamltables: ./tables.yamllanding:  read:    trigger: customerdetailscomplete-{{filename_date_format}}*.flg    trigger_type: file    database: landing_dbx_patterns    table: &quot;{{table}}&quot;    container: datalake    root: &quot;/mnt/{{container}}/data/landing/dbx_patterns/{{table}}/{{path_date_format}}&quot;    filename: &quot;{{table}}-{{filename_date_format}}*.csv&quot;    filename_date_format: &quot;%Y%m%d&quot;    path_date_format: &quot;%Y%m%d&quot;    format: cloudFiles    spark_schema: ../Schema/{{table.lower()}}.yaml    options:      # autoloader      cloudFiles.format: csv      cloudFiles.schemaLocation:  /mnt/{{container}}/checkpoint/{{checkpoint}}      cloudFiles.useIncrementalListing: auto      # schema      inferSchema: false      enforceSchema: true      columnNameOfCorruptRecord: _corrupt_record      # csv      header: false      mode: PERMISSIVE      encoding: windows-1252      delimiter: &quot;,&quot;      escape: '&quot;'      nullValue: &quot;&quot;      quote: '&quot;'      emptyValue: &quot;&quot;    raw:  delta_lake:    # delta table properties can be set at stage level or table level    delta_properties:      delta.appendOnly: true      delta.autoOptimize.autoCompact: true          delta.autoOptimize.optimizeWrite: true        delta.enableChangeDataFeed: false    database: raw_dbx_patterns    table: &quot;{{table}}&quot;    container: datalake    root: /mnt/{{container}}/data/raw    path: &quot;{{database}}/{{table}}&quot;    options:      checkpointLocation: /mnt/{{container}}/checkpoint/{{database}}_{{table}}      mergeSchema: true```Import the config objects into you pipeline:```pythonfrom dbxconfig import Config, Timeslice, StageType# build path to configuration filepattern = &quot;auto_load_schema&quot;config_path = f&quot;../Config&quot;# create a timeslice object for slice loading. Use * for all time (supports hrs, mins, seconds and sub-second).timeslice = Timeslice(day=&quot;*&quot;, month=&quot;*&quot;, year=&quot;*&quot;)# parse and create a config objectsconfig = Config(config_path=config_path, pattern=pattern)# get the configuration for a table mapping to load.table_mapping = config.get_table_mapping(    timeslice=timeslice,     stage=StageType.raw,     table=&quot;customers&quot;)print(table_mapping)```## Development Setup```pip install -r requirements.txt```## Unit TestsTo run the unit tests with a coverage report.```pip install -e .pytest test/unit --junitxml=junit/test-results.xml --cov=dbxconfig --cov-report=xml --cov-report=html```## Build```python setup.py sdist bdist_wheel```## Publish```twine upload dist/*```</longdescription>
</pkgmetadata>