<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Team and repository tags========================.. image:: https://governance.openstack.org/tc/badges/monasca-notification.svg    :target: https://governance.openstack.org/tc/reference/tags/index.html.. Change things from this point onNotification Engine===================This engine reads alarms from Kafka and then notifies the customer usingthe configured notification method. Multiple notification and retryengines can run in parallel, up to one per available Kafka partition.Zookeeper is used to negotiate access to the Kafka partitions whenever anew process joins or leaves the working set.Architecture============The notification engine generates notifications using the followingsteps:1. Read Alarms from Kafka, with no auto commit. -   monasca\_common.kafka.KafkaConsumer class2. Determine notification type for an alarm. Done by reading from mysql. - AlarmProcessor class3. Send notification. - NotificationProcessor class4. Add successful notifications to a sent notification topic. - NotificationEngine class5. Add failed notifications to a retry topic. - NotificationEngine class6. Commit offset to Kafka - KafkaConsumer classThe notification engine uses three Kafka topics:1. alarm\_topic: Alarms inbound to the notification engine.2. notification\_topic: Successfully sent notifications.3. notification\_retry\_topic: Failed notifications.A retry engine runs in parallel with the notification engine and givesany failed notification a configurable number of extra chances atsuccess.The retry engine generates notifications using the following steps:1. Read notification json data from Kafka, with no auto commit. - KafkaConsumer class2. Rebuild the notification that failed. - RetryEngine class3. Send notification. - NotificationProcessor class4. Add successful notifications to a sent notification topic. - RetryEngine class5. Add failed notifications that have not hit the retry limit back to the retry topic. -   RetryEngine class6. Discard failed notifications that have hit the retry limit. - RetryEngine class7. Commit offset to Kafka. - KafkaConsumer classThe retry engine uses two Kafka topics:1. notification\_retry\_topic: Notifications that need to be retried.2. notification\_topic: Successfully sent notifications.Fault Tolerance---------------When reading from the alarm topic, no committing is done. The committingis done only after processing. This allows the processing to continueeven though some notifications can be slow. In the event of acatastrophic failure some notifications could be sent but the alarmshave not yet been acknowledged. This is an acceptable failure mode,better to send a notification twice than not at all.The general process when a major error is encountered is to exit thedaemon which should allow the other processes to renegotiate access tothe Kafka partitions. It is also assumed that the notification enginewill be run by a process supervisor which will restart it in case of afailure. In this way, any errors which are not easy to recover from areautomatically handled by the service restarting and the active daemonswitching to another instance.Though this should cover all errors, there is the risk that an alarm ora set of alarms can be processed and notifications are sent out multipletimes. To minimize this risk a number of techniques are used:-  Timeouts are implemented for all notification types.-  An alarm TTL is utilized. Any alarm older than the TTL is not   processed.Operation=========``oslo.config`` is used for handling configuration options. A sampleconfiguration file ``etc/monasca/notification.conf.sample`` can begenerated by running:::    tox -e genconfigTo run the service using the default config file locationof `/etc/monasca/notification.conf`:::    monasca-notificationTo run the service and explicitly specify the config file:::    monasca-notification --config-file /etc/monasca/monasca-notification.confMonitoring----------StatsD is incorporated into the daemon and will send all stats to theStatsD server launched by monasca-agent. Default host and port points to**localhost:8125**.-  Counters   -  ConsumedFromKafka   -  AlarmsFailedParse   -  AlarmsNoNotification   -  NotificationsCreated   -  NotificationsSentSMTP   -  NotificationsSentWebhook   -  NotificationsSentPagerduty   -  NotificationsSentFailed   -  NotificationsInvalidType   -  AlarmsFinished   -  PublishedToKafka-  Timers   -  ConfigDBTime   -  SendNotificationTimePlugins-------The following notification plugins are available:- Email- HipChat- Jira- PagerDuty- Slack- WebhookThe plugins can be configured via the Monasca Notification config file. Ingeneral you will need to follow these steps to enable a plugin:- Make sure that the plugin is enabled in the config file- Make sure that the plugin is configured in the config file- Restart the Monasca Notification servicePagerDuty plugin----------------The PagerDuty plugin supports the PagerDuty v1 Events API. The first stepis to `configure`_ a service in PagerDuty which uses this API. Onceconfigured, the service will be assigned an integration key. This key should beused as the `ADDRESS` field when creating the notification type, for example:::    monasca notification-create pd_notification pagerduty a30d5560c5ce4239a6f52a01a15850caThe default settings for the plugin, including the v1 Events API URL shouldbe sufficient to get started, but it is worth checking that the PagerDutyEvents v1 API URL matches that provided in the example Monasca Notificationconfig file.Slack plugin~~~~~~~~~~~~To use the Slack plugin you must first configure an incoming `webhook`_for the Slack channel you wish to post notifications to. The notification canthen be created as follows:::    monasca notification-create slack_notification slack https://hooks.slack.com/services/MY/SECRET/WEBHOOK/URLNote that whilst it is also possible to use a token instead of a webhook,this approach is now `deprecated`_.By default the Slack notification will dump all available information intothe alert. For example, a notification may be posted to Slack which lookslike this:::    {      &quot;metrics&quot;:[         {            &quot;dimensions&quot;:{               &quot;hostname&quot;:&quot;operator&quot;            },            &quot;id&quot;:null,            &quot;name&quot;:&quot;cpu.user_perc&quot;         }      ],      &quot;alarm_id&quot;:&quot;20a54a65-44b8-4ac9-a398-1f2d888827d2&quot;,      &quot;state&quot;:&quot;ALARM&quot;,      &quot;alarm_timestamp&quot;:1556703552,      &quot;tenant_id&quot;:&quot;62f7a7a314904aa3ab137d569d6b4fde&quot;,      &quot;old_state&quot;:&quot;OK&quot;,      &quot;alarm_description&quot;:&quot;Dummy alarm&quot;,      &quot;message&quot;:&quot;Thresholds were exceeded for the sub-alarms: count(cpu.user_perc, deterministic) &gt;= 1.0 with the values: [1.0]&quot;,      &quot;alarm_definition_id&quot;:&quot;78ce7b53-f7e6-4b51-88d0-cb741e7dc906&quot;,      &quot;alarm_name&quot;:&quot;dummy_alarm&quot;    }The format of the above message can be customised with a Jinja template. All fieldsfrom the raw Slack message are available in the template. For example, you mayconfigure the plugin as follows:::    [notification_types]    enabled = slack    [slack_notifier]    message_template = /etc/monasca/slack_template.j2    timeout = 10    ca_certs = /etc/ssl/certs/ca-bundle.crt    insecure = FalseWith the following contents of `/etc/monasca/slack_template.j2`:::    {{ alarm_name }} has triggered on {% for item in metrics %}host {{ item.dimensions.hostname }}{% if not loop.last %}, {% endif %}{% endfor %}.With this configuration, the raw Slack message above would be transformedinto:::   dummy_alarm has triggered on host(s): operator.Future Considerations=====================- More extensive load testing is needed:   - How fast is the mysql db? How much load do we put on it. Initially I     think it makes most sense to read notification details for each alarm     but eventually I may want to cache that info.   - How expensive are commits to Kafka for every message we read? Should     we commit every N messages?   - How efficient is the default Kafka consumer batch size?   - Currently we can get ~200 notifications per second per     NotificationEngine instance using webhooks to a local http server. Is     that fast enough?   - Are we putting too much load on Kafka at ~200 commits per second?.. _webhook: https://api.slack.com/incoming-webhooks.. _deprecated: https://api.slack.com/custom-integrations/legacy-tokens.. _configure: https://support.pagerduty.com/docs/services-and-integrations#section-events-api-v1</longdescription>
</pkgmetadata>