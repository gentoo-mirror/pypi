<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># clarku-youtube-crawlerClark University YouTube crawler and JSON decoder for YouTube json. Please read documentation in ``DOCS``Pypi page: &quot;https://pypi.org/project/clarku-youtube-crawler/&quot;## InstallingTo install,``pip install clarku-youtube-crawler``The crawler needs multiple other packages to function. If missing requirements (I already include all dependencies so it shouldn't happen), download &lt;a href=&quot;https://github.com/ClarkUniversity-NiuLab/clarku-youtube-crawler/blob/master/requirements.txt&quot;&gt;``requirements.txt`` &lt;/a&gt; .Navigate to the folder where it contains requirements.txt and run ``pip install -r requirements.txt``## UpgradingTo upgrade``pip install clarku-youtube-crawler --upgrade``Go to the project folder, delete config.ini if it is already there.## YouTube API Key- Go to https://cloud.google.com/, click console, and create a project. Under Credentials, copy the API key.- In your project folder, create a &quot;DEVELOPER_KEY.txt&quot; file (must be this file name) and paste your API key. - You can use multiple API keys by putting them on different lines in DEVELOPER_KEY.txt. - The crawler will use up all quotas of one key and try next one, until all quotas are used up.## Example usageCase 1: crawl videos by keywords, ```import clarku_youtube_crawler as cu# Crawl all JSONscrawler = cu.RawCrawler()crawler.build(&quot;low visibility&quot;)crawler.crawl(&quot;low visibility&quot;, start_date=14, start_month=12, start_year=2020, day_count=5)crawler.crawl(&quot;blind&quot;, start_date=14, start_month=12, start_year=2020, day_count=5)crawler.merge_to_workfile()crawler.crawl_videos_in_list(comment_page_count=1)crawler.merge_all(save_to='low visibility/all_videos.json')# Convert JSON to CSVdecoder = cu.JSONDecoder()decoder.json_to_csv(data_file='low visibility/all_videos.json')# Crawl subtitles from CSV# If you don't need subtitles, delete the following linessubtitleCrawler = cu.SubtitleCrawler()subtitleCrawler.build(&quot;low visibility&quot;)subtitleCrawler.crawl_csv(    videos_to_collect=&quot;low visibility/videos_to_collect.csv&quot;,    video_id=&quot;videoId&quot;,    sub_title_dir=&quot;low visibility/subtitles/&quot;)```Case 2: crawl a videos by a list of ids specified by videoId column in an input CSV```import clarku_youtube_crawler as cucrawler = cu.RawCrawler()work_dir = &quot;blind&quot;crawler.build(work_dir)# update videos_to_collect.csv to your csv file. Specify the column of video id by video_id# video ids must be &quot;:&quot; + YouTube video id. E.g., &quot;:wl4m1Rqmq-Y&quot;crawler.crawl_videos_in_list(video_list_workfile=&quot;videos_to_collect.csv&quot;,                             comment_page_count=1,                             search_key=&quot;blind&quot;,                             video_id=&quot;videoId&quot;                             )crawler.merge_all(save_to='all_raw_data.json')decoder = cu.JSONDecoder()decoder.json_to_csv(data_file='all_raw_data.json')# Crawl subtitles from CSV# If you don't need subtitles, delete the following linessubtitleCrawler = cu.SubtitleCrawler()subtitleCrawler.build(work_dir)subtitleCrawler.crawl_csv(    videos_to_collect=&quot;videos_to_collect.csv&quot;,    video_id=&quot;videoId&quot;,    sub_title_dir=f&quot;YouTube_CSV/subtitles/&quot;)```Case 3: Search a list of channels by search keys, then crawl all videos belonging to those channels.```import clarku_youtube_crawler as cuchCrawler = cu.ChannelCrawler()work_dir = &quot;low visibility&quot;chCrawler.build(work_dir)# You can search different channels. All results will be mergedchCrawler.search_channel(&quot;low visibility&quot;)chCrawler.search_channel(&quot;blind&quot;)chCrawler.merge_to_workfile()chCrawler.crawl()# Crawl videos posted by selected channels. channels_to_collect.csv file has which search keys find each channelcrawler = cu.RawCrawler()crawler.build(work_dir)crawler.merge_to_workfile(file_dir=work_dir + &quot;/video_search_list/&quot;)crawler.crawl_videos_in_list(comment_page_count=1)crawler.merge_all()# Convert JSON to CSVdecoder = cu.JSONDecoder()decoder.json_to_csv(data_file=work_dir + '/all_videos_visibility.json')# Crawl subtitles from CSV# If you don't need subtitles, delete the following linessubtitleCrawler = cu.SubtitleCrawler()subtitleCrawler.build(work_dir)subtitleCrawler.crawl_csv(    videos_to_collect=work_dir+&quot;/videos_to_collect.csv&quot;,    video_id=&quot;videoId&quot;,    sub_title_dir=work_dir+&quot;/subtitles/&quot;)```Case 4: You already have a list of channels. You want to crawl all videos of the channels in the list:```import clarku_youtube_crawler as cuwork_dir = 'disability'chCrawler = cu.ChannelCrawler()chCrawler.build(work_dir)chCrawler.crawl(filename='mturk_test.csv', channel_header=&quot;Input.channelId&quot;)# Crawl videos posted by selected channelscrawler = cu.RawCrawler()crawler.build(work_dir)crawler.merge_to_workfile(file_dir=work_dir + &quot;/video_search_list/&quot;)crawler.crawl_videos_in_list(comment_page_count=10)  # 100 comments per page, 10 page will crawl 1000 commentscrawler.merge_all()## Convert JSON to CSVdecoder = cu.JSONDecoder()decoder.json_to_csv(data_file=work_dir + '/all_videos.json')# Crawl subtitles from CSVsubtitleCrawler = cu.SubtitleCrawler()subtitleCrawler.build(work_dir)subtitleCrawler.crawl_csv(    videos_to_collect=work_dir + &quot;/videos_to_collect.csv&quot;,    video_id=&quot;videoId&quot;,    sub_title_dir=work_dir + &quot;/subtitles/&quot;)```</longdescription>
</pkgmetadata>