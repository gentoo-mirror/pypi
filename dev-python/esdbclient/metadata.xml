<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Python gRPC Client for EventStoreDBThis package provides a Python gRPC client for[EventStoreDB](https://www.eventstore.com/).This client has been developed in collaboration with the EventStoreDBteam. It has been tested to work with EventStoreDB LTS versions 21.10,without and without SSL/TLS, and with Python versions 3.7 to 3.11. Thereis 100% test coverage including branches.All the Python code in this package has typing annotations. The static typingannotations are checked relatively strictly with mypy. The code is formattedwith black and isort, and also checked with flake8. Poetry is used for packagemanagement during development, and for building and publishing distributions to[PyPI](https://pypi.org/project/esdbclient/).Not all the features of the EventStoreDB API are presentedby this client in its current form, however many of the mostuseful aspects are presented in an easy-to-use interface (see below).## SynopsisThe `ESDBClient` class can be imported from the `esdbclient` package.Probably the three most useful methods of `ESDBClient` are:* `append_events()` This method can be used to recordevents in a particular &quot;stream&quot;. This is useful when executing a commandin an application. Either all or none of the events will be recorded.* `read_stream_events()` This method can be used to retrieve all the recordedevents in a &quot;stream&quot;. This is useful for reconstructing an aggregate beforeexecuting a command in an application.* `subscribe_all_events()` This method can be used to receive all recordedevents across all &quot;streams&quot;. This is useful in event-processing components,and supports processing events with &quot;exactly-once&quot; semantics.The example below uses an &quot;insecure&quot; EventStoreDB server running locally on port 2114.```pythonimport esdbclient, uuid# Construct ESDBClient with an EventStoreDB URI.client = esdbclient.ESDBClient(&quot;esdb://localhost:2114?Tls=false&quot;)# Append events to a new stream.stream_name = str(uuid.uuid4())event1 = esdbclient.NewEvent(type='OrderCreated', data=b'data1')client.append_events(    stream_name=stream_name,    expected_position=None,    events=[event1],)# Append more events to an existing stream.event2 = esdbclient.NewEvent(type='OrderUpdated', data=b'data2')event3 = esdbclient.NewEvent(type='OrderDeleted', data=b'data3')client.append_events(    stream_name=stream_name,    expected_position=0,    events=[event2, event3],)# Read all events recorded in a stream.recorded = list(    client.read_stream_events(        stream_name=stream_name    ))assert len(recorded) == 3assert recorded[0].data == event1.dataassert recorded[1].data == event2.dataassert recorded[2].data == event3.dataassert recorded[0].type == event1.typeassert recorded[1].type == event2.typeassert recorded[2].type == event3.type# In an event-processing component, use a &quot;catch-up&quot; subscription# to receive all events across all streams, including events that# have not yet been recorded, starting from the component's last# saved &quot;commit position&quot;.last_saved_commit_position = 0subscription = client.subscribe_all_events(    commit_position=last_saved_commit_position)# To implement &quot;exactly-once&quot; semantics, iterate over the# &quot;catch-up&quot; subscription. Process each received event,# in turn, through an event-processing policy. Save the# value of the commit_position attribute of the processed# event with new state generated by the policy in the same# atomic transaction. Use the last saved &quot;commit position&quot;# when restarting the &quot;catch-up&quot; subscription.received = []for event in subscription:    received.append(event)    if event.id == event3.id:        breakassert received[-3].data == event1.dataassert received[-2].data == event2.dataassert received[-1].data == event3.dataassert received[-3].type == event1.typeassert received[-2].type == event2.typeassert received[-1].type == event3.typeassert received[-3].commit_position &gt; 0assert received[-2].commit_position &gt; received[-3].commit_positionassert received[-1].commit_position &gt; received[-2].commit_position# Close the client after use.client.close()```See below for more details.For an example of usage, see the [eventsourcing-eventstoredb](https://github.com/pyeventsourcing/eventsourcing-eventstoredb) package.## Table of contents&lt;!-- TOC --&gt;* [Install package](#install-package)  * [From PyPI](#from-pypi)  * [With Poetry](#with-poetry)* [Server container](#server-container)  * [Run container](#run-container)  * [Stop container](#stop-container)* [Client class](#client-class)  * [Import class from package](#import-class-from-package)  * [Construct client class](#construct-client-class)* [Streams](#streams)  * [Append events](#append-events)  * [Append event](#append-event)  * [Idempotent append operations](#idempotent-append-operations)  * [Read stream events](#read-stream-events)  * [Read all events](#read-all-events)  * [Get current stream position](#get-current-stream-position)  * [Get current commit position](#get-current-commit-position)  * [Get stream metadata](#get-stream-metadata)  * [Set stream metadata](#set-stream-metadata)  * [Delete stream](#delete-stream)  * [Tombstone stream](#tombstone-stream)* [Catch-up subscriptions](#catch-up-subscriptions)  * [How to implement exactly-once event processing](#how-to-implement-exactly-once-event-processing)  * [Subscribe all events](#subscribe-all-events)  * [Subscribe stream events](#subscribe-stream-events)* [Persistent subscriptions](#persistent-subscriptions)  * [Create subscription](#create-subscription)  * [Read subscription](#read-subscription)  * [Get subscription info](#get-subscription-info)  * [List subscriptions](#list-subscriptions)  * [Delete subscription](#delete-subscription)  * [Create stream subscription](#create-stream-subscription)  * [Read stream subscription](#read-stream-subscription)  * [Get stream subscription info](#get-stream-subscription-info)  * [List stream subscriptions](#list-stream-subscriptions)  * [Delete stream subscription](#delete-stream-subscription)* [Connection](#connection)  * [Reconnect](#reconnect)  * [Close](#close)* [Notes](#notes)  * [Connection strings](#connection-strings)  * [Regular expression filters](#regular-expression-filters)  * [New event objects](#new-event-objects)  * [Recorded event objects](#recorded-event-objects)* [Contributors](#contributors)  * [Install Poetry](#install-poetry)  * [Setup for PyCharm users](#setup-for-pycharm-users)  * [Setup from command line](#setup-from-command-line)  * [Project Makefile commands](#project-makefile-commands)&lt;!-- TOC --&gt;## Install packageIt is recommended to install Python packages into a Python virtual environment.### From PyPIYou can use pip to install this package directly from[the Python Package Index](https://pypi.org/project/esdbclient/).    $ pip install esdbclient### With PoetryYou can use Poetry to add this package to your pyproject.toml and install it.    $ poetry add esdbclient## Server containerThe EventStoreDB server can be run locally using the official Docker container image.### Run containerUse Docker to run EventStoreDB using the official Docker container image on DockerHub.For development, you can start a &quot;secure&quot; server locally, on port 2113.    $ docker run -d --name my-eventstoredb -it -p 2113:2113 --env &quot;HOME=/tmp&quot; eventstore/eventstore:22.10.0-buster-slim --devYou can also start an &quot;insecure&quot; server locally, on port 2114.    $ docker run -d --name my-eventstoredb -it -p 2114:2113 eventstore/eventstore:22.10.0-buster-slim --insecureTo connect to the &quot;insecure&quot; local server using the client in this package, you just needto know the local hostname and the port number. To connect to the &quot;secure&quot; localdevelopment server, you will also need to know that the username is &quot;admin&quot; andthe password is &quot;changeit&quot;. You will also need to get the SSL/TLS certificate fromthe server. You can get the server certificate with the following command.    $ python -c &quot;import ssl; print(ssl.get_server_certificate(addr=('localhost', 2113)))&quot;### Stop containerTo stop and remove the `my-eventstoredb` container created above, use the following Docker commands.    $ docker stop my-eventstoredb$ docker rm my-eventstoredb## Client classThis client is implemented as the Python class `ESDBClient`.### Import class from packageThe `ESDBClient` class can be imported from the `esdbclient` package.```pythonfrom esdbclient import ESDBClient```### Construct client classThe `ESDBClient` class can be constructed with a required `uri` argument, and anoptional `root_certificates` (by default the client will attempt to create a &quot;secure&quot;connection to the server, and in this case the `root_certificates` value is required).The `uri` argument is required, and is expected to conform with the standardEventStoreDB &quot;esdb&quot; or &quot;esdb+discover&quot; URI schemes. You can generate EventStoreDBconnection strings using the online tool.For example, the URI below specifies that the client should attempt to connect to&quot;localhost&quot; on port 2113, using call credentials &quot;username&quot; and &quot;password&quot;.```esdb://username:password@localhost:2113```By using the [query string syntax](https://en.wikipedia.org/wiki/Query_string), the`uri` connection string can specify connection options. See the Notes section belowfor details of these options.For example, the URI below uses the &quot;Tls&quot; options to specify that the client shouldcreate an &quot;insecure&quot; gRPC connection to a &quot;follower&quot; node.```esdb://username:password@localhost:2113?Tls=false&amp;NodePreference=follower```By default, the client will attempt to create a &quot;secure&quot; connection to a &quot;leader&quot;.Unless the `uri` argument specifies `Tls=false`, the `root_certificates` clientconstructor argument is also required. It is expected to be a Python `str` containingPEM encoded SSL/TLS root certificates used for server authentication. This value ispassed directly to `grpc.ssl_channel_credentials()`. It is commonly the certificateof the certificate authority that was responsible for generating the SSL/TLS certificateused by the EventStoreDB server.In the example below, the constructor argument values are taken from the operatingsystem environment (the examples in this document are tested with botha &quot;secure&quot; and an &quot;insecure&quot; server).```pythonimport osclient = ESDBClient(    uri=os.getenv(&quot;ESDB_URI&quot;),    root_certificates=os.getenv(&quot;ESDB_ROOT_CERTIFICATES&quot;),)```## StreamsIn EventStoreDB, a &quot;stream&quot; is a sequence of recorded events that all havethe same &quot;stream name&quot;. Each recorded event has a &quot;stream position&quot; in its stream,and a &quot;commit position&quot; in the database. The stream positions of the recorded eventsin a stream is a gapless sequence starting from zero. The commit positions of therecorded events in the database form a sequence that is not gapless.The methods `append_events()`, `read_stream_events()` and `read_all_events()` canbe used to record and read events in the database.### Append eventsThe `append_events()` method can be used to write a sequence of new events atomicallyto a &quot;stream&quot;. Writing new events either creates a stream, or appends events to the endof a stream. This method is idempotent (see below).This method can be used to record atomically all the newevents that are generated when executing a command in an application.Three arguments are required, `stream_name`, `expected_position`and `events`.The `stream_name` argument is required, and is expected to be a Python`str` object that uniquely identifies the stream in the database.The `expected_position` argument is required, is expected to be: `None`if events are being written to a new stream, and otherwise an Python `int`equal to the position in the stream of the last recorded event in the stream.The `events` argument is required, and is expected to be a sequence of newevent objects to be appended to the named stream. The `NewEvent` class shouldbe used to construct new event objects (see below).This method takes an optional `timeout` argument, which is a Python `float` that setsa deadline for the completion of the gRPC operation.Streams are created by writing events. The correct value of the `expected_position`argument when writing the first event of a new stream is `None`. Please note, it isnot possible to somehow create an &quot;empty&quot; stream in EventStoreDB.The stream positions of recorded events in a stream start from zero, and form a gaplesssequence of integers. The stream position of the first recorded event in a stream is`0`. And so when appending the second new event to a stream that has one recorded event,the correct value of the `expected_position` argument is `0`. Similarly, the streamposition of the second recorded event in a stream is `1`, and so when appending thethird new event to a stream that has two recorded events, the correct value of the`expected_position` argument is `1`. And so on... (There is a theoretical maximumnumber of recorded events that any stream can have, but I'm not sure what it is;maybe 9,223,372,036,854,775,807 because it is implemented as a `long` in C#?)If there is a mismatch between the given value of the `expected_position` argumentand the position of the last recorded event in a stream, then an `ExpectedPositionError`exception will be raised. This effectively accomplishes optimistic concurrency control.If you wish to disable optimistic concurrency control when appending new events, youcan set the `expected_position` to a negative integer.If you need to discover the current position of the last recorded event in a stream,you can use the `get_stream_position()` method (see below).Please note, the append events operation is atomic, so that either allor none of the given new events will be recorded. By design, it is onlypossible with EventStoreDB to atomically record new events in one stream.In the example below, a new event is appended to a new stream.```pythonfrom uuid import uuid4from esdbclient import NewEvent# Construct new event object.event1 = NewEvent(type='OrderCreated', data=b'data1')# Define stream name.stream_name1 = str(uuid4())# Append list of events to new stream.commit_position1 = client.append_events(    stream_name=stream_name1,    expected_position=None,    events=[event1],)```In the example below, two subsequent events are appended to an existingstream.```pythonevent2 = NewEvent(type='OrderUpdated', data=b'data2')event3 = NewEvent(type='OrderDeleted', data=b'data3')commit_position2 = client.append_events(    stream_name=stream_name1,    expected_position=0,    events=[event2, event3],)```If the operation is successful, this method returns an integerrepresenting the overall &quot;commit position&quot; as it was when the operationwas completed. Otherwise, an exception will be raised.A &quot;commit position&quot; is a monotonically increasing integer representingthe position of the recorded event in a &quot;total order&quot; of all recordedevents in the database across all streams. It is the actual positionof the event record on disk, and there are usually large differencesbetween successive commits. In consequence, the sequence of commitpositions is not gapless. Indeed, there are usually large differencesbetween the commit positions of successive recorded events.The &quot;commit position&quot; returned by `append_events()` is that of the lastrecorded event in the given batch of new events.The &quot;commit position&quot; returned in this way can therefore be used to waitfor a downstream component to have processed all the events that were recorded.For example, consider a user interface command that results in the recordingof new events, and a query into an eventually consistent materializedview in a downstream component that is updated from these events. If the newevents have not yet been processed, the view would be stale. The &quot;commit position&quot;can be used by the user interface to poll the downstream component until it hasprocessed those new events, after which time the view will not be stale.### Append eventThe `append_event()` method can be used to write a single new event to a stream.Three arguments are required, `stream_name`, `expected_position` and `event`.This method works in the same way as `append_events()`, however `event` is expectedto be a single `NewEvent`.This method takes an optional `timeout` argument, which is a Python `float` that setsa deadline for the completion of the gRPC operation.Since the handling of a command in your application may result in one or manynew events, and the results of handling a command should be recorded atomically,and the writing of new events generated by a command handler is usually a concernthat is factored out and used everywhere in a project, it is quite usual in a projectto only use `append_events()` to record new events. For this reason, an example isnot provided here.### Idempotent append operationsSometimes it may happen that a new event is successfully recorded and then somehowthe connection to the database gets interrupted before the successful call can returnsuccessfully to the client. In case of an error when appending an event, it may bedesirable to retry appending the same event at the same position. If the event wasin fact successfully recorded, it is convenient for the retry to return successfullywithout raising an error due to optimistic concurrency control (as described above).The example below shows the `append_events()` method being called again with`event3` and `expected_position=2`. We can see that repeating the call to`append_events()` returns successfully.```python# Retry appending event3.commit_position_retry = client.append_events(    stream_name=stream_name1,    expected_position=0,    events=[event2, event3],)```We can see that the same commit position is returned as above.```pythonassert commit_position_retry == commit_position2```We can also see the stream has been unchanged despite calling the append_events()twice with the same arguments, by calling `read_stream_events()`. That is, thereare still only three events in the stream.```pythonresponse = client.read_stream_events(    stream_name=stream_name1)events = list(response)assert len(events) == 3```This idempotent behaviour is activated because the `NewEvent` class has an `id`attribute that, by default, is assigned a new and unique version-4 UUID when aninstance of `NewEvent` is constructed. If events with the same `id` are appendedat the same `expected_position`, the stream will be unchanged, the operation willcomplete successfully, and the same commit position will be returned to the caller.```pythonfrom uuid import UUIDassert isinstance(event1.id, UUID)assert isinstance(event2.id, UUID)assert isinstance(event3.id, UUID)assert event1.id != event2.idassert event2.id != event3.idassert events[0].id == event1.idassert events[1].id == event2.idassert events[2].id == event3.id```It is possible to set the `id` constructor argument of `NewEvent` when instantiatingthe `NewEvent` class, but in the examples above we have been using the defaultbehaviour, which is that the `id` value is generated when the `NewEvent` class isinstantiated.### Read stream eventsThe `read_stream_events()` method can be used to read the recorded events of a stream.This method can be used to retrieve all the recorded events for an aggregate beforeexecuting a command in an application.This method has one required argument, `stream_name`, which is the name ofthe stream from which to read events. By default, the recorded events in thestream are returned in the order they were recorded.The method `read_stream_events()` also supports four optional arguments,`stream_position`, `backwards`, `limit`, and `timeout`.The optional `stream_position` argument is an optional integer that can be used toindicate the position in the stream from which to start reading. This argument is`None` by default, which means the stream will be read either from the start of thestream (the default behaviour), or from the end of the stream if `backwards` is`True` (see below). When reading a stream from a specific position in the stream, therecorded event at that position WILL be included, both when reading forwardsfrom that position, and when reading backwards from that position.The optional argument `backwards` is a boolean, by default `False`, which means thestream will be read forwards by default, so that events are returned in theorder they were appended, If `backwards` is `True`, the stream will be readbackwards, so that events are returned in reverse order.The optional argument `limit` is an integer which limits the number of events that willbe returned. The default value is `sys.maxint`.The optional argument `timeout` is a Python `float` which sets a deadline for the completion ofthe gRPC operation.This method returns a Python iterable object that yields `RecordedEvent` objects.These recorded event objects are instances of the `RecordedEvent` class (see below)The example below shows how to read the recorded events of a streamforwards from the start of the stream to the end of the stream. Thename of a stream is given when calling the method. In this example,the iterable response object is converted into a Python `list`, whichcontains all the recorded event objects that were read from the stream.```pythonresponse = client.read_stream_events(    stream_name=stream_name1)events = list(response)```Now that we have a list of event objects, we can check we got thethree events that were appended to the stream, and that they areordered exactly as they were appended.```pythonassert len(events) == 3assert events[0].stream_name == stream_name1assert events[0].stream_position == 0assert events[0].type == event1.typeassert events[0].data == event1.dataassert events[1].stream_name == stream_name1assert events[1].stream_position == 1assert events[1].type == event2.typeassert events[1].data == event2.dataassert events[2].stream_name == stream_name1assert events[2].stream_position == 2assert events[2].type == event3.typeassert events[2].data == event3.data```The example below shows how to read recorded events in a stream forwards froma specific stream position to the end of the stream.```pythonevents = list(    client.read_stream_events(        stream_name=stream_name1,        stream_position=1,    ))assert len(events) == 2assert events[0].stream_name == stream_name1assert events[0].stream_position == 1assert events[0].type == event2.typeassert events[0].data == event2.dataassert events[1].stream_name == stream_name1assert events[1].stream_position == 2assert events[1].type == event3.typeassert events[1].data == event3.data```The example below shows how to read the recorded events in a stream backwards fromthe end of the stream to the start of the stream.```pythonevents = list(    client.read_stream_events(        stream_name=stream_name1,        backwards=True,    ))assert len(events) == 3assert events[0].stream_name == stream_name1assert events[0].stream_position == 2assert events[0].type == event3.typeassert events[0].data == event3.dataassert events[1].stream_name == stream_name1assert events[1].stream_position == 1assert events[1].type == event2.typeassert events[1].data == event2.data```The example below shows how to read a limited number (two) of the recorded eventsin a stream forwards from the start of the stream.```pythonevents = list(    client.read_stream_events(        stream_name=stream_name1,        limit=2,    ))assert len(events) == 2assert events[0].stream_name == stream_name1assert events[0].stream_position == 0assert events[0].type == event1.typeassert events[0].data == event1.dataassert events[1].stream_name == stream_name1assert events[1].stream_position == 1assert events[1].type == event2.typeassert events[1].data == event2.data```The example below shows how to read a limited number (one) of the recordedevents in a stream backwards from a given stream position.```pythonevents = list(    client.read_stream_events(        stream_name=stream_name1,        stream_position=2,        backwards=True,        limit=1,    ))assert len(events) == 1assert events[0].stream_name == stream_name1assert events[0].stream_position == 2assert events[0].type == event3.typeassert events[0].data == event3.data```### Read all eventsThe method `read_all_events()` can be used to read all recorded eventsin the database in the order they were recorded. An iterable object ofrecorded events is returned. This iterable object will stop when it hasyielded the last recorded event.This method supports six optional arguments, `commit_position`, `backwards`,`filter_exclude`, `filter_include`, `limit`, and `timeout`.The optional argument `commit_position` is an optional integer that can be used tospecify the commit position from which to start reading. This argument is `None` bydefault, meaning that all the events will be read either from the start, orfrom the end if `backwards` is `True` (see below). Please note, if specified,the specified position must be an actually existing commit position, becauseany other number will result in a server error (at least in EventStoreDB v21.10).The optional argument `backwards` is a boolean which is by default `False` meaning theevents will be read forwards by default, so that events are returned in theorder they were committed, If `backwards` is `True`, the events will be readbackwards, so that events are returned in reverse order.The optional argument `filter_exclude` is a sequence of regular expressions thatmatch the type strings of recorded events that should not be included. By default,this argument will match &quot;system events&quot;, so that they will not be included.This argument is ignored if `filter_include` is set to a non-empty sequence.The optional argument `filter_include` is a sequence of regular expressionsthat match the type strings of recorded events that should be included. Bydefault, this argument is an empty tuple. If this argument is set to anon-empty sequence, the `filter_exclude` argument is ignored.The optional argument `limit` is an integer which limits the number of events that willbe returned. The default value is `sys.maxint`.The optional argument `timeout` is a Python `float` which sets a deadline for the completion ofthe gRPC operation.The filtering of events is done on the EventStoreDB server. The`limit` argument is applied on the server after filtering. See below formore information about filter regular expressions.When reading forwards from a specific commit position, the event at the specifiedposition WILL be included. However, when reading backwards, the event at thespecified position will NOT be included. (This non-inclusive behaviour, of excludingthe specified commit position when reading all streams backwards, differs from thebehaviour when reading a stream backwards from a specific stream position, I'mnot sure why.)The example below shows how to read all events in the database in theorder they were recorded.```pythonevents = list(client.read_all_events())assert len(events) &gt;= 3```The example below shows how to read all recorded events from a specific commit position.```pythonevents = list(    client.read_all_events(        commit_position=commit_position1    ))assert len(events) == 3assert events[0].stream_name == stream_name1assert events[0].stream_position == 0assert events[0].type == event1.typeassert events[0].data == event1.dataassert events[1].stream_name == stream_name1assert events[1].stream_position == 1assert events[1].type == event2.typeassert events[1].data == event2.dataassert events[2].stream_name == stream_name1assert events[2].stream_position == 2assert events[2].type == event3.typeassert events[2].data == event3.data```The example below shows how to read all recorded events in reverse order.```pythonevents = list(    client.read_all_events(        backwards=True    ))assert len(events) &gt;= 3assert events[0].stream_name == stream_name1assert events[0].stream_position == 2assert events[0].type == event3.typeassert events[0].data == event3.dataassert events[1].stream_name == stream_name1assert events[1].stream_position == 1assert events[1].type == event2.typeassert events[1].data == event2.dataassert events[2].stream_name == stream_name1assert events[2].stream_position == 0assert events[2].type == event1.typeassert events[2].data == event1.data```The example below shows how to read a limited number (one) of the recorded eventsin the database forwards from a specific commit position. Please note, when readingall events forwards from a specific commit position, the event at the specifiedposition WILL be included.```pythonevents = list(    client.read_all_events(        commit_position=commit_position1,        limit=1,    ))assert len(events) == 1assert events[0].stream_name == stream_name1assert events[0].stream_position == 0assert events[0].type == event1.typeassert events[0].data == event1.dataassert events[0].commit_position == commit_position1```The example below shows how to read a limited number (one) of the recorded eventsin the database backwards from the end. This gives the last recorded event.```pythonevents = list(    client.read_all_events(        backwards=True,        limit=1,    ))assert len(events) == 1assert events[0].stream_name == stream_name1assert events[0].stream_position == 2assert events[0].type == event3.typeassert events[0].data == event3.data```The example below shows how to read a limited number (one) of the recorded eventsin the database backwards from a specific commit position. Please note, when readingall events backwards from a specific commit position, the event at the specifiedposition WILL NOT be included.```pythonevents = list(    client.read_all_events(        commit_position=commit_position2,        backwards=True,        limit=1,    ))assert len(events) == 1assert events[0].commit_position &lt; commit_position2```### Get current stream positionThe `get_stream_position()` method can be used toget the &quot;stream position&quot; of the last recorded event in a stream.This method has a `stream_name` argument, which is required.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.The sequence of positions in a stream is gapless. It is zero-based,so that a stream with one recorded event has a current streamposition of `0`. The current stream position is `1` when a stream hastwo events, and it is `2` when there are events, and so on.In the example below, the current stream position is obtained of thestream to which events were appended in the examples above.Because the sequence of stream positions is zero-based, and becausethree events were appended, so the current stream position is `2`.```pythonstream_position = client.get_stream_position(    stream_name=stream_name1)assert stream_position == 2```If a stream does not exist, the returned stream position value is `None`,which matches the required expected position when appending the first eventof a new stream (see above).```pythonstream_position = client.get_stream_position(    stream_name=str(uuid4()))assert stream_position == None```This method takes an optional argument `timeout` which is a Python `float` that setsa deadline for the completion of the gRPC operation.### Get current commit positionThe method `get_commit_position()` can be used to get the currentcommit position of the database.```pythoncommit_position = client.get_commit_position()```This method takes an optional argument `timeout` which is a Python `float` that setsa deadline for the completion of the gRPC operation.This method can be useful to measure progress of a downstream componentthat is processing all recorded events, by comparing the current commitposition with the recorded commit position of the last successfully processedevent in a downstream component.The value of the `commit_position` argument when reading events either by usingthe `read_all_events()` method or by using a catch-up subscription would usuallybe determined by the recorded commit position of the last successfully processedevent in a downstream component.### Get stream metadataThe method `get_stream_metadata()` gets the metadata for a stream, alongwith the version of the stream metadata.```pythonmetadata, metadata_version = client.get_stream_metadata(stream_name=stream_name1)```The returned `metadata` value is a Python `dict`. The returned `metadata_version`value is either an `int`, or `None` if the stream does not exist. These values canbe passed into `set_stream_metadata()`.### Set stream metadataThe method `set_stream_metadata()` sets the metadata for a stream, alongwith the version of the stream metadata.```pythonmetadata[&quot;foo&quot;] = &quot;bar&quot;client.set_stream_metadata(    stream_name=stream_name1,    metadata=metadata,    expected_position=metadata_version,)```The `expected_position` argument should be the current version of the stream metadata.Please refer to the EventStoreDB documentation for more information about streammetadata.### Delete streamThe method `delete_stream()` can be used to &quot;delete&quot; a stream.```pythoncommit_position = client.delete_stream(stream_name=stream_name1, expected_position=2)```After deleting a stream, it's still possible to append new events. Reading from adeleted stream will return only events that have been appended after it wasdeleted.### Tombstone streamThe method `tombstone_stream()` can be used to &quot;tombstone&quot; a stream.```pythoncommit_position = client.tombstone_stream(stream_name=stream_name1, expected_position=2)```After tombstoning a stream, it's not possible to append new events.## Catch-up subscriptionsA &quot;catch-up subscription&quot; can be used to receive already recorded events, butit will also return events that are recorded after the subscription was started.The method `subscribe_all_events()` starts a catch-up subscription to receive allevents in the database. The method `subscribe_stream_events()` starts a catch-upsubscription to receive events from a specific stream.Catch-up subscriptions are simply a streaming gRPC call which iskept open by the server, with newly recorded events sent to the clientas the client iterates over the subscription.Many catch-up subscriptions can be created, concurrently or successively, and allwill receive all the recorded events they have been requested to receive.Received recorded events are instances of the `RecordedEvent` class (see below).Recorded event objects have a commit position, amonst other attributes.### How to implement exactly-once event processingThe commit positions of recorded events that are received and processed by adownstream component are usefully recorded by the downstream component so thatthe commit position of last processed event can be determined.The last recorded commit position can be used to specify the commit position from whichto subscribe when processing is resumed. Since this commit position will represent theposition of the last successfully processed event in a downstream component, so itwill be usual to want the next event after this position, because that is the nextevent that has not yet been processed. For this reason, when subscribing for eventsfrom a specific commit position using a catch-up subscription in EventStoreDB, therecorded event at the specified commit position will NOT be included in the sequenceof recorded events that are received.To accomplish &quot;exactly-once&quot; processing of recorded events in a downstreamcomponent when using a catch-up subscription, the commit position of a recordedevent should be recorded atomically and uniquely along with the result of processingrecorded events, for example in the same database as materialised views whenimplementing eventually-consistent CQRS, or in the same database as a downstreamanalytics or reporting or archiving application. By recording the commit positionof recorded events atomically with the new state that results from processingrecorded events, &quot;dual writing&quot; in the consumption of recorded events can beavoided. By also recording the commit position uniquely, the new state cannot berecorded twice, and hence the recorded state of the downstream component will beupdated only once for any recorded event. By using the greatest recorded commitposition to resume a catch-up subscription, all recorded events will eventuallybe processed. The combination of the &quot;at-most-once&quot; condition and the &quot;at-least-once&quot;condition gives the &quot;exactly-once&quot; condition.The danger with &quot;dual writing&quot; in the consumption of recorded events is that if arecorded event is successfully processed and new state recorded atomically in onetransaction with the commit position recorded in a separate transaction, one mayhappen and not the other. If the new state is recorded but the position is lost,and then the processing is stopped and resumed, the recorded event may be processedtwice. On the other hand, if the commit position is recorded but the new state islost, the recorded event may effectively not be processed at all. By eitherprocessing an event more than once, or by failing to process an event, the recordedstate of the downstream component might be inaccurate, or possibly inconsistent, andperhaps catastrophically so. Such consequences may or may not matter in your situation.But sometimes inconsistencies may halt processing until the issue is resolved. You canavoid &quot;dual writing&quot; in the consumption of events by atomically recording the commitposition of a recorded event along with the new state that results from processing thatevent in the same atomic transaction. By making the recording of the commit positionsunique, so that transactions will be rolled back when there is a conflict, you willprevent the results of any duplicate processing of a recorded event being committed.Recorded events received from a catch-up subscription cannot be acknowledged backto the EventStoreDB server. Acknowledging events, however, is an aspect of &quot;persistentsubscriptions&quot; (see below). Hoping to rely on acknowledging events to an upstreamcomponent is an example of dual writing.### Subscribe all eventsThe`subscribe_all_events()` method can be used to start a &quot;catch-up&quot; subscriptionthat can return all events in the database.This method can be used by a downstream componentto process recorded events with exactly-once semantics.This method takes an optional `commit_position` argument, which can beused to specify a commit position from which to subscribe forrecorded events. The default value is `None`, which meansthe subscription will operate from the first recorded eventin the database. If a commit position is given, it must matchan actually existing commit position in the database. The eventsthat are obtained will not include the event recorded at that commitposition.This method also takes three other optional arguments, `filter_exclude`,`filter_include`, and `timeout`.The argument `filter_exclude` is a sequence of regular expressions matchingthe type strings of recorded events that should be excluded. By default,this argument will match &quot;system events&quot;, so that they will not be included.This argument is ignored if `filter_include` is set to a non-empty sequence.The argument `filter_include` is a sequence of regular expressionsmatching the type strings of recorded events that should be included. Bydefault, this argument is an empty tuple. If this argument is set to anon-empty sequence, the `filter_exclude` argument is ignored.Please note, the filtering happens on the EventStoreDB server, and the`limit` argument is applied on the server after filtering. See below formore information about filter regular expressions.The argument `timeout` is a Python `float` which sets a deadline for the completion ofthe gRPC operation. This probably isn't very useful, but is included forcompleteness and consistency with the other methods.This method returns a Python iterator that yields recorded events, including eventsthat are recorded after the subscription was created. Iterating over this object willtherefore not stop, unless the connection to the database is lost. The connection willbe closed when the iterator object is deleted from memory, which will happen when theiterator object goes out of scope or is explicitly deleted (see below). The connectionmay also be closed by the server.The subscription object can be used directly, but it might be used within a threadedloop dedicated to receiving events that can be stopped in a controlled way, withrecorded events put on a queue for processing in a different thread. This packagedoesn't provide such a threaded or queuing object class. Just make sure to reconstructthe subscription (and the queue) using the last recorded commit position when resumingthe subscription after an error, to be sure all events are processed once.The example below shows how to subscribe to receive all recordedevents from the start, and then resuming from a specific commit position.Three already-recorded events are received, and then three new events arerecorded, which are then received via the subscription.```python# Append an event to a new stream.stream_name2 = str(uuid4())event4 = NewEvent(type='OrderCreated', data=b'data4')client.append_events(    stream_name=stream_name2,    expected_position=None,    events=[event4],)# Subscribe from the first recorded event in the database.subscription = client.subscribe_all_events()received_events = []# Process events received from the catch-up subscription.for event in subscription:    last_commit_position = event.commit_position    received_events.append(event)    if event.id == event4.id:        breakassert received_events[-4].id == event1.idassert received_events[-3].id == event2.idassert received_events[-2].id == event3.idassert received_events[-1].id == event4.id# Append subsequent events to the stream.event5 = NewEvent(type='OrderUpdated', data=b'data5')client.append_events(    stream_name=stream_name2,    expected_position=0,    events=[event5],)# Receive subsequent events from the subscription.for event in subscription:    last_commit_position = event.commit_position    received_events.append(event)    if event.id == event5.id:        breakassert received_events[-5].id == event1.idassert received_events[-4].id == event2.idassert received_events[-3].id == event3.idassert received_events[-2].id == event4.idassert received_events[-1].id == event5.id# Append more events to the stream.event6 = NewEvent(type='OrderDeleted', data=b'data6')client.append_events(    stream_name=stream_name2,    expected_position=1,    events=[event6],)# Resume subscribing from the last commit position.subscription = client.subscribe_all_events(    commit_position=last_commit_position)# Catch up by receiving the new event from the subscription.for event in subscription:    received_events.append(event)    if event.id == event6.id:        breakassert received_events[-6].id == event1.idassert received_events[-5].id == event2.idassert received_events[-4].id == event3.idassert received_events[-3].id == event4.idassert received_events[-2].id == event5.idassert received_events[-1].id == event6.id# Append three more events to a new stream.stream_name3 = str(uuid4())event7 = NewEvent(type='OrderCreated', data=b'data7')event8 = NewEvent(type='OrderUpdated', data=b'data8')event9 = NewEvent(type='OrderDeleted', data=b'data9')client.append_events(    stream_name=stream_name3,    expected_position=None,    events=[event7, event8, event9],)# Receive the three new events from the resumed subscription.for event in subscription:    received_events.append(event)    if event.id == event9.id:        breakassert received_events[-9].id == event1.idassert received_events[-8].id == event2.idassert received_events[-7].id == event3.idassert received_events[-6].id == event4.idassert received_events[-5].id == event5.idassert received_events[-4].id == event6.idassert received_events[-3].id == event7.idassert received_events[-2].id == event8.idassert received_events[-1].id == event9.id```The catch-up subscription gRPC operation is ended as soon as the subscription objectgoes out of scope or is explicitly deleted from memory.```python# End the subscription.del subscription```### Subscribe stream eventsThe`subscribe_stream_events()` method can be used to start a &quot;catch-up&quot; subscriptionthat can return all events in a stream.This method takes a `stream_name` argument, which specifies the name of the streamfrom which recorded events will be received.This method takes an optional `stream_position` argument, which specifies astream position in the stream from which recorded events will be received. Theevent at the specified stream position will not be included.This method takes an optional `timeout` argument, which is a Python `float` that setsa deadline for the completion of the gRPC operation.The example below shows how to start a catch-up subscription to a stream.```python# Subscribe to events from stream2, from the start.subscription = client.subscribe_stream_events(stream_name=stream_name2)# Read from the subscription.events = []for event in subscription:    events.append(event)    if event.id == event6.id:        break# Check we got events only from stream2.assert len(events) == 3events[0].stream_name == stream_name2events[1].stream_name == stream_name2events[2].stream_name == stream_name2# Append another event to stream3.event10 = NewEvent(type=&quot;OrderUndeleted&quot;, data=b'data10')client.append_events(    stream_name=stream_name3,    expected_position=2,    events=[event10],)# Append another event to stream2.event11 = NewEvent(type=&quot;OrderUndeleted&quot;, data=b'data11')client.append_events(    stream_name=stream_name2,    expected_position=2,    events=[event11])# Continue reading from the subscription.for event in subscription:    events.append(event)    if event.id == event11.id:        break# Check we got events only from stream2.assert len(events) == 4events[0].stream_name == stream_name2events[1].stream_name == stream_name2events[2].stream_name == stream_name2events[3].stream_name == stream_name2```The example below shows how to start a catch-up subscription to a stream from aspecific stream position.```python# Subscribe to events from stream2, from the start.subscription = client.subscribe_stream_events(    stream_name=stream_name2,    stream_position=1,)# Read event from the subscription.events = []for event in subscription:    events.append(event)    if event.id == event11.id:        break# Check we got events only after position 1.assert len(events) == 2events[0].id == event6.idevents[0].stream_position == 2events[0].stream_name == stream_name2events[1].id == event11.idevents[1].stream_position == 3events[1].stream_name == stream_name2```## Persistent subscriptions### Create subscriptionThe method `create_subscription()` can be used to create a&quot;persistent subscription&quot; to EventStoreDB.This method takes a required `group_name` argument, which is thename of a &quot;group&quot; of consumers of the subscription.This method takes an optional `from_end` argument, which can beused to specify that the group of consumers of the subscription shouldonly receive events that were recorded after the subscription was created.This method takes an optional `commit_position` argument, which can beused to specify a commit position from which the group of consumers ofthe subscription should receive events. Please note, the recorded eventat the specified commit position MAY be included in the recorded eventsreceived by the group of consumers.If neither `from_end` or `commit_position` are specified, the group of consumersof the subscription will receive all recorded events.This method also takes option `filter_exclude`, `filter_include`arguments, which work in the same way as they do in the `subscribe_all_events()`method.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.The method `create_subscription()` does not return a value, becauserecorded events are obtained by the group of consumers of the subscriptionusing the `read_subscription()` method.*Please note, in this version of this client the &quot;consumer strategy&quot; isset to &quot;DispatchToSingle&quot;. Support for choosing other consumer strategiessupported by EventStoreDB will in future be supported in this client.*In the example below, a persistent subscription is created.```python# Create a persistent subscription.group_name = f&quot;group-{uuid4()}&quot;client.create_subscription(group_name=group_name)```### Read subscriptionThe method `read_subscription()` can be used by a group of consumers to receiverecorded events from a persistent subscription created using `create_subscription`.This method takes a required `group_name` argument, which isthe name of a &quot;group&quot; of consumers of the subscription specifiedwhen `create_subscription()` was called.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.This method returns a 2-tuple: a &quot;read request&quot; object and a &quot;read response&quot; object.```pythonread_req, read_resp = client.read_subscription(group_name=group_name)```The &quot;read response&quot; object is an iterator that yields recorded events fromthe specified commit position.The &quot;read request&quot; object has an `ack()` method that can be used by a consumerin a group to acknowledge to the server that it has received and successfullyprocessed a recorded event. This will prevent that recorded event being receivedby another consumer in the same group. The `ack()` method takes an `event_id`argument, which is the ID of the recorded event that has been received.The example below iterates over the &quot;read response&quot; object, and calls `ack()`on the &quot;read response&quot; object. The for loop breaks when we have receivedthe last event, so that we can continue with the examples below.```pythonevents = []for event in read_resp:    events.append(event)    # Acknowledge the received event.    read_req.ack(event_id=event.id)    # Break when the last event has been received.    if event.id == event11.id:        break```The received events are the events we appended above.```pythonassert events[-11].id == event1.idassert events[-10].id == event2.idassert events[-9].id == event3.idassert events[-8].id == event4.idassert events[-7].id == event5.idassert events[-6].id == event6.idassert events[-5].id == event7.idassert events[-4].id == event8.idassert events[-3].id == event9.idassert events[-2].id == event10.idassert events[-1].id == event11.id```The &quot;read request&quot; object also has an `nack()` method that can be used by a consumerin a group to acknowledge to the server that it has failed successfully toprocess a recorded event. This will allow that recorded event to be receivedby this or another consumer in the same group.It might be more useful to encapsulate the request and response objects and to iterateover the &quot;read response&quot; in a separate thread, to call back to a handler function whena recorded event is received, and call `ack()` if the handler does not raise anexception, and to call `nack()` if an exception is raised. The example below shows howthis might be done.```pythonfrom threading import Threadclass SubscriptionReader:    def __init__(self, client, group_name, callback):        self.client = client        self.group_name = group_name        self.callback = callback        self.thread = Thread(target=self.read_subscription, daemon=True)        self.error = None    def start(self):        self.thread.start()    def join(self):        self.thread.join()    def read_subscription(self):        req, resp = self.client.read_subscription(group_name=self.group_name)        for event in resp:            try:                self.callback(event)            except Exception as e:                # req.nack(event.id)  # not yet implemented....                self.error = e                break            else:                req.ack(event.id)# Create another persistent subscription.group_name = f&quot;group-{uuid4()}&quot;client.create_subscription(group_name=group_name)events = []def handle_event(event):    events.append(event)    if event.id == event11.id:        raise Exception()reader = SubscriptionReader(    client=client,    group_name=group_name,    callback=handle_event)reader.start()reader.join()assert events[-1].id == event11.id```Please note, when processing events in a downstream component, the commit position ofthe last successfully processed event is usefully recorded by the downstream componentso that the commit position can be determined by the downstream component from its ownrecorded when it is restarted. This commit position can be used to specify the commitposition from which to subscribe. Since this commit position represents the position ofthe last successfully processed event in a downstream component, so it will be usual towant to read from the next event after this position, because that is the next eventthat needs to be processed. However, when subscribing for events using a persistentsubscription in EventStoreDB, the event at the specified commit position MAY be returnedas the first event in the received sequence of recorded events, and so it maybe necessary to check the commit position of the received events and to discardany  recorded event object that has a commit position equal to the commit positionspecified in the request.Whilst there are some advantages of persistent subscriptions, in particular theprocessing of recorded events by a group of consumers, by tracking in theupstream server the position in the commit sequence of events that have been processed,there is a danger of &quot;dual writing&quot; in the consumption of events. Reliabilityin processing of recorded events by a group of consumers will rely instead onidempotent handling of duplicate messages, and resilience to out-of-order delivery.### Get subscription infoThe `get_subscription_info()` method can be used to get information for apersistent subscription.This method has one required argument, `group_name`, whichshould match the value of the argument used when calling `create_subscription()`.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.```pythonsubscription_info = client.get_subscription_info(    group_name=group_name,)```The returned value is a `SubscriptionInfo` object.### List subscriptionsThe `list_subscriptions()` method can be used to get information for allexisting persistent subscriptions.This method takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.```pythonsubscriptions = client.list_subscriptions()```The returned value is a list of `SubscriptionInfo` objects.### Delete subscriptionThe `delete_subscription()` method can be used to delete a persistentsubscription.This method has one required argument, `group_name`, whichshould match the value of argument used when calling `create_subscription()`.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.```pythonclient.delete_subscription(    group_name=group_name,)```### Create stream subscriptionThe `create_stream_subscription()` method can be used to create a persistentsubscription for a stream.This method has two required arguments, `group_name` and `stream_name`. The`group_name` argument names the group of consumers that will receive eventsfrom this subscription. The `stream_name` argument specifies which streamthe subscription will follow. The values of both these arguments are expectedto be Python `str` objects.This method has an optional `stream_position` argument, which specifies astream position from which to subscribe. The recorded event at this streamposition will be received when reading the subscription.This method has an optional `from_end` argument, which is a Python `bool`.By default, the value of this argument is False. If this argument is setto a True value, reading from the subscription will receive only eventsrecorded after the subscription was created. That is, it is not inclusiveof the current stream position.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.This method does not return a value. Events can be received by iteratingover the value returned by calling `read_stream_subscription()` (see below).The example below creates a persistent stream subscription from the start of the stream.```python# Create a persistent stream subscription from start of the stream.group_name1 = f&quot;group-{uuid4()}&quot;client.create_stream_subscription(    group_name=group_name1,    stream_name=stream_name2,)```The example below creates a persistent stream subscription from a stream position.```python# Create a persistent stream subscription from a stream position.group_name2 = f&quot;group-{uuid4()}&quot;client.create_stream_subscription(    group_name=group_name2,    stream_name=stream_name2,    stream_position=2)```The example below creates a persistent stream subscription from the end of the stream.```python# Create a persistent stream subscription from end of the stream.group_name3 = f&quot;group-{uuid4()}&quot;client.create_stream_subscription(    group_name=group_name3,    stream_name=stream_name2,    from_end=True)```### Read stream subscriptionThe `read_stream_subscription()` method can be used to create a persistentsubscription for a stream.This method has two required arguments, `group_name` and `stream_name`, whichshould match the values of arguments used when calling `create_stream_subscription()`.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.Just like `read_subscription`, this method returns a 2-tuple: a &quot;read request&quot; objectand a &quot;read response&quot; object.```pythonread_req, read_resp = client.read_stream_subscription(    group_name=group_name1,    stream_name=stream_name2,)```The example below iterates over the &quot;read response&quot; object, and calls `ack()`on the &quot;read response&quot; object. The for loop breaks when we have receivedthe last event in the stream, so that we can finish the examples in thisdocumentation.```pythonevents = []for event in read_resp:    events.append(event)    # Acknowledge the received event.    read_req.ack(event_id=event.id)    # Break when the last event has been received.    if event.id == event11.id:        break```We can check we received all the events that were appended to `stream_name2`in the examples above.```pythonassert len(events) == 4assert events[0].stream_name == stream_name2assert events[0].id == event4.idassert events[1].stream_name == stream_name2assert events[1].id == event5.idassert events[2].stream_name == stream_name2assert events[2].id == event6.idassert events[3].stream_name == stream_name2assert events[3].id == event11.id```### Get stream subscription infoThe `get_stream_subscription_info()` method can be used to get information for apersistent subscription for a stream.This method has two required arguments, `group_name` and `stream_name`, whichshould match the values of arguments used when calling `create_stream_subscription()`.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.```pythonsubscription_info = client.get_stream_subscription_info(    group_name=group_name1,    stream_name=stream_name2,)```The returned value is a `SubscriptionInfo` object.### List stream subscriptionsThe `list_stream_subscriptions()` method can be used to get information for allthe persistent subscriptions for a stream.This method has one required argument, `stream_name`.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.```pythonsubscriptions = client.list_stream_subscriptions(    stream_name=stream_name2,)```The returned value is a list of `SubscriptionInfo` objects.### Delete stream subscriptionThe `delete_stream_subscription()` method can be used to delete a persistentsubscription for a stream.This method has two required arguments, `group_name` and `stream_name`, whichshould match the values of arguments used when calling `create_stream_subscription()`.This method also takes an optional `timeout` argument, thatis expected to be a Python `float`, which sets a deadlinefor the completion of the gRPC operation.```pythonclient.delete_stream_subscription(    group_name=group_name1,    stream_name=stream_name2,)```## Connection### ReconnectThe `reconnect()` method can be used to manually reconnect the client to asuitable EventStoreDB node. This method uses the same routine, for discovering thecluster nodes and connecting to a suitable node according to the node preferencespecified in the connection string, that is used when the client isinstantiated. This method is thread-safe, and it is &quot;conservative&quot;in that only one reconnection will occur. Concurrent attempts to reconnectwill block until the client has reconnected successfully, and then they willall return normally.```pythonclient.reconnect()```An example of when it might be desirable to reconnect manually is when (for performancereasons) the node preference is for the client to be connected to a follower node in thecluster, and, after a cluster leader election, the follower node becomes a leader node.Reconnecting to a follower node in this case is currently beyond the capabilities ofthis client, but this behavior might be implemented in a future release.Please note, all the client methods use an `@autoreconnect` decorator (which calls the`reconnect()` method) and a `@retry` decorator that will retry operations that faildue to connectivity issues. This means, for example, that when the node preferenceis for the client to be connected to a leader (which is the default) and when, after acluster leader election, the node to which the client is connected becomes a follower,so that write operations will begin to fail because the client is no longer connectedto a leader, then the client will automatically reconnect to the new leader and alsothe client will retry the failed write operations. The client also will reconnectaccording to the node preference when there are connectivity issues causing readoperations to fail with the current connection.Please also note, an event-processing component that uses a catch-up subscription willneed to be monitored for errors, and, if it fails after the subscription started, itwill need to be restarted from the last saved commit position. In this case, theclient will automatically reconnect to a node in the cluster when the subsequent callto start a catch-up subscription is made. You just need to catch the error, readthe last saved commit position, and restart the event processing, using the same`ESDBClient` instance, but with a new call to `subscribe_all_events()`.### CloseThe `close()` method can be used to cleanly close the gRPC connection.```pythonclient.close()```## Notes### Connection stringsThe EventStoreDB connection string is a URI comprising a scheme, followed by anetwork location, optionally followed by a query string.The scheme will be separated from the network location with the characters &quot;://&quot;.If it exists, the query string will be separated from the network location withthe &quot;?&quot; character.There are two URI schemes used by EventStoreDB clients: the &quot;esdb&quot; scheme, and the&quot;esdb+discover&quot; scheme.In both the &quot;esdb&quot; and &quot;esdb+discover&quot; schemes, the network location string maystart with a user info string. If it exists in the URI, the user info stringmust be formed from a username and a password. The username and password stringswill be separated in the user info string with a &quot;:&quot; character. The user info stringwill be separated from the rest of the network location string with the &quot;@&quot; character.In the &quot;esdb&quot; scheme, after the user info string, the rest of the network location willbe a list of gRPC targets. The gRPC targets will be separated from each other in thenetwork location string by the &quot;,&quot; character. Each gRPC target should indicate anEventStoreDB gRPC server socket, by specifying a network address and a port number,separated with the &quot;:&quot; character. The network address may be an IP address or ahostname that can be resolved to an IP address.In the &quot;esdb+discover&quot; scheme, after the user info string, the rest of the networklocation will be a fully-qualified domain name, which identifies a cluster ofEventStoreDB servers. The client will use a DNS server to resolve the full-qualifieddomain name to a list of addresses of EventStoreDB servers. In this case, the portnumber &quot;2113&quot; will be used to construct gRPC targets from the addresses obtained fromthe DNS server.In both the &quot;esdb&quot; and &quot;esdb+discover&quot; schemes, the query string will be a list offield-value arguments, separated from each other with the &quot;&amp;&quot; character. Eachfield-value argument must include a key and a value separated by the &quot;=&quot; character.The table below describes the query arguments supported by this client.| Field               | Value                                                                 | Description                                                                                                   ||---------------------|-----------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|| Tls                 | &quot;true&quot;, &quot;false&quot; (default: &quot;true&quot;)                                     | Use a secure gRPC channel.                                                                                    || TlsVerifyCert       | &quot;true&quot;, &quot;false&quot; (default: &quot;true&quot;)                                     | NOT IMPLEMENTED                                                                                               || ConnectionName      | any string (default: auto-generated version-4 UUID)                   | Identifies the client to the cluster.                                                                         || NodePreference      | &quot;leader&quot;, &quot;follower&quot;, &quot;readonlyreplica&quot;, &quot;random&quot; (default: &quot;leader&quot;) | The node state preferred by the client.                                                                       || DefaultDeadline     | integer (default: `None`)                                             | The default value (in seconds) of the `timeout` argument of client &quot;write&quot; methods such as `append_events()`. || GossipTimeout       | integer (default: 5)                                                  | The default value (in seconds) of the `timeout` argument of gossip read methods, such as `read_gossip()`.     || MaxDiscoverAttempts | integer (default: 10)                                                 | The number of attempts to read gossip when connecting or reconnecting to a cluster member.                    || DiscoveryInterval   | integer (default: 100)                                                | How long to wait (in milliseconds) between gossip retries.                                                    || KeepAliveInterval   | integer (default: `None`)                                             | gRPC channel option: &quot;grpc.keepalive_ms&quot;                                                                      || KeepAliveTimeout    | integer (default: `None`)                                             | gRPC channel option: &quot;grpc.keepalive_timeout_ms&quot;                                                              |Please note, the field names are case-insensitive. If fields are repeated in thequery string, the query string will be parsed without error. However, the connectionoptions used by the client will use the value of the first field. All theother field-values in the query string with the same field name will be ignored.Please note, if NodePreference is &quot;leader&quot; and the node becomes a follower, the clientwill attempt to reconnect to the current leader. The HTTP header &quot;requires-leader&quot; isset to &quot;true&quot; for &quot;write&quot; requests, this header is observed by the server, and a nodewhich is not a leader that receives such a request will return an error. This erroris detected by the client, which will then close the current gRPC connection and createa new connection to the leader. The request will then be sent to the leader, and willbe expected to succeed.Please note, if NodePreference is &quot;follower&quot; and there are no follower nodes in thecluster, the client will fail to connect.Please note, if NodePreference is &quot;readonlyreplica&quot; and there are no read-only replicanodes in the cluster, the client will fail to connect.Please note, the gRPC channel option &quot;grpc.max_receive_message_length&quot; is automaticallyconfigured to the value `17 * 1024 * 1024`. This value cannot be changed.### Regular expression filtersThe filter arguments in `read_all_events()`, `subscribe_all_events()`,`create_subscription()` and `commit_position()` are applied to the `type`attribute of recorded events.The default value of the `filter_exclude` arguments is designed to excludeEventStoreDB &quot;system&quot; and &quot;persistence subscription config&quot; events, whichotherwise would be included. System events generated by EventStoreDB allhave `type` strings that start with the `$` sign. Persistence subscriptionevents generated when manipulating persistence subscriptions all have `type`strings that start with `PersistentConfig`.For example, to match the type of EventStoreDB system events, use the regularexpression `r'\$.+'`. Please note, the constant `ESDB_SYSTEM_EVENTS_REGEX` isset to `r'\$.+'`. You can import this value(`from esdbclient import ESDB_SYSTEM_EVENTS_REGEX`) and useit when building longer sequences of regular expressions.Similarly, to match the type of EventStoreDB persistence subscription events, use theregular expression `r'PersistentConfig\d+'`. The constant `ESDB_PERSISTENT_CONFIG_EVENTS_REGEX`is set to `r'PersistentConfig\d+'`. You can also import this value(`from esdbclient import ESDB_PERSISTENT_CONFIG_EVENTS_REGEX`) and use it when buildinglonger sequences of regular expressions.The constant `DEFAULT_EXCLUDE_FILTER` is a sequence of regular expressions that matchthe events that EventStoreDB generates internally, events that are extraneous to thosewhich you append using the `append_events()` method.For example, to exclude system events and persistence subscription configuration events,and snapshots, you might use the sequence `DEFAULT_EXCLUDE_FILTER + ['.*Snapshot']` asthe value of the `filter_exclude` argument when calling `read_all_events()`,`subscribe_all_events()`, `create_subscription()` or `get_commit_position()`.### New event objectsThe `NewEvent` class is used when appending events.The required argument `type` is a Python `str` object, used to indicate the type ofthe event that will be recorded.The required argument `data` is a Python `bytes` object, used to indicate the data ofthe event that will be recorded.The optional argument `metadata` is a Python `bytes` object, used to indicate anymetadata of the event that will be recorded. The default value is an empty `bytes`object.The optional argument `content_type` is a Python `str` object, used to indicate thetype of the data that will be recorded for this event. The default value is`application/json`, which indicates that the `data` was serialised using JSON.An alternative value for this argument is `application/octet-stream`.The optional argument `id` is a Python `UUID` object, used to specify the unique IDof the event that will be recorded. This value will default to a new version-4 UUID.```pythonnew_event1 = NewEvent(    type='OrderCreated',    data=b'{&quot;name&quot;: &quot;Greg&quot;}',)assert new_event1.type == 'OrderCreated'assert new_event1.data == b'{&quot;name&quot;: &quot;Greg&quot;}'assert new_event1.metadata == b''assert new_event1.content_type == 'application/json'assert isinstance(new_event1.id, UUID)event_id = uuid4()new_event2 = NewEvent(    type='ImageCreated',    data=b'01010101010101',    metadata=b'{&quot;a&quot;: 1}',    content_type='application/octet-stream',    id=event_id,)assert new_event2.type == 'ImageCreated'assert new_event2.data == b'01010101010101'assert new_event2.metadata == b'{&quot;a&quot;: 1}'assert new_event2.content_type == 'application/octet-stream'assert new_event2.id == event_id```### Recorded event objectsThe `RecordedEvent` class is used when reading events.The attribute `type` is a Python `str` object, used to indicate the type of eventthat was recorded.The attribute `data` is a Python `bytes` object, used to indicate the data of theevent that was recorded.The attribute `metadata` is a Python `bytes` object, used to indicate the metadata ofthe event that was recorded.The attribute `content_type` is a Python `str` object, used to indicate the type ofdata that was recorded for this event (usually `application/json` to indicate thatthis data can be parsed as JSON, but alternatively `application/octet-stream` toindicate that it is something else).The attribute `id` is a Python `UUID` object, used to indicate the unique ID of theevent that was recorded. Please note, when recorded events are returned from a callto `read_stream_events()` in EventStoreDB v21.10, the commit position is not actuallyset in the response. This attribute is typed as an optional value (`Optional[UUID]`),and in the case of using EventStoreDB v21.10 the value of this attribute will be `None`when reading recorded events from a stream. Recorded events will however have thisvalues set when reading recorded events from `read_all_events()` and from bothcatch-up and persistent subscriptions.The attribute `stream_name` is a Python `str` object, used to indicate the name of thestream in which the event was recorded.The attribute `stream_position` is a Python `int`, used to indicate the position in thestream at which the event was recorded.The attribute `commit_position` is a Python `int`, used to indicate the commit positionat which the event was recorded.```pythonfrom esdbclient.events import RecordedEventrecorded_event = RecordedEvent(    type='OrderCreated',    data=b'{}',    metadata=b'',    content_type='application/json',    id=uuid4(),    stream_name='stream1',    stream_position=0,    commit_position=512,)```## Contributors### Install PoetryThe first thing is to check you have Poetry installed.    $ poetry --versionIf you don't, then please [install Poetry](https://python-poetry.org/docs/#installing-with-the-official-installer).    $ curl -sSL https://install.python-poetry.org | python3 -It will help to make sure Poetry's bin directory is in your `PATH` environment variable.But in any case, make sure you know the path to the `poetry` executable. The Poetryinstaller tells you where it has been installed, and how to configure your shell.Please refer to the [Poetry docs](https://python-poetry.org/docs/) for guidance onusing Poetry.### Setup for PyCharm usersYou can easily obtain the project files using PyCharm (menu &quot;Git &gt; Clone...&quot;).PyCharm will then usually prompt you to open the project.Open the project in a new window. PyCharm will then usually prompt you to createa new virtual environment.Create a new Poetry virtual environment for the project. If PyCharm doesn't alreadyknow where your `poetry` executable is, then set the path to your `poetry` executablein the &quot;New Poetry Environment&quot; form input field labelled &quot;Poetry executable&quot;. In the&quot;New Poetry Environment&quot; form, you will also have the opportunity to select whichPython executable will be used by the virtual environment.PyCharm will then create a new Poetry virtual environment for your project, usinga particular version of Python, and also install into this virtual environment theproject's package dependencies according to the project's `poetry.lock` file.You can add different Poetry environments for different Python versions, and switchbetween them using the &quot;Python Interpreter&quot; settings of PyCharm. If you want to usea version of Python that isn't installed, either use your favourite package manager,or install Python by downloading an installer for recent versions of Python directlyfrom the [Python website](https://www.python.org/downloads/).Once project dependencies have been installed, you should be able to run testsfrom within PyCharm (right-click on the `tests` folder and select the 'Run' option).Because of a conflict between pytest and PyCharm's debugger and the coverage tool,you may need to add ``--no-cov`` as an option to the test runner template. Alternatively,just use the Python Standard Library's ``unittest`` module.You should also be able to open a terminal window in PyCharm, and run the project'sMakefile commands from the command line (see below).### Setup from command lineObtain the project files, using Git or suitable alternative.In a terminal application, change your current working directoryto the root folder of the project files. There should be a Makefilein this folder.Use the Makefile to create a new Poetry virtual environment for theproject and install the project's package dependencies into it,using the following command.    $ make install-packagesIt's also possible to also install the project in 'editable mode'.    $ make installPlease note, if you create the virtual environment in this way, and then try toopen the project in PyCharm and configure the project to use this virtualenvironment as an &quot;Existing Poetry Environment&quot;, PyCharm sometimes has someissues (don't know why) which might be problematic. If you encounter suchissues, you can resolve these issues by deleting the virtual environmentand creating the Poetry virtual environment using PyCharm (see above).### Project Makefile commandsYou can start EventStoreDB using the following command.    $ make start-eventstoredbYou can run tests using the following command (needs EventStoreDB to be running).    $ make testYou can stop EventStoreDB using the following command.    $ make stop-eventstoredbYou can check the formatting of the code using the following command.    $ make lintYou can reformat the code using the following command.    $ make fmtTests belong in `./tests`. Code-under-test belongs in `./esdbclient`.Edit package dependencies in `pyproject.toml`. Update installed packages (and the`poetry.lock` file) using the following command.    $ make update-packages</longdescription>
</pkgmetadata>