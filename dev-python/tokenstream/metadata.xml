<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># tokenstream[![GitHub Actions](https://github.com/vberlier/tokenstream/workflows/CI/badge.svg)](https://github.com/vberlier/tokenstream/actions)[![PyPI](https://img.shields.io/pypi/v/tokenstream.svg)](https://pypi.org/project/tokenstream/)[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/tokenstream.svg)](https://pypi.org/project/tokenstream/)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)&gt; A versatile token stream for handwritten parsers.```pythonfrom tokenstream import TokenStreamdef parse_sexp(stream: TokenStream):    &quot;&quot;&quot;A basic S-expression parser.&quot;&quot;&quot;    with stream.syntax(brace=r&quot;\(|\)&quot;, number=r&quot;\d+&quot;, name=r&quot;\w+&quot;):        brace, number, name = stream.expect((&quot;brace&quot;, &quot;(&quot;), &quot;number&quot;, &quot;name&quot;)        if brace:            return [parse_sexp(stream) for _ in stream.peek_until((&quot;brace&quot;, &quot;)&quot;))]        elif number:            return int(number.value)        elif name:            return name.valueprint(parse_sexp(TokenStream(&quot;(hello (world 42))&quot;)))  # ['hello', ['world', 42]]```## IntroductionWriting recursive-descent parsers by hand can be quite elegant but it's often a bit more verbose than expected, especially when it comes to handling indentation and reporting proper syntax errors. This package provides a powerful general-purpose token stream that addresses these issues and more.### Features- Define the set of recognizable tokens dynamically with regular expressions- Transparently skip over irrelevant tokens- Expressive API for matching, collecting, peeking, and expecting tokens- Clean error reporting with line numbers and column numbers- Contextual support for indentation-based syntax- Checkpoints for backtracking parsers- Works well with Python 3.10+ match statementsCheck out the [`examples`](https://github.com/vberlier/tokenstream/tree/main/examples) directory for practical examples.## InstallationThe package can be installed with `pip`.```bashpip install tokenstream```## Getting startedYou can define tokens with the `syntax()` method. The keyword arguments associate regular expression patterns to token types. The method returns a context manager during which the specified tokens will be recognized.```pythonstream = TokenStream(&quot;hello world&quot;)with stream.syntax(word=r&quot;\w+&quot;):    print([token.value for token in stream])  # ['hello', 'world']```Check out the full [API reference](https://vberlier.github.io/tokenstream/api_reference/) for more details.### Expecting tokensThe token stream is iterable and will yield all the extracted tokens one after the other. You can also retrieve tokens from the token stream one at a time by using the `expect()` method.```pythonstream = TokenStream(&quot;hello world&quot;)with stream.syntax(word=r&quot;\w+&quot;):    print(stream.expect().value)  # &quot;hello&quot;    print(stream.expect().value)  # &quot;world&quot;```The `expect()` method lets you ensure that the extracted token matches a specified type and will raise an exception otherwise.```pythonstream = TokenStream(&quot;hello world&quot;)with stream.syntax(number=r&quot;\d+&quot;, word=r&quot;\w+&quot;):    print(stream.expect(&quot;word&quot;).value)  # &quot;hello&quot;    print(stream.expect(&quot;number&quot;).value)  # UnexpectedToken: Expected number but got word 'world'```### Filtering the streamNewlines and whitespace are ignored by default. You can reject interspersed whitespace by intercepting the built-in `newline` and `whitespace` tokens.```pythonstream = TokenStream(&quot;hello world&quot;)with stream.syntax(word=r&quot;\w+&quot;), stream.intercept(&quot;newline&quot;, &quot;whitespace&quot;):    print(stream.expect(&quot;word&quot;).value)  # &quot;hello&quot;    print(stream.expect(&quot;word&quot;).value)  # UnexpectedToken: Expected word but got whitespace ' '```The opposite of the `intercept()` method is `ignore()`. It allows you to ignore tokens and handle comments pretty easily.```pythonstream = TokenStream(    &quot;&quot;&quot;    # this is a comment    hello # also a comment    world    &quot;&quot;&quot;)with stream.syntax(word=r&quot;\w+&quot;, comment=r&quot;#.+$&quot;), stream.ignore(&quot;comment&quot;):    print([token.value for token in stream])  # ['hello', 'world']```### IndentationTo enable indentation you can use the `indent()` method. The stream will now yield balanced pairs of `indent` and `dedent` tokens when the indentation changes.```pythonsource = &quot;&quot;&quot;hello    world&quot;&quot;&quot;stream = TokenStream(source)with stream.syntax(word=r&quot;\w+&quot;), stream.indent():    stream.expect(&quot;word&quot;)    stream.expect(&quot;indent&quot;)    stream.expect(&quot;word&quot;)    stream.expect(&quot;dedent&quot;)```To prevent some tokens from triggering unwanted indentation changes you can use the `skip` argument.```pythonsource = &quot;&quot;&quot;hello        # some comment    world&quot;&quot;&quot;stream = TokenStream(source)with stream.syntax(word=r&quot;\w+&quot;, comment=r&quot;#.+$&quot;), stream.indent(skip=[&quot;comment&quot;]):    stream.expect(&quot;word&quot;)    stream.expect(&quot;comment&quot;)    stream.expect(&quot;indent&quot;)    stream.expect(&quot;word&quot;)    stream.expect(&quot;dedent&quot;)```### CheckpointsThe `checkpoint()` method returns a context manager that resets the stream to the current token at the end of the `with` statement. You can use the returned `commit()` function to keep the state of the stream at the end of the `with` statement.```pythonstream = TokenStream(&quot;hello world&quot;)with stream.syntax(word=r&quot;\w+&quot;):    with stream.checkpoint():        print([token.value for token in stream])  # ['hello', 'world']    with stream.checkpoint() as commit:        print([token.value for token in stream])  # ['hello', 'world']        commit()    print([token.value for token in stream])  # []```### Match statementsMatch statements make it very intuitive to process tokens extracted from the token stream. If you're using Python 3.10+ give it a try and see if you like it.```pythonfrom tokenstream import TokenStream, Tokendef parse_sexp(stream: TokenStream):    &quot;&quot;&quot;A basic S-expression parser that uses Python 3.10+ match statements.&quot;&quot;&quot;    with stream.syntax(brace=r&quot;\(|\)&quot;, number=r&quot;\d+&quot;, name=r&quot;\w+&quot;):        match stream.expect_any((&quot;brace&quot;, &quot;(&quot;), &quot;number&quot;, &quot;name&quot;):            case Token(type=&quot;brace&quot;):                return [parse_sexp(stream) for _ in stream.peek_until((&quot;brace&quot;, &quot;)&quot;))]            case Token(type=&quot;number&quot;) as number :                return int(number.value)            case Token(type=&quot;name&quot;) as name:                return name.value```## ContributingContributions are welcome. Make sure to first open an issue discussing the problem or the new feature before creating a pull request. The project uses [`poetry`](https://python-poetry.org/).```bash$ poetry install```You can run the tests with `poetry run pytest`.```bash$ poetry run pytest```The project must type-check with [`pyright`](https://github.com/microsoft/pyright). If you're using VSCode the [`pylance`](https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance) extension should report diagnostics automatically. You can also install the type-checker locally with `npm install` and run it from the command-line.```bash$ npm run watch$ npm run check$ npm run verifytypes```The code follows the [`black`](https://github.com/psf/black) code style. Import statements are sorted with [`isort`](https://pycqa.github.io/isort/).```bash$ poetry run isort tokenstream examples tests$ poetry run black tokenstream examples tests$ poetry run black --check tokenstream examples tests```---License - [MIT](https://github.com/vberlier/tokenstream/blob/main/LICENSE)</longdescription>
</pkgmetadata>