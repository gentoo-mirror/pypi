<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># `light-curve` processing toolbox for PythonThe Python wrapper for Rust [`light-curve-feature`](https://github.com/light-curve/light-curve-feature) and [`light-curve-dmdt`](https://github.com/light-curve/light-curve-dmdt) packages which gives a collection of high-performant time-series feature extractors.[![PyPI version](https://badge.fury.io/py/light-curve.svg)](https://pypi.org/project/light-curve/)![testing](https://github.com/light-curve/light-curve-python/actions/workflows/test.yml/badge.svg)[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/light-curve/light-curve-python/master.svg)](https://results.pre-commit.ci/latest/github/light-curve/light-curve-python/master)## Installation```shpython3 -mpip install 'light-curve[full]'````full` extras would install the package with all optional Python dependencies required by experimental features.We also provide `light-curve-python` package which is just an &quot;alias&quot; to the main `light-curve[full]` package.Minimum supported Python version is 3.7.We provide binary wheels via [PyPi](https://pypi.org/project/light-curve/) for number of platforms and architectures, both for CPython and PyPy.We also provide binary wheels for stable CPython ABI, so the package is guaranteed to work with all future CPython3 versions.### Support matrix| Arch \ OS   | Linux glibc | Linux musl                     | macOS                                                          | Windows https://github.com/light-curve/light-curve-python/issues/186 || ----------- |-------------|--------------------------------|----------------------------------------------------------------|----------------------------------------------------------------------|| **x86-64**  | wheel (MKL) | wheel (MKL)                    | wheel                                                          | wheel (no Ceres, no GSL)                                             || **i686**    | src         | src                            | —                                                              | not tested                                                           || **aarch64** | wheel       | wheel                          | src https://github.com/light-curve/light-curve-python/issues/5 | not tested                                                           || **ppc64le** | wheel       | not tested (no Rust toolchain) | —                                                              | —                                                                    |- &quot;wheel&quot;: binary wheel is available on pypi.org, local building is not required for the platform, the only pre-requirement is a recent `pip` version. For Linux x86-64 we provide binary wheels built with Intel MKL for better periodogram performance, which is not a default build option. For Windows x86-64 we provide wheel with no Ceres and no GSL support, which is not a default build option.- &quot;src&quot;: the package is confirmed to be built and pass unit tests locally, but testing and package building is not supported by CI. It is required to have the [GNU scientific library (GSL)](https://www.gnu.org/software/gsl/) v2.1+ and the [Rust toolchain](https://rust-lang.org) v1.62+ to install it via `pip install`.- &quot;not tested&quot;: building from the source code is not tested, please report us building status via issue/PR/email.We build aarch64 macOS 12.0+ Python 3.8+ wheels locally and submit them running this command in `light-curve` directory:```rm -rf ./wheelhousecurl https://sh.rustup.rs -sSf | sh -s -- --default-toolchain stable -ybrew install gslCIBW_BUILD='cp3*-macosx_arm64' CIBW_ENVIRONMENT=&quot;MATURIN_PEP517_ARGS='--locked --no-default-features --features ceres-source,fftw-source,gsl'&quot; CIBW_BEFORE_ALL='' python3 -mcibuildwheel --platform macosCIBW_BUILD='cp310-macosx_arm64' CIBW_ENVIRONMENT=&quot;MATURIN_PEP517_ARGS='--locked --no-default-features --features ceres-source,fftw-source,gsl,abi3'&quot; CIBW_BEFORE_ALL='' python3 -mcibuildwheel --platform macostwine upload wheelhouse/*.whl```## Feature evaluatorsMost of the classes implement various feature evaluators useful for light-curve basedastrophysical source classification and characterisation.&lt;!-- name: test_feature_evaluators_basic --&gt;```pythonimport light_curve as lcimport numpy as np# Time values can be non-evenly separated but must be an ascending arrayn = 101t = np.linspace(0.0, 1.0, n)perfect_m = 1e3 * t + 1e2err = np.sqrt(perfect_m)m = perfect_m + np.random.normal(0, err)# Half-amplitude of magnitudeamplitude = lc.Amplitude()# Fraction of points beyond standard deviations from meanbeyond_std = lc.BeyondNStd(nstd=1)# Slope, its error and reduced chi^2 of linear fitlinear_fit = lc.LinearFit()# Feature extractor, it will evaluate all features in more efficient wayextractor = lc.Extractor(amplitude, beyond_std, linear_fit)# Array with all 5 extracted featuresresult = extractor(t, m, err, sorted=True, check=False)print('\n'.join(f&quot;{name} = {value:.2f}&quot; for name, value in zip(extractor.names, result)))# Run in parallel for multiple light curves:results = amplitude.many(    [(t[:i], m[:i], err[:i]) for i in range(n // 2, n)],    n_jobs=-1,    sorted=True,    check=False,)print(&quot;Amplitude of amplitude is {:.2f}&quot;.format(np.ptp(results)))```If you confident in your inputs you could use `sorted = True` (`t` is in ascending order)and `check = False` (no NaNs in inputs, no infs in `t` or `m`) for better performance.Note that if your inputs are not valid and are not validated by`sorted=None` and `check=True` (default values) then all kind of bad things could happen.Print feature classes list&lt;!-- name: test_feature_evaluators_list --&gt;```pythonimport light_curve as lcprint([x for x in dir(lc) if hasattr(getattr(lc, x), &quot;names&quot;)])```Read feature docs&lt;!-- name: test_feature_evaluators_help --&gt;```pythonimport light_curve as lchelp(lc.BazinFit)```### Experimental extractorsFrom the technical point of view the package consists of two parts: a wrapper for [`light-curve-feature` Rust crate](https://crates.io/crates/light-curve-feature) (`light_curve_ext` sub-package) and pure Python sub-package `light_curve_py`.We use the Python implementation of feature extractors to test Rust implementation and to implement new experimental extractors.Please note, that the Python implementation is much slower for the most of the extractors and doesn't provide the same functionality as the Rust implementation.However, the Python implementation provides some new feature extractors you can find useful.You can manually use extractors from both implementations:&lt;!-- name: test_experimental_extractors --&gt;```pythonimport numpy as npfrom numpy.testing import assert_allclosefrom light_curve.light_curve_ext import LinearTrend as RustLinearTrendfrom light_curve.light_curve_py import LinearTrend as PythonLinearTrendrust_fe = RustLinearTrend()py_fe = PythonLinearTrend()n = 100t = np.sort(np.random.normal(size=n))m = 3.14 * t - 2.16 + np.random.normal(size=n)assert_allclose(rust_fe(t, m), py_fe(t, m),                err_msg=&quot;Python and Rust implementations must provide the same result&quot;)```This should print a warning about experimental status of the Python class### Available featuresSee the complite list of evailable feature evaluators and documentation in [`light-curve-feature` Rust crate docs](https://docs.rs/light-curve-feature/latest/light_curve_feature/features/index.html).&lt;table&gt;  &lt;tr&gt;    &lt;th&gt;Feature name&lt;/th&gt;    &lt;th&gt;Description&lt;/th&gt;    &lt;th&gt;Min data points&lt;/th&gt;    &lt;th&gt;Features number&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;Amplitude&lt;/td&gt;    &lt;td&gt;Half amplitude of magnitude: &lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{\max (m)-\min (m)}{2}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;AndersonDarlingNormal&lt;/td&gt;    &lt;td&gt;Unbiased Anderson–Darling normality test statistic:&lt;p align=&quot;left&quot;&gt;$\displaystyle \left( 1+\frac{4}{N} -\frac{25}{N^{2}}\right) \times$&lt;p&gt;&lt;p align=&quot;right&quot;&gt;$\times \left( -N-\frac{1}{N}\sum\limits_{i=0}^{N-1} (2i+1)\ln \Phi _{i} +(2(N-i)-1)\ln (1-\Phi _{i} )\right) ,$&lt;p&gt; where $\Phi _{i\ } \equiv \Phi (( m_{i} \ -\ \langle m\rangle ) /\sigma _{m})$ is the commutative distribution function of the standard normal distribution, $N-$ the number of observations, $\langle m\rangle -$ mean magnitude and $\sigma _{m} =\sqrt{\sum\limits_{i=0}^{N-1}( m_{i} -\langle m\rangle )^{2} /( N-1) \ }$ is the magnitude standard deviation&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;BazinFit&lt;/td&gt;    &lt;td&gt;Five fit parameters and goodness of fit (reduced $\chi ^{2}$ of the Bazin function developed for core-collapsed supernovae:&lt;p align=&quot;center&quot;&gt;$\displaystyle f(t)=A\frac{\mathrm{e}^{-(t-t_{0} )/\tau _{fall}}}{1+\mathrm{e}^{-(t-t_{0} )/\tau _{rise}}} +B,$&lt;/p&gt; where $f(t)-$ flux observation&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;6&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;BeyondNStd&lt;/td&gt;    &lt;td&gt;Fraction of observations beyond $n\sigma _{m}$ from the mean magnitude $\langle m\rangle $:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{\sum _{i} I_{|m-\langle m\rangle | &gt;n\sigma _{m}} (m_{i} )}{N},$&lt;/p&gt; where $I-$ an indicator function&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;   &lt;tr&gt;    &lt;td&gt;Cusum&lt;/td&gt;    &lt;td&gt;A range of cumulative sums:&lt;p align=&quot;center&quot;&gt;$\displaystyle \max(S) -\min(S),$&lt;/p&gt; where $S_{j} \equiv \frac{1}{N\sigma _{m}}\sum\limits _{i=0}^{j} (m_{i} -\langle m\rangle )$, $j\in \{1..N-1\}$&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;Eta&lt;/td&gt;    &lt;td&gt;Von Neummann $\eta $:&lt;p align=&quot;center&quot;&gt;$\displaystyle \eta \equiv \frac{1}{(N-1)\sigma _{m}^{2}}\sum\limits _{i=0}^{N-2} (m_{i+1} -m_{i} )^{2}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;EtaE&lt;/td&gt;    &lt;td&gt;Modernisation of &lt;b&gt;Eta&lt;/b&gt; for unevenly time series:&lt;p align=&quot;center&quot;&gt;$\displaystyle \eta ^{e} \equiv \frac{(t_{N-1} -t_{0} )^{2}}{(N-1)^{3}}\frac{\sum\limits_{i=0}^{N-2}\left(\frac{m_{i+1} -m_{i}}{t_{i+1} -t_{i}}\right)^{2}}{\sigma _{m}^{2}}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;ExcessVariance&lt;/td&gt;    &lt;td&gt;Measure of the variability amplitude:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{\sigma _{m}^{2} -\langle \delta ^{2} \rangle }{\langle m\rangle ^{2}},$&lt;/p&gt; where $\langle \delta ^{2} \rangle -$ mean squared error&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;InterPercentileRange&lt;/td&gt;    &lt;td&gt;&lt;p align=&quot;center&quot;&gt;$\displaystyle Q(1-p)-Q(p),$&lt;/p&gt; where $Q(n)$ and $Q(d)-$ $n$-th and $d$-th quantile of magnitude sample&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;Kurtosis&lt;/td&gt;    &lt;td&gt;Excess kurtosis of magnitude:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{N(N+1)}{(N-1)(N-2)(N-3)}\frac{\sum _{i} (m_{i} -\langle m\rangle )^{4}}{\sigma _{m}^{2}} -3\frac{(N+1)^{2}}{(N-2)(N-3)}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;LinearFit&lt;/td&gt;    &lt;td&gt;The slope, its error and reduced $\chi ^{2}$ of the light curve in the linear fit of a magnitude light curve with respect to the observation error $\{\delta _{i}\}$:&lt;p align=&quot;center&quot;&gt;$\displaystyle m_{i} \ =\ c\ +\ \text{slope} \ t_{i} \ +\ \delta _{i} \varepsilon _{i} ,$&lt;/p&gt; where $c$ is a constant, $\{\varepsilon _{i}\}$ are standard distributed random variables&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;3&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;3&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;LinearTrend&lt;/td&gt;    &lt;td&gt;The slope and its error of the light curve in the linear fit of a magnitude light curve without respect to the observation error $\{\delta _{i}\}$:&lt;p align=&quot;center&quot;&gt;$\displaystyle m_{i} \ =\ c\ +\ \text{slope} \ t_{i} \ +\ \Sigma \varepsilon _{i} ,$&lt;/p&gt; where $c$ and $\Sigma$ are constants, $\{\varepsilon _{i}\}$  are standard distributed random variables.&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;MagnitudePercentageRatio&lt;/td&gt;    &lt;td&gt;Magnitude percentage ratio:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{Q(1-n)-Q(n)}{Q(1-d)-Q(d)}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;MaximumSlope&lt;/td&gt;    &lt;td&gt;Maximum slope between two sub-sequential observations:&lt;p align=&quot;center&quot;&gt;$\displaystyle \max_{i=0\dotsc N-2}\left| \frac{m_{i+1} -m_{i}}{t_{i+1} -t_{i}}\right|$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;Mean&lt;/td&gt;    &lt;td&gt;Mean magnitude:&lt;p align=&quot;center&quot;&gt;$\displaystyle \langle m\rangle =\frac{1}{N}\sum\limits _{i} m_{i}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;MeanVariance&lt;/td&gt;    &lt;td&gt;Standard deviation to mean ratio:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{\sigma _{m}}{\langle m\rangle }$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;Median&lt;/td&gt;    &lt;td&gt;Median magnitude    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;MedianAbsoluteDeviation&lt;/td&gt;    &lt;td&gt;Median of the absolute value of the difference between magnitude and its median:&lt;p align=&quot;center&quot;&gt;$\displaystyle \mathrm{Median} (|m_{i} -\mathrm{Median} (m)|)$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;MedianBufferRangePercentage&lt;/td&gt;    &lt;td&gt;&lt;p align=&quot;center&quot;&gt;$\displaystyle \mathrm{Median} (m)\pm q\times (\max (m)-\min (m))/2$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;OtsuSplit&lt;/td&gt;    &lt;td&gt;Difference of subset means, standard deviation of the lower subset, standard deviation of the uppersubset and lower-to-all observation count ratio for two subsets of magnitudes obtained by Otsu's method split.&lt;br&gt;&lt;br&gt;Otsu's method is used to perform automatic thresholding. The algorithm returns a single threshold that separate values into two classes. This threshold is determined by minimizing intra-class intensity variance $\sigma^2_{W}=w_0\sigma^2_0+w_1\sigma^2_1$, or equivalently, by maximizing inter-class variance $\sigma^2_{B}=w_0 w_1 (\mu_1-\mu_0)^2$. There can be more than one extremum. In this case, the algorithm returns the minimum threshold.   &lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;PercentAmplitude&lt;/td&gt;    &lt;td&gt;Maximum deviation of magnitude from its median:&lt;p align=&quot;center&quot;&gt;$\displaystyle \max_{i} |m_{i} \ -\ \text{Median}( m) |$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;PercentDifferenceMagnitudePercentile&lt;/td&gt;    &lt;td&gt;Ratio of $p$-th inter-percentile range to the median:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{Q( 1-p) -Q( p)}{\text{Median}( m)}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;ReducedChi2&lt;/td&gt;    &lt;td&gt;Reduced $\chi ^{2}$ of magnitude measurements:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{1}{N-1}\sum _{i}\left(\frac{m_{i} -\overline{m}}{\delta _{i}}\right)^{2} ,$&lt;/p&gt; where $\overline{m} -$ weighted mean magnitude&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;Skew&lt;/td&gt;    &lt;td&gt;Skewness of magnitude:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{N}{(N-1)(N-2)}\frac{\sum _{i} (m_{i} -\langle m\rangle )^{3}}{\sigma _{m}^{3}}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;3&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;StandardDeviation&lt;/td&gt;    &lt;td&gt;Standard deviation of magnitude:&lt;p align=&quot;center&quot;&gt;$\displaystyle \sigma _{m} \equiv \sqrt{\sum _{i} (m_{i} -\langle m\rangle )^{2} /(N-1)}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;StetsonK&lt;/td&gt;    &lt;td&gt;&lt;b&gt;Stetson K&lt;/b&gt; coefficient described light curve shape:&lt;p align=&quot;center&quot;&gt;$\displaystyle \frac{\sum _{i}\left| \frac{m_{i} -\langle m\rangle }{\delta _{i}}\right| }{\sqrt{N\ \chi ^{2}}}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;VillarFit&lt;/td&gt;    &lt;td&gt;Seven fit parameters and goodness of fit (reduced $\chi ^{2}$) of the Villar function developed for supernovae classification:&lt;p align=&quot;center&quot;&gt;$f(t)=c+\frac{A}{1+\exp\frac{-(t-t_{0} )}{\tau _{rise}}} \times f_{fall}(t),$&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;$f_{fall}(t) = 1-\frac{\nu (t-t_{0} )}{\gamma }, ~~~ t&lt; t_{0} +\gamma,$&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;$f_{fall}(t) = (1-\nu )\exp\frac{-(t-t_{0} -\gamma )}{\tau _{fall}}, ~~~ t \geq t_{0} + \gamma.$&lt;/p&gt;where $f(t) -$ flux observation, $A, \gamma , \tau _{rise} , \tau _{fall}  &gt;0$, $\nu \in [0;1)$&lt;/p&gt;Here we introduce a new dimensionless parameter $\nu$ instead of the plateau slope $\beta$ from the original paper: $\nu \equiv -\beta \gamma /A$&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;8&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;8&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;WeightedMean&lt;/td&gt;    &lt;td&gt;Weighted mean magnitude:&lt;p align=&quot;center&quot;&gt;$\displaystyle \overline{m} \equiv \frac{\sum _{i} m_{i} /\delta _{i}^{2}}{\sum _{i} 1/\delta _{i}^{2}}$&lt;/p&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;    &lt;td align=&quot;center&quot;&gt;1&lt;/td&gt;  &lt;/tr&gt;&lt;/table&gt;### Meta-featuresMeta-features can accept another feature extractors and apply them to pre-processed data.#### PeriodogramA number of features based on Lomb–Scargle periodogram as it was an evenly separeated uncertancy-less lime series.Periodogram $P(\omega)$ is an estimate of spectral density of unevenly time series.`Periodogram`'s `peaks` argument corresponds to a number of the most significant spectral density peaks to return.For each peak its period and &quot;signal to noise&quot; ratio is returned.$$\text{signal to noise of peak} \equiv \frac{P(\omega_\mathrm{peak}) - \langle P(\omega) \rangle}{\sigma\_{P(\omega)}}$$`features` argument accepts a list of additional feature evaluators.#### BinsBinning time series to bins with width $\mathrm{window}$ with respect to some $\mathrm{offset}$.$j-th$ bin boundaries are $[j \cdot \mathrm{window} + \mathrm{offset}; (j + 1) \cdot \mathrm{window} + \mathrm{offset}]$.Binned time series is defined by$$t_j^* = (j + \frac12) \cdot \mathrm{window} + \mathrm{offset},$$$$m_j^* = \frac{\sum{m_i / \delta_i^2}}{\sum{\delta_i^{-2}}},$$$$\delta_j^* = \frac{N_j}{\sum{\delta_i^{-2}}},$$where $N_j$ is a number of sampling observations and all sums are over observations inside considering bin.### BenchmarksYou can run all benchmarks from the Python project folder with `python3 -mpytest --benchmark-enable tests/test_w_bench.py`, or with slow benchmarks disabled `python3 -mpytest -m &quot;not (nobs or multi)&quot; --benchmark-enable tests/test_w_bench.py`.Here we benchmark the Rust implementation (`rust`) versus [`feets`](https://feets.readthedocs.io/en/latest/) package and our own Python implementation (`lc_py`) for a light curve having n=1000 observations.![Benchmarks, Rust is much faster](https://github.com/light-curve/light-curve-python/raw/readme-benchs/light-curve/.readme/benchplot_v2.png)The plot shows that the Rust implementation of the package outperforms other ones by a factor of 1.5—50.This allows to extract a large set of &quot;cheap&quot; features well under one ms for n=1000.The performance of parametric fits (`BazinFit` and `VillarFit`) and `Periodogram` depend on their parameters, but the typical timescale of feature extraction including these features is 20—50 ms for few hundred observations.![Benchmark for different number of observations](https://github.com/light-curve/light-curve-python/raw/readme-benchs/light-curve/.readme/nobs_bench_v2.png)Benchmark results of several features for both the pure-Python and Rust implementations of the ”light-curve” package, as a function of the number of observations in a light curve. Both the x-axis and y-axis are on a logarithmic scale.![Benchmark for multithreading and multiprocessing](https://github.com/light-curve/light-curve-python/raw/readme-benchs/light-curve/.readme/multi_bench_v2.png)Processing time per a single light curve for extraction of features subset presented in first benchmark versus the number of CPU cores used. The dataset consists of 10,000 light curves with 1,000 observations in each.See benchmarks' descriptions in more details in [&quot;Performant feature extraction for photometric time series&quot;](https://arxiv.org/abs/2302.10837).## dm-dt mapClass `DmDt` provides dm–dt mapper (based on [Mahabal et al. 2011](https://ui.adsabs.harvard.edu/abs/2011BASI...39..387M/abstract), [Soraisam et al. 2020](https://ui.adsabs.harvard.edu/abs/2020ApJ...892..112S/abstract)). It is a Python wrapper for [`light-curve-dmdt` Rust crate](https://crates.io/crates/light-curve-dmdt).&lt;!-- name: test_dmdt --&gt;```pythonimport numpy as npfrom light_curve import DmDtfrom numpy.testing import assert_array_equaldmdt = DmDt.from_borders(min_lgdt=0, max_lgdt=np.log10(3), max_abs_dm=3, lgdt_size=2, dm_size=4, norm=[])t = np.array([0, 1, 2], dtype=np.float32)m = np.array([0, 1, 2], dtype=np.float32)desired = np.array(    [        [0, 0, 2, 0],        [0, 0, 0, 1],    ])actual = dmdt.points(t, m)assert_array_equal(actual, desired)```## CitationIf you found this project useful for your research please cite [Malanchev et al., 2021](https://ui.adsabs.harvard.edu/abs/2021MNRAS.502.5147M/abstract)```bibtex@ARTICLE{2021MNRAS.502.5147M,       author = {{Malanchev}, K.~L. and {Pruzhinskaya}, M.~V. and {Korolev}, V.~S. and {Aleo}, P.~D. and {Kornilov}, M.~V. and {Ishida}, E.~E.~O. and {Krushinsky}, V.~V. and {Mondon}, F. and {Sreejith}, S. and {Volnova}, A.~A. and {Belinski}, A.~A. and {Dodin}, A.~V. and {Tatarnikov}, A.~M. and {Zheltoukhov}, S.~G. and {(The SNAD Team)}},        title = &quot;{Anomaly detection in the Zwicky Transient Facility DR3}&quot;,      journal = {\mnras},     keywords = {methods: data analysis, astronomical data bases: miscellaneous, stars: variables: general, Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},         year = 2021,        month = apr,       volume = {502},       number = {4},        pages = {5147-5175},          doi = {10.1093/mnras/stab316},archivePrefix = {arXiv},       eprint = {2012.01419}, primaryClass = {astro-ph.IM},       adsurl = {https://ui.adsabs.harvard.edu/abs/2021MNRAS.502.5147M},      adsnote = {Provided by the SAO/NASA Astrophysics Data System}}```</longdescription>
</pkgmetadata>