<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># What is this?I needed an efficient data logger for my machine learning experiments. Specifically one that- could log in a hierarchical way (not one big global logging variable)- while still having a flat table-like structure for performing queries/summaries- without having tons of duplicated dataThis library would likely work well with PySpark# What is a Use-case Example?Lets say you're going to perform- 3 experiments- each experiment has 10 episodes- each episode has 100,000 timesteps- there is an an `x` and a `y` value at each timestep &lt;br&gt;#### Example goal:- We want to get the average `x` value across all timesteps in episode 2 (we don't care what experiment they're from)Our timestamp data could look like:```pythonrecord1 = { &quot;x&quot;:1, &quot;y&quot;:1 } # first timesteprecord2 = { &quot;x&quot;:2, &quot;y&quot;:2 } # second timesteprecord3 = { &quot;x&quot;:3, &quot;y&quot;:3 } # third timestep```#### ProblemThose records don't contain the experiment number or the episode number (and we need those for our goal)#### Bad SolutionDuplicating the data would provide a flat structure, but (for 100,000 timesteps) thats a huge memory cost```pythonrecord1 = { &quot;x&quot;:1, &quot;y&quot;:1, &quot;episode&quot;:1, &quot;experiment&quot;: 1, } # first timesteprecord2 = { &quot;x&quot;:2, &quot;y&quot;:2, &quot;episode&quot;:1, &quot;experiment&quot;: 1, } # second timesteprecord3 = { &quot;x&quot;:3, &quot;y&quot;:3, &quot;episode&quot;:1, &quot;experiment&quot;: 1, } # third timestep```#### Good-ish SolutionWe could use references to be both more efficient and allow adding parent data after the fact```python# parent dataexperiment_data = { &quot;experiment&quot;: 1 }episode_data    = { &quot;episode&quot;:1, &quot;parent&quot;: experiment_data }record1 = { &quot;x&quot;:1, &quot;y&quot;:1, &quot;parent&quot;: episode_data } # first timesteprecord2 = { &quot;x&quot;:2, &quot;y&quot;:2, &quot;parent&quot;: episode_data } # second timesteprecord3 = { &quot;x&quot;:3, &quot;y&quot;:3, &quot;parent&quot;: episode_data } # third timestep```We could reduce the cost of key duplication by having shared keys```python# parent dataexperiment_data = { &quot;experiment&quot;: 1 }episode_data    = { &quot;episode&quot;:1, &quot;parent&quot;: experiment_data }episode_keeper = {&quot;parent&quot;: episode_data} # timestep 0episode_keeper = { &quot;x&quot;:[1],     &quot;y&quot;:[1],     &quot;parent&quot;: episode_data} # first timestep (keys added on-demand)episode_keeper = { &quot;x&quot;:[1,2],   &quot;y&quot;:[1,2],   &quot;parent&quot;: episode_data} # second timestepepisode_keeper = { &quot;x&quot;:[1,2,3], &quot;y&quot;:[1,2,3], &quot;parent&quot;: episode_data} # third timestep```#### How does Rigorous Recorder Fix This?The &quot;Good-ish Solution&quot; above is still crude, this library cleans it up1. The `Recorder` class in this library is the core/pure data structure2. The `ExperimentCollection` class automates common boilerplate for saving (python pickle), catching errors, managing experiments, etc```pythonfrom rigorous_recorder import Recorderrecorder = Recorder()# parent dataexperiment_recorder = Recorder(experiment=1).set_parent(recorder)episode_recorder    = Recorder(episode=1).set_parent(experiment_recorder)episode_recorder.push(x=1, y=1) # timestep1episode_recorder.push(x=2, y=2) # timestep2episode_recorder.push(x=3, y=3) # timestep3recorder.save_to(&quot;where/ever/you_want.pickle&quot;)```# How do I use this?`pip install rigorous-recorder````pythonfrom rigorous_recorder import RecordKeeper, ExperimentCollectionfrom statistics import mean as averagefrom random import random, sample, choicescollection = ExperimentCollection(&quot;data/my_study&quot;) # &lt;- this string is a filepath number_of_new_experiments = 1for _ in range(number_of_new_experiments):        # at the end (even when an error is thrown), all data is saved to disk automatically    # experiment number increments based on the last saved-to-disk experiment number    # running again (after error) won't double-increment the experiment number (same number until non-error run is achieved)    with collection.new_experiment() as experiment_recorder:        # we can create a hierarchy like this:        #         #                          experiment_recorder        #                           /              \        #               model1_recorder           model2_recorder        #                /        |                 |           \        # m1_train_recorder m1_test_recorder   m2_test_recorder m2_train_recorder        #         model1_recorder = RecordKeeper(model=&quot;model1&quot;).set_parent(experiment_recorder)        model2_recorder = RecordKeeper(model=&quot;model2&quot;).set_parent(experiment_recorder)                #         # training        #         model1_train_recorder = RecordKeeper(training=True).set_parent(model1_recorder)        model2_train_recorder = RecordKeeper(training=True).set_parent(model2_recorder)        for each_index in range(100_000):            # one approach            model1_train_recorder.push(index=each_index, loss=random())                        # alternative approach (same outcome)            model2_train_recorder.add(index=each_index)            # - this way is very handy for adding data in one method (like a loss func)            #   while calling .commit() in a different method (like update weights)            model2_train_recorder.add({ &quot;loss&quot;: random() })            model2_train_recorder.commit()                    #         # testing        #         model1_test_recorder = RecordKeeper(testing=True).set_parent(model1_recorder)        model2_test_recorder = RecordKeeper(testing=True).set_parent(model2_recorder)        for each_index in range(500):            # one method            model1_test_recorder.push(                index=each_index,                accuracy=random(),            )                        # alternative way (same outcome)            model2_test_recorder.add(index=each_index, accuracy=random())            model2_test_recorder.commit()# # # Analysis# # all_records = collection.recordsprint(&quot;first record&quot;, all_records[0]) # behaves just like a regular dictionary# slice across both models (first 500 training records from both models)records_first_half_of_time = tuple(each for each in all_records if each[&quot;training&quot;] and each[&quot;index&quot;] &lt; 500)# average loss across both modelsfirst_half_average_loss = average(tuple(each[&quot;loss&quot;] for each in records_first_half_of_time))# average only for model 1model1_first_half_loss = average(tuple(each[&quot;loss&quot;] for each in records_first_half_of_time if each[&quot;model&quot;] == &quot;model1&quot;))# average only for model 2model2_first_half_loss = average(tuple(each[&quot;loss&quot;] for each in records_first_half_of_time if each[&quot;model&quot;] == &quot;model2&quot;))```# What are some other details?The `ExperimentCollection` adds 6 keys as a parent to every record:```experiment_number     # interror_number          # int, is only incremented for back-to-back error runshad_error             # boolean for easy filteringexperiment_start_time # the output of time.time() from python's time moduleexperiment_end_time   # the output of time.time() from python's time moduleexperiment_duration   # the difference between start and end (for easy graphing/filtering)```</longdescription>
</pkgmetadata>