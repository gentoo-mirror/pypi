<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># delta_table_utilsDelta table utilities.The basic use case for this library is if you are working in Databricks and want to do upserts using [AutoLoader](https://docs.databricks.com/ingestion/auto-loader/index.html).Basic usage:```pythonfrom delta_table.delta_table_utils import DeltaTableColumn, DeltaTableschema_name = 'my_schema'table_name = 'my_table'# Define the delta table schemacolumn_list = [    DeltaTableColumn('id', data_type='STRING', nulls_allowed=False, is_unique_id=True),    DeltaTableColumn('col1', data_type='STRING', nulls_allowed=False),    DeltaTableColumn('col2', data_type='DOUBLE'),    DeltaTableColumn('col3', data_type='DOUBLE'),    DeltaTableColumn('col4', data_type='DOUBLE'),    DeltaTableColumn('created_at', data_type='TIMESTAMP'),    DeltaTableColumn('updated_at', data_type='TIMESTAMP')]# Create the DeltaTable objectdelta_table = DeltaTable(schema_name=schema_name, table_name=table_name, upload_path=&quot;&lt;location_of_data_in_s3&gt;&quot;, column_list=column_list)# Create the table and start the streamdelta_table.create_if_not_exists(sqlContext)delta_table.stream(spark, cloudFiles_format='csv')```## Additional notesBy default, when you use the `stream` method in this library, it stops as soon as no new data is detected. This is useful if you don't want a cluster running all the time and rather you just want to update your delta tables on some sort of a schedule.</longdescription>
</pkgmetadata>