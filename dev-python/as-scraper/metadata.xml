<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># as-scraper[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/as-scraper.svg)](https://pypi.org/project/as-scraper/)[![PyPI - Downloads](https://img.shields.io/pypi/dm/as-scraper)](https://pypi.org/project/as-scraper/)Python library for scraping using Selenium&gt; If you are looking for the library implemented inside airflow, go to https://github.com/Avila-Systems/as-scraper-airflow.# InstallationThe **as-scraper** library uses Geckodriver (Firefox) for scraping with the Selenium library.In order to use it, you need to have an Geckodriver dependency. Check the [selenium documentation](https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/) for details about how to install the Firefox browser driver.# Usage## Creating a simple scraperLets say that we want to scrap [yellowpages.com](https://www.yellowpages.com). Our target data would be the popular cities that we can find in the [sitemap](https://www.yellowpages.com/sitemap) url.Our output data will have two columns: `name` of the city and `url` which is linked to the city. For example, for *Houston*, we would want the following output:| name | url ||:-----|:----||Houston|https://www.yellowpages.com/houston-tx|### Declaring our Scraper ClassSo first we create a scraper that extends from the Scraper class, and define the `COLUMNS` variable to `['name', 'url']`.Create the *scrapers/yellowpages.py* file and type the following code into it:```pythonfrom as_scraper.scraper import Scraperclass YellowPagesScraper(Scraper):    COLUMNS = ['name', 'url']```### Deciding wether to load javascript or notNow, there are two execution options when running scrapers. We can either *load javascript* which uses the **Selenium** library, or not load javascript and use the *requests* library for http requests.For this example, let's go ahead and use the **Selenium** library. To configure this, simply add the following variable to your scraper:```pythonfrom as_scraper.scraper import Scraperclass YellowPagesScraper(Scraper):    COLUMNS = ['name', 'url']    LOAD_JAVASCRIPT = True```### Defining the `scrape_handler`And the magic comes in the next step. We will define the `scrape_handler` method in our class, which will have the responsibility to scrape a given url and extract the data from it.&gt; All scrapers must define the `scrape_handler` method.```pythonfrom typing import Optionalfrom selenium.webdriver import Firefoxfrom selenium.webdriver.common.by import Byimport pandas as pdfrom as_scraper.scraper import Scraperclass YellowPagesScraper(Scraper):    COLUMNS = ['name', 'url']    LOAD_JAVASCRIPT = True    def scrape_handler(self, url: str, html: Optional[str] = None, driver: Optional[Firefox] = None, **kwargs) -&gt; pd.DataFrame:        rows = []        div_tag = driver.find_element(By.CLASS_NAME, &quot;row-content&quot;)        div_tag = div_tag.find_element(By.CLASS_NAME, &quot;row&quot;)        section_tags = div_tag.find_elements(By.TAG_NAME, &quot;section&quot;)        for section_tag in section_tags:            a_tags = section_tag.find_elements(By.TAG_NAME, &quot;a&quot;)            for a_tag in a_tags:                city_name = a_tag.text                city_url = a_tag.get_attribute(&quot;href&quot;)                rows.append({&quot;name&quot;: city_name, &quot;url&quot;: city_url})        df = pd.DataFrame(rows, columns=self.COLUMNS)        return df```### ExecutionFinally, to execute the scraper you must call the **execute* method.```pythonfrom typing import Optionalfrom selenium.webdriver import Firefoxfrom selenium.webdriver.common.by import Byimport pandas as pdfrom as_scraper.scraper import Scraperclass YellowPagesScraper(Scraper):    COLUMNS = ['name', 'url']    LOAD_JAVASCRIPT = True    def scrape_handler(self, url: str, html: Optional[str] = None, driver: Optional[Firefox] = None, **kwargs) -&gt; pd.DataFrame:        rows = []        div_tag = driver.find_element(By.CLASS_NAME, &quot;row-content&quot;)        div_tag = div_tag.find_element(By.CLASS_NAME, &quot;row&quot;)        section_tags = div_tag.find_elements(By.TAG_NAME, &quot;section&quot;)        for section_tag in section_tags:            a_tags = section_tag.find_elements(By.TAG_NAME, &quot;a&quot;)            for a_tag in a_tags:                city_name = a_tag.text                city_url = a_tag.get_attribute(&quot;href&quot;)                rows.append({&quot;name&quot;: city_name, &quot;url&quot;: city_url})        df = pd.DataFrame(rows, columns=self.COLUMNS)        return dfif __name__ == '__main__':    urls = ['https://www.yellowpages.com/sitemap']    scraper = YellowPagesScraper(urls)    results, errors = scraper.execute()    print(results)    print(errors)```Now go ahead and run `python scrapers/yellowpages.py`. Have fun!</longdescription>
</pkgmetadata>