<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># NGBoost: Natural Gradient Boosting for Probabilistic Prediction&lt;h4 align=&quot;center&quot;&gt;![Python package](https://github.com/stanfordmlgroup/ngboost/workflows/Python%20package/badge.svg)[![GitHub Repo Size](https://img.shields.io/github/repo-size/stanfordmlgroup/ngboost?label=Repo+Size)](https://github.com/stanfordmlgroup/ngboost/graphs/contributors)[![Github License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![PyPI](https://img.shields.io/pypi/v/ngboost?logo=pypi&amp;logoColor=white)](https://pypi.org/project/ngboost)[![PyPI Downloads](https://img.shields.io/pypi/dm/ngboost?logo=icloud&amp;logoColor=white)](https://pypistats.org/packages/ngboost)&lt;/h4&gt;ngboost is a Python library that implements Natural Gradient Boosting, as described in [&quot;NGBoost: Natural Gradient Boosting for Probabilistic Prediction&quot;](https://stanfordmlgroup.github.io/projects/ngboost/). It is built on top of [Scikit-Learn](https://scikit-learn.org/stable/), and is designed to be scalable and modular with respect to choice of proper scoring rule, distribution, and base learner. A didactic introduction to the methodology underlying NGBoost is available in this [slide deck](https://docs.google.com/presentation/d/1Tn23Su0ygR6z11jy3xVNiLGv0ggiUQue/edit?usp=share_link&amp;ouid=102290675300480810195&amp;rtpof=true&amp;sd=true).## Installation```shvia pippip install --upgrade ngboostvia conda-forgeconda install -c conda-forge ngboost```## UsageProbabilistic regression example on the Boston housing dataset:```pythonfrom ngboost import NGBRegressorfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorX, Y = load_boston(True)X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)ngb = NGBRegressor().fit(X_train, Y_train)Y_preds = ngb.predict(X_test)Y_dists = ngb.pred_dist(X_test)# test Mean Squared Errortest_MSE = mean_squared_error(Y_preds, Y_test)print('Test MSE', test_MSE)# test Negative Log Likelihoodtest_NLL = -Y_dists.logpdf(Y_test).mean()print('Test NLL', test_NLL)```Details on available distributions, scoring rules, learners, tuning, and model interpretation are available in our [user guide](https://stanfordmlgroup.github.io/ngboost/intro.html), which also includes numerous usage examples and information on how to add new distributions or scores to NGBoost.## License[Apache License 2.0](https://github.com/stanfordmlgroup/ngboost/blob/master/LICENSE).## ReferenceTony Duan, Anand Avati, Daisy Yi Ding, Khanh K. Thai, Sanjay Basu, Andrew Y. Ng, Alejandro Schuler. 2019.NGBoost: Natural Gradient Boosting for Probabilistic Prediction.[arXiv](https://arxiv.org/abs/1910.03225)</longdescription>
</pkgmetadata>