<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># qabotQuery local or remote files with natural language queries powered byOpenAI's `gpt` and `duckdb` ðŸ¦†.Can query Wikidata and local files.## Command Line Usage```bash$ EXPORT OPENAI_API_KEY=sk-...$ EXPORT QABOT_MODEL_NAME=gpt-4$ qabot -w -q &quot;How many Hospitals are there located in Beijing&quot;Query: How many Hospitals are there located in BeijingThere are 39 hospitals located in Beijing.Total tokens 1749 approximate cost in USD: 0.05562```## Python Usage```pythonfrom qabot import ask_wikidata, ask_fileprint(ask_wikidata(&quot;How many hospitals are there in New Zealand?&quot;))print(ask_file(&quot;How many men were aboard the titanic?&quot;, 'data/titanic.csv'))```Output:```textThere are 54 hospitals in New Zealand.There were 577 male passengers on the Titanic.```## FeaturesWorks on local CSV files:![](.github/local_csv_query.png)remote CSV files:```$ qabot -f https://duckdb.org/data/holdings.csv -q &quot;Tell me how many Apple holdings I currently have&quot; ðŸ¦† Creating local DuckDB database... ðŸ¦† Loading data...create view 'holdings' as select * from 'https://duckdb.org/data/holdings.csv'; ðŸš€ Sending query to LLM ðŸ§‘ Tell me how many Apple holdings I currently have ðŸ¤– You currently have 32.23 shares of Apple.This information was obtained by summing up all the Apple ('APPL') shares in the holdings table.SELECT SUM(shares) as total_shares FROM holdings WHERE ticker = 'APPL'```Even on (public) data stored in S3:![](.github/external_s3_data.png)You can even load data from disk/URL via the natural language query:```qabot -q &quot;Load the file 'data/titanic.csv' into a table called 'raw_passengers'. Create a view of the raw passengers table for just the male passengers. What was the average fare for surviving male passengers?&quot; -v ðŸ¦† Creating local DuckDB database... ðŸ¤– Using model: gpt-3.5-turbo. Max LLM/function iterations before answer 20 ðŸš€ Sending query to LLM ðŸ§‘ Load the file 'data/titanic.csv' into a table called 'raw_passengers'. Create a view of the raw passengers table for just the male passengers. What was the average fare for surviving male passengers? ðŸ¤– load_data{'files': ['data/titanic.csv']} ðŸ¦† Imported with SQL:[&quot;create table 'titanic' as select * from 'data/titanic.csv';&quot;] ðŸ¤– show_tables ðŸ¦† nameqabot_queriestitanic ðŸ¤– describe_table{'table': 'titanic'} ðŸ¦† titaniccolumn_name,data_typePassengerId,BIGINTSurvived,BIGINTPclass,BIGINTName,VARCHARSex,VARCHARAge,DOUBLESibSp,BIGINTParch,BIGINTTicket,VARCHARFare,DOUBLECabin,VARCHAREmbarked,VARCHARselect count(*) from 'titanic';count_star()891select * from 'titanic' limit 3;PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked1,0,3,Braund, Mr. Owen Harris,male,22.0,1,0,A/5 21171,7.25,None,S2,1,1,Cumings, Mrs. John Bradley (Florence Briggs Thayer),female,38.0,1,0,PC 17599,71.2833,C85,C3,1,3,Heikkinen, Miss. Laina,female,26.0,0,0,STON/O2. 3101282,7.925,None,S ðŸ¤– execute_sql{'query': &quot;CREATE VIEW male_passengers AS SELECT * FROM titanic WHERE Sex = 'male'&quot;} ðŸ¦† No output ðŸ¤– describe_table{'table': 'male_passengers'} ðŸ¦† male_passengerscolumn_name,data_typePassengerId,BIGINTSurvived,BIGINTPclass,BIGINTName,VARCHARSex,VARCHARAge,DOUBLESibSp,BIGINTParch,BIGINTTicket,VARCHARFare,DOUBLECabin,VARCHAREmbarked,VARCHARselect count(*) from 'male_passengers';count_star()577select * from 'male_passengers' limit 3;PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked1,0,3,Braund, Mr. Owen Harris,male,22.0,1,0,A/5 21171,7.25,None,S5,0,3,Allen, Mr. William Henry,male,35.0,0,0,373450,8.05,None,S6,0,3,Moran, Mr. James,male,None,0,0,330877,8.4583,None,Q ðŸ¤– execute_sql{'query': 'SELECT AVG(Fare) AS average_fare FROM male_passengers WHERE Survived = 1'} ðŸ¦† average_fare40.82148440366974 ðŸ¦† {'summary': 'The average fare for surviving male passengers was $40.82.', 'detail': &quot;To calculate the average fare for surviving male passengers, I created a view called 'male_passengers' that contains only the male passengers from the 'titanic' table. Then, I executed the SQL query `SELECT AVG(Fare) AS average_fare FROM male_passengers WHERE Survived = 1` to calculate the average fare for the surviving male passengers.&quot;} ðŸš€ Question: ðŸ§‘ Load the file 'data/titanic.csv' into a table called 'raw_passengers'. Create a view of the raw passengers table for just the male passengers. What was the average fare for surviving male passengers? ðŸ¤– The average fare for surviving male passengers was $40.82.To calculate the average fare for surviving male passengers, I created a view called 'male_passengers' that contains only the male passengers from the 'titanic' table. Then, I executed the SQL query `SELECT AVG(Fare) AS average_fare FROM male_passengers WHERE Survived = 1` to calculate the average fare for the surviving male passengers.```## QuickstartYou need to set the `OPENAI_API_KEY` environment variable to your OpenAI API key, which you can get from [here](https://platform.openai.com/account/api-keys).Install the `qabot` command line tool using pip/poetry:```bash$ pip install -U qabot```Then run the `qabot` command with either local files (`-f my-file.csv`) or `-w` to query wikidata.## Examples### Local CSV file/s```bash$ qabot -q &quot;how many passengers survived by gender?&quot; -f data/titanic.csvðŸ¦† Loading data from files...Loading data/titanic.csv into table titanic...Query: how many passengers survived by gender?Result:There were 233 female passengers and 109 male passengers who survived. ðŸš€ any further questions? [y/n] (y): y ðŸš€ Query: what was the largest family who did not survive? Query: what was the largest family who did not survive?Result:The largest family who did not survive was the Sage family, with 8 members. ðŸš€ any further questions? [y/n] (y): n```## Query WikiDataUse the `-w` flag to query wikidata. For best results use the `gpt-4` model.```bash$ EXPORT QABOT_MODEL_NAME=gpt-4$ qabot -w -q &quot;How many Hospitals are there located in Beijing&quot;```## Intermediate steps and database queriesUse the `-v` flag to see the intermediate steps and database queries.Sometimes it takes a long route to get to the answer, but it's interesting to see how it gets there.```qabot -f data/titanic.csv -q &quot;how many passengers survived by gender?&quot; -v```## Data accessed via http/s3Use the `-f &lt;url&gt;` flag to load data from a url, e.g. a csv file on s3:```bash$ qabot -f s3://covid19-lake/enigma-jhu-timeseries/csv/jhu_csse_covid_19_timeseries_merged.csv -q &quot;how many confirmed cases of covid are there?&quot; -vðŸ¦† Loading data from files...create table jhu_csse_covid_19_timeseries_merged as select * from 's3://covid19-lake/enigma-jhu-timeseries/csv/jhu_csse_covid_19_timeseries_merged.csv';Result:264308334 confirmed cases```## Ideas- streaming mode to output results as they come in- token limits- Supervisor agent - assess whether a query is &quot;safe&quot; to run, could ask for user confirmation to run anything that gets flagged.- Often we can zero-shot the question and get a single query out - perhaps we try this before the MKL chain- test each zeroshot agent individually- Generate and pass back assumptions made to the user- Add an optional &quot;clarify&quot; tool to the chain that asks the user to clarify the question- Create a query checker tool that checks if the query looks valid and/or safe- Inject AWS credentials into duckdb so we can access private resources in S3- Automatic publishing to pypi. Look at https://blog.pypi.org/posts/2023-04-20-introducing-trusted-publishers/</longdescription>
</pkgmetadata>