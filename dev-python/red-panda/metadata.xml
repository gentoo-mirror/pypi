<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Red Panda![image](https://raw.githubusercontent.com/jucyai/red-panda/dev/red-panda.jpg)[![docs](https://readthedocs.org/projects/red-panda/badge/?version=dev)](https://red-panda.readthedocs.io/en/dev/?badge=dev)[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/jucyao/red-panda/blob/master/LICENSE)Easily interact with cloud (AWS) in your Data Science workflow.## Features- DataFrame/files to and from S3 and Redshift.- Run queries on Redshift in Python.- Use built-in Redshift admin queries, such as checking running queries and errors.- Use Redshift utility functions to easily accomplish common tasks such as creating a table.- Manage files on S3.- Query data on S3 directly with Athena.- Pandas DataFrame utility functions.## Installation```shpip install red-panda```## Using red-pandaImport `red-panda` and create an instance of `RedPanda`. If you create the instance with `dryrun=True` (i.e. `rp = RedPanda(redshift_conf, s3_conf, dryrun=True)`), `red-panda` will print the planned queries instead of executing them.```pythonfrom red_panda import RedPandaredshift_conf = {    &quot;user&quot;: &quot;awesome-developer&quot;,    &quot;password&quot;: &quot;strong-password&quot;,    &quot;host&quot;: &quot;awesome-domain.us-east-1.redshift.amazonaws.com&quot;,    &quot;port&quot;: 5432,    &quot;dbname&quot;: &quot;awesome-db&quot;,}aws_conf = {    &quot;aws_access_key_id&quot;: &quot;your-aws-access-key-id&quot;,    &quot;aws_secret_access_key&quot;: &quot;your-aws-secret-access-key&quot;,    # &quot;aws_session_token&quot;: &quot;temporary-token-if-you-have-one&quot;,}rp = RedPanda(redshift_conf, aws_conf)```Load your Pandas DataFrame into Redshift as a new table.```pythonimport pandas as pddf = pd.DataFrame(data={&quot;col1&quot;: [1, 2], &quot;col2&quot;: [3, 4]})s3_bucket = &quot;s3-bucket-name&quot;s3_path = &quot;parent-folder/child-folder&quot; # optional, if you don't have any sub folderss3_file_name = &quot;test.csv&quot; # optional, randomly generated if not providedrp.df_to_redshift(df, &quot;test_table&quot;, bucket=s3_bucket, path=s3_path, append=False)```It is also possible to:- Upload a DataFrame or flat file to S3.- Delete files from S3.- Load S3 data into Redshift.- Unload a Redshift query result to S3.- Obtain a Redshift query result as a DataFrame.- Run any query on Redshift.- Download S3 file to local.- Read S3 file in memory as DataFrame.- Run built-in Redshift admin queries, such as getting running query information.- Use utility functions such as `create_table` to quickly create tables in Redshift.- Run queries against S3 data directly with Athena using `AthenaUtils`.- Use features separately with `RedshiftUtils`, `S3Utils`, `AthenaUtils`.```pythons3_key = s3_path + &quot;/&quot; + s3_file_name# DataFrame uploaded to S3rp.df_to_s3(df, s3_bucket, s3_key)# Delete a file on S3rp.delete_from_s3(s3_bucket, s3_key)# Upload a local file to S3pd.to_csv(df, &quot;test_data.csv&quot;, index=False)rp.file_to_s3(&quot;test_data.csv&quot;, s3_bucket, s3_key)# Populate a Redshift table from S3 files# Use a dictionary for column definition, here we minimally define only data_typeredshift_column_definition = {    &quot;col1&quot;: {data_type: &quot;int&quot;},    &quot;col2&quot;: {data_type: &quot;int&quot;},}rp.s3_to_redshift(    s3_bucket, s3_key, &quot;test_table&quot;, column_definition=redshift_column_definition)# Unload Redshift query result to S3sql = &quot;select * from test_table&quot;rp.redshift_to_s3(sql, s3_bucket, s3_path+&quot;/unload&quot;, prefix=&quot;unloadtest_&quot;)# Obtain Redshift query result as a DataFramedf = rp.redshift_to_df(&quot;select * from test_table&quot;)# Run queries on Redshiftrp.run_query(&quot;create table test_table_copy as select * from test_table&quot;)# Download S3 file to localrp.s3_to_file(s3_bucket, s3_key, &quot;local_file_name.csv&quot;)# Read S3 file in memory as DataFramedf = rp.s3_to_df(s3_bucket, s3_key, delimiter=&quot;,&quot;) # csv file in this example# Since we are only going to use Redshift functionalities, we can just use RedshiftUtilsfrom red_panda.red_panda import RedshiftUtilsru = RedshiftUtils(redshift_conf)# Run built-in Redshift admin queries, such as getting running query informationload_errors = ru.get_load_error(as_df=True)# Use utility functions such as create_table to quickly create tables in Redshiftru.create_table(&quot;test_table&quot;, redshift_column_definition, sortkey=[&quot;col2&quot;], drop_first=True)```For full API documentation, visit &lt;https://red-panda.readthedocs.io/en/latest/&gt;.## TODOIn no particular order:- Support more data formats for copy. Currently only support delimited files.- Support more data formats for s3 to df. Currently only support delimited files.- Improve tests and docs.- Better ways of inferring data types from dataframe to Redshift.- Explore using `S3 Transfer Manager`'s `upload_fileobj` for `df_to_s3` to take advantage of automatic multipart upload.- Add COPY from S3 manifest file, in addition to COPY from S3 source path.- Support multi-cloud.- Take advantage of Redshift slices for parallel processing. Split files for COPY.</longdescription>
</pkgmetadata>