<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># TF2JAX![CI status](https://github.com/deepmind/tf2jax/workflows/ci/badge.svg)![pypi](https://img.shields.io/pypi/v/tf2jax)TF2JAX is an experimental library for converting [TensorFlow] functions/graphsto [JAX] functions.Specifically, it aims to transform a `tf.function`, e.g.```python@tf.functiondef tf_fn(x):  return tf.sin(tf.cos(x))```to a python function equivalent to the following JAX code.```pythondef jax_fn(x):  return jnp.sin(jnp.cos(x))```Users are able to apply additional JAX transforms (e.g. `jit`, `grad`, `vmap`,`make_jaxpr`, etc.) to the converted function as they would any other codewritten in JAX.[TOC]## InstallationYou can install the latest released version of TF2JAX from PyPI via:```shpip install tf2jax```or you can install the latest development version from GitHub:```shpip install git+https://github.com/deepmind/tf2jax.git```## MotivationsTF2JAX enables existing TensorFlow functions and models (including[SavedModel](https://www.tensorflow.org/guide/saved_model) and[TensorFlow Hub](https://www.tensorflow.org/hub/tf1_hub_module)) to be reusedand/or fine-tuned within JAX codebases. The conversion process is transparentto the users, which is useful for debugging and introspection.This also provide a pathway for JAX users to integrate JAX functions serializedvia `jax2tf.convert`, back into their existing JAX codebases.See [section](#alternatives) at the end for comparison with an alternativeapproach provided by `jax2tf.call_tf`.## DisclaimerThis is experimental code with potentially unstable API, and there are noguarantees for using it at this point in time. We highly recommend youthoroughly test the resulting JAX functions to ensure they meet yourrequirements.## Quick startThe rest of this document assumes the following imports:```pythonimport jaximport jax.numpy as jnpimport numpy as npimport tensorflow as tf  # Assumes this is v2.import tf2jax```An example using the `convert` API and the Sonnet v2 MLP.```pythonimport sonnet.v2 as sntmodel = snt.nets.MLP((64, 10,))@tf.functiondef forward(x):  return model(x)x = np.random.normal(size=(128, 16)).astype(np.float32)# TF -&gt; JAX, jax_params are the network parameters of the MLPjax_func, jax_params = tf2jax.convert(forward, np.zeros_like(x))# Call JAX, also return updated jax_params (e.g. variable, batchnorm stats)jax_outputs, jax_params = jax_func(jax_params, x)````tf2jax.convert` has the signature `convert(fn: tf.Function, *args, **kwargs)`,where `fn(*args, **kwargs)` is used to trace the function `fn` and generates thecorresponding `tf.GraphDef`. The `zeros_like` is not necessary, only used hereto demonstrate the JAX function is not memorizing the outputs.### Example with a pure functionIf your function is pure, i.e. it does not capture any variables, then you candrop the parameters from the inputs and outputs of the converted function with`tf2jax.convert_functional`.```python@tf.functiondef forward(x):  return tf.sin(tf.cos(x))jax_func = tf2jax.convert_functional(forward, np.zeros_like(x))jax_outputs = jax_func(x)```## Randomness and PRNG KeysA TensorFlow function that make use of random ops will be converted to a JAXfunction that takes a PRNG key as a keyword-only argument. TF2JAX willcomplain loudly if a PRNG key is required but not provided.```pythonjax_outputs, jax_params = jax_func(jax_params, x, rng=jax.random.PRNGKey(42))```## Custom GradientCustom gradient support is highly experimental, please report any errors.```python@tf.function@tf.custom_gradientdef forward(x):  e = tf.exp(x)  def grad(dy):    return dy * tf.sin(x) + e  # # This is deliberately the wrong gradient.  return tf.reduce_sum(e), gradwith tf2jax.override_config(&quot;convert_custom_gradient&quot;, True):  jax_func = tf2jax.convert_functional(forward, np.zeros_like(x))jax_grads = jax.grad(jax_func)(x)```## Support for Serialization Formats### SavedModel[SavedModel](https://www.tensorflow.org/guide/saved_model) is the preferredformat for serializing TF2 functions.```pythonmodel = tf.Module()model.f = forwardmodel.f(x)  # Dummy call.tf.saved_model.save(model, &quot;/tmp/blah&quot;)restored = tf.saved_model.load(&quot;/tmp/blah&quot;)jax_func, jax_params = tf2jax.convert(restored.f, np.zeros_like(x))```If the restored function has an unambiguous signature, i.e. it was only tracedonce prior to export. Then TF2JAX can convert the function directly from itsGraphDef without tracing it again.```pythonjax_func, jax_params = tf2jax.convert_from_restored(restored.f)```### TF-HubThe (legacy, TF1) [TF-Hub](https://www.tensorflow.org/hub/tf1_hub_module)format is supported with minor boilerplate.```pythonimport tensorflow_hub as hubhub_model = hub.load(&quot;/tmp/blah&quot;)jax_func, jax_params = tf2jax.convert(tf.function(hub_model), tf.zeros_like(x))jax_outputs, updated_jax_params = jax_func(jax_params, x)```## JAX to TensorFlow and Back Again.`tf2jax.convert_functional` can convert the outputs of `jax2tf.convert` backinto JAX code.```python# Some JAX function.def forward(*inputs):  ...# JAX -&gt; TFtf_func = jax2tf.convert(forward)# JAX -&gt; TF -&gt; JAXjax_func = tf2jax.convert_functional(tf.function(tf_func), *tree.map_structure(np.zeros_like, inputs))# JAX -&gt; TF -&gt; SavedModel -&gt; TFmodel = tf.Module()model.f = tf.function(tf_func)model.f(*tree.map_structure(tf.zeros_like, inputs))  # Dummy call.tf.saved_model.save(model, &quot;/tmp/blah&quot;)restored = tf.saved_model.load(&quot;/tmp/blah&quot;)# JAX -&gt; TF -&gt; SavedModel -&gt; TF -&gt; JAXjax_too_func = tf2jax.convert_functional(restored.f, *tree.map_structure(np.zeros_like, inputs))```## Additional ConfigurationThe behaviour of TF2JAX can be configured globally via `tf2jax.update_config`,or configured locally via the context manager `tf2jax.override_config`.### Strict shape and dtype checkingBy default, TF2JAX will assert that the input shapes to the converted functionare compatible with the input shapes of the original function. This is becausesome functions have shape dependent behaviours that will silently return theincorrect outputs after conversion, e.g. some batchnorm implementation.```pythonjax_func = tf2jax.convert_functional(forward, np.zeros((10, 5), np.float32))# This will raise an error.jax_func(np.zeros((20, 5), np.float32))# This will not.with tf2jax.override_config(&quot;strict_shape_check&quot;, False):  jax_func(np.zeros((20, 5), np.float32))```The input dtypes are not currently checked but this may change in the future.```pythonjax_func = tf2jax.convert_functional(forward, np.zeros((10, 5), np.float32))# This will not raise an error.jax_func(np.zeros((20, 5), np.int32))# This will.with tf2jax.override_config(&quot;strict_dtype_check&quot;, True):  jax_func(np.zeros((20, 5), np.int32))```### Convert constants to bfloat16TF2JAX allows users to trace the converted function with parameters and inputsof different precision than the original function, e.g. `bfloat16` instead of`float32`. To aid this, the configuration `force_const_float32_to_bfloat16`and `force_const_float64_to_bfloat16` can be used to force float constants inthe original function into `bfloat16` precision, to avoid accidental typepromotion.```python@tf.functiondef forward(x):  return x + tf.constant(3.14, dtype=tf.float32)with tf2jax.override_config(&quot;force_const_float32_to_bfloat16&quot;, True):  jax_func = tf2jax.convert_functional(forward, np.zeros_like(x))jax_bf16_outputs = jax_func(jnp.asarray(x, jnp.bfloat16))```### Disable PreventGradientIf `jax2tf.convert(..., with_gradient=False)` is used to produce the initial TFfunction (and possibly exported as SavedModel), then TF2JAX will respect theinserted `tf.raw_ops.PreventGradient` ops and raise `LookupError` when computinggradients.This can be disabled by setting the configuration `raise_on_prevent_gradient` tofalse (default is true), so that TF2JAX will only log a warning but otherwiseallow the gradient to be computed as though the `PreventGradient` ops were notpresent.```python@tf.functiondef prevent(x):  return tf.raw_ops.PreventGradient(input=x * x, message=&quot;prevented&quot;)jax_func = tf2jax.convert_functional(prevent, 0.0)jax.grad(jax_func)(3.14)  # Raise LookupError.with tf2jax.config.override_config(&quot;raise_on_prevent_gradient&quot;, False):  jax_func = tf2jax.convert_functional(prevent, 0.0)g = jax.grad(jax_func)(3.14)  # Returns 6.28.```### Infer Cumulative ReductionsIf the `infer_cumulative_reduction_from_jax2tf` flag is true (default) thenTF2JAX will attempt to infer `cummax`, `cummin`, `cumprod` and `cumsum`operations from `reduce_window` operations generated by JAX2TF. This providesbetter performance because `reduce_window` implementation of these ops haveO(N^2) complexity on CPU and GPU backends, and can suffer from long compilationtimes due to aggressive constant folding.See [jax2tf_cumulative_reduction] for more context.## LimitationsCurrently, only a subset of TensorFlow ops are supported, and not necessarilyall functionalities are supported for some ops. The code will fail fast. Supportfor additional TensorFlow ops are added on a as-needed basis. Please submit yourrequests via Github issues or send in your pull requests.There will likely to be some cases where the resulting JAX code is notequivalent to the TensorFlow code, both in terms of performance and numericaloutputs. The goal is to minimise differences in the latter for model endpoints,ahead of improving performance.TF2 control flows are supported with some limitations, e.g. for while loops,the `cond` and `body` functions cannot have side effects such as assigning tovariables.TF1 control flows are not supported.## Alternatives### `jax2tf.call_tf``jax2tf` now also offers the experimental `call_tf` function which allows JAX tocall TensorFlow functions. For compiled code, this works by staging outTensorFlow functions to XLA.From the [jax2tf documentation], as of 2022-07-22:&gt; The function `call_tf` allows JAX functions to call TensorFlow functions.&gt; These functions can be called anywhere in a JAX computation, including in&gt; staging contexts `jax.jit`, `jax.pmap`, `jax.xmap`, or inside JAX's&gt; control-flow primitives. In non-staging contexts, the TensorFlow function is&gt; called in eager mode. For now, only reverse-mode autodiff is supported for&gt; these functions (no forward-mode autodiff, nor `vmap`).The advantage of `call_tf` is that it implicitly covers all TensorFlow ops andsupports `custom_gradient` by deferring to TensorFlow during eager execution andto XLA for compiled code.The disadvantage is that it only supports a limited set of JAX transforms(`jit`, `grad`, `pmap`, `remat`) and otherwise appears as a &quot;black box&quot; toJAX (e.g. `vmap` is not supported, nor custom transforms). A TensorFlow functionmust be compileable to XLA if it is to be jitted after `call_tf`.## Citing TF2JAXThis repository is part of the [DeepMind JAX Ecosystem], to cite TF2JAX pleaseuse the [DeepMind JAX Ecosystem citation].## ContributingWe are happy to receive pull requests that improve our coverage of TensorFlowops.[DeepMind JAX Ecosystem]: https://deepmind.com/blog/article/using-jax-to-accelerate-our-research &quot;DeepMind JAX Ecosystem&quot;[DeepMind JAX Ecosystem citation]: https://github.com/deepmind/jax/blob/main/deepmind2020jax.txt &quot;Citation&quot;[JAX]: https://github.com/google/jax &quot;JAX on GitHub&quot;[TensorFlow]: https://github.com/tensorflow/tensorflow &quot;TensorFlow on GitHub&quot;[jax2tf documentation]: https://github.com/google/jax/blob/master/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax &quot;jax2tf documentation&quot;[jax2tf_cumulative_reduction]: https://github.com/google/jax/blob/main/jax/experimental/jax2tf/jax2tf.py#L2172</longdescription>
</pkgmetadata>