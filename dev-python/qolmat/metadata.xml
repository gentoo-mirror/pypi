<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>hlbotterman@quantmetry.com,jroussel@quantmetry.com,tmorzadec@quantmetry.com,rhajou@quantmetry.com,fdakhli@quantmetry.comLicense: new BSDClassifier: Intended Audience :: Science/ResearchClassifier: Intended Audience :: DevelopersClassifier: License :: OSI ApprovedClassifier: Topic :: Software DevelopmentClassifier: Topic :: Scientific/EngineeringClassifier: Operating System :: Microsoft :: WindowsClassifier: Operating System :: POSIXClassifier: Operating System :: UnixClassifier: Operating System :: MacOSClassifier: Programming Language :: Python :: 3.7Classifier: Programming Language :: Python :: 3.8Classifier: Programming Language :: Python :: 3.9Classifier: Programming Language :: Python :: 3.10Requires-Python: &gt;=3.8Description-Content-Type: text/x-rstProvides-Extra: testsProvides-Extra: docs##############################################RPCA for anomaly detection and data imputation##############################################What is robust principal component analysis?============================================Robust Principal Component Analysis (RPCA) is a modification of thestatistical procedure of `principal component analysis(PCA) &lt;https://en.wikipedia.org/wiki/Principal_component_analysis&gt;`__which allows to work with grossly corrupted observations.Suppose we are given a large data matrix :math:`\mathbf{D}`, and knowthat it may be decomposed as.. math::   \mathbf{D} = \mathbf{X}^* + \mathbf{A}^*where :math:`\mathbf{X}^*` has low-rank and :math:`\mathbf{A}^*` issparse. We do not know the low-dimensional column and row space of:math:`\mathbf{X}^*`, not even their dimension. Similarly, for thenon-zero entries of :math:`\mathbf{A}^*`, we do not know theirlocation, magnitude or even their number. Are the low-rank and sparseparts possible to recover both *accurately* and *efficiently*?Of course, for the separation problem to make sense, the low-rank partcannot be sparse and analogously, the sparse part cannot be low-rank.See `here &lt;https://arxiv.org/abs/0912.3599&gt;`__ for more details.Formally, the problem is expressed as.. math::   \begin{align*}   &amp; \text{minimise} \quad \text{rank} (\mathbf{X}) + \lambda \Vert \mathbf{A} \Vert_0 \\   &amp; \text{s.t.} \quad \mathbf{D} = \mathbf{X} + \mathbf{A}   \end{align*}Unfortunately this optimization problem is a NP-hard problem due to itsnonconvexity and discontinuity. So then, a widely used solving scheme isreplacing rank(:math:`\mathbf{X}`) by its convex envelope —the nuclearnorm :math:`\Vert \mathbf{X} \Vert_*`— and the :math:`\ell_0`penalty is replaced with the :math:`\ell_1`-norm, which is good atmodeling the sparse noise and has high efficient solution. Therefore,the problem becomes.. math::   \begin{align*}   &amp; \text{minimise} \quad \Vert \mathbf{X} \Vert_* + \lambda \Vert \mathbf{A} \Vert_1 \\   &amp; \text{s.t.} \quad \mathbf{D} = \mathbf{X} + \mathbf{A}   \end{align*}Theoretically, this is guaranteed to work even if the rank of:math:`\mathbf{X}^*` grows almost linearly in the dimension of thematrix, and the errors in :math:`\mathbf{A}^*` are up to a constantfraction of all entries. Algorithmically, the above problem can besolved by efficient and scalable algorithms, at a cost not so muchhigher than the classical PCA. Empirically, a number of simulations andexperiments suggest this works under surprisingly broad conditions formany types of real data.Some examples of real-life applications are background modelling fromvideo surveillance, face recognition, speech recognition. We here focuson anomaly detection in time series.What’s in this repo?====================Some classes are implemented:**RPCA** class based on `RPCA &lt;https://arxiv.org/abs/0912.3599&gt;`_ p.29... math::   \begin{align*}   &amp; \text{minimise} \quad \Vert \mathbf{X} \Vert_* + \lambda \Vert \mathbf{A} \Vert_1 \\   &amp; \text{s.t.} \quad \mathbf{D} = \mathbf{X} + \mathbf{A}   \end{align*}**GraphRPCA** class based on  `GraphRPCA &lt;https://arxiv.org/abs/1507.08173&gt;`_... math::   \begin{align*}   &amp; \text{minimise} \quad  \Vert \mathbf{A} \Vert_1 + \gamma_1 \text{tr}(\mathbf{X} \mathbf{\mathcal{L}_1} \mathbf{X}^T) + \gamma_2 \text{tr}(\mathbf{X}^T \mathbf{\mathcal{L}_2} \mathbf{X}) \\   &amp; \text{s.t.} \quad \mathbf{D} = \mathbf{X} + \mathbf{A}   \end{align*}**TemporalRPCA** class based on  `Link 1 &lt;https://arxiv.org/abs/2001.05484&gt;`__ and this `Link 2 &lt;https://www.hindawi.com/journals/jat/2018/7191549/&gt;`__). The optimisation problem is the following.. math::   \text{minimise} \quad \Vert P_{\Omega}(\mathbf{X}+\mathbf{A}-\mathbf{D}) \Vert_F^2 + \lambda_1 \Vert \mathbf{X} \Vert_* + \lambda_2 \Vert \mathbf{A} \Vert_1 + \sum_{k=1}^K \eta_k \Vert \mathbf{XH_k} \Vert_pwhere :math:`\Vert \mathbf{XH_k} \Vert_p` is either :math:`\Vert \mathbf{XH_k} \Vert_1` or  :math:`\Vert \mathbf{XH_k} \Vert_F^2`.The operator :math:`P_{\Omega}` is the projection operator such that:math:`P_{\Omega}(\mathbf{M})` is the projection of:math:`\mathbf{M}` on the set of observed data :math:`\Omega`. Thisallows to deal with missing values. Each of these classes is adapted totake as input either a time series or a matrix directly. If a timeseries is passed, a pre-processing is done.See the examples folder for a first overview of the implemented classes.Installation============Install directly from the gitlab repository:Contributing============Feel free to open an issue or contact us at pnom@quantmetry.comReferences==========[1] Candès, Emmanuel J., et al. “Robust principal component analysis?.”Journal of the ACM (JACM) 58.3 (2011): 1-37,(`pdf &lt;https://arxiv.org/abs/0912.3599&gt;`__)[2] Wang, Xuehui, et al. “An improved robust principal componentanalysis model for anomalies detection of subway passenger flow.”Journal of advanced transportation 2018 (2018).(`pdf &lt;https://www.hindawi.com/journals/jat/2018/7191549/&gt;`__)[3] Chen, Yuxin, et al. “Bridging convex and nonconvex optimization inrobust PCA: Noise, outliers, and missing data.” arXiv preprintarXiv:2001.05484 (2020), (`pdf &lt;https://arxiv.org/abs/2001.05484&gt;`__)[4] Shahid, Nauman, et al. “Fast robust PCA on graphs.” IEEE Journal ofSelected Topics in Signal Processing 10.4 (2016): 740-756.(`pdf &lt;https://arxiv.org/abs/1507.08173&gt;`__)</longdescription>
</pkgmetadata>