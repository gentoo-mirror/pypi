<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Welcome to DYNAMAX!![Logo](https://raw.githubusercontent.com/probml/dynamax/main/logo/logo.gif)![Test Status](https://github.com/probml/dynamax/actions/workflows/run_tests.yml/badge.svg?branch=main)Dynamax is a library for probabilistic state space models (SSMs) writtenin [JAX](https://github.com/google/jax). It has code for inference(state estimation) and learning (parameter estimation) in a variety ofSSMs, including:-   Hidden Markov Models (HMMs)-   Linear Gaussian State Space Models (aka Linear Dynamical Systems)-   Nonlinear Gaussian State Space Models-   Generalized Gaussian State Space Models (with non-Gaussian emission    models)The library consists of a set of core, functionally pure, low-levelinference algorithms, as well as a set of model classes which provide amore user-friendly, object-oriented interface. It is compatible withother libraries in the JAX ecosystem, such as[optax](https://github.com/deepmind/optax) (used for estimatingparameters using stochastic gradient descent), and[Blackjax](https://github.com/blackjax-devs/blackjax) (used forcomputing the parameter posterior using Hamiltonian Monte Carlo (HMC) orsequential Monte Carlo (SMC)).## DocumentationFor tutorials and API documentation, see: https://probml.github.io/dynamax/.## Installation and TestingTo install the latest releast of dynamax from PyPi:``` {.console}pip install dynamax                 # Install dynamax and core dependencies, orpip install dynamax[notebooks]      # Install with demo notebook dependencies```To install the latest development branch:``` {.console}pip install git+https://github.com/probml/dynamax.git```Finally, if you\'re a developer, you can install dynamax along with thetest and documentation dependencies with:``` {.console}git clone git@github.com:probml/dynamax.gitcd dynamaxpip install -e '.[dev]'```To run the tests:``` {.console}pytest dynamax                         # Run all testspytest dynamax/hmm/inference_test.py   # Run a specific testpytest -k lgssm                        # Run tests with lgssm in the name```## What are state space models?A state space model or SSM is a partially observed Markov model, inwhich the hidden state, $z_t$, evolves over time according to a Markovprocess, possibly conditional on external inputs / controls /covariates, $u_t$, and generates an observation, $y_t$. This isillustrated in the graphical model below.&lt;p align=&quot;center&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/probml/dynamax/main/docs/figures/LDS-UZY.png&quot;&gt;&lt;/p&gt;The corresponding joint distribution has the following form (in dynamax,we restrict attention to discrete time systems):$$p(y_{1:T}, z_{1:T} | u_{1:T}) = p(z_1 | u_1) p(y_1 | z_1, u_1) \prod_{t=1}^T p(z_t | z_{t-1}, u_t) p(y_t | z_t, u_t)$$Here $p(z_t | z_{t-1}, u_t)$ is called the transition or dynamics model,and $p(y_t | z_{t}, u_t)$ is called the observation or emission model.In both cases, the inputs $u_t$ are optional; furthermore, theobservation model may have auto-regressive dependencies, in which casewe write $p(y_t | z_{t}, u_t, y_{1:t-1})$.We assume that we see the observations $y_{1:T}$, and want to infer thehidden states, either using online filtering (i.e., computing$p(z_t|y_{1:t})$ ) or offline smoothing (i.e., computing$p(z_t|y_{1:T})$ ). We may also be interested in predicting futurestates, $p(z_{t+h}|y_{1:t})$, or future observations,$p(y_{t+h}|y_{1:t})$, where h is the forecast horizon. (Note that byusing a hidden state to represent the past observations, the model canhave \&quot;infinite\&quot; memory, unlike a standard auto-regressive model.) Allof these computations can be done efficiently using our library, as wediscuss below. In addition, we can estimate the parameters of thetransition and emission models, as we discuss below.More information can be found in these books:&gt; -   \&quot;Machine Learning: Advanced Topics\&quot;, K. Murphy, MIT Press 2023.&gt;     Available at &lt;https://probml.github.io/pml-book/book2.html&gt;.&gt; -   \&quot;Bayesian Filtering and Smoothing\&quot;, S. Särkkä, Cambridge&gt;     University Press, 2013. Available at&gt;     &lt;https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf&gt;## Example usageDynamax includes classes for many kinds of SSM. You can use these modelsto simulate data, and you can fit the models using standard learningalgorithms like expectation-maximization (EM) and stochastic gradientdescent (SGD). Below we illustrate the high level (object-oriented) APIfor the case of an HMM with Gaussian emissions. (See [thisnotebook](https://github.com/probml/dynamax/blob/main/docs/notebooks/hmm/gaussian_hmm.ipynb)for a runnable version of this code.)```pythonimport jax.numpy as jnpimport jax.random as jrimport matplotlib.pyplot as pltfrom dynamax.hidden_markov_model import GaussianHMMkey1, key2, key3 = jr.split(jr.PRNGKey(0), 3)num_states = 3emission_dim = 2num_timesteps = 1000# Make a Gaussian HMM and sample data from ithmm = GaussianHMM(num_states, emission_dim)true_params, _ = hmm.initialize(key1)true_states, emissions = hmm.sample(true_params, key2, num_timesteps)# Make a new Gaussian HMM and fit it with EMparams, props = hmm.initialize(key3, method=&quot;kmeans&quot;, emissions=emissions)params, lls = hmm.fit_em(params, props, emissions, num_iters=20)# Plot the marginal log probs across EM iterationsplt.plot(lls)plt.xlabel(&quot;EM iterations&quot;)plt.ylabel(&quot;marginal log prob.&quot;)# Use fitted model for posterior inferencepost = hmm.smoother(params, emissions)print(post.smoothed_probs.shape) # (1000, 3)```JAX allows you to easily vectorize these operations with `vmap`.For example, you can sample and fit to a batch of emissions as shown below.```pythonfrom functools import partialfrom jax import vmapnum_seq = 200batch_true_states, batch_emissions = \    vmap(partial(hmm.sample, true_params, num_timesteps=num_timesteps))(        jr.split(key2, num_seq))print(batch_true_states.shape, batch_emissions.shape) # (200,1000) and (200,1000,2)# Make a new Gaussian HMM and fit it with EMparams, props = hmm.initialize(key3, method=&quot;kmeans&quot;, emissions=batch_emissions)params, lls = hmm.fit_em(params, props, batch_emissions, num_iters=20)```These examples demonstrate the dynamax models, but we can also call the low-levelinference code directly.## ContributingPlease see [this page](https://github.com/probml/dynamax/blob/main/CONTRIBUTING.md) for detailson how to contribute.## AboutCore team: Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, Kevin Murphy.Other contributors: Adrien Corenflos, Elizabeth DuPre, Gerardo Duran-Martin, Colin Schlager, Libby Zhang and other people [listed here](https://github.com/probml/dynamax/graphs/contributors)MIT License. 2022</longdescription>
</pkgmetadata>