<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>aiohttp-s3-client================[![PyPI - License](https://img.shields.io/pypi/l/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![Wheel](https://img.shields.io/pypi/wheel/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![Mypy](http://www.mypy-lang.org/static/mypy_badge.svg)]() [![PyPI](https://img.shields.io/pypi/v/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![PyPI](https://img.shields.io/pypi/pyversions/aiohttp-s3-client)](https://pypi.org/project/aiohttp-s3-client) [![Coverage Status](https://coveralls.io/repos/github/mosquito/aiohttp-s3-client/badge.svg?branch=master)](https://coveralls.io/github/mosquito/aiohttp-s3-client?branch=master) ![tox](https://github.com/mosquito/aiohttp-s3-client/workflows/tox/badge.svg?branch=master)The simple module for putting and getting object from Amazon S3 compatible endpoints## Installation```bashpip install aiohttp-s3-client```## Usage```pythonfrom http import HTTPStatusfrom aiohttp import ClientSessionfrom aiohttp_s3_client import S3Clientasync with ClientSession(raise_for_status=True) as session:    client = S3Client(        url=&quot;http://s3-url&quot;,        session=session,        access_key_id=&quot;key-id&quot;,        secret_access_key=&quot;hackme&quot;,        region=&quot;us-east-1&quot;    )    # Upload str object to bucket &quot;bucket&quot; and key &quot;str&quot;    async with client.put(&quot;bucket/str&quot;, &quot;hello, world&quot;) as resp:        assert resp.status == HTTPStatus.OK    # Upload bytes object to bucket &quot;bucket&quot; and key &quot;bytes&quot;    async with await client.put(&quot;bucket/bytes&quot;, b&quot;hello, world&quot;) as resp:        assert resp.status == HTTPStatus.OK    # Upload AsyncIterable to bucket &quot;bucket&quot; and key &quot;iterable&quot;    async def gen():        yield b'some bytes'    async with client.put(&quot;bucket/file&quot;, gen()) as resp:        assert resp.status == HTTPStatus.OK    # Upload file to bucket &quot;bucket&quot; and key &quot;file&quot;    async with client.put_file(&quot;bucket/file&quot;, &quot;/path_to_file&quot;) as resp:        assert resp.status == HTTPStatus.OK    # Check object exists using bucket+key    async with client.head(&quot;bucket/key&quot;) as resp:        assert resp == HTTPStatus.OK    # Get object by bucket+key    async with client.get(&quot;bucket/key&quot;) as resp:        data = await resp.read()    # Delete object using bucket+key    async with client.delete(&quot;bucket/key&quot;) as resp:        assert resp == HTTPStatus.NO_CONTENT    # List objects by prefix    async for result in client.list_objects_v2(&quot;bucket/&quot;, prefix=&quot;prefix&quot;):        # Each result is a list of metadata objects representing an object        # stored in the bucket.        do_work(result)```Bucket may be specified as subdomain or in object name:```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientclient = S3Client(url=&quot;http://bucket.your-s3-host&quot;,                  session=aiohttp.ClientSession())async with client.put(&quot;key&quot;, gen()) as resp:    ...client = S3Client(url=&quot;http://your-s3-host&quot;,                  session=aiohttp.ClientSession())async with await client.put(&quot;bucket/key&quot;, gen()) as resp:    ...client = S3Client(url=&quot;http://your-s3-host/bucket&quot;,                  session=aiohttp.ClientSession())async with client.put(&quot;key&quot;, gen()) as resp:    ...```Auth may be specified with keywords or in URL:```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientclient_credentials_as_kw = S3Client(    url=&quot;http://your-s3-host&quot;,    access_key_id=&quot;key_id&quot;,    secret_access_key=&quot;access_key&quot;,    session=aiohttp.ClientSession(),)client_credentials_in_url = S3Client(    url=&quot;http://key_id:access_key@your-s3-host&quot;,    session=aiohttp.ClientSession(),)```## CredentialsBy default `S3Client` trying to collect all available credentials from keywordarguments like `access_key_id=` and `secret_access_key=`, after that from theusername and password from passed `url` argument, so the nex step is environmentvariables parsing and the last source for collection is the config file.You can pass credentials explicitly using `aiohttp_s3_client.credentials`module.### `aiohttp_s3_client.credentials.StaticCredentials````pythonimport aiohttpfrom aiohttp_s3_client import S3Clientfrom aiohttp_s3_client.credentials import StaticCredentialscredentials = StaticCredentials(    access_key_id='aaaa',    secret_access_key='bbbb',    region='us-east-1',)client = S3Client(    url=&quot;http://your-s3-host&quot;,    session=aiohttp.ClientSession(),    credentials=credentials,)```### `aiohttp_s3_client.credentials.URLCredentials````pythonimport aiohttpfrom aiohttp_s3_client import S3Clientfrom aiohttp_s3_client.credentials import URLCredentialsurl = &quot;http://key@hack-me:your-s3-host&quot;credentials = URLCredentials(url, region=&quot;us-east-1&quot;)client = S3Client(    url=&quot;http://your-s3-host&quot;,    session=aiohttp.ClientSession(),    credentials=credentials,)```### `aiohttp_s3_client.credentials.EnvironmentCredentials````pythonimport aiohttpfrom aiohttp_s3_client import S3Clientfrom aiohttp_s3_client.credentials import EnvironmentCredentialscredentials = EnvironmentCredentials(region=&quot;us-east-1&quot;)client = S3Client(    url=&quot;http://your-s3-host&quot;,    session=aiohttp.ClientSession(),    credentials=credentials,)```### `aiohttp_s3_client.credentials.ConfigCredentials`Using user config file:```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientfrom aiohttp_s3_client.credentials import ConfigCredentialscredentials = ConfigCredentials()   # Will be used ~/.aws/credentials configclient = S3Client(    url=&quot;http://your-s3-host&quot;,    session=aiohttp.ClientSession(),    credentials=credentials,)```Using the custom config location:```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientfrom aiohttp_s3_client.credentials import ConfigCredentialscredentials = ConfigCredentials(&quot;~/.my-custom-aws-credentials&quot;)client = S3Client(    url=&quot;http://your-s3-host&quot;,    session=aiohttp.ClientSession(),    credentials=credentials,)```### `aiohttp_s3_client.credentials.merge_credentials`This function collect all passed credentials instances and return a new onewhich contains all non-blank fields from passed instances. The first argumenthas more priority.```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientfrom aiohttp_s3_client.credentials import (    ConfigCredentials, EnvironmentCredentials, merge_credentials)credentials = merge_credentials(    EnvironmentCredentials(),    ConfigCredentials(),)client = S3Client(    url=&quot;http://your-s3-host&quot;,    session=aiohttp.ClientSession(),    credentials=credentials,)```### `aiohttp_s3_client.credentials.MetadataCredentials`Trying to get credentials from the metadata service:```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientfrom aiohttp_s3_client.credentials import MetadataCredentialscredentials = MetadataCredentials()# start refresh credentials from metadata serverawait credentials.start()client = S3Client(    url=&quot;http://your-s3-host&quot;,    session=aiohttp.ClientSession(),)await credentials.stop()```## Multipart uploadFor uploading large files [multipart uploading](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)can be used. It allows you to asynchronously upload multiple parts of a fileto S3.S3Client handles retries of part uploads and calculates part hash for integrity checks.```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientclient = S3Client(url=&quot;http://your-s3-host&quot;, session=aiohttp.ClientSession())await client.put_file_multipart(    &quot;test/bigfile.csv&quot;,    headers={        &quot;Content-Type&quot;: &quot;text/csv&quot;,    },    workers_count=8,)```## Parallel download to fileS3 supports `GET` requests with `Range` header. It's possible to downloadobjects in parallel with multiple connections for speedup.S3Client handles retries of partial requests and makes sure that file won'tbe changed during download with `ETag` header.If your system supports `pwrite` syscall (Linux, macOS, etc.) it will be used towrite simultaneously to a single file. Otherwise, each worker will have own filewhich will be concatenated after downloading.```pythonimport aiohttpfrom aiohttp_s3_client import S3Clientclient = S3Client(url=&quot;http://your-s3-host&quot;, session=aiohttp.ClientSession())await client.get_file_parallel(    &quot;dump/bigfile.csv&quot;,    &quot;/home/user/bigfile.csv&quot;,    workers_count=8,)```</longdescription>
</pkgmetadata>