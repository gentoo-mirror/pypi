<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;  Intel¬Æ Extension for Transformers===========================&lt;h3&gt;An Innovative Transformer-based Toolkit to Accelerate GenAI/LLM Everywhere&lt;/h3&gt;[![](https://dcbadge.vercel.app/api/server/Wxk3J3ZJkU?compact=true&amp;style=flat-square)](https://discord.gg/Wxk3J3ZJkU)[![Release Notes](https://img.shields.io/github/v/release/intel/intel-extension-for-transformers)](https://github.com/intel/intel-extension-for-transformers/releases)[üè≠Architecture](./docs/architecture.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üí¨NeuralChat](./intel_extension_for_transformers/neural_chat)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üòÉInference](./intel_extension_for_transformers/llm/runtime/graph)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üíªExamples](./docs/examples.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[üìñDocumentations](https://intel.github.io/intel-extension-for-transformers/latest/docs/Welcome.html)&lt;/div&gt;## üöÄLatest News* [2023/10] LLM runtime, an Intel-optimized [GGML](https://github.com/ggerganov/ggml) compatible runtime, demonstrates **up to 15x performance gain in 1st token generation and 1.5x in other token generation** over the default [llama.cpp](https://github.com/ggerganov/llama.cpp).* [2023/10] LLM runtime now supports LLM inference with **infinite-length inputs up to 4 million tokens**, inspired from [StreamingLLM](https://arxiv.org/abs/2309.17453).* [2023/09] NeuralChat has been showcased in [**Intel Innovation‚Äô23 Keynote**](https://www.youtube.com/watch?v=RbKRELWP9y8&amp;t=2954s) and [Google Cloud Next'23](https://cloud.google.com/blog/topics/google-cloud-next/welcome-to-google-cloud-next-23) to demonstrate GenAI/LLM capabilities on Intel Xeon Scalable Processors.* [2023/08] NeuralChat supports **custom chatbot development and deployment within minutes** on broad Intel HWs such as Xeon Scalable Processors, Gaudi2,¬†Xeon CPU Max Series,¬†Data Center GPU Max Series, Arc Series, and Core Processors. Check out [Notebooks](./intel_extension_for_transformers/neural_chat/docs/full_notebooks.md).* [2023/07] LLM runtime extends Hugging Face Transformers API to provide seamless low precision inference for popular LLMs, supporting low precision data types such as INT3/INT4/FP4/NF4/INT5/INT8/FP8.---&lt;div align=&quot;left&quot;&gt;## üèÉInstallation### Quick Install from Pypi```bashpip install intel-extension-for-transformers```&gt; For more installation methods, please refer to [Installation Page](./docs/installation.md)## üåüIntroductionIntel¬Æ Extension for Transformers is an innovative toolkit to accelerate Transformer-based models on Intel platforms, in particular, effective on 4th Intel Xeon Scalable processor¬†Sapphire Rapids (codenamed [Sapphire Rapids](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html)). The toolkit provides the below key features and examples:*  Seamless user experience of model compressions on Transformer-based models by extending [Hugging Face transformers](https://github.com/huggingface/transformers)¬†APIs and leveraging [Intel¬Æ Neural Compressor](https://github.com/intel/neural-compressor)*  Advanced software optimizations and unique compression-aware runtime (released with NeurIPS 2022's paper [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) and [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114), and NeurIPS 2021's paper [Prune Once for All: Sparse Pre-Trained Language Models](https://arxiv.org/abs/2111.05754))*  Optimized Transformer-based model packages such as [Stable Diffusion](examples/huggingface/pytorch/text-to-image/deployment/stable_diffusion), [GPT-J-6B](examples/huggingface/pytorch/text-generation/deployment), [GPT-NEOX](examples/huggingface/pytorch/language-modeling/quantization#2-validated-model-list), [BLOOM-176B](examples/huggingface/pytorch/language-modeling/inference#BLOOM-176B), [T5](examples/huggingface/pytorch/summarization/quantization#2-validated-model-list), [Flan-T5](examples/huggingface/pytorch/summarization/quantization#2-validated-model-list), and end-to-end workflows such as [SetFit-based text classification](docs/tutorials/pytorch/text-classification/SetFit_model_compression_AGNews.ipynb) and [document level sentiment analysis (DLSA)](workflows/dlsa) *  [NeuralChat](intel_extension_for_transformers/neural_chat), a customizable chatbot framework to create your own chatbot within minutes by leveraging a rich set of plugins [Knowledge Retrieval](./intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md), [Speech Interaction](./intel_extension_for_transformers/neural_chat/pipeline/plugins/audio/README.md), [Query Caching](./intel_extension_for_transformers/neural_chat/pipeline/plugins/caching/README.md), [Security Guardrail](./intel_extension_for_transformers/neural_chat/pipeline/plugins/security/README.md).*  [Inference](intel_extension_for_transformers/llm/runtime/graph) of Large Language Model (LLM) in pure C/C++ with weight-only quantization kernels, supporting [GPT-NEOX](intel_extension_for_transformers/llm/runtime/graph/models/gptneox), [LLAMA](intel_extension_for_transformers/llm/runtime/graph/models/llama), [MPT](intel_extension_for_transformers/llm/runtime/graph/models/mpt), [FALCON](intel_extension_for_transformers/llm/runtime/graph/models/falcon), [BLOOM-7B](intel_extension_for_transformers/llm/runtime/graph/models/bloom), [OPT](intel_extension_for_transformers/llm/runtime/graph/models/opt), [ChatGLM2-6B](intel_extension_for_transformers/llm/runtime/graph/models/chatglm), [GPT-J-6B](intel_extension_for_transformers/llm/runtime/graph/models/gptj) and [Dolly-v2-3B](intel_extension_for_transformers/llm/runtime/graph/models/gptneox)## üå±Getting StartedBelow is the sample code to enable the chatbot. See more [examples](intel_extension_for_transformers/neural_chat/docs/full_notebooks.md).### Chatbot ```python# pip install intel-extension-for-transformersfrom intel_extension_for_transformers.neural_chat import build_chatbotchatbot = build_chatbot()response = chatbot.predict(&quot;Tell me about Intel Xeon Scalable Processors.&quot;)```Below is the sample code to enable weight-only INT4/INT8 inference. See more [examples](intel_extension_for_transformers/llm/runtime/graph).### INT4 Inference ```pythonfrom transformers import AutoTokenizer, TextStreamerfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfigmodel_name = &quot;Intel/neural-chat-7b-v1-1&quot;     # Hugging Face model_id or local modelconfig = WeightOnlyQuantConfig(compute_dtype=&quot;int8&quot;, weight_dtype=&quot;int4&quot;)prompt = &quot;Once upon a time, there existed a little girl,&quot;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_idsstreamer = TextStreamer(tokenizer)model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)```### INT8 Inference```pythonfrom transformers import AutoTokenizer, TextStreamerfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfigmodel_name = &quot;Intel/neural-chat-7b-v1-1&quot;     # Hugging Face model_id or local modelconfig = WeightOnlyQuantConfig(compute_dtype=&quot;bf16&quot;, weight_dtype=&quot;int8&quot;)prompt = &quot;Once upon a time, there existed a little girl,&quot;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_idsstreamer = TextStreamer(tokenizer)model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)```## üéØValidated  ModelsYou can access the latest int4 performance and accuracy at [int4 blog](https://medium.com/@NeuralCompressor/llm-performance-of-intel-extension-for-transformers-f7d061556176).Additionally, we are preparing to introduce Baichuan, Mistral, and other models into [LLM Runtime (Intel Optimized llamacpp)](./intel_extension_for_transformers/llm/runtime/graph). For comprehensive accuracy and performance data, though not the most up-to-date, please refer to the [Release data](./docs/release_data.md). ## üìñDocumentation&lt;table&gt;&lt;thead&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;OVERVIEW&lt;/th&gt;  &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;  &lt;tr&gt;    &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/neural_chat&quot;&gt;NeuralChat&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/graph&quot;&gt;LLM Runtime&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;NEURALCHAT&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_spr.ipynb&quot;&gt;Chatbot on Intel CPU&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;3&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_xpu.ipynb&quot;&gt;Chatbot on Intel GPU&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;3&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/neural_chat/docs/notebooks/deploy_chatbot_on_habana_gaudi.ipynb&quot;&gt;Chatbot on Gaudi&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/neural_chat/examples/talkingbot_pc/build_talkingbot_on_pc.ipynb&quot;&gt;Chatbot on Client&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/neural_chat/docs/full_notebooks.md&quot;&gt;More Notebooks&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;LLM RUNTIME&lt;/th&gt;  &lt;/tr&gt; &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/graph/README.md&quot;&gt;LLM Runtime&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/graph/README.md#2-run-llm-with-python-api&quot;&gt;Streaming LLM&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/graph/core/README.md&quot;&gt;Low Precision Kernels&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/graph/tensor_parallelism.md&quot;&gt;Tensor Parallelism&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;LLM COMPRESSION&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/smoothquant.md&quot;&gt;SmoothQuant (INT8)&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;3&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/weightonlyquant.md&quot;&gt;Weight-only Quantization (INT4/FP4/NF4/INT8)&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;3&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/qloracpu.md&quot;&gt;QLoRA on CPU&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;GENERAL COMPRESSION&lt;/th&gt;  &lt;tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/quantization.md&quot;&gt;Quantization&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/pruning.md&quot;&gt;Pruning&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/distillation.md&quot;&gt;Distillation&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/text-classification/orchestrate_optimizations/README.md&quot;&gt;Orchestration&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/language-modeling/nas/README.md&quot;&gt;Neural Architecture Search&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/export.md&quot;&gt;Export&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/metrics.md&quot;&gt;Metrics&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/objectives.md&quot;&gt;Objectives&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/pipeline.md&quot;&gt;Pipeline&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/question-answering/dynamic/README.md&quot;&gt;Length Adaptive&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/examples.md#early-exit&quot;&gt;Early Exit&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/data_augmentation.md&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/td&gt;      &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;TUTORIALS &amp; RESULTS&lt;/a&gt;&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/tutorials/pytorch&quot;&gt;Tutorials&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/graph#supported-models&quot;&gt;LLM List&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/examples.md&quot;&gt;General Model List&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.md&quot;&gt;Model Performance&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;## üôåDemo* Infinite inference (up to 4M tokens)https://github.com/intel/intel-extension-for-transformers/assets/109187816/1698dcda-c9ec-4f44-b159-f4e9d67ab15b## üìÉSelected Publications/Events* Blog published on Medium: [NeuralChat: Simplifying Supervised Instruction Fine-tuning and Reinforcement Aligning for Chatbots](https://medium.com/@NeuralCompressor/neuralchat-simplifying-supervised-instruction-fine-tuning-and-reinforcement-aligning-for-chatbots-d034bca44f69) (Sep 2023)* Intel Innovation'23 Keynote: [Intel Innovation 2023 Keynote by Greg Lavender](https://www.youtube.com/watch?v=RbKRELWP9y8&amp;t=2954s) (Sep 2023)* Blog on Intel Community: [NeuralChat: A Customizable Chatbot Framework](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/NeuralChat-A-Customizable-Chatbot-Framework/post/1526789) (Sep 2023)* Blog published on Medium: [NeuralChat: A Customizable Chatbot Framework](https://medium.com/intel-analytics-software/make-your-own-chatbot-within-a-few-minutes-with-neuralchat-a-customizable-chatbot-framework-139b4bdec8d1) (Sep 2023)* Blog published on Medium: [Faster Stable Diffusion Inference with Intel Extension for Transformers](https://medium.com/intel-analytics-software/faster-stable-diffusion-inference-with-intel-extension-for-transformers-on-intel-platforms-7e0f563186b0) (July 2023)* Blog of Intel Developer News: [The Moat Is Trust, Or Maybe Just Responsible AI](https://www.intel.com/content/www/us/en/developer/articles/technical/moat-is-trust-minimizing-risks-generative-ai.html) (July 2023)* Blog of Intel Developer News: [Create Your Own Custom Chatbot](https://www.intel.com/content/www/us/en/developer/articles/technical/train-large-language-models-create-custom-chatbot.html) (July 2023)* Blog of Intel Developer News: [Accelerate Llama 2 with Intel AI Hardware and Software Optimizations](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html) (July 2023)* Arxiv: [An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs](https://arxiv.org/abs/2306.16601) (June 2023)* Blog published on Medium: [Simplify Your Custom Chatbot Deployment](https://medium.com/intel-analytics-software/simplify-your-custom-chatbot-deployment-on-intel-platforms-c8a911d906cf) (June 2023)&gt; View [Full Publication List](./docs/publication.md).## Additional Content* [Release Information](./docs/release.md)* [Contribution Guidelines](./docs/contributions.md)* [Legal Information](./docs/legal.md)* [Security Policy](SECURITY.md)## Acknowledgements* Excellent open-source projects: [bitsandbytes](https://github.com/TimDettmers/bitsandbytes), [FastChat](https://github.com/lm-sys/FastChat), [fastRAG](https://github.com/IntelLabs/fastRAG), [ggml](https://github.com/ggerganov/ggml), [gptq](https://github.com/IST-DASLab/gptq), [llama.cpp](https://github.com/ggerganov/llama.cpp), [lm-evauation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [peft](https://github.com/huggingface/peft), [trl](https://github.com/huggingface/trl), [streamingllm](https://github.com/mit-han-lab/streaming-llm) and many others.* Thanks to all the contributors including [Ikko Eltociear Ashimine](https://github.com/eltociear), [Hardik Kamboj](https://github.com/hardikkamboj), [Sangjune Park](https://github.com/JJukE), [Kevin Ta](https://github.com/kta-intel), [Huiyan Cao](https://github.com/huiyan2021), [Xigui Wang](https://github.com/xiguiw), [Jiafu Zhang](https://github.com/jiafuzha), [Tyler Titsworth](https://github.com/tylertitsworth), [Yi Wang](https://github.com/sywangyi), [Samanway Sadhu](https://github.com/SamanwaySadhu), [Jiqing Feng](https://github.com/jiqing-feng), [Jonathan Mamou](https://github.com/jmamou) and [Niroop Ammbashankar](https://github.com/nammbash).## üíÅCollaborationsWelcome to raise any interesting ideas on model compression techniques and LLM-based chatbot development! Feel free to reach [us](mailto:itrex.maintainers@intel.com), and we look forward to our collaborations on Intel Extension for Transformers!</longdescription>
</pkgmetadata>