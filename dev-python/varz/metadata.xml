<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># [Varz](http://github.com/wesselb/varz)[![CI](https://github.com/wesselb/varz/workflows/CI/badge.svg?branch=master)](https://github.com/wesselb/varz/actions?query=workflow%3ACI)[![Coverage Status](https://coveralls.io/repos/github/wesselb/varz/badge.svg?branch=master&amp;service=github)](https://coveralls.io/github/wesselb/varz?branch=master)[![Latest Docs](https://img.shields.io/badge/docs-latest-blue.svg)](https://wesselb.github.io/varz)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)Painless optimisation of constrained variables in AutoGrad, TensorFlow, PyTorch, and JAX* [Requirements and Installation](#requirements-and-installation)* [Manual](#manual)    - [Basics](#basics)    - [Naming](#naming)    - [Constrained Variables](#constrained-variables)    - [Automatic Naming of Variables](#automatic-naming-of-variables)        - [Sequential Specification](#sequential-specification)        - [Parametrised Specification](#parametrised-specification)        - [Namespaces](#namespaces)        - [Structlike Specification](#structlike-specification)    - [Optimisers](#optimisers)    - [PyTorch Specifics](#pytorch-specifics)    - [Getting and Setting Latent Representations of Variables as a Vector](#getting-and-setting-latent-representations-of-variables-as-a-vector)    - [Getting Variables from a Source](#get-variables-from-a-source) * [Examples](#examples)    - [Minimise a Function Using L-BFGS-B in AutoGrad](#minimise-a-function-using-l-bfgs-b-in-autograd)    - [Minimise a Function Using L-BFGS-B in TensorFlow](#minimise-a-function-using-l-bfgs-b-in-tensorflow)    - [Minimise a Function Using L-BFGS-B in PyTorch](#minimise-a-function-using-l-bfgs-b-in-pytorch)    - [Minimise a Function Using L-BFGS-B in JAX](#minimise-a-function-using-l-bfgs-b-in-jax)    - [Tracking the Learning Curve in JAX](#tracking-the-learning-curve-in-jax)    ## Requirements and InstallationSee [the instructions here](https://gist.github.com/wesselb/4b44bf87f3789425f96e26c4308d0adc).Then simply```bashpip install varz```## Manual### Basics```pythonfrom varz import Vars```To begin with, create a *variable container* of the right data type.For use with AutoGrad, use a `np.*` data type;for use with PyTorch, use a `torch.*` data type;for use with TensorFlow, use a `tf.*` data type;and for use with JAX, use a `jnp.*` data type.In this example we'll use AutoGrad.```python&gt;&gt;&gt; vs = Vars(np.float64)```Now a variable can be created by requesting it, giving it an initial value anda name. ```python&gt;&gt;&gt; vs.unbounded(np.random.randn(2, 2), name=&quot;x&quot;)array([[ 1.04404354, -1.98478763],       [ 1.14176728, -3.2915562 ]])```If the same variable is created again, because a variable with the name `x` already exists, the existing variable will be returned, even if you again pass it aninitial value.```python&gt;&gt;&gt; vs.unbounded(np.random.randn(2, 2), name=&quot;x&quot;)array([[ 1.04404354, -1.98478763],       [ 1.14176728, -3.2915562 ]])&gt;&gt;&gt; vs.unbounded(name=&quot;x&quot;)array([[ 1.04404354, -1.98478763],       [ 1.14176728, -3.2915562 ]])```Alternatively, indexing syntax may be used to get the existing variable `x`.This asserts that a variable with the name `x` already exists and will throw a`KeyError` otherwise.```python&gt;&gt;&gt; vs[&quot;x&quot;]array([[ 1.04404354, -1.98478763],       [ 1.14176728, -3.2915562 ]])       &gt;&gt;&gt; vs[&quot;y&quot;]KeyError: 'y'```The value of `x` can be changed by assigning it a different value.```python&gt;&gt;&gt; vs.assign(&quot;x&quot;, np.random.randn(2, 2))array([[ 1.43477728,  0.51006941],       [-0.74686452, -1.05285767]])```By default, assignment is non-differentiable and _overwrites_ data.The variable can be deleted by passing its name to `vs.delete`:```python&gt;&gt;&gt; vs.delete(&quot;x&quot;)&gt;&gt;&gt; vs[&quot;x&quot;]KeyError: 'x'```When a variable is first created, you can set the keyword argument `visible`to `False` if you want to make the variable invisible to thevariable-aggregating operations [`vs.get_latent_vars`](#constrained-variables)and[`vs.get_latent_vector`](#getting-and-setting-latent-representations-of-variables-as-a-vector).These variable-aggregating operations are used in optimisers to get the intendedcollection of variable to optimise.Therefore, setting `visible` to `False` will prevent a variable from beingoptimised.Finally, a variable container can be copied with `vs.copy()`.Copies are lightweight and _share their variables with the originals_.As a consequence, however, assignment in a copy will also mutate the original.[Differentiable assignment, however, will not.](#differentiable-assignment)### NamingVariables may be organised by naming them hierarchically using `.`s. For example, you could name like `group1.bar`, `group1.foo`, and `group2.bar`.This is helpful for extracting collections of variables, where wildcards may be used to match names.For example, `*.bar` would match `group1.bar` and `group2.bar`, and `group1.*` would match `group1.bar` and `group1.foo`.See also [here](#getting-and-setting-latent-representations-of-variables-as-a-vector).The names of all variables can be obtained with `Vars.names`, and variables can be printed with `Vars.print`.Example:```python&gt;&gt;&gt; vs = Vars(np.float64)&gt;&gt;&gt; vs.unbounded(1, name=&quot;x1&quot;)array(1.)&gt;&gt;&gt; vs.unbounded(2, name=&quot;x2&quot;)array(2.)&gt;&gt;&gt; vs.unbounded(3, name=&quot;y&quot;)array(3.)&gt;&gt;&gt; vs.names['x1', 'x2', 'y']&gt;&gt;&gt; vs.print()x1:         1.0x2:         2.0y:          3.0```### Constrained Variables* **Unbounded variables:**  A variable that is unbounded can be created using  `Vars.unbounded` or `Vars.ubnd`.    ```python    &gt;&gt;&gt; vs.ubnd(name=&quot;normal_variable&quot;)    0.016925610008314832    ```* **Positive variables:**    A variable that is constrained to be *positive* can be created using    `Vars.positive` or `Vars.pos`.    ```python    &gt;&gt;&gt; vs.pos(name=&quot;positive_variable&quot;)    0.016925610008314832    ```* **Bounded variables:**    A variable that is constrained to be *bounded* can be created using    `Vars.bounded` or `Vars.bnd`.    ```python    &gt;&gt;&gt; vs.bnd(name=&quot;bounded_variable&quot;, lower=1, upper=2)    1.646772663807718    ```  * **Lower-triangular matrix:**    A matrix variable that is constrained to be *lower triangular* can be    created using `Vars.lower_triangular` or `Vars.tril`. Either an    initialisation or a shape of square matrix must be given.        ```python    &gt;&gt;&gt; vs.tril(shape=(2, 2), name=&quot;lower_triangular&quot;)    array([[ 2.64204459,  0.        ],           [-0.14055559, -1.91298679]])    ```  * **Positive-definite matrix:**    A matrix variable that is contrained to be *positive definite* can be    created using `Vars.positive_definite` or `Vars.pd`. Either an    initialisation or a shape of square matrix must be given.        ```python    &gt;&gt;&gt; vs.pd(shape=(2, 2), name=&quot;positive_definite&quot;)    array([[ 1.64097496, -0.52302151],           [-0.52302151,  0.32628302]])    ```  * **Orthogonal matrix:**    A matrix variable that is constrained to be *orthogonal* can be created using    `Vars.orthogonal` or `Vars.orth`. Either an initialisation or a    shape of square matrix must be given.        ```python    &gt;&gt;&gt; vs.orth(shape=(2, 2), name=&quot;orthogonal&quot;)    array([[ 0.31290403, -0.94978475],           [ 0.94978475,  0.31290403]])    ```These constrained variables are created by transforming some *latent unconstrained representation* to the desired constrained space.The latent variables can be obtained using `Vars.get_latent_vars`.```python&gt;&gt;&gt; vs.get_latent_vars(&quot;positive_variable&quot;, &quot;bounded_variable&quot;)[array(-4.07892742), array(-0.604883)]```To illustrate the use of wildcards, the following is equivalent:```python&gt;&gt;&gt; vs.get_latent_vars(&quot;*_variable&quot;)[array(-4.07892742), array(-0.604883)]```Variables can be excluded by prepending a dash:```python&gt;&gt;&gt; vs.get_latent_vars(&quot;*_variable&quot;, &quot;-bounded_*&quot;)[array(-4.07892742)]```### Automatic Naming of VariablesTo parametrise functions, a common pattern is the following:```pythondef objective(vs):    x = vs.unbounded(5, name=&quot;x&quot;)    y = vs.unbounded(10, name=&quot;y&quot;)        return (x * y - 5) ** 2 + x ** 2```The names for `x` and `y` are necessary, because otherwise new variables will be created and initialised every time `objective` is run.Varz offers two ways to not having to specify a name for every variable: sequential and parametrised specification.#### Sequential SpecificationSequential specification can be used if, upon execution of `objective`, variables are always obtained in the *same order*.This means that variables can be identified with their position in this orderand hence be named accordingly.To use sequential specification, decorate the function with `sequential`.Example:```pythonfrom varz import sequential@sequentialdef objective(vs):    x = vs.unbounded(5)  # Initialise to 5.    y = vs.unbounded()   # Initialise randomly.        return (x * y - 5) ** 2 + x ** 2``````python&gt;&gt;&gt; vs = Vars(np.float64)&gt;&gt;&gt; objective(vs)68.65047879833773&gt;&gt;&gt; objective(vs)  # Running the objective again reuses the same variables.68.65047879833773&gt;&gt;&gt; vs.names['var0', 'var1']&gt;&gt;&gt; vs.print()var0:       5.0      # This is `x`.var1:       -0.3214  # This is `y`.```#### Parametrised SpecificationSequential specification still suffers from boilerplate code like`x = vs.unbounded(5)` and `y = vs.unbounded()`.This is the problem that parametrised specification addresses, which allows you to specify variables as *arguments to your function*.Import `from varz.spec import parametrised`.To indicate that an argument of the function is a variable, as opposed to a regular argument, the argument's type hint must be set accordingly, as follows:* **Unbounded variables:**    ```python    @parametrised    def f(vs, x: Unbounded):        ...    ```* **Positive variables:**    ```python    @parametrised    def f(vs, x: Positive):        ...    ```* **Bounded variables:**    The following two specifications are possible. The former uses the    default bounds and the latter uses specified bounds.         ```python    @parametrised    def f(vs, x: Bounded):        ...    ```      ```python    @parametrised    def f(vs, x: Bounded(lower=1, upper=10)):        ...    ```    * **Lower-triangular variables:**    ```python    @parametrised    def f(vs, x: LowerTriangular(shape=(2, 2))):        ...    ```* **Positive-definite variables:**    ```python    @parametrised    def f(vs, x: PositiveDefinite(shape=(2, 2))):        ...    ```  * **Orthogonal variables:**    ```python    @parametrised    def f(vs, x: Orthogonal(shape=(2, 2))):        ...    ```    As can be seen from the above, the variable container must also be an argument of the function, because that is where the variables will be obtained from.A variable can be given an initial value in the way you would expect:```python@parametriseddef f(vs, x: Unbounded = 5):    ...```Variable arguments and regular arguments can be mixed.If `f` is called, variable arguments must not be specified, because they will be obtained automatically.Regular arguments, however, must be specified.To use parametrised specification, decorate the function with `parametrised`.Example:```pythonfrom varz import parametrised, Unbounded, Bounded@parametriseddef objective(vs, x: Unbounded, y: Bounded(lower=1, upper=3) = 2, option=None):    print(&quot;Option:&quot;, option)    return (x * y - 5) ** 2 + x ** 2``````python&gt;&gt;&gt; vs = Vars(np.float64)&gt;&gt;&gt; objective(vs)Option: None9.757481795615316&gt;&gt;&gt; objective(vs, &quot;other&quot;)Option: other9.757481795615316&gt;&gt;&gt; objective(vs, option=&quot;other&quot;)Option: other9.757481795615316&gt;&gt;&gt; objective(vs, x=5)  # This is not valid, because `x` will be obtained automatically from `vs`.ValueError: 1 keyword argument(s) not parsed: x.&gt;&gt;&gt; vs.print()x:          1.025y:          2.0```#### NamespacesNamespaces can be used to group all variables in a function together.Example:```pythonfrom varz import namespace@namespace(&quot;test&quot;)def objective(vs):    x = vs.unbounded(5, name=&quot;x&quot;)    y = vs.unbounded(name=&quot;y&quot;)        return x + y``````python&gt;&gt;&gt; vs = Vars(np.float64)&gt;&gt;&gt; objective(vs)6.12448906632577&gt;&gt;&gt; vs.names['test.x', 'test.y']&gt;&gt;&gt; vs.print()test.x:     5.0test.y:     1.124```You can combine namespace with other specification methods:```pythonfrom varz import namespace@namespace(&quot;test&quot;)@sequentialdef objective(vs):    x = vs.unbounded(5)    y = vs.unbounded()        return x + y``````python&gt;&gt;&gt; vs = Vars(np.float64)&gt;&gt;&gt; objective(vs)4.812730329303665&gt;&gt;&gt; vs.names['test.var0', 'test.var1']&gt;&gt;&gt; vs.print()test.var0:  5.0test.var1:  -0.1873```#### Structlike SpecificationFor any variable container `vs`, `vs.struct` gives an object which you can treat likenested struct, list, or dictionary to automatically generate variable names.For example, `vs.struct.model[&quot;a&quot;].variance.positive()` would be equivalent to`vs.positive(name=&quot;model[a].variance&quot;)`.After variables have been defined in this way, they also be extracted via `vs.struct`:`vs.struct.model[&quot;a&quot;].variance()` would be equivalent to `vs[&quot;model[a].variance&quot;]`.Example:```pythondef objective(vs):    params = vs.struct        x = params.x.unbounded()    y = params.y.unbounded()        for model_params, model in zip(params.models, [object(), object(), object()]):        model_params.specific_parameter1.positive()        model_params.specific_parameter2.positive()        return x + y``````python&gt;&gt;&gt; vs = Vars(np.float64)&gt;&gt;&gt; objective(vs)-0.08322955725015702&gt;&gt;&gt; vs.names['x', 'y', 'models[0].specific_parameter1', 'models[0].specific_parameter2', 'models[1].specific_parameter1', 'models[1].specific_parameter2', 'models[2].specific_parameter1', 'models[2].specific_parameter2']&gt;&gt;&gt; vs.print()x:          -0.8963y:          0.8131models[0].specific_parameter1: 0.01855models[0].specific_parameter2: 0.6644models[1].specific_parameter1: 0.3542models[1].specific_parameter2: 0.3642models[2].specific_parameter1: 0.5807models[2].specific_parameter2: 0.5977&gt;&gt;&gt; vs.struct.models[0].specific_parameter1()0.018551827512328086&gt;&gt;&gt; vs.struct.models[0].specific_parameter2()0.6643533007198247```There are a few methods available for convenient manipulation of the variable struct.In the following, let `params = vs.struct`.* _Go up a directory_:    `params.a.b.c.up()` goes up one directory and gives `params.a.b`.    If you want to be sure about which directory you are going up, you can pass    the name of the directory you want to go up as an argument:    `params.a.b.c.up(&quot;c&quot;)` will give the intended result, but    `params.a.b.c.up(&quot;b&quot;)` will result in an assertion error.* _Get all variables in a path_:    `params.a.all()` gives the regex `a.*`.* _Check if a variable exists_:    `bool(params.a)` gives `True` if `a` is a defined variable and `False`    otherwise.* _Assign a value to a variable_:    `params.a.assign(1)` assigns `1` to `a`.* _Delete a variable_:    `params.a.delete()` deletes `a`.### OptimisersThe following optimisers are available:```varz.{autograd,tensorflow,torch,jax}.minimise_l_bfgs_b (L-BFGS-B)varz.{autograd,tensorflow,torch,jax}.minimise_adam     (ADAM)```The L-BFGS-B algorithm is recommended for deterministic objectives and ADAMis recommended for stochastic objectives.See the examples for an illustration of how these optimisers canbe used.Some commonly used keyword arguments are as follows:| Keyword Argument | Description || - | - || `iters` | Number of iterations || `trace` | Show progress || `jit` | Use a JIT to compile the gradient |See the API for a detailed description of the keyword arguments that theseoptimisers accept.### PyTorch SpecificsAll the variables held by a container can be detached from the current computation graph with `Vars.detach` .To make a copy of the container with detached versions of the variables, use`Vars.copy` with `detach=True` instead.Whether variables require gradients can be configured with `Vars.requires_grad`.By default, no variable requires a gradient.### Getting and Setting Latent Representations of Variables as a VectorIt may be desirable to get the latent representations of a collection of variables as a single vector, e.g. when feeding them to an optimiser.This can be achieved with `Vars.get_latent_vector`.```python&gt;&gt;&gt; vs.get_latent_vector(&quot;x&quot;, &quot;*_variable&quot;)array([0.12500578, -0.21510423, -0.61336039, 1.23074066, -4.07892742,       -0.604883])```Similarly, to update the latent representation of a collection of variables,`Vars.set_latent_vector` can be used.```python&gt;&gt;&gt; vs.set_latent_vector(np.ones(6), &quot;x&quot;, &quot;*_variable&quot;)[array([[1., 1.],        [1., 1.]]), array(1.), array(1.)]&gt;&gt;&gt; vs.get_latent_vector(&quot;x&quot;, &quot;*_variable&quot;)array([1., 1., 1., 1., 1., 1.])```#### Differentiable AssignmentBy default, `Vars.set_latent_vector` will overwrite the variables, just like`Vars.assign`.This has as an unfortunate consequence that you cannot differentiate with respect tothe assigned values.To be able to differentiable with respect to the assigned values, set the keyword`differentiable=True` in the call to `Vars.set_latent_vector`.Unlike regular assignment, if the variable container is a copy of some original,differentiable assignment will not mutate the variables in the original.### Get Variables from a SourceThe keyword argument `source` can set to a tensor from which the latent variables will be obtained.Example:```python&gt;&gt;&gt; vs = Vars(np.float32, source=np.array([1, 2, 3, 4, 5]))&gt;&gt;&gt; vs.unbounded()array(1., dtype=float32)&gt;&gt;&gt; vs.unbounded(shape=(3,))array([2., 3., 4.], dtype=float32)&gt;&gt;&gt; vs.pos()148.41316&gt;&gt;&gt; np.exp(5).astype(np.float32)148.41316```## GPU SupportTo create and optimise variables on a GPU,[set the active device to a GPU](https://github.com/wesselb/lab#devices).The easiest way of doing this is to `import lab as B` and`B.set_global_device(&quot;gpu:0&quot;)`.## Examples### Minimise a Function Using L-BFGS-B in AutoGrad```pythonimport autograd.numpy as npfrom varz.autograd import Vars, minimise_l_bfgs_btarget = 5.0 def objective(vs):    # Get a variable named &quot;x&quot;, which must be positive, initialised to 10.    x = vs.pos(10.0, name=&quot;x&quot;)      return (x ** 0.5 - target) ** 2  ``````python&gt;&gt;&gt; vs = Vars(np.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, vs)3.17785950743424e-19  # Final objective function value.&gt;&gt;&gt; vs['x'] - target ** 2-5.637250666268301e-09```### Minimise a Function Using L-BFGS-B in TensorFlow```pythonimport tensorflow as tffrom varz.tensorflow import Vars, minimise_l_bfgs_btarget = 5.0def objective(vs):    # Get a variable named &quot;x&quot;, which must be positive, initialised to 10.    x = vs.pos(10.0, name=&quot;x&quot;)      return (x ** 0.5 - target) ** 2  ``````python&gt;&gt;&gt; vs = Vars(tf.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, vs)3.17785950743424e-19  # Final objective function value.&gt;&gt;&gt; vs['x'] - target ** 2&lt;tf.Tensor: id=562, shape=(), dtype=float64, numpy=-5.637250666268301e-09&gt;&gt;&gt;&gt; vs = Vars(tf.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, vs, jit=True)  # Speed up optimisation with TF's JIT!3.17785950743424e-19```### Minimise a Function Using L-BFGS-B in PyTorch```pythonimport torchfrom varz.torch import Vars, minimise_l_bfgs_btarget = torch.tensor(5.0, dtype=torch.float64)def objective(vs):    # Get a variable named &quot;x&quot;, which must be positive, initialised to 10.    x = vs.pos(10.0, name=&quot;x&quot;)      return (x ** 0.5 - target) ** 2  ``````python&gt;&gt;&gt; vs = Vars(torch.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, vs)array(3.17785951e-19)  # Final objective function value.&gt;&gt;&gt; vs[&quot;x&quot;] - target ** 2tensor(-5.6373e-09, dtype=torch.float64)&gt;&gt;&gt; vs = Vars(torch.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, vs, jit=True)  # Speed up optimisation with PyTorch's JIT!array(3.17785951e-19)```### Minimise a Function Using L-BFGS-B in JAX```pythonimport jax.numpy as jnpfrom varz.jax import Vars, minimise_l_bfgs_btarget = 5.0def objective(vs):    # Get a variable named &quot;x&quot;, which must be positive, initialised to 10.    x = vs.pos(10.0, name=&quot;x&quot;)      return (x ** 0.5 - target) ** 2  ``````python&gt;&gt;&gt; vs = Vars(jnp.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, vs)array(3.17785951e-19)  # Final objective function value.&gt;&gt;&gt; vs[&quot;x&quot;] - target ** 2-5.637250666268301e-09&gt;&gt;&gt; vs = Vars(jnp.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, vs, jit=True)  # Speed up optimisation with Jax's JIT!array(3.17785951e-19)```### Tracking the Learning Curve in JAX```pythonimport jax.numpy as jnpfrom varz.jax import Vars, minimise_l_bfgs_btarget = 5.0def objective(vs, prev_x):    # Get a variable named &quot;x&quot;, which must be positive, initialised to 10.    x = vs.pos(10.0, name=&quot;x&quot;)    # In addition to the objective function value, also return `x` so that      # we can log it.    return (x ** 0.5 - target) ** 2, x  objs = []xs = []def callback(obj, x):    objs.append(obj)    xs.append(x)    # Return a dictionary of extra information to show in the progress display.    return {&quot;x&quot;: x}``````python&gt;&gt;&gt; vs = Vars(jnp.float64)&gt;&gt;&gt; minimise_l_bfgs_b(objective, (vs, 0), trace=True, jit=True, callback=callback)Minimisation of &quot;objective&quot;:    Iteration 1/1000:        Time elapsed: 0.0 s        Time left:  19.0 s        Objective value: 0.04567        x:          27.18    Iteration 6/1000:        Time elapsed: 0.1 s        Time left:  7.4 s        Objective value: 4.520e-04        x:          24.99    Done!Termination message:    CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL(array(3.17785951e-19), DeviceArray(24.99999999, dtype=float64))&gt;&gt;&gt; vs[&quot;x&quot;] - target ** 2DeviceArray(-5.63725067e-09, dtype=float64)&gt;&gt;&gt; objs[array(3.3772234), array(0.04567386), array(0.03582296), array(0.00014534), array(5.18203996e-07), array(6.81622668e-12), array(3.17785951e-19)]&gt;&gt;&gt; xs[DeviceArray(10., dtype=float64), DeviceArray(27.18281828, dtype=float64), DeviceArray(23.14312757, dtype=float64), DeviceArray(24.87958747, dtype=float64), DeviceArray(25.00719916, dtype=float64), DeviceArray(24.99997389, dtype=float64), DeviceArray(24.99999999, dtype=float64)]```</longdescription>
</pkgmetadata>