<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Text GenerationThe Hugging Face Text Generation Python library provides a convenient way of interfacing with a`text-generation-inference` instance running on[Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints) or on the Hugging Face Hub.## Get Started### Install```shellpip install text-generation```### Inference API Usage```pythonfrom text_generation import InferenceAPIClientclient = InferenceAPIClient(&quot;bigscience/bloomz&quot;)text = client.generate(&quot;Why is the sky blue?&quot;).generated_textprint(text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```or with the asynchronous client:```pythonfrom text_generation import InferenceAPIAsyncClientclient = InferenceAPIAsyncClient(&quot;bigscience/bloomz&quot;)response = await client.generate(&quot;Why is the sky blue?&quot;)print(response.generated_text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;async for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```Check all currently deployed models on the Huggingface Inference API with `Text Generation` support:```pythonfrom text_generation.inference_api import deployed_modelsprint(deployed_models())```### Hugging Face Inference Endpoint usage```pythonfrom text_generation import Clientendpoint_url = &quot;https://YOUR_ENDPOINT.endpoints.huggingface.cloud&quot;client = Client(endpoint_url)text = client.generate(&quot;Why is the sky blue?&quot;).generated_textprint(text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```or with the asynchronous client:```pythonfrom text_generation import AsyncClientendpoint_url = &quot;https://YOUR_ENDPOINT.endpoints.huggingface.cloud&quot;client = AsyncClient(endpoint_url)response = await client.generate(&quot;Why is the sky blue?&quot;)print(response.generated_text)# ' Rayleigh scattering'# Token Streamingtext = &quot;&quot;async for response in client.generate_stream(&quot;Why is the sky blue?&quot;):    if not response.token.special:        text += response.token.textprint(text)# ' Rayleigh scattering'```### Types```python# Request Parametersclass Parameters:    # Activate logits sampling    do_sample: bool    # Maximum number of generated tokens    max_new_tokens: int    # The parameter for repetition penalty. 1.0 means no penalty.    # See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.    repetition_penalty: Optional[float]    # Whether to prepend the prompt to the generated text    return_full_text: bool    # Stop generating tokens if a member of `stop_sequences` is generated    stop: List[str]    # Random sampling seed    seed: Optional[int]    # The value used to module the logits distribution.    temperature: Optional[float]    # The number of highest probability vocabulary tokens to keep for top-k-filtering.    top_k: Optional[int]    # If set to &lt; 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or    # higher are kept for generation.    top_p: Optional[float]    # truncate inputs tokens to the given size    truncate: Optional[int]    # Typical Decoding mass    # See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information    typical_p: Optional[float]    # Generate best_of sequences and return the one if the highest token logprobs    best_of: Optional[int]    # Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)    watermark: bool    # Get decoder input token logprobs and ids    decoder_input_details: bool    # Return the N most likely tokens at each step    top_n_tokens: Optional[int] # Decoder input tokensclass InputToken:    # Token ID from the model tokenizer    id: int    # Token text    text: str    # Logprob    # Optional since the logprob of the first token cannot be computed    logprob: Optional[float]# Generated tokensclass Token:    # Token ID from the model tokenizer    id: int    # Token text    text: str    # Logprob    logprob: float    # Is the token a special token    # Can be used to ignore tokens when concatenating    special: bool# Generation finish reasonclass FinishReason(Enum):    # number of generated tokens == `max_new_tokens`    Length = &quot;length&quot;    # the model generated its end of sequence token    EndOfSequenceToken = &quot;eos_token&quot;    # the model generated a text included in `stop_sequences`    StopSequence = &quot;stop_sequence&quot;# Additional sequences when using the `best_of` parameterclass BestOfSequence:    # Generated text    generated_text: str    # Generation finish reason    finish_reason: FinishReason    # Number of generated tokens    generated_tokens: int    # Sampling seed if sampling was activated    seed: Optional[int]    # Decoder input tokens, empty if decoder_input_details is False    prefill: List[InputToken]    # Generated tokens    tokens: List[Token]    # Most likely tokens    top_tokens: Optional[List[List[Token]]] # `generate` detailsclass Details:    # Generation finish reason    finish_reason: FinishReason    # Number of generated tokens    generated_tokens: int    # Sampling seed if sampling was activated    seed: Optional[int]    # Decoder input tokens, empty if decoder_input_details is False    prefill: List[InputToken]    # Generated tokens    tokens: List[Token]    # Most likely tokens    top_tokens: Optional[List[List[Token]]]    # Additional sequences when using the `best_of` parameter    best_of_sequences: Optional[List[BestOfSequence]]# `generate` return valueclass Response:    # Generated text    generated_text: str    # Generation details    details: Details# `generate_stream` detailsclass StreamDetails:    # Generation finish reason    finish_reason: FinishReason    # Number of generated tokens    generated_tokens: int    # Sampling seed if sampling was activated    seed: Optional[int]# `generate_stream` return valueclass StreamResponse:    # Generated token    token: Token    # Most likely tokens    top_tokens: Optional[List[Token]]     # Complete generated text    # Only available when the generation is finished    generated_text: Optional[str]    # Generation details    # Only available when the generation is finished    details: Optional[StreamDetails]# Inference API currently deployed modelclass DeployedModel:    model_id: str    sha: str```</longdescription>
</pkgmetadata>