<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1a0pSD-1tWhMmeJeeoyZM1A-HCW3yf1xR?usp=sharing) [![Website](https://img.shields.io/badge/www-Website-green)](https://agarwl.github.io/rliable) [![Blog](https://img.shields.io/badge/b-Blog-blue)](https://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html)`rliable` is an open-source Python library for reliable evaluation, even with a *handfulof runs*, on reinforcement learning and machine learnings benchmarks. | **Desideratum** | **Current evaluation approach** |  **Our Recommendation**    || --------------------------------- | ----------- | --------- || Uncertainty in aggregate performance | **Point estimates**: &lt;ul&gt; &lt;li&gt; Ignore statistical uncertainty &lt;/li&gt; &lt;li&gt; Hinder *results reproducibility* &lt;/li&gt;&lt;/ul&gt; | Interval estimates using **stratified bootstrap confidence intervals** (CIs) ||Performance variability across tasks and runs| **Tables with task mean scores**: &lt;ul&gt;&lt;li&gt; Overwhelming beyond a few tasks &lt;/li&gt; &lt;li&gt; Standard deviations frequently omitted &lt;/li&gt; &lt;li&gt; Incomplete picture for multimodal and heavy-tailed distributions &lt;/li&gt; &lt;/ul&gt; | **Score distributions** (*performance profiles*): &lt;ul&gt; &lt;li&gt; Show tail distribution of scores on combined runs across tasks &lt;/li&gt; &lt;li&gt; Allow qualitative comparisons &lt;/li&gt; &lt;li&gt; Easily read any score percentile &lt;/li&gt; &lt;/ul&gt;||Aggregate metrics for summarizing benchmark performance | **Mean**:  &lt;ul&gt;&lt;li&gt; Often dominated by performance on outlier tasks &lt;/li&gt;&lt;/ul&gt; &amp;nbsp; **Median**: &lt;ul&gt; &lt;li&gt; Statistically inefficient (requires a large number of runs to claim improvements) &lt;/li&gt;  &lt;li&gt; Poor indicator of overall performance: 0 scores on nearly half the tasks doesn't change it &lt;/li&gt; &lt;/ul&gt;| **Interquartile Mean (IQM)** across all runs: &lt;ul&gt; &lt;li&gt; Performance on middle 50% of combined runs &lt;/li&gt; &lt;li&gt; Robust to outlier scores but more statistically efficient than median &lt;/li&gt; &lt;/ul&gt; To show other aspects of performance gains, report *Probability of improvement* and *Optimality gap* |`rliable` provides support for: * Stratified Bootstrap Confidence Intervals (CIs) * Performance Profiles (with plotting functions) * Aggregate metrics   * Interquartile Mean (IQM) across all runs   * Optimality Gap   * Probability of Improvement&lt;div align=&quot;left&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/google-research/rliable/master/images/aggregate_metric.png&quot;&gt;&lt;/div&gt;## Interactive colabWe provide a colab at [bit.ly/statistical_precipice_colab](https://colab.research.google.com/drive/1a0pSD-1tWhMmeJeeoyZM1A-HCW3yf1xR?usp=sharing),which shows how to use the library with examples of published algorithms onwidely used benchmarks including Atari 100k, ALE, DM Control and Procgen.### PaperFor more details, refer to the accompanying **NeurIPS 2021** paper (**Outstanding Paper** Award):[Deep Reinforcement Learning at the Edge of the Statistical Precipice](https://arxiv.org/pdf/2108.13264.pdf).### InstallationTo install `rliable`, run:```pythonpip install -U rliable```To install latest version of `rliable` as a package, run:```pythonpip install git+https://github.com/google-research/rliable```To import `rliable`, we suggest:```pythonfrom rliable import library as rlyfrom rliable import metricsfrom rliable import plot_utils```### Aggregate metrics with 95% Stratified Bootstrap CIs##### IQM, Optimality Gap, Median, Mean```pythonalgorithms = ['DQN (Nature)', 'DQN (Adam)', 'C51', 'REM', 'Rainbow',              'IQN', 'M-IQN', 'DreamerV2']# Load ALE scores as a dictionary mapping algorithms to their human normalized# score matrices, each of which is of size `(num_runs x num_games)`.atari_200m_normalized_score_dict = ...aggregate_func = lambda x: np.array([  metrics.aggregate_median(x),  metrics.aggregate_iqm(x),  metrics.aggregate_mean(x),  metrics.aggregate_optimality_gap(x)])aggregate_scores, aggregate_score_cis = rly.get_interval_estimates(  atari_200m_normalized_score_dict, aggregate_func, reps=50000)fig, axes = plot_utils.plot_interval_estimates(  aggregate_scores, aggregate_score_cis,  metric_names=['Median', 'IQM', 'Mean', 'Optimality Gap'],  algorithms=algorithms, xlabel='Human Normalized Score')```&lt;div align=&quot;left&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/google-research/rliable/master/images/ale_interval_estimates.png&quot;&gt;&lt;/div&gt;##### Probability of Improvement```python# Load ProcGen scores as a dictionary containing pairs of normalized score# matrices for pairs of algorithms we want to compareprocgen_algorithm_pairs = {.. , 'x,y': (score_x, score_y), ..}average_probabilities, average_prob_cis = rly.get_interval_estimates(  procgen_algorithm_pairs, metrics.probability_of_improvement, reps=2000)plot_utils.plot_probability_of_improvement(average_probabilities, average_prob_cis)```&lt;div align=&quot;center&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/google-research/rliable/master/images/procgen_probability_of_improvement.png&quot;&gt;&lt;/div&gt;#### Sample Efficiency Curve```pythonalgorithms = ['DQN (Nature)', 'DQN (Adam)', 'C51', 'REM', 'Rainbow',              'IQN', 'M-IQN', 'DreamerV2']# Load ALE scores as a dictionary mapping algorithms to their human normalized# score matrices across all 200 million frames, each of which is of size# `(num_runs x num_games x 200)` where scores are recorded every million frame.ale_all_frames_scores_dict = ...frames = np.array([1, 10, 25, 50, 75, 100, 125, 150, 175, 200]) - 1ale_frames_scores_dict = {algorithm: score[:, :, frames] for algorithm, score                          in ale_all_frames_scores_dict.items()}iqm = lambda scores: np.array([metrics.aggregate_iqm(scores[..., frame])                               for frame in range(scores.shape[-1])])iqm_scores, iqm_cis = rly.get_interval_estimates(  ale_frames_scores_dict, iqm, reps=50000)plot_utils.plot_sample_efficiency_curve(    frames+1, iqm_scores, iqm_cis, algorithms=algorithms,    xlabel=r'Number of Frames (in millions)',    ylabel='IQM Human Normalized Score')```&lt;div align=&quot;center&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/google-research/rliable/master/images/ale_legend.png&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/google-research/rliable/master/images/atari_sample_efficiency_iqm.png&quot;&gt;&lt;/div&gt;### Performance Profiles```python# Load ALE scores as a dictionary mapping algorithms to their human normalized# score matrices, each of which is of size `(num_runs x num_games)`.atari_200m_normalized_score_dict = ...# Human normalized score thresholdsatari_200m_thresholds = np.linspace(0.0, 8.0, 81)score_distributions, score_distributions_cis = rly.create_performance_profile(    atari_200m_normalized_score_dict, atari_200m_thresholds)# Plot score distributionsfig, ax = plt.subplots(ncols=1, figsize=(7, 5))plot_utils.plot_performance_profiles(  score_distributions, atari_200m_thresholds,  performance_profile_cis=score_distributions_cis,  colors=dict(zip(algorithms, sns.color_palette('colorblind'))),  xlabel=r'Human Normalized Score $(\tau)$',  ax=ax)```&lt;div align=&quot;center&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/google-research/rliable/master/images/ale_legend.png&quot;&gt;  &lt;img src=&quot;https://raw.githubusercontent.com/google-research/rliable/master/images/ale_score_distributions_new.png&quot;&gt;&lt;/div&gt;The above profile can also be plotted with non-linear scaling as follows:```pythonplot_utils.plot_performance_profiles(  perf_prof_atari_200m, atari_200m_tau,  performance_profile_cis=perf_prof_atari_200m_cis,  use_non_linear_scaling=True,  xticks = [0.0, 0.5, 1.0, 2.0, 4.0, 8.0]  colors=dict(zip(algorithms, sns.color_palette('colorblind'))),  xlabel=r'Human Normalized Score $(\tau)$',  ax=ax)```### DependenciesThe code was tested under `Python&gt;=3.7` and uses these packages:- arch == 5.3.0- scipy &gt;= 1.7.0- numpy &gt;= 0.9.0- absl-py &gt;= 1.16.4- seaborn &gt;= 0.11.2Citing------If you find this open source release useful, please reference in your paper:    @article{agarwal2021deep,      title={Deep Reinforcement Learning at the Edge of the Statistical Precipice},      author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel              and Courville, Aaron and Bellemare, Marc G},      journal={Advances in Neural Information Processing Systems},      year={2021}    }Disclaimer: This is not an official Google product.</longdescription>
</pkgmetadata>