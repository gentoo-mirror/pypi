<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># yetlWebsite: https://www.yetl.io/# IntroductionInstall```pip install yetl-framework```Configuration framework for databricks pipelines.Define configuration and table dependencies in yaml config then get the table mappings config model:Define your tables.```yamlversion: 1.3.0audit_control:  delta_lake:    raw_dbx_patterns_control:      header_footer:        sql: ../sql/{{database}}/{{table}}.sql        depends_on:          - raw.raw_dbx_patterns.*      raw_audit:        sql: ../sql/{{database}}/{{table}}.sql        depends_on:          - raw.raw_dbx_patterns.*          - audit_control.raw_dbx_patterns_control.header_footerlanding:  read:    landing_dbx_patterns:      customer_details_1: null      customer_details_2: nullraw:  delta_lake:    raw_dbx_patterns:      customers:        ids: id        depends_on:          - landing.landing_dbx_patterns.customer_details_1          - landing.landing_dbx_patterns.customer_details_2        warning_thresholds:          invalid_ratio: 0.1          invalid_rows: 0          max_rows: 100          min_rows: 5        exception_thresholds:          invalid_ratio: 0.2          invalid_rows: 2          max_rows: 1000          min_rows: 0        custom_properties:          process_group: 1```Define you load configuration:```yamlversion: 1.3.0tables: ./tables.yamlaudit_control:  delta_lake:    # delta table properties can be set at stage level or table level    delta_properties:        delta.appendOnly: true        delta.autoOptimize.autoCompact: true        delta.autoOptimize.optimizeWrite: true    managed: false    create_table: true    container: datalake    location: /mnt/{{container}}/data/raw    checkpoint_location: &quot;/mnt/{{container}}/checkpoint/{{checkpoint}}&quot;    path: &quot;{{database}}/{{table}}&quot;    options:      checkpointLocation: defaultlanding:  read:    trigger: customerdetailscomplete-{{filename_date_format}}*.flg    trigger_type: file    container: datalake    location: &quot;/mnt/{{container}}/data/landing/dbx_patterns/{{table}}/{{path_date_format}}&quot;    filename: &quot;{{table}}-{{filename_date_format}}*.csv&quot;    filename_date_format: &quot;%Y%m%d&quot;    path_date_format: &quot;%Y%m%d&quot;    format: cloudFiles    spark_schema: ../schema/{{table.lower()}}.yaml    options:      # autoloader      cloudFiles.format: csv      cloudFiles.schemaLocation:  /mnt/{{container}}/checkpoint/{{checkpoint}}      cloudFiles.useIncrementalListing: auto      # schema      inferSchema: false      enforceSchema: true      columnNameOfCorruptRecord: _corrupt_record      # csv      header: false      mode: PERMISSIVE      encoding: windows-1252      delimiter: &quot;,&quot;      escape: '&quot;'      nullValue: &quot;&quot;      quote: '&quot;'      emptyValue: &quot;&quot;    raw:  delta_lake:    # delta table properties can be set at stage level or table level    delta_properties:      delta.appendOnly: true      delta.autoOptimize.autoCompact: true          delta.autoOptimize.optimizeWrite: true        delta.enableChangeDataFeed: false    managed: false    create_table: true    container: datalake    location: /mnt/{{container}}/data/raw    path: &quot;{{database}}/{{table}}&quot;    checkpoint_location: &quot;/mnt/{{container}}/checkpoint/{{checkpoint}}&quot;    options:      mergeSchema: truebase:  delta_lake:    container: datalake    location: /mnt/{{container}}/data/base    path: &quot;{{database}}/{{table}}&quot;    options: null```Import the config objects into you pipeline:```pythonfrom yetl import Config, StageTypepipeline = &quot;auto_load_schema&quot;project = &quot;test_project&quot;config = Config(    project=project, pipeline=pipeline)table_mapping = config.get_table_mapping(    stage=StageType.raw, table=&quot;customers&quot;)print(table_mapping)```Use even less code and use the decorator pattern:```python@yetl_flow(        project=&quot;test_project&quot;,         stage=StageType.raw)def auto_load_schema(table_mapping:TableMapping):    # &lt;&lt; ADD YOUR PIPELINE LOGIC HERE - USING TABLE MAPPING CONFIG &gt;&gt;    return table_mapping # return whatever you want here.result = auto_load_schema(table=&quot;customers&quot;)```## Development Setup```pip install -r requirements.txt```## Unit TestsTo run the unit tests with a coverage report.```pip install -e .pytest test/unit --junitxml=junit/test-results.xml --cov=yetl --cov-report=xml --cov-report=html```## Integration TestsTo run the integration tests with a coverage report.```pip install -e .pytest test/integration --junitxml=junit/test-results.xml --cov=yetl --cov-report=xml --cov-report=html```## Build```python setup.py sdist bdist_wheel```## Publish```twine upload dist/*```</longdescription>
</pkgmetadata>