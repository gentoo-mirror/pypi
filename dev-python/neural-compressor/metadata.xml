<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;Intel® Neural Compressor===========================&lt;h3&gt; An open-source Python library supporting popular model compression techniques on all mainstream deep learning frameworks (TensorFlow, PyTorch, ONNX Runtime, and MXNet)&lt;/h3&gt;[![python](https://img.shields.io/badge/python-3.7%2B-blue)](https://github.com/intel/neural-compressor)[![version](https://img.shields.io/badge/release-2.3.1-green)](https://github.com/intel/neural-compressor/releases)[![license](https://img.shields.io/badge/license-Apache%202-blue)](https://github.com/intel/neural-compressor/blob/master/LICENSE)[![coverage](https://img.shields.io/badge/coverage-85%25-green)](https://github.com/intel/neural-compressor)[![Downloads](https://static.pepy.tech/personalized-badge/neural-compressor?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=green&amp;left_text=downloads)](https://pepy.tech/project/neural-compressor)[Architecture](./docs/source/design.md#architecture)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Workflow](./docs/source/design.md#workflow)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Results](./docs/source/validated_model_list.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Examples](./examples/README.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Documentations](https://intel.github.io/neural-compressor)---&lt;div align=&quot;left&quot;&gt;Intel® Neural Compressor aims to provide popular model compression techniques such as quantization, pruning (sparsity), distillation, and neural architecture search on mainstream frameworks such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), [ONNX Runtime](https://onnxruntime.ai/), and [MXNet](https://mxnet.apache.org/),as well as Intel extensions such as [Intel Extension for TensorFlow](https://github.com/intel/intel-extension-for-tensorflow) and [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch).In particular, the tool provides the key features, typical examples, and open collaborations as below:* Support a wide range of Intel hardware such as [Intel Xeon Scalable Processors](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html), [Intel Xeon CPU Max Series](https://www.intel.com/content/www/us/en/products/details/processors/xeon/max-series.html), [Intel Data Center GPU Flex Series](https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/flex-series.html), and [Intel Data Center GPU Max Series](https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html) with extensive testing; support AMD CPU, ARM CPU, and NVidia GPU through ONNX Runtime with limited testing* Validate popular LLMs such as LLama2, [LLama](examples/onnxrt/nlp/huggingface_model/text_generation/llama/quantization/ptq_static), [MPT](https://github.com/intel/intel-extension-for-transformers/blob/main/examples/huggingface/pytorch/text-generation/quantization/README.md), [Falcon](https://github.com/intel/intel-extension-for-transformers/blob/main/examples/huggingface/pytorch/language-modeling/quantization/README.md), [GPT-J](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/ptq_static/fx), [Bloom](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/ptq_static/ipex/smooth_quant), [OPT](/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/ptq_static/ipex/smooth_quant), and more than 10,000 broad models such as [Stable Diffusion](/examples/pytorch/nlp/huggingface_models/text-to-image/quantization), [BERT-Large](/examples/pytorch/nlp/huggingface_models/text-classification/quantization/ptq_static/fx), and [ResNet50](/examples/pytorch/image_recognition/torchvision_models/quantization/ptq/cpu/fx) from popular model hubs such as [Hugging Face](https://huggingface.co/), [Torch Vision](https://pytorch.org/vision/stable/index.html), and [ONNX Model Zoo](https://github.com/onnx/models#models), by leveraging zero-code optimization solution [Neural Coder](/neural_coder#what-do-we-offer) and automatic [accuracy-driven](/docs/source/design.md#workflow) quantization strategies* Collaborate with cloud marketplace such as [Google Cloud Platform](https://console.cloud.google.com/marketplace/product/bitnami-launchpad/inc-tensorflow-intel?project=verdant-sensor-286207), [Amazon Web Services](https://aws.amazon.com/marketplace/pp/prodview-yjyh2xmggbmga#pdp-support), and [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bitnami.inc-tensorflow-intel), software platforms such as [Alibaba Cloud](https://www.intel.com/content/www/us/en/developer/articles/technical/quantize-ai-by-oneapi-analytics-on-alibaba-cloud.html), [Tencent TACO](https://new.qq.com/rain/a/20221202A00B9S00) and [Microsoft Olive](https://github.com/microsoft/Olive), and open AI ecosystem such as [Hugging Face](https://huggingface.co/blog/intel), [PyTorch](https://pytorch.org/tutorials/recipes/intel_neural_compressor_for_pytorch.html), [ONNX](https://github.com/onnx/models#models), [ONNX Runtime](https://github.com/microsoft/onnxruntime), and [Lightning AI](https://github.com/Lightning-AI/lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst)## Installation### Install from pypi```Shellpip install neural-compressor```&gt; More installation methods can be found at [Installation Guide](./docs/source/installation_guide.md). Please check out our [FAQ](./docs/source/faq.md) for more details.## Getting Started### Quantization with Python API```shell# Install Intel Neural Compressor and TensorFlowpip install neural-compressorpip install tensorflow# Prepare fp32 modelwget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb``````pythonfrom neural_compressor.data import DataLoader, Datasetsfrom neural_compressor.config import PostTrainingQuantConfigdataset = Datasets(&quot;tensorflow&quot;)[&quot;dummy&quot;](shape=(1, 224, 224, 3))dataloader = DataLoader(framework=&quot;tensorflow&quot;, dataset=dataset)from neural_compressor.quantization import fitq_model = fit(    model=&quot;./mobilenet_v1_1.0_224_frozen.pb&quot;,    conf=PostTrainingQuantConfig(),    calib_dataloader=dataloader,)```## Documentation&lt;table class=&quot;docutils&quot;&gt;  &lt;thead&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot;&gt;Overview&lt;/th&gt;  &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/design.md#architecture&quot;&gt;Architecture&lt;/a&gt;&lt;/td&gt;      &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/design.md#workflow&quot;&gt;Workflow&lt;/a&gt;&lt;/td&gt;      &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;examples/README.md&quot;&gt;Examples&lt;/a&gt;&lt;/td&gt;      &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;https://intel.github.io/neural-compressor/latest/docs/source/api-doc/apis.html&quot;&gt;APIs&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th colspan=&quot;8&quot;&gt;Python-based APIs&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/quantization.md&quot;&gt;Quantization&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/mixed_precision.md&quot;&gt;Advanced Mixed Precision&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/pruning.md&quot;&gt;Pruning (Sparsity)&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/distillation.md&quot;&gt;Distillation&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/orchestration.md&quot;&gt;Orchestration&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/benchmark.md&quot;&gt;Benchmarking&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/distributed.md&quot;&gt;Distributed Compression&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/export.md&quot;&gt;Model Export&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th colspan=&quot;8&quot;&gt;Neural Coder (Zero-code Optimization)&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./neural_coder/docs/PythonLauncher.md&quot;&gt;Launcher&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./neural_coder/extensions/neural_compressor_ext_lab/README.md&quot;&gt;JupyterLab Extension&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./neural_coder/extensions/neural_compressor_ext_vscode/README.md&quot;&gt;Visual Studio Code Extension&lt;/a&gt;&lt;/td&gt;        &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./neural_coder/docs/SupportMatrix.md&quot;&gt;Supported Matrix&lt;/a&gt;&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;  &lt;thead&gt;      &lt;tr&gt;        &lt;th colspan=&quot;8&quot;&gt;Advanced Topics&lt;/th&gt;      &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;      &lt;tr&gt;          &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/adaptor.md&quot;&gt;Adaptor&lt;/a&gt;&lt;/td&gt;          &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/tuning_strategies.md&quot;&gt;Strategy&lt;/a&gt;&lt;/td&gt;          &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/distillation_quantization.md&quot;&gt;Distillation for Quantization&lt;/a&gt;&lt;/td&gt;          &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/smooth_quant.md&quot;&gt;SmoothQuant&lt;/td&gt;      &lt;/tr&gt;      &lt;tr&gt;          &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./docs/source/quantization_weight_only.md&quot;&gt;Weight-Only Quantization (INT8/INT4/FP4/NF4) &lt;/td&gt;          &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/intel/neural-compressor/blob/fp8_adaptor/docs/source/fp8.md&quot;&gt;FP8 Quantization &lt;/td&gt;      &lt;/tr&gt;  &lt;/tbody&gt;  &lt;thead&gt;      &lt;tr&gt;        &lt;th colspan=&quot;8&quot;&gt;Innovations for Productivity&lt;/th&gt;      &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;      &lt;tr&gt;          &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./neural_insights/README.md&quot;&gt;Neural Insights&lt;/a&gt;&lt;/td&gt;          &lt;td colspan=&quot;4&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;./neural_solution/README.md&quot;&gt;Neural Solution&lt;/a&gt;&lt;/td&gt;      &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&gt; More documentations can be found at [User Guide](./docs/source/user_guide.md).## Selected Publications/Events* EMNLP'2023 (Under Review): [TEQ: Trainable Equivalent Transformation for Quantization of LLMs](https://openreview.net/forum?id=iaI8xEINAf&amp;referrer=%5BAuthor%20Console%5D) (Sep 2023)* arXiv: [Efficient Post-training Quantization with FP8 Formats](https://arxiv.org/abs/2309.14592) (Sep 2023)* arXiv: [Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs](https://arxiv.org/abs/2309.05516) (Sep 2023)* Post on Social Media: [ONNXCommunityMeetup2023: INT8 Quantization for Large Language Models with Intel Neural Compressor](https://www.youtube.com/watch?v=luYBWA1Q5pQ)  (July 2023)* Blog by Intel: [Accelerate Llama 2 with Intel AI Hardware and Software Optimizations](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html) (July 2023)* NeurIPS'2022: [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) (Oct 2022)* NeurIPS'2022: [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114) (Oct 2022)&gt; View [Full Publication List](./docs/source/publication_list.md).## Additional Content* [Release Information](./docs/source/releases_info.md)* [Contribution Guidelines](./docs/source/CONTRIBUTING.md)* [Legal Information](./docs/source/legal_information.md)* [Security Policy](SECURITY.md)## Communication - [GitHub Issues](https://github.com/intel/neural-compressor/issues): mainly for bugs report, new feature request, question asking, etc.- [Email](mailto:inc.maintainers@intel.com): welcome to raise any interesting research ideas on model compression techniques by email for collaborations.  - [WeChat group](/docs/source/imgs/wechat_group.jpg): scan the QA code to join the technical discussion.</longdescription>
</pkgmetadata>