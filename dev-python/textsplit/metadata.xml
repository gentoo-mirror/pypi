<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># IntroductionThis library contains simple functionality to tackle the problem of segmentingdocuments into coherent parts. Imagine you don't have a good paragraphannotation in your documents, as it is often the case for scraped pdfs or htmldocuments. For NLP tasks you want to split them at points where the topicchanges. Good results have been achieved using topic representations, but theyinvolve a further step of topic modeling which is quite domain dependent. Thisapproach uses only word embeddings which are assumed to be less domain specific.See [https://arxiv.org/pdf/1503.05543.pdf] for an overview and an approach verysimilar to the one presented here.The algorithm uses word embeddings to find a segmentation where the splits arechosen such that the segments are coherent. This coherence can be described asaccumulated weighted cosine similarity of the words of a segment to the meanvector of that segment.  More formally segments are chosen as to maximize thequantity |v|, where v is a segment vector and |.| denotes the l2-norm. Theaccumulated weighted cosine similarity turns up by a simple transformation:|v| = 1/|v| &lt;v, v&gt; = &lt;v, v/|v|&gt; = \sum_i &lt;w_i, v/|v|&gt; = \sum_i |w_i| &lt;w_i/|w_i|, v/|v|&gt;,where v = \sum_i w_i is the definition of the segment vector from word vectorsw_i. The expansion gives a good intuition of what we try to achieve. As weusually compare word embeddings with cosine similarity, the last scalar product&lt;w_i/|w_i|, v/|v|&gt; is just the cosine similarity of a word w_i to the segmentvector v. The weighting with the length of w_i suppresses frequent noise words,that typically have a shorter length.This leads to the interpretation that coherence corresponds to segment vectorlength, in the sense that two segment vectors of same length contain the sameamount of information. This interpretation is of course only capturinginformation that we are given as input by means of the word embeddings, but itserves as an abstraction.# FormalizationTo optimize for segment vector length |v|, we look for a sequence of splitpositions such that the sum of l2-norms of the segment vectors formed by summingthe words between the splits is maximal. Given this objective withoutconstraints, the optimal solution is to split the document between every twosubsequent words (triangle inequality). We have to impose some limit on thegranularity of the segmentation to get useful results. This is done by a penaltyfor every split made, that counts against the vector norms, i.e. is subtractedfrom the sum of vector norms.Let Seg := {(0 = t_0 &lt; t_i &lt; ... &lt; t_n = L) | s_i natural number} where L is adocuments length. A segment [a, b[ comprises the words at positions a, a+1, ...,b-1. Let l(j, k) := |\sum_i=j^{k-1} w_i| denote the vector of segment [i, j[. Weoptimize the function f mapping elements of Seg to the real numbers withf: (t_0, ..., t_n) \mapsto \sum_{i=0}^{n-1} (l(t_{i-1}, t_i) + l(t_i, t_{i+1}) - penalty).# AlgorithmsThere are two variants, a greedy that is fast and a dynamic programming approachthat computes the optimal segmentation. Both depend on a penalty hyperparameter,that defined the granularity of the split.## GreedySplit the text iteratively at the position where the gain is highest until thisgain would be below a given penalty threshold. The gain is the sum of norms ofthe left and right segments minus the norm of the segment that is to be split.## Optimal (Dynamic Programming)Iteratively construct a data structure storing the results of optimallysplitting a prefix of the document. This results in a matrix storing a scorefor making a segment from position i to j, given a optimal segmentation up to i.# Tools## Penalty hyperparameter choiceThe greedy implementation does not need the penalty parameter, but can also berun by limiting the number of segments. This is leveraged by the `get_penalty`function to approximately determine a penalty parameter for a desired averagesegment length computed over a set of documents.## Measure accuracy of segmentation against referenceTo measure the accuracy of an algorithm against a given reference segmentation`P_k` is a commonly used metric described e.g. in above paper.## Apply segmentation definition to documentThe function `get_segments` simply applies a segmentation determined by one ofthe algorithms to e.g. the sentences of a text used when generating thesegmentation.# Usage## InputThe algorithms are fed a matrix `docmat` containing vectors representing thecontent of a text. These vectors are supposed to have cosine similarity as anatural similarity measure and length roughly corresponding to the contentlength of a text particle. Particles could be words in which case word2vecembeddings are a good choice as vectors. The width of `docmat` is the embeddingdimension and the height the number of particles.## Split along sentence bordersIf you want to split text into paragraphs, you most likely already have a goodidea of what potential sentence borders are. It makes sense not to give the wordvectors as input but sentence vectors formed by e.g. the sum of word vectors, asit is usual practice.# Getting StartedIn the Jupyter notebook HowTo.ipynb you find code that demonstrates the use ofthe module. It downloads a corpus to trains word2vec vectors on and an exampletext for segmentation. You achieve better results if you compute word vectors ona larger corpus.</longdescription>
</pkgmetadata>