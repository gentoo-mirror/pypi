<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;    &lt;img alt=&quot;cometLLM&quot; src=&quot;https://github.com/comet-ml/comet-llm/raw/main/logo.svg&quot;&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://pypi.org/project/comet-llm&quot;&gt;        &lt;img src=&quot;https://img.shields.io/pypi/v/comet-llm&quot; alt=&quot;PyPI version&quot;&gt;    &lt;/a&gt;    &lt;a rel=&quot;nofollow&quot; href=&quot;https://opensource.org/license/mit/&quot;&gt;        &lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/badge/License-MIT-blue.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://www.comet.com/docs/v2/guides/large-language-models/overview/&quot; rel=&quot;nofollow&quot;&gt;        &lt;img src=&quot;https://img.shields.io/badge/cometLLM-Docs-blue.svg&quot; alt=&quot;cometLLM Documentation&quot;&gt;    &lt;/a&gt;    &lt;a rel=&quot;nofollow&quot; href=&quot;https://pepy.tech/project/comet-llm&quot;&gt;        &lt;img style=&quot;max-width: 100%;&quot; src=&quot;https://static.pepy.tech/badge/comet-llm&quot; alt=&quot;Downloads&quot;&gt;    &lt;/a&gt;    &lt;a rel=&quot;nofollow&quot; href=&quot;https://colab.research.google.com/github/comet-ml/comet-llm/blob/main/examples/CometLLM_Prompts.ipynb&quot;&gt;        &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;    &lt;/a&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;b&gt;CometLLM&lt;/b&gt; is a tool to log and visualize your LLM prompts and chains. Use CometLLM to identify effective prompt strategies, streamline your troubleshooting, and ensure reproducible workflows!&lt;/p&gt;&lt;/p&gt;![CometLLM Preview](https://github.com/comet-ml/comet-llm/raw/main/comet_llm.gif)## ‚ö°Ô∏è QuickstartInstall `comet_llm` Python library with pip:```bashpip install comet_llm```If you don't have already, [create your free Comet account](https://www.comet.com/signup/?utm_source=comet_llm&amp;utm_medium=referral&amp;utm_content=github&amp;framework=llm) and grab your API Key from the account settings page.Now you are all set to log your first prompt and response:```pythonimport comet_llmcomet_llm.log_prompt(    prompt=&quot;What is your name?&quot;,    output=&quot; My name is Alex.&quot;,    api_key=&quot;&lt;YOUR_COMET_API_KEY&gt;&quot;,)```## üéØ Features- [x] Log your prompts and responses, including prompt template, variables, timestamps and duration and any metadata that you need.- [x] Visualize your prompts and responses in the UI.- [x] Log your chain execution down to the level of granularity that you need.- [x] Visualize your chain execution in the UI.- [ ] Diff your prompts and chain execution in the UI.## üëÄ ExamplesTo log a single LLM call as an individual prompt, use `comet_llm.log_prompt`. If you require more granularity, you can log a chain of executions that may include more than one LLM call, context retrieval, or data pre- or post-processing with `comet_llm.start_chain`.### Log a full prompt and response```pythonimport comet_llmcomet_llm.log_prompt(    prompt=&quot;Answer the question and if the question can't be answered, say \&quot;I don't know\&quot;\n\n---\n\nQuestion: What is your name?\nAnswer:&quot;,    prompt_template=&quot;Answer the question and if the question can't be answered, say \&quot;I don't know\&quot;\n\n---\n\nQuestion: {{question}}?\nAnswer:&quot;,    prompt_template_variables={&quot;question&quot;: &quot;What is your name?&quot;},    metadata= {        &quot;usage.prompt_tokens&quot;: 7,        &quot;usage.completion_tokens&quot;: 5,        &quot;usage.total_tokens&quot;: 12,    },    output=&quot; My name is Alex.&quot;,    duration=16.598,)```[Read the full documentation for more details about logging a prompt](https://www.comet.com/docs/v2/guides/large-language-models/llm-project/#logging-prompts-to-llm-projects).### Log a LLM chain```pythonfrom comet_llm import Span, end_chain, start_chainimport datetimefrom time import sleepdef retrieve_context(user_question):    if &quot;open&quot; in user_question:        return &quot;Opening hours: 08:00 to 17:00 all days&quot;def llm_answering(user_question, current_time, context):    prompt_template = &quot;&quot;&quot;You are a helpful chatbot. You have access to the following context:    {context}    The current time is: {current_time}    Analyze the following user question and decide if you can answer it, if the question can't be answered, say \&quot;I don't know\&quot;:    {user_question}    &quot;&quot;&quot;    prompt = prompt_template.format(        user_question=user_question, current_time=current_time, context=context    )    with Span(        category=&quot;llm-call&quot;,        inputs={&quot;prompt_template&quot;: prompt_template, &quot;prompt&quot;: prompt},    ) as span:        # Call your LLM model here        sleep(0.1)        result = &quot;Yes we are currently open&quot;        usage = {&quot;prompt_tokens&quot;: 52, &quot;completion_tokens&quot;: 12, &quot;total_tokens&quot;: 64}        span.set_outputs(outputs={&quot;result&quot;: result}, metadata={&quot;usage&quot;: usage})    return resultdef main(user_question, current_time):    start_chain(inputs={&quot;user_question&quot;: user_question, &quot;current_time&quot;: current_time})    with Span(        category=&quot;context-retrieval&quot;,        name=&quot;Retrieve Context&quot;,        inputs={&quot;user_question&quot;: user_question},    ) as span:        context = retrieve_context(user_question)        span.set_outputs(outputs={&quot;context&quot;: context})    with Span(        category=&quot;llm-reasoning&quot;,        inputs={            &quot;user_question&quot;: user_question,            &quot;current_time&quot;: current_time,            &quot;context&quot;: context,        },    ) as span:        result = llm_answering(user_question, current_time, context)        span.set_outputs(outputs={&quot;result&quot;: result})    end_chain(outputs={&quot;result&quot;: result})main(&quot;Are you open?&quot;, str(datetime.datetime.now().time()))```[Read the full documentation for more details about logging a chain](https://www.comet.com/docs/v2/guides/large-language-models/llm-project/#logging-chains-to-llm-projects).## ‚öôÔ∏è ConfigurationYou can configure your Comet credentials and where you are logging data to:| Name                 | Python parameter name | Environment variable name || -------------------- | --------------------- | ------------------------- || Comet API KEY        | api_key               | COMET_API_KEY             || Comet Workspace name | workspace             | COMET_WORKSPACE           || Comet Project name   | project               | COMET_PROJECT_NAME        |## üìù LicenseCopyright (c) [Comet](https://www.comet.com/site/) 2023-present. `cometLLM` is free and open-source software licensed under the [MIT License](https://github.com/comet-ml/comet-llm/blob/master/LICENSE).</longdescription>
</pkgmetadata>