<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://s1.ax1x.com/2023/03/07/ppZfEmq.png&quot; border=&quot;0&quot; width=600px/&gt;&lt;/div&gt;------&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;#overview&quot;&gt;Overview&lt;/a&gt; •  &lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; •  &lt;a href=&quot;https://arxiv.org/abs/2303.02913&quot;&gt;Paper&lt;/a&gt; •  &lt;a href=&quot;https://github.com/Shark-NLP/OpenICL/tree/main/examples&quot;&gt;Examples&lt;/a&gt; •  &lt;a href=&quot;https://openicl.readthedocs.io/en/latest/index.html&quot;&gt;Docs&lt;/a&gt; •  &lt;a href=&quot;#citation&quot;&gt;Citation&lt;/a&gt; &lt;/p&gt;![version](https://img.shields.io/badge/version-0.1.6-blue)## OverviewOpenICL provides an easy interface for in-context learning, with many state-of-the-art retrieval and inference methods built in to facilitate systematic comparison of LMs and fast research prototyping. Users can easily incorporate different retrieval and inference methods, as well as different prompt instructions into their workflow. &lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://s1.ax1x.com/2023/03/07/ppZWjmt.jpg&quot;  border=&quot;0&quot; /&gt;&lt;/div&gt;## InstallationNote: OpenICL requires Python 3.8+**Using Pip**```pip install openicl```**Installation for local development:**```git clone https://github.com/Shark-NLP/OpenICLcd OpenICLpip install -e .```## Quick StartFollowing example shows you how to perform ICL on sentiment classification dataset.  More examples and tutorials can be found at [examples](https://github.com/Shark-NLP/OpenICL/tree/main/examples)#### Step 1: Load and prepare data```pythonfrom datasets import load_datasetfrom openicl import DatasetReader# Loading dataset from huggingfacedataset = load_dataset('gpt3mix/sst2')# Define a DatasetReader, with specified column names where input and output are stored.data = DatasetReader(dataset, input_columns=['text'], output_column='label')```#### Step 2: Define the prompt template (Optional)```pythonfrom openicl import PromptTemplatetp_dict = {    0: &quot;&lt;/E&gt;Positive Movie Review: &lt;/text&gt;&quot;,    1: &quot;&lt;/E&gt;Negative Movie Review: &lt;/text&gt;&quot; }template = PromptTemplate(tp_dict, {'text': '&lt;/text&gt;'}, ice_token='&lt;/E&gt;')```The placeholder `&lt;/E&gt;` and `&lt;/text&gt;` will be replaced by in-context examples and testing input, respectively. For more detailed information about `PromptTemplate` (such as string-type template) , please see [tutorial1](https://github.com/Shark-NLP/OpenICL/blob/main/examples/tutorials/openicl_tutorial1_getting_started.ipynb).#### Step 3: Initialize the Retriever```pythonfrom openicl import TopkRetriever# Define a retriever using the previous `DataLoader`.# `ice_num` stands for the number of data in in-context examples.retriever = TopkRetriever(data, ice_num=8)```Here we use the popular &lt;a href=&quot;https://arxiv.org/abs/2101.06804&quot;&gt;TopK&lt;/a&gt; method to build the retriever. #### Step 4: Initialize the Inferencer ```pythonfrom openicl import PPLInferencerinferencer = PPLInferencer(model_name='distilgpt2')```#### Step 5: Inference and scoring```pythonfrom openicl import AccEvaluator# the inferencer requires retriever to collect in-context examples, as well as a template to wrap up these examples.predictions = inferencer.inference(retriever, ice_template=template)# compute accuracy for the predictionscore = AccEvaluator().score(predictions=predictions, references=data.references)print(score)```## Docs**(updating...)**[OpenICL Documentation](https://openicl.readthedocs.io/en/latest/index.html)## CitationIf you find this repository helpful, feel free to cite our paper:```bibtex@article{wu2023openicl,  title={OpenICL: An Open-Source Framework for In-context Learning},  author={Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu Qiao, Zhiyong Wu},  journal={arXiv preprint arXiv:2303.02913},  year={2023}}```</longdescription>
</pkgmetadata>