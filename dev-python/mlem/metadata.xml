<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![image](https://user-images.githubusercontent.com/6797716/165590476-994d4d93-8e98-4afb-b5f8-6f42b9d56efc.png)[![Check, test and release](https://github.com/iterative/mlem/actions/workflows/check-test-release.yml/badge.svg)](https://github.com/iterative/mlem/actions/workflows/check-test-release.yml)[![codecov](https://codecov.io/gh/iterative/mlem/branch/main/graph/badge.svg?token=WHU4OAB6O2)](https://codecov.io/gh/iterative/mlem)[![PyPi](https://img.shields.io/pypi/v/mlem.svg?label=pip&amp;logo=PyPI&amp;logoColor=white)](https://pypi.org/project/mlem)[![License: Apache 2.0](https://img.shields.io/github/license/iterative/mlem)](https://github.com/iterative/mlem/blob/master/LICENSE)&lt;!-- [![Maintainability](https://codeclimate.com/github/iterative/mlem/badges/gpa.svg)](https://codeclimate.com/github/iterative/mlem) --&gt;MLEM helps you package and deploy machine learning models.It saves ML models in a standard format that can be used in a variety of production scenarios such as real-time REST serving or batch processing.- **Run your ML models anywhere:**  Wrap models as a Python package or Docker Image, or deploy them to Heroku, SageMaker or Kubernetes (more platforms coming soon).  Switch between platforms transparently, with a single command.- **Model metadata into YAML automatically:**  Automatically include Python requirements and input data needs into a human-readable, deployment-ready format.  Use the same metafile on any ML framework.- **Stick to your training workflow:**  MLEM doesn't ask you to rewrite model training code.  Add just two lines around your Python code: one to import the library and one to save the model.- **Developer-first experience:**  Use the CLI when you feel like DevOps, or the API if you feel like a developer.## Why is MLEM special?The main reason to use MLEM instead of other tools is to adopt a **GitOps approach** to manage model lifecycles.- **Git as a single source of truth:**  MLEM writes model metadata to a plain text file that can be versioned in Git along with code.  This enables GitFlow and other software engineering best practices.- **Unify model and software deployment:**  Release models using the same processes used for software updates (branching, pull requests, etc.).- **Reuse existing Git infrastructure:**  Use familiar hosting like Github or Gitlab for model management, instead of having separate services.- **UNIX philosophy:**  MLEM is a modular tool that solves one problem very well.  It integrates well into a larger toolset from Iterative.ai, such as [DVC](https://dvc.org/) and [CML](https://cml.dev/).## UsageThis a quick walkthrough showcasing deployment functionality of MLEM.Please read [Get Started guide](https://mlem.ai/doc/get-started) for a full version.### InstallationMLEM requires Python 3.```console$ python -m pip install mlem```&gt; To install the pre-release version:&gt;&gt; ```console&gt; $ python -m pip install git+https://github.com/iterative/mlem&gt; ```### Saving the model```python# train.pyfrom mlem.api import savefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import load_irisdef main():    data, y = load_iris(return_X_y=True, as_frame=True)    rf = RandomForestClassifier(        n_jobs=2,        random_state=42,    )    rf.fit(data, y)    save(        rf,        &quot;models/rf&quot;,        sample_data=data,    )if __name__ == &quot;__main__&quot;:    main()```### CodificationCheck out what we have:```shell$ ls models/rfrf.mlem$ cat rf.mlem```&lt;details&gt;  &lt;summary&gt; Click to show `cat` output&lt;/summary&gt;```yamlartifacts:  data:    hash: ea4f1bf769414fdacc2075ef9de73be5    size: 163651    uri: rfmodel_type:  methods:    predict:      args:      - name: data        type_:          columns:          - sepal length (cm)          - sepal width (cm)          - petal length (cm)          - petal width (cm)          dtypes:          - float64          - float64          - float64          - float64          index_cols: []          type: dataframe      name: predict      returns:        dtype: int64        shape:        - null        type: ndarray    predict_proba:      args:      - name: data        type_:          columns:          - sepal length (cm)          - sepal width (cm)          - petal length (cm)          - petal width (cm)          dtypes:          - float64          - float64          - float64          - float64          index_cols: []          type: dataframe      name: predict_proba      returns:        dtype: float64        shape:        - null        - 3        type: ndarray  type: sklearnobject_type: modelrequirements:- module: sklearn  version: 1.0.2- module: pandas  version: 1.4.1- module: numpy  version: 1.22.3```&lt;/details&gt;### Deploying the modelIf you want to follow this Quick Start, you'll need to sign up on https://heroku.com,create an API_KEY and populate `HEROKU_API_KEY` env var (or run `heroku login` in command line).Besides, you'll need to run `heroku container:login`. This will log you in to Herokucontainer registry.Now we can [deploy the model with `mlem deploy`](https://mlem.ai/doc/get-started/deploying)(you need to use different `app_name`, since it's going to be published on https://herokuapp.com):```shell$ mlem deployment run heroku app.mlem \  --model models/rf \  --app_name example-mlem-get-started-app‚è≥Ô∏è Loading model from models/rf.mlem‚è≥Ô∏è Loading deployment from app.mlemüõ† Creating docker image for heroku  üõ† Building MLEM wheel file...  üíº Adding model files...  üõ† Generating dockerfile...  üíº Adding sources...  üíº Generating requirements file...  üõ† Building docker image registry.heroku.com/example-mlem-get-started-app/web...  ‚úÖ  Built docker image registry.heroku.com/example-mlem-get-started-app/web  üîº Pushing image registry.heroku.com/example-mlem-get-started-app/web to registry.heroku.com  ‚úÖ  Pushed image registry.heroku.com/example-mlem-get-started-app/web to registry.heroku.comüõ† Releasing app example-mlem-get-started-app formation‚úÖ  Service example-mlem-get-started-app is up. You can check it out at https://example-mlem-get-started-app.herokuapp.com/```## ContributingContributions are welcome! Please see our [Contributing Guide](https://mlem.ai/doc/contributing/core)for more details.Thanks to all our contributors!## CopyrightThis project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).By submitting a pull request to this project, you agree to license your contribution under the Apache license version 2.0 to this project.</longdescription>
</pkgmetadata>