<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># tensorizerModule, Model, and Tensor Serialization/Deserialization## TLDRExtremely fast model loads from HTTP/HTTPS, Redis, and S3 endpoints.GPT-J (`20GB`) loads at wire-speed (`~5GB/s`) on a 40GbE network, andis only bottlenecked by the Linux kernel TCP stack.## RationaleCoreWeave and our customers use KNative to deploy models as serverlessfunctions. How long a model takes to load is a major factor in the latencyof KNative scale-up. `tensorizer` is a tool to serialize models and theirassociated tensors into a single file that can be loaded quickly andefficiently off an HTTP/HTTPS or S3 endpoint.By not embedding the model in the container image, we can reduce thecontainer image size and the time it takes to load the model. This isespecially important for models that are large in size, such as[EleutherAI/gpt-neox-20B](https://huggingface.co/EleutherAI/gpt-neox-20B)that weighs in at `~40GB`.This decoupling of the model from the container image also allows us toupdate the model without having to rebuild the container image. This allowsus to quickly iterate on the model and deploy new versions without havingto wait for the container image to build or for the container image cacheto be populated.`tensorizer` has S3 support, so we can store the serialized model in S3object storage, and perform streaming loads from S3. This allows us tostream the model directly from S3 into the container without having todownload the model to the container's local filesystem. This alsopertains to HTTP/HTTPS endpoints, as S3 is just an HTTP/HTTPS endpoint.`tensorizer` also has support for loading models from a local filesystem,so you can use it to serialize models locally and load them locally. Thisis extremely fast, as the same principles that make it fast for HTTP/HTTPSand S3 endpoints also apply to local filesystems.`tensorizer` has preliminary support for Redis, but it is not recommendedfor model deployment due to the lack of distributed caching. It is intendedfor sharing state between inference pods, or for loading data on a per-requestbasis from a Redis cache.## Installation### From PyPI`tensorizer` can be installed from PyPI with `pip`:```bashpython -m pip install tensorizer```### From SourceYou can also install `tensorizer` from source using `pip`.To clone the repository and install `tensorizer` in[editable mode](https://pip.pypa.io/en/stable/topics/local-project-installs/#editable-installs),run:```bashgit clone https://github.com/coreweave/tensorizercd tensorizerpython -m pip install -e .```Or, run the following for `pip` to install `tensorizer`[directly from GitHub](https://pip.pypa.io/en/stable/topics/vcs-support/#git):```bashpython -m pip install git+https://github.com/coreweave/tensorizer```## Basic UsageSerialization is done with the `TensorSerializer` class. It takes a`path_uri` argument that can be a local filesystem path, an HTTP/HTTPSendpoint, or an S3 endpoint.`write_module` is the main method of the `TensorSerializer` class. Ittakes a `torch.nn.Module` and serializes the tensors to the `path_uri`endpoint.The below example serializes the `EleutherAI/gpt-j-6B` model to an S3endpoint. It assumes that you have already configured your S3credentials in `~/.s3cfg`.**NOTE:** Loading and serializing `gpt-j-6B` will take a lot of CPU RAM,up to `~20GB`. Additionally, when loading `gpt-j-6B` into a GPU, youwill need about `~16GB` of VRAM. If you don't have that much RAM or VRAM,you can use the smaller `gpt-neo-125M` model instead.**NOTE2:** The below examples require the `transformers` and `accelerate`libraries. You can install them with `pip`:```bashpython -m pip install transformers accelerate```[serialize.py](examples/serialize.py)```pythonimport torchfrom tensorizer import TensorSerializerfrom transformers import AutoModelForCausalLMmodel_ref = &quot;EleutherAI/gpt-j-6B&quot;# For less intensive requirements, swap above with the line below:# model_ref = &quot;EleutherAI/gpt-neo-125M&quot;model_name = model_ref.split(&quot;/&quot;)[-1]# Change this to your S3 bucket.s3_bucket = &quot;bucket&quot;s3_uri = f&quot;s3://{s3_bucket}/{model_name}.tensors&quot;model = AutoModelForCausalLM.from_pretrained(    model_ref,    revision=&quot;float16&quot;,    torch_dtype=torch.float16,    low_cpu_mem_usage=True,)serializer = TensorSerializer(s3_uri)serializer.write_module(model)serializer.close()```Conversely, deserialization is done with the `TensorDeserializer` class.It takes a `path_uri` argument that can be a local filesystem path, anHTTP/HTTPS endpoint, or an S3 endpoint.`load_into_module` is the main method of the `TensorDeserializer` class.It takes a `torch.nn.Module` and loads the tensors from the `path_uri`endpoint into the `torch.nn.Module`.The below example loads the `EleutherAI/gpt-j-6B` model from an S3endpoint.[deserialize.py](examples/deserialize.py)```pythonimport timeimport torchfrom tensorizer import TensorDeserializerfrom tensorizer.utils import no_init_or_tensor, convert_bytes, get_mem_usagefrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfigmodel_ref = &quot;EleutherAI/gpt-j-6B&quot;# To run this at home, swap this with the line below for a smaller example:# model_ref = &quot;EleutherAI/gpt-neo-125M&quot;model_name = model_ref.split(&quot;/&quot;)[-1]# Change this to your S3 bucket.s3_bucket = &quot;bucket&quot;s3_uri = f&quot;s3://{s3_bucket}/{model_name}.tensors&quot;config = AutoConfig.from_pretrained(model_ref)# This ensures that the model is not initialized.with no_init_or_tensor():    model = AutoModelForCausalLM.from_config(config)before_mem = get_mem_usage()# Lazy load the tensors from S3 into the model.start = time.time()deserializer = TensorDeserializer(s3_uri, plaid_mode=True)deserializer.load_into_module(model)end = time.time()# Brag about how fast we are.total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)duration = end - startper_second = convert_bytes(deserializer.total_tensor_bytes / duration)after_mem = get_mem_usage()deserializer.close()print(f&quot;Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s&quot;)print(f&quot;Memory usage before: {before_mem}&quot;)print(f&quot;Memory usage after: {after_mem}&quot;)# Tokenize and generatemodel.eval()tokenizer = AutoTokenizer.from_pretrained(model_ref)eos = tokenizer.eos_token_idinput_ids = tokenizer.encode(    &quot;¡Hola! Encantado de conocerte. hoy voy a&quot;, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)with torch.no_grad():    output = model.generate(        input_ids, max_new_tokens=50, do_sample=True, pad_token_id=eos    )print(f&quot;Output: {tokenizer.decode(output[0], skip_special_tokens=True)}&quot;)```It should produce output similar to the following, with GPT-J-6B:```Deserialized model in 6.25 secondsTest Output: ¡Hola! Encantado de conocerte. hoy voy a comentar por primeravez una teoría de trineo, que quizá te parezcaalgo desconocido, ya que en este mundo hanllegado a dominar tantos```More practical examples for the usage of `tensorizer` can be found in[examples/hf_serialization.py](examples/hf_serialization.py),where `df_main()` serializes models from[HuggingFace Diffusers](https://github.com/huggingface/diffusers)and `hf_main()` serializes[HuggingFace Transformers](https://github.com/huggingface/transformers) models.## BenchmarksYou can run your own benchmarks on CoreWeave or your own Kubernetes clusterby using the `benchmark.yaml` file in the `examples/benchmark_buffer_size`directory. Please see the [README](examples/benchmark_buffer_size/README.md).## Available Pre-Tensorized Models on the CoreWeave CloudThe following models are available on the CoreWeave Cloud for free, and can beused with the `TensorDeserializer` class. The S3 support defaults to the`accel-object.ord1.coreweave.com` endpoint, and the bucket to use as `tensorized`.We name the keys in the S3 bucket after the HuggingFace model identifier, andappend the `/fp16` suffix for the half-precision version.For example, the S3 URI for the `EleutherAI/gpt-j-6B` model is:`s3://tensorized/EleutherAI/gpt-j-6B/fp16/model.tensors`The below table shows the available models and their S3 URIs.### Large Language Models| Model                                                                                   | Precision | S3 URI                                                              ||-----------------------------------------------------------------------------------------|-----------|---------------------------------------------------------------------|| [EleutherAI/gpt-neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125M)               | `fp32`    | `s3://tensorized/EleutherAI/gpt-neo-125M/model.tensors`             || [EleutherAI/gpt-neo-125M](https://huggingface.co/EleutherAI/gpt-neo-125M)               | `fp16`    | `s3://tensorized/EleutherAI/gpt-neo-125M/fp16/model.tensors`        || [EleutherAI/gpt-neo-1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B)               | `fp32`    | `s3://tensorized/EleutherAI/gpt-neo-1.3B/model.tensors`             || [EleutherAI/gpt-neo-1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B)               | `fp16`    | `s3://tensorized/EleutherAI/gpt-neo-1.3B/fp16/model.tensors`        || [EleutherAI/gpt-neo-2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B)               | `fp32`    | `s3://tensorized/EleutherAI/gpt-neo-2.7B/model.tensors`             || [EleutherAI/gpt-neo-2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B)               | `fp16`    | `s3://tensorized/EleutherAI/gpt-neo-2.7B/fp16/model.tensors`        || [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B)                       | `fp32`    | `s3://tensorized/EleutherAI/gpt-j-6B/model.tensors`                 || [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B)                       | `fp16`    | `s3://tensorized/EleutherAI/gpt-j-6B/fp16/model.tensors`            || [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b)               | `fp32`    | `s3://tensorized/EleutherAI/gpt-neox-20b/model.tensors`             || [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b)               | `fp16`    | `s3://tensorized/EleutherAI/gpt-neox-20b/fp16/model.tensors`        || [EleutherAI/pythia-70m](https://huggingface.co/EleutherAI/pythia-70m)                   | `fp32`    | `s3://tensorized/EleutherAI/pythia-70m/model.tensors`               || [EleutherAI/pythia-70m](https://huggingface.co/EleutherAI/pythia-70m)                   | `fp16`    | `s3://tensorized/EleutherAI/pythia-70m/fp16/model.tensors`          || [EleutherAI/pythia-1.4b](https://huggingface.co/EleutherAI/pythia-1.4b)                 | `fp32`    | `s3://tensorized/EleutherAI/pythia-1.4b/model.tensors`              || [EleutherAI/pythia-1.4b](https://huggingface.co/EleutherAI/pythia-1.4b)                 | `fp16`    | `s3://tensorized/EleutherAI/pythia-1.4b/fp16/model.tensors`         || [EleutherAI/pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b)                 | `fp32`    | `s3://tensorized/EleutherAI/pythia-2.8b/model.tensors`              || [EleutherAI/pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b)                 | `fp16`    | `s3://tensorized/EleutherAI/pythia-2.8b/fp16/model.tensors`         || [EleutherAI/pythia-6.9b](https://huggingface.co/EleutherAI/pythia-6.9b)                 | `fp32`    | `s3://tensorized/EleutherAI/pythia-6.9b/model.tensors`              || [EleutherAI/pythia-6.9b](https://huggingface.co/EleutherAI/pythia-6.9b)                 | `fp16`    | `s3://tensorized/EleutherAI/pythia-6.9b/fp16/model.tensors`         || [EleutherAI/pythia-12b](https://huggingface.co/EleutherAI/pythia-12b)                   | `fp32`    | `s3://tensorized/EleutherAI/pythia-12b/model.tensors`               || [EleutherAI/pythia-12b](https://huggingface.co/EleutherAI/pythia-12b)                   | `fp16`    | `s3://tensorized/EleutherAI/pythia-12b/fp16/model.tensors`          || [EleutherAI/pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped)   | `fp32`    | `s3://tensorized/EleutherAI/pythia-70m-deduped/model.tensors`       || [EleutherAI/pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped)   | `fp16`    | `s3://tensorized/EleutherAI/pythia-70m-deduped/fp16/model.tensors`  || [EleutherAI/pythia-1.4b-deduped](https://huggingface.co/EleutherAI/pythia-1.4b-deduped) | `fp32`    | `s3://tensorized/EleutherAI/pythia-1.4b-deduped/model.tensors`      || [EleutherAI/pythia-1.4b-deduped](https://huggingface.co/EleutherAI/pythia-1.4b-deduped) | `fp16`    | `s3://tensorized/EleutherAI/pythia-1.4b-deduped/fp16/model.tensors` || [EleutherAI/pythia-2.8b-deduped](https://huggingface.co/EleutherAI/pythia-2.8b-deduped) | `fp32`    | `s3://tensorized/EleutherAI/pythia-2.8b-deduped/model.tensors`      || [EleutherAI/pythia-2.8b-deduped](https://huggingface.co/EleutherAI/pythia-2.8b-deduped) | `fp16`    | `s3://tensorized/EleutherAI/pythia-2.8b-deduped/fp16/model.tensors` || [EleutherAI/pythia-6.9b-deduped](https://huggingface.co/EleutherAI/pythia-6.9b-deduped) | `fp32`    | `s3://tensorized/EleutherAI/pythia-6.9b-deduped/model.tensors`      || [EleutherAI/pythia-6.9b-deduped](https://huggingface.co/EleutherAI/pythia-6.9b-deduped) | `fp16`    | `s3://tensorized/EleutherAI/pythia-6.9b-deduped/fp16/model.tensors` || [EleutherAI/pythia-12b-deduped](https://huggingface.co/EleutherAI/pythia-12b-deduped)   | `fp32`    | `s3://tensorized/EleutherAI/pythia-12b-deduped/model.tensors`       || [EleutherAI/pythia-12b-deduped](https://huggingface.co/EleutherAI/pythia-12b-deduped)   | `fp16`    | `s3://tensorized/EleutherAI/pythia-12b-deduped/fp16/model.tensors`  || [KoboldAI/fairseq-dense-125M](https://huggingface.co/KoboldAI/fairseq-dense-125M)       | `fp32`    | `s3://tensorized/KoboldAI/fairseq-dense-125M/model.tensors`         || [KoboldAI/fairseq-dense-125M](https://huggingface.co/KoboldAI/fairseq-dense-125M)       | `fp16`    | `s3://tensorized/KoboldAI/fairseq-dense-125M/fp16/model.tensors`    || [KoboldAI/fairseq-dense-355M](https://huggingface.co/KoboldAI/fairseq-dense-355M)       | `fp32`    | `s3://tensorized/KoboldAI/fairseq-dense-355M/model.tensors`         || [KoboldAI/fairseq-dense-355M](https://huggingface.co/KoboldAI/fairseq-dense-355M)       | `fp16`    | `s3://tensorized/KoboldAI/fairseq-dense-355M/fp16/model.tensors`    || [KoboldAI/fairseq-dense-2.7B](https://huggingface.co/KoboldAI/fairseq-dense-2.7B)       | `fp32`    | `s3://tensorized/KoboldAI/fairseq-dense-2.7B/model.tensors`         || [KoboldAI/fairseq-dense-2.7B](https://huggingface.co/KoboldAI/fairseq-dense-2.7B)       | `fp16`    | `s3://tensorized/KoboldAI/fairseq-dense-2.7B/fp16/model.tensors`    || [KoboldAI/fairseq-dense-6.7B](https://huggingface.co/KoboldAI/fairseq-dense-6.7B)       | `fp32`    | `s3://tensorized/KoboldAI/fairseq-dense-6.7B/model.tensors`         || [KoboldAI/fairseq-dense-6.7B](https://huggingface.co/KoboldAI/fairseq-dense-6.7B)       | `fp16`    | `s3://tensorized/KoboldAI/fairseq-dense-6.7B/fp16/model.tensors`    || [KoboldAI/fairseq-dense-13B](https://huggingface.co/KoboldAI/fairseq-dense-13B)         | `fp32`    | `s3://tensorized/KoboldAI/fairseq-dense-13B/model.tensors`          || [KoboldAI/fairseq-dense-13B](https://huggingface.co/KoboldAI/fairseq-dense-13B)         | `fp16`    | `s3://tensorized/KoboldAI/fairseq-dense-13B/fp16/model.tensors`     || [Salesforce/codegen-350M-mono](https://huggingface.co/Salesforce/codegen-350M-mono)     | `fp32`    | `s3://tensorized/Salesforce/codegen-350M-mono/model.tensors`        || [Salesforce/codegen-350M-mono](https://huggingface.co/Salesforce/codegen-350M-mono)     | `fp16`    | `s3://tensorized/Salesforce/codegen-350M-mono/fp16/model.tensors`   || [Salesforce/codegen-350M-multi](https://huggingface.co/Salesforce/codegen-350M-multi)   | `fp32`    | `s3://tensorized/Salesforce/codegen-350M-multi/model.tensors`       || [Salesforce/codegen-350M-multi](https://huggingface.co/Salesforce/codegen-350M-multi)   | `fp16`    | `s3://tensorized/Salesforce/codegen-350M-multi/fp16/model.tensors`  || [Salesforce/codegen-2B-multi](https://huggingface.co/Salesforce/codegen-2B-multi)       | `fp32`    | `s3://tensorized/Salesforce/codegen-2B-multi/model.tensors`         || [Salesforce/codegen-2B-multi](https://huggingface.co/Salesforce/codegen-2B-multi)       | `fp16`    | `s3://tensorized/Salesforce/codegen-2B-multi/fp16/model.tensors`    || [Salesforce/codegen-6B-mono](https://huggingface.co/Salesforce/codegen-6B-mono)         | `fp32`    | `s3://tensorized/Salesforce/codegen-6B-mono/model.tensors`          || [Salesforce/codegen-6B-mono](https://huggingface.co/Salesforce/codegen-6B-mono)         | `fp16`    | `s3://tensorized/Salesforce/codegen-6B-mono/fp16/model.tensors`     || [Salesforce/codegen-6B-multi](https://huggingface.co/Salesforce/codegen-6B-multi)       | `fp32`    | `s3://tensorized/Salesforce/codegen-6B-multi/model.tensors`         || [Salesforce/codegen-6B-multi](https://huggingface.co/Salesforce/codegen-6B-multi)       | `fp16`    | `s3://tensorized/Salesforce/codegen-6B-multi/fp16/model.tensors`    || [Salesforce/codegen-16B-mono](https://huggingface.co/Salesforce/codegen-16B-mono)       | `fp32`    | `s3://tensorized/Salesforce/codegen-16B-mono/model.tensors`         || [Salesforce/codegen-16B-mono](https://huggingface.co/Salesforce/codegen-16B-mono)       | `fp16`    | `s3://tensorized/Salesforce/codegen-16B-mono/fp16/model.tensors`    || [Salesforce/codegen-16B-multi](https://huggingface.co/Salesforce/codegen-16B-multi)     | `fp32`    | `s3://tensorized/Salesforce/codegen-16B-multi/model.tensors`        || [Salesforce/codegen-16B-multi](https://huggingface.co/Salesforce/codegen-16B-multi)     | `fp16`    | `s3://tensorized/Salesforce/codegen-16B-multi/fp16/model.tensors`   |### Generative Diffusion Models| Model                                                                                                       | Component    | Precision | S3 URI                                                                                 ||-------------------------------------------------------------------------------------------------------------|--------------|-----------|----------------------------------------------------------------------------------------|| [RunwayML/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)                     | `VAE`        | `fp32`    | `s3://tensorized/runwayml/stable-diffusion-v1-5/vae.tensors`                           || [RunwayML/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)                     | `UNet`       | `fp32`    | `s3://tensorized/runwayml/stable-diffusion-v1-5/unet.tensors`                          || [RunwayML/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)                     | `TextEnc`    | `fp32`    | `s3://tensorized/runwayml/stable-diffusion-v1-5/text_encoder.tensors`                  || [RunwayML/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)                     | `VAE`        | `fp16`    | `s3://tensorized/runwayml/stable-diffusion-v1-5/fp16/vae.tensors`                      || [RunwayML/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)                     | `UNet`       | `fp16`    | `s3://tensorized/runwayml/stable-diffusion-v1-5/fp16/unet.tensors`                     || [RunwayML/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)                     | `TextEnc`    | `fp16`    | `s3://tensorized/runwayml/stable-diffusion-v1-5/fp16/text_encoder.tensors`             || [StabilityAI/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)                 | `VAE`        | `fp32`    | `s3://tensorized/stabilityai/stable-diffusion-2-1/vae.tensors`                         || [StabilityAI/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)                 | `UNet`       | `fp32`    | `s3://tensorized/stabilityai/stable-diffusion-2-1/unet.tensors`                        || [StabilityAI/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)                 | `TextEnc`    | `fp32`    | `s3://tensorized/stabilityai/stable-diffusion-2-1/text_encoder.tensors`                || [StabilityAI/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)                 | `VAE`        | `fp16`    | `s3://tensorized/stabilityai/stable-diffusion-2-1/fp16/vae.tensors`                    || [StabilityAI/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)                 | `UNet`       | `fp16`    | `s3://tensorized/stabilityai/stable-diffusion-2-1/fp16/unet.tensors`                   || [StabilityAI/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)                 | `TextEnc`    | `fp16`    | `s3://tensorized/stabilityai/stable-diffusion-2-1/fp16/text_encoder.tensors`           || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `VAE`        | `fp32`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/vae.tensors`                 || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `UNet`       | `fp32`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/unet.tensors`                || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `TextEnc`    | `fp32`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/text_encoder.tensors`        || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `TextEnc2`   | `fp32`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/text_encoder_2.tensors`      || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `VAE`        | `fp16`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/fp16/vae.tensors`            || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `UNet`       | `fp16`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/fp16/unet.tensors`           || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `TextEnc`    | `fp16`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/fp16/text_encoder.tensors`   || [StabilityAI/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) | `TextEnc2`   | `fp16`    | `s3://tensorized/stabilityai/stable-diffusion-xl-base-1.0/fp16/text_encoder_2.tensors` |## S3 Usage Notes`tensorizer` uses the `boto3` library to interact with S3. The easiest wayto use `tensorizer` with S3 is to configure your S3 credentials in`~/.s3cfg`.If you don't want to use `~/.s3cfg`, or wish to use a `.s3cfg` config filesaved at a nonstandard location (e.g. under `/var/run`), you can also specifyyour S3 credentials using the `tensorizer.stream_io.open_stream()` function,and then pass that into the `TensorSerializer` or `TensorDeserializer`constructor.The `stream_io.open_stream()` function takes a `path_uri` argument, which canbe an `s3://` URI, and accepts the following keyword arguments:* `s3_access_key_id`: S3 access key ID* `s3_secret_access_key`: S3 secret access key* `s3_endpoint`: S3 endpoint*Or,** `s3_config_path`: Alternative filesystem path to a `.s3cfg` config fileFor example:```pythonTensorSerializer(    open_stream(s3_uri,                &quot;wb&quot;,                s3_access_key_id=ACCESS_KEY,                s3_secret_access_key=SECRET_KEY,                s3_endpoint=&quot;object.ord1.coreweave.com&quot;))```and...```pythonTensorDeserializer(    open_stream(s3_uri,                &quot;rb&quot;,                s3_access_key_id=ACCESS_KEY,                s3_secret_access_key=SECRET_KEY,                s3_endpoint=&quot;object.ord1.coreweave.com&quot;))```**NOTE:** For faster object downloads in the CoreWeave Cloud, you can usethe `accel-object.ord1.coreweave.com` endpoint. This endpoint is optimizedfor object downloads, and will be faster than the `object.ord1.coreweave.com`endpoint once the object is cached.**NOTE2:** The cache above does not get invalidated when the object is updatedin S3. If you update an object in S3, you will need to wait for the cache toexpire before you can download the updated object. This takes 24 hours sincethe last download.For this reason, it is recommended to use a unique S3 key for each versionof a model if you use the `accel-object.ord1.coreweave.com` endpoint.## Additional Features`tensorizer` has a few additional features that make it more useful thanjust a serialization/deserialization tool.### Plaid Mode`tensorizer` has a `plaid_mode` argument that can be passed to the`TensorDeserializer` class. When `plaid_mode` is `True`, `tensorizer`will load the tensors extremely fast. This is done by loading the tensorsinto a `torch.nn.Module` that is not initialized, by overriding the`__init__` method of the `torch.nn.Module` to do nothing.The tensors are them loaded into a buffer, and the buffer is zero-copiedinto the uninitialized `torch.nn.Module`. This is unsafe, and should onlybe used in inference cases where the model is not being trained.### `state_dict` SupportThe `TensorDeserializer` object can be used as-is as a `state_dict` for`torch.nn.Module.load_state_dict`. This is useful for loading the tensorsinto a `torch.nn.Module` that is already initialized, or for inspection.Keep in mind that `load_state_dict` is not a fast operation, and willlikely be much slower than `load_into_module`.The `state_dict` can also be used to initialize a HuggingFace TransformersAutoModel. But HuggingFace Transformers performs three or more copies ofthe data, so memory use will explode.### `bfloat16` SupportTensorizer supports models using the `bfloat16` data type. However, tensorizeruses numpy to save the tensors as binary and numpy doesn't support `bfloat16`.This means that special conversions need to be applied.To be saved, the torch tensor is cast to `int16` before being converted tonumpy, which doesn't change any of the underlying data. When serialized, theoriginal `bfloat16` datatype string is also saved so that it will be cast backto `bfloat16` during the deserialization process.The `complex32` datatype is supported in a similar way, by casting to `int32`.The quantized datatypes (`qint8`, `qint32`, etc.) are not currently supportedby tensorizer as they would require supplemental quantization parameters to bedeserialized correctly.**NOTE:** The exact choice of intermediate types as `int16` and `int32` isconsidered an implementation detail, and is subject to change,so they should not be relied upon.**NOTE2:** This does not interfere with storing actual `int` datatypesused in tensors in tensorized files.### Numpy SupportTensorizer can be used with `numpy` directly to read and write`numpy.ndarray`s.The serializer's `write_tensor` function handles supplying both`torch.Tensor`s and `numpy.ndarray`s.The deserializer has a separate function `read_numpy_arrays` that will returnthe data as `numpy.ndarray`s.As explained above in [bfloat16 support](#bfloat16-support), tensorizer usesspecial conversions to write &quot;opaque&quot; datatypes, those not supported by numpy.Therefore, special considerations need to be taken when loading such data as`numpy.ndarray`s.By default, the `TensorDeserializer.read_numpy_arrays` function sets its`allow_raw_data` parameter to `False`. This means that if a file containsopaque datatypes, a `ValueError` will be raised during deserialization.If you want to return the raw data regardless, set `allow_raw_data` to `True`.Otherwise, the file may be read with `TensorDeserializer.read_tensors`instead, which yields `torch.Tensor` objects of the correct datatype.A fifth and sixth variable are also returned by the `read_numpy_arrays`generator. The fifth is a `bool` that indicates whether the returned arrayhas an opaque datatype and requires special handling (only legal when`allow_raw_data=True`). The sixth is a string describing the true, non-numpydatatype that the raw data should be interpreted as in such cases.For all other datatypes that require no special handling, these are returned as`False` and `None`, respectively.The exact numpy datatypes used by the returned opaque `numpy.ndarray` objectsis not guaranteed, and should not be relied upon.## Running Tests`tensorizer` uses `unittest` for testing.The tests have their own set of dependencies, which can be installed with`pip install -r tests/requirements.txt`.Some tests require a GPU, and will be skipped if no GPU is available.To run the tests, run the following in the root of the repository:```bashpython -m pip install -e .python -m pip install -r tests/requirements.txtpython -m unittest discover tests/ --verbose```</longdescription>
</pkgmetadata>