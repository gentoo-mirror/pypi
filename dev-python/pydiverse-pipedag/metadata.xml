<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pydiverse.pipedag[![CI](https://github.com/pydiverse/pydiverse.pipedag/actions/workflows/tests.yml/badge.svg)](https://github.com/pydiverse/pydiverse.pipedag/actions/workflows/tests.yml)A pipeline orchestration library executing tasks within one python session. It takes care of SQL table(de)materialization, caching and cache invalidation. Blob storage is supported as well for examplefor storing model files.This is an early stage version 0.x which lacks documentation. Please contacthttps://github.com/orgs/pydiverse/teams/code-owners if you like to become an early adopteror to contribute early stage usage examples.## Usagepydiverse.pipedag can either be installed via pypi with `pip install pydiverse-pipedag` or via conda-forgewith `conda install pydiverse-pipedag -c conda-forge`.## ExampleA flow can look like this (i.e. put this in a file named `run_pipeline.py`):```pythonimport pandas as pdimport sqlalchemy as safrom pydiverse.pipedag import Flow, Stage, Table, materializefrom pydiverse.pipedag.context import StageLockContext@materialize(lazy=True)def lazy_task_1():    return sa.select(        sa.literal(1).label(&quot;x&quot;),        sa.literal(2).label(&quot;y&quot;),    )@materialize(lazy=True, input_type=sa.Table)def lazy_task_2(input1: sa.Table, input2: sa.Table):    query = sa.select(        (input1.c.x * 5).label(&quot;x5&quot;),        input2.c.a,    ).select_from(input1.outerjoin(input2, input2.c.x == input1.c.x))    return Table(query, name=&quot;task_2_out&quot;, primary_key=[&quot;a&quot;])@materialize(lazy=True, input_type=sa.Table)def lazy_task_3(input1: sa.Table):    return sa.text(f&quot;SELECT * FROM {input1.original.schema}.{input1.name}&quot;)@materialize(lazy=True, input_type=sa.Table)def lazy_task_4(input1: sa.Table):    return sa.text(f&quot;SELECT * FROM {input1.original.schema}.{input1.name}&quot;)@materialize(nout=2, version=&quot;1.0.0&quot;)def eager_inputs():    dfA = pd.DataFrame(        {            &quot;a&quot;: [0, 1, 2, 4],            &quot;b&quot;: [9, 8, 7, 6],        }    )    dfB = pd.DataFrame(        {            &quot;a&quot;: [2, 1, 0, 1],            &quot;x&quot;: [1, 1, 2, 2],        }    )    return Table(dfA, &quot;dfA&quot;), Table(dfB, &quot;dfB_%%&quot;)@materialize(version=&quot;1.0.0&quot;, input_type=pd.DataFrame)def eager_task(tbl1: pd.DataFrame, tbl2: pd.DataFrame):    return tbl1.merge(tbl2, on=&quot;x&quot;)def main():    with Flow() as f:        with Stage(&quot;stage_1&quot;):            lazy_1 = lazy_task_1()            a, b = eager_inputs()        with Stage(&quot;stage_2&quot;):            lazy_2 = lazy_task_2(lazy_1, b)            lazy_3 = lazy_task_3(lazy_2)            eager = eager_task(lazy_1, b)        with Stage(&quot;stage_3&quot;):            lazy_4 = lazy_task_4(lazy_2)        _ = lazy_3, lazy_4, eager  # unused terminal output tables    # Run flow    result = f.run()    assert result.successful    # Run in a different way for testing    with StageLockContext():        result = f.run()        assert result.successful        assert result.get(lazy_1, as_type=pd.DataFrame)[&quot;x&quot;][0] == 1if __name__ == &quot;__main__&quot;:    main()```Create a file called `pipedag.yaml` in the same directory:```yamlinstances:  __any__:    network_interface: &quot;127.0.0.1&quot;    auto_table:      - &quot;pandas.DataFrame&quot;      - &quot;sqlalchemy.sql.expression.TextClause&quot;      - &quot;sqlalchemy.sql.expression.Selectable&quot;    fail_fast: true    instance_id: pipedag_default    table_store:      class: &quot;pydiverse.pipedag.backend.table.SQLTableStore&quot;      args:        url: &quot;postgresql://sa:Pydiverse23@127.0.0.1:6543/{instance_id}&quot;        create_database_if_not_exists: True        print_materialize: true        print_sql: true      local_table_cache:        store_input: true        store_output: true        use_stored_input_as_cache: true        class: &quot;pydiverse.pipedag.backend.table.cache.ParquetTableCache&quot;        args:          base_path: &quot;/tmp/pipedag/table_cache&quot;    blob_store:      class: &quot;pydiverse.pipedag.backend.blob.FileBlobStore&quot;      args:        base_path: &quot;/tmp/pipedag/blobs&quot;    lock_manager:      class: &quot;pydiverse.pipedag.backend.lock.DatabaseLockManager&quot;    orchestration:      class: &quot;pydiverse.pipedag.engine.SequentialEngine&quot;```If you don't have a postgres database at hand, you can start a postgres database, with the following `docker-compose.yaml` file:```yamlversion: &quot;3.9&quot;services:  postgres:    image: postgres    environment:      POSTGRES_USER: sa      POSTGRES_PASSWORD: Pydiverse23    ports:      - &quot;6543:5432&quot;```Run `docker-compose up` in the directory of your `docker-compose.yaml` and then executethe flow script as follows with a shell like `bash` and a python environment thatincludes `pydiverse-pipedag`, `pandas`, and `sqlalchemy`:```bashpoetry run python run_pipeline.py```Finally, you may connect to your localhost postgres database `pipedag_default` andlook at tables in schemas `stage_1`..`stage_3`.If you don't have a SQL UI at hand, you may use `psql` command line tool inside the docker container.Check out the `NAMES` column in `docker ps` output. If the name of your postgres container is`example_postgres_1`, then you can look at output tables like this:```bashdocker exec example_postgres_1 psql --username=sa --dbname=pipedag_default -c 'select * from stage_1.dfa;'```Or more interactively:```bashdocker exec -t -i example_postgres_1 bashpsql --username=sa --dbname=pipedag_default\dt stage_*.*select * from stage_2.task_2_out;```## Troubleshooting### Installing mssql odbc driver for linuxInstalling withinstructions [here](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16#suse18)worked.But `odbcinst -j` revealed that it installed the configuration in `/etc/unixODBC/*`. But conda installed pyodbc bringsits own `odbcinst` executable and that shows odbc config files are expected in `/etc/*`. Symlinks were enough to fix theproblem. Try `python -c 'import pyodbc;print(pyodbc.drivers())'` and see whether you get more than an empty list.Furthermore, make sure you use 127.0.0.1 instead of localhost. It seems that /etc/hosts is ignored.</longdescription>
</pkgmetadata>