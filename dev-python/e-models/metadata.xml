<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>e-models========Suite of tools to assist in the build of extraction models with scrapy spidersInstallation:```$ pip install e-models```## scrapyutils modulescrapyutils module provides two classes, one for extending `scrapy.http.TextResponse` and another forextending `scrapy.loader.ItemLoader`. The extensions provide methods that:1. Allow to extract item data in the text (markdown) domain instead of the html source domain. For this purpose, a new type of selector has been added.2. The main purpose of this approach is the generation of datasets suitable for training transformer models for text extraction (aka extractive question answering, EQA)3. As a secondary objective, it provides an alternative kind of selector to xpath and css selectors for extraction of data from the html source, that may be more suitable and readable for humans.4. In many situations, and specially when there is not an id or a class to spot accurately the text, the expresion in terms of regular expressions in the domain of markdown can be simpler.5. The additional cost of maintenance is zero. Even if you are introducing a new kind of selector that may be unknown for most people, this only characterizes single lines and does not condition   any other line of the spider code. If the selector still works, it is not needed to be changed. If it doesn't work anymore, it can be replaced by common and known selectors without affecting   anything else, except that the corresponding field will be missing from the dataset generation that the new selector type aim for.#### Set upInstead of subclass your item loaders from `scrapy.loader.ItemLoader`, use `emodels.scrapyutils.loader.ExtractItemLoader`. This action will not affect the working of itemloaders and will enable theproperties just described above. In addition, in order to save the collected extraction data, it is required to set the environment variable `EMODELS_SAVE_EXTRACT_ITEMS` to 1. The collectedextraction data will be stored at `&lt;user home folder&gt;/.datasets/items/&lt;item class name&gt;/&lt;sequence number&gt;.jl.gz`. The base folder `&lt;user home folder&gt;/.datasets` is the default one. You cancustomize it via the environment variable `EMODELS_DIR`.So, in order to maintain a clean and organized dataset, only enable extract items saving when you are sure you have the correct extraction selectors. Then run locally:```EMODELS_SAVE_EXTRACT_ITEMS=1 scrapy crawl myspider```In addition, in order to have your dataset organized, you may want to choose the same item class name for same item schema, even accross multiple projects. And avoid to repeat it among items with differentschema. However, in general you will use extraction data from all classes of items at same time in order to train a transformer model, as this is the way how transformers learn to generalize. In additionavoid to create multiple collections for same item class and target site, as they will not provide diversity and you may commit the mistake of having same sample distribution in train and test datasets.At the end you will have a transformer model that is suited to extract any kind of item, as they are trained not to extract &quot;data from x item&quot; but instead to recognize and extract based on fields.So, even if you didn't train the transformer to extract a specific item class, it will do great if you trained it to extract its fields, if it already learned to extract same fields fromother item classes. You only need to ask the correct question. For example, given an html page as a context, you can ask the model: `which is the phone number?`. You don't need to specifywhich kind of data (a business? a person? an organization?) you expect to find there.For instructions on how to create datasets for training your transformers, go to the datasets module sections.#### Markdown SelectorsThe new kind of selector is based on regular expressions, but it also admits id and class specifications for further guiding the selector extraction. Many examples can be seen in`tests/test_scrapyutils.py`. This selector does not operate on the html source, but on a text rendered (markdown) version of it. The loader function that applies this selector is(copied from `emodels/scrapyutils/loaders.py`):```python    def add_text_re(        self,        attr: str,        reg: str = &quot;(.+?)&quot;,        tid: Optional[str] = None,        flags: int = 0,        skip_prefix: str = DEFAULT_SKIP_PREFIX,        strict_tid: bool = False,        idx: int = 0,        *processors,        **kw,    ):        &quot;&quot;&quot;        attr - item attribute where selector extracted value will be assigned to.        reg - Optional regular expression (it is optional because you can also use tid alone)        tid - Optional css id or class specification (start with either # or .). When this parameter is present,              regular expression match will be restricted to the text region with the specified id or class. Note:              when the tid string starts with #, it is also able to match the itemprop attribute, not only the id.        flags - Optional regular expression flag        skip_prefix - This prefix is added to the provided regular expression in order to skip it from the result.              The default one is any non alphanumeric character at begining of the line and in most cases              you will use this value. Provided for convenience, in order to avoid to repeat it frequently              in the regular expression parameter, making it more natural.        strict_tid - The default behavior of selectors is to match the regex against full single entire lines (except              when using for example the flag re.S), even when there are multiple ids or classes in same line. If you              want a stricter match against regions inside lines, set this parameter to True. Of course, this              parameter has no effect if you don't use the optional parameter tid.        idx - Regex selectors only return a single match, and by default it is the first one (idx=0). If you want              instead to extract a different match, set the appropiate index with this parameter.        *processors - Extraction processors passed to the method (same as in usual loaders)        **kw - Additional extract parameters passed to the method (same as in usual loaders)        &quot;&quot;&quot;```The docstring aim is to be enough explicative, so no need to repeat here.#### Testing selectorsIn most cases, as usual with selectors, you will first need to test different selectors in order to set the definitive one to hardcode in the spider. Typically with css and xpath selectors,there are three alternatives used by developers:1. The browser development console2. The fetch command in the `scrapy shell` console3. Put testing code in the spider to save responses and open it from a python consoleEach one has its pros and cons, and which to use depends on specific needs.1. For example, the browser development console is readily accesible but frequently the page rendered in the browser is not exactly the one that will be downloaded by the spider(not only because of rendering itself, but also bans, session issues, etc). However, when applicable, it allows to easily identify id, itemprop and class attributes that can bereadily used in the `add_text_re()` `tid` parameter. In many situations, however, it is not as straighforward as that, and you may get unexpected results, dirty extraction withundesired characters, etc. However, with experience you will be able to make most of the extraction using only the browser. See instructions on usage of tid parameter in the selectordoctest.2. Scrapy shell```$ scrapy shell&gt; fetch(&lt;url&gt;)&gt; from emodels.scrapyutils.response import ExtractTextResponse&gt; response = response.replace(cls=ExtractTextResponse)```The newly created response has available the method `text_re()` for testing extraction with markdown selectors.3. Put testing code in the spider to save responses and open it from a python console``````## datasets moduleThis module contains utils for creating datasets and training models.#### 1. Creating a dataset from collected items.As we mentioned before, the way to collect extraction data from a spider run is by running the spider with the appropiate environment variable set:```bashEMODELS_SAVE_EXTRACT_ITEMS=1 scrapy crawl myspider```This will save in the local datasets directory, within the folder `items/&lt;scrapy item name associacted to the loader&gt;`, the extraction data generated from the the fields extractedwith the `add_text_re()` method. Each time you run a spider in this way, a new file will be generated within the mentioned folder.At any moment, you can build a joint from all the individual files, with the following lines in a python console:```python&gt; from emodels.datasets.utils import ExtractDatasetFilename&gt; eds = ExtractDatasetFilename.build_from_items(&quot;items&quot;, &quot;myproject&quot;)```The joint dataset will be saved to the dataset file represented by eds variable.```python&gt; eds'/home/myuser/.datasets/myproject/items.jl.gz'```This operation also assigns randomly the samples to train/test/validation buckets, according to the`dataset_ratio` parameter, which by default assigns 70% to training bucket, 30% to test bucket and 0 to validation bucket.Provided `EMODELS_DATASET_DIR` is the same, you can also recover back the dataset later:```python&gt; eds = ExtractDatasetFilename.local_by_name(&quot;items&quot;, &quot;myproject&quot;)```If not, you can also do:```python&gt; eds = ExtractDatasetFilename(&quot;/home/myuser/.datasets/myproject/items.jl.gz&quot;)```#### 2. Preparing dataset to train a HuggingFace transformer modelConvert extract dataset to HuggingFace DatasetDict:```python&gt; from emodels.datasets.hugging import to_hfdataset, prepare_datasetdict&gt; hf = to_hfdataset(eds)```And then prepare the dataset for usage in transformers training:```python&gt; from transformers import AutoTokenizer&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)&gt; hff = prepare_datasetdict(hf, tokenizer)```This preparation includes truncation of samples in order to fit to the target HuggingFace model size (in this case, `deepset/roberta-base-squad2`), and set of appropiate sample fieldsrequired for training.Both `hf` and `hff` in the examples above are instances of HuggingFace DatasetDict class. So you can save them to disk and recover them at any time. I.e:```python&gt; hff.save_to_disk(&quot;./my_prepared_dataset&quot;)```Later, for recovering:```&gt; from datasets import DatasetDict&gt; hff = DatasetDict.load_from_disk(&quot;./my_prepared_dataset&quot;)```(Notice that the `datasets` module here is not the same as `emodels.datasets` module. The former comes from the HuggingFace package.)#### 3. Get trainer and do the train stuff.```python&gt; from emodels.datasets.hugging import get_qatransformer_trainer&gt; model, trainer, test_data = get_qatransformer_trainer(hff, &quot;deepset/roberta-base-squad2&quot;, &quot;mytransformer_cache_dir&quot;)&gt; trainer.train()   # this will take long time...&gt; trainer.evaluate(test_data)```Once trained, the model can be saved:```python&gt; model.save_pretrained(&quot;./mytunned_model&quot;)```Save the tokenizer along with the model, so everything is packed in the same place:```python&gt; tokenizer.save_pretrained(&quot;./mytunned_model&quot;)```This will avoid the requirement to manually load the tokenizer on each further operation.And later be recovered:```python&gt; from transformers import AutoModelForQuestionAnswering&gt; model = AutoModelForQuestionAnswering.from_pretrained(&quot;./mytunned_model&quot;)```#### 4. Extracting with the model.```python&gt; from transformers import AutoTokenizer&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)&gt; from emodels.datasets.hugging import QuestionAnswerer&gt; question_answerer = QuestionAnswerer(&quot;./mytunned_model&quot;)&gt; question_answerer.answer(question=&quot;Which is the name?&quot;, context=&lt;target markdown&gt;)```#### 5. Evaluating the model.Optionally, evaluate the extraction with the base untunned model:```python&gt; from emodels.datasets.hugging import evaluate&gt; evaluate(eds.iter(), model_path=&quot;deepset/roberta-base-squad2&quot;)```This will return a score dictionary, one item per dataset bucket. Then, do the same for the tunned model:```python&gt; evaluate(eds.iter(), model=&quot;./mytunned_model&quot;)```if everything went ok, the score of the tunned model should be bigger than the pretrained one.This evaluation is different than the one performed by trainer.evaluate() above. While the last evaluates by comparingindexes results, which are the output and target of the model, the one here evaluates actual results as extracted text.Results however should be similar.## html2text moduleThis is a modified and tuned copy of the html2text python library, with modifications required to work for the purpose of this package. You won't usually need to import this module directly. It ismostly there for correct working of the other modules in this package.</longdescription>
</pkgmetadata>