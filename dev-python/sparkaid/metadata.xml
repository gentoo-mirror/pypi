<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Spark - Dataframe with complex schema# Changelog### Version 1.0.0:* **_[Breaking changes]_** `flatten()` now stops the unpacking of nested data at ArrayType  (i.e: any field with DataType = ArrayType will have its nested elements as-is).  To have the same result as in the previous version - flatten all array fields, add the param `arrays_to_unpack = [&quot;*&quot;]`.* Added `snake_case()`* Added `json_schema_to_spark_schema()`* Added support for providing the param `nested_struct_separator` to `flatten()`.  Example: When provided with the value &quot;___&quot;, the raw schema `{&quot;parent&quot;: {&quot;child&quot;: &quot;some_value&quot;}}` will be unpacked to `{&quot;parrent___child&quot;: &quot;some_value&quot;}`# Problem descriptionA Spark DataFrame can have a simple schema, where every single column is of a simple datatype like `IntegerType, BooleanType, StringType`,...However, a column can be of one of the complex types: `ArrayType`, `MapType`, or `StructType`. The schema itself is, actually, an instance of the type `StructType`. So, when a schema has column(s) with DataType is StructType, we have a nested schema.Working with nested schema is not always easy. Some notable problems are:* Complex SQL queries* Difficult to rename/cast datatype of nested columns* Unnecessary high IO when reading only some nested columns from Parquet files (_https://issues.apache.org/jira/browse/SPARK-17636_)The page *https://docs.databricks.com/delta/data-transformation/complex-types.html* provides a lot of useful tips on dealing with dataframes having a complex schema.&lt;br&gt;This page will provide some further tips/utils to work on dataframes with more complex schema:* Renaming or casting/parsing nested columns* Flattening_Please note the term **Flattening** in this post **only** means getting rid of StructType column(s) in our DataFrame. This does not include the act of eliminating ArrayType and MapType in the schema, which is usually called **Exploding** in Spark documents. Also, Spark 2.4 introduced the function flatten [https://spark.apache.org/docs/2.4.0/api/java/org/apache/spark/sql/functions.html#flatten-org.apache.spark.sql.Column], which is used on a nested array (an array with 2 layers, an array of arrays of DataType) to make it flat (an array with 1 layer)._# Solutions## Renaming nested columnsRenaming a column at root level is simple: use the function `withColumnRenamed`.However, with a nested column, that function does not give any error, but also does not make any effect:df_struct = spark.createDataFrame([Row(structA=Row(field1=10, field2=1.5), structB=Row(field3=&quot;one&quot;,field4=False))])    df_struct.printSchema()root     |-- structA: struct (nullable = true)     |    |-- field1: long (nullable = true)     |    |-- field2: double (nullable = true)     |-- structB: struct (nullable = true)     |    |-- field3: string (nullable = true)     |    |-- field4: boolean (nullable = true)        df_struct.withColumnRenamed(&quot;structA.field1&quot;, &quot;structA.newField1&quot;) \        .withColumnRenamed(&quot;structB&quot;, &quot;newStructB&quot;) \        .printSchema()root     |-- structA: struct (nullable = true)     |    |-- field1: long (nullable = true)     |    |-- field2: double (nullable = true)     |-- newStructB: struct (nullable = true)     |    |-- field3: string (nullable = true)     |    |-- field4: boolean (nullable = true)To change the names of nested columns, there are some options:1. By building a new struct column on the flight with the `struct()` function:from pyspark.sql.functions import struct, coldf_renamed = df_struct.withColumn(&quot;structA&quot;, struct(col(&quot;structA.field1&quot;).alias(&quot;newField1&quot;),                                                    col(&quot;structA.field2&quot;)))2. By creating a new *schema* (a `StructType()` object) and use type casting on the original struct column:from pyspark.sql.types import *newStructASchema = StructType([                            StructField(&quot;newField1&quot;, LongType()),                            StructField(&quot;field2&quot;, DoubleType())                        ])    df_renamed = df_struct.withColumn(&quot;structA&quot;, col(&quot;structA&quot;).cast(newStructASchema)).printSchema()    Both options yield the same schema    root     |-- structA: struct (nullable = true)     |    |-- newField1: long (nullable = true)     |    |-- field2: double (nullable = true)     |-- structB: struct (nullable = true)     |    |-- field3: string (nullable = true)     |    |-- field4: boolean (nullable = true)The 2nd option is more convenient when building a recursive function to recreate the multi-layer nested schema with new columns names.---## Flattening### StructTypeSample DataFrame:    from pyspark.sql import Row    from pyspark.sql.functions import col    df_struct = spark.createDataFrame([Row(structA=Row(field1=10, field2=1.5),                                           structB=Row(field3=&quot;one&quot;,field4=False))])df_struct.printSchema()root |-- structA: struct (nullable = true) |    |-- field1: long (nullable = true) |    |-- field2: double (nullable = true) |-- structB: struct (nullable = true) |    |-- field3: string (nullable = true) |    |-- field4: boolean (nullable = true)Spark allows selecting nested columns by using the dot `.` notation:df_struct.select(&quot;structA.*&quot;, &quot;structB.field3&quot;).printSchema()root     |-- field1: long (nullable = true)     |-- field2: double (nullable = true)     |-- field3: string (nullable = true)Please note here that the current Spark implementation (2.4.3 or below) doesn't keep the outer layer fieldname (e.g: structA) in the output dataframe### ArrayTypeTo select only some elements from an ArrayType column, either *`getItem()`* or using brackets (as selecting elements from a legacy array: `[]` in Python `()` in Scala)  would do the trick:df_array = spark.createDataFrame([Row(arrayA=[1,2,3,4,5],fieldB=&quot;foo&quot;)])df_array.select(col(&quot;arrayA&quot;).getItem(0).alias(&quot;element0&quot;), col(&quot;arrayA&quot;)[4].alias(&quot;element5&quot;), col(&quot;fieldB&quot;)).show()+--------+--------+------+    |element0|element5|fieldB|    +--------+--------+------+    |       1|       5|   foo|    +--------+--------+------+### MapTypeElements from a MapType column can be selected the same way as in the case of ArrayType, but using the key instead of index number. The dot notation (`.`) could also be used instead of `getItem()` or brackets:    df_map = spark.createDataFrame([Row(mapA={2: &quot;TWO&quot;, 3: &quot;THREE&quot;, 0: &quot;ZERO&quot;}, fieldB=&quot;foo&quot;)])    df_map.select(col(&quot;mapA&quot;)[3].alias(&quot;element3&quot;), col(&quot;mapA&quot;).getItem(2).alias(&quot;element2&quot;), col(&quot;mapA.0&quot;).alias(&quot;element0&quot;), col(&quot;mapA&quot;).getItem(1).alias(&quot;element1&quot;)).show()+--------+--------+--------+--------+    |element3|element2|element0|element1|    +--------+--------+--------+--------+    |   THREE|     TWO|    ZERO|    null|    +--------+--------+--------+--------+### StructType nested in StructTypeAs Spark DataFrame.select() supports passing an array of columns to be selected, to fully unflatten a multi-layer nested dataframe, a recursive call would do the trick.Here is a detailed discussion on StackOverFlow on how to do this:https://stackoverflow.com/questions/37471346/automatically-and-elegantly-flatten-dataframe-in-spark-sql### StructType nested in ArrayTypedf_nested = spark.createDataFrame([    Row(        arrayA=[            Row(childStructB=Row(field1=1, field2=&quot;foo&quot;)),            Row(childStructB=Row(field1=2, field2=&quot;bar&quot;))        ]    )])df_nested.printSchema()root     |-- arrayA: array (nullable = true)     |    |-- element: struct (containsNull = true)     |    |    |-- childStructB: struct (nullable = true)     |    |    |    |-- field1: long (nullable = true)     |    |    |    |-- field2: string (nullable = true)         df_nested.show(1, False)        +------------------------+    |arrayA                  |    +------------------------+    |[[[1, foo]], [[2, bar]]]|    +------------------------+     Selecting *field1* or *field2* can be done as with normal structs (not nested inside an array), by using that dot `.` annotation. The result would be of the type `ArrayType[ChildFieldType]`, which has been **_vertically sliced_** from the original arraydf_child = df_nested.select(&quot;arrayA.childStructB.field1&quot;, &quot;arrayA.childStructB.field2&quot;)df_child.printSchema()root     |-- field1: array (nullable = true)     |    |-- element: long (containsNull = true)     |-- field2: array (nullable = true)     |    |-- element: string (containsNull = true)         df_child.show()    +------+----------+    |field1|    field2|    +------+----------+    |[1, 2]|[foo, bar]|    +------+----------+### StructType nested in MapTypeAs each MapType column has two components, the keys and the values, selecting nested column inside a MapType column is not straight forward - we cannot just use that `.` to take the nested fields as that has already been used for denoting the key.     df_map_nested = spark.createDataFrame([Row(mapA={&quot;2&quot;: Row(type_name=&quot;Arabic number&quot;, equivalent=2), &quot;THREE&quot;: Row(type_name=&quot;English Text&quot;, equivalent=3)}, fieldB=&quot;foo&quot;)])    df_map_nested.select(col(&quot;mapA.type_name&quot;), col(&quot;mapA.THREE.type_name&quot;)).show()        +---------+------------+    |type_name|   type_name|    +---------+------------+    |     null|English Text|    +---------+------------+    A solution for this is to use the builtin function `map_values()` which has been introduced since Spark 2.3. Note the type of the result column: ArrayType    from pyspark.sql.functions import map_values    result = df_map_nested.select(map_values(&quot;mapA&quot;)[&quot;type_name&quot;], col(&quot;mapA.THREE.type_name&quot;))    result.show(2,False)    result.printSchema()    +-----------------------------+------------+    |map_values(mapA).type_name   |type_name   |    +-----------------------------+------------+    |[Arabic number, English Text]|English Text|    +-----------------------------+------------+        root     |-- map_values(mapA).type_name: array (nullable = true)     |    |-- element: string (containsNull = true)     |-- type_name: string (nullable = true)     ## HurdlesThe above steps would work well for most of dataframes. The only dataframes that it fails (as of Spark 2.4.3 or lower) are the ones with a StructType nested inside MORE THAN ONE layers of ArrayType.Like this one:df_nested_B = spark.createDataFrame([        Row(            arrayA=[[                Row(childStructB=Row(field1=1, field2=&quot;foo&quot;)),                Row(childStructB=Row(field1=2, field2=&quot;bar&quot;))            ]]        )])    df_nested_B.printSchema()        root     |-- arrayA: array (nullable = true)     |    |-- element: array (containsNull = true)     |    |    |-- element: struct (containsNull = true)     |    |    |    |-- childStructB: struct (nullable = true)     |    |    |    |    |-- field1: long (nullable = true)     |    |    |    |    |-- field2: string (nullable = true)     Or this onedf_nested_C = spark.createDataFrame([        Row(            arrayA=[                Row(childStructB=Row(childArrayC=[Row(field1=1, field2=&quot;foo&quot;)])),                Row(childStructB=Row(childArrayC=[Row(field1=2, field2=&quot;bar&quot;)])),            ]        )])    df_nested_C.printSchema()        root     |-- arrayA: array (nullable = true)     |    |-- element: struct (containsNull = true)     |    |    |-- childStructB: struct (nullable = true)     |    |    |    |-- childArrayC: array (nullable = true)     |    |    |    |    |-- element: struct (containsNull = true)     |    |    |    |    |    |-- field1: long (nullable = true)     |    |    |    |    |    |-- field2: string (nullable = true)     Selecting `arrayA.childStructB.field1` from `df_nested_B` fails with the error message: `AnalysisException: No such struct field field1 in childStructB`.&lt;br&gt;While selecting `arrayA.childStructB.childArrayC.field1` from `df_nested_C` throws the `AnalysisException`: `cannot resolve 'arrayA.childStructB.childArrayC['field1']' due to data type mismatch: argument 2 requires integral type, however, ''field1'' is of string type.`## (More) SolutionsWith the introduction of the SQL function `transform` in Spark 2.4, the error above can be solved by applying `transform` on every layer of the array.A comprehensive implementation of a flatten function can be found in the Python package `sparkaid`:from sparkaid import flattenflatten(df_nested_B).printSchema()root |-- arrayA__childStructB_field1: array (nullable = true) |    |-- element: array (containsNull = true) |    |    |-- element: long (containsNull = true) |-- arrayA__childStructB_field2: array (nullable = true) |    |-- element: array (containsNull = true) |    |    |-- element: string (containsNull = true)&lt;p&gt;flatten(df_nested_B).show()+---------------------------+---------------------------+    |arrayA__childStructB_field1|arrayA__childStructB_field2|    +---------------------------+---------------------------+    |                   [[1, 2]]|               [[foo, bar]]|    +---------------------------+---------------------------+&lt;p&gt;      flatten(df_nested_C).printSchema()    root     |-- arrayA_childStructB_childArrayC_field1: array (nullable = true)     |    |-- element: array (containsNull = true)     |    |    |-- element: long (containsNull = true)     |-- arrayA_childStructB_childArrayC_field2: array (nullable = true)     |    |-- element: array (containsNull = true)     |    |    |-- element: string (containsNull = true)[]: https://spark.apache.org/docs/2.4.0/api/java/org/apache/spark/sql/functions.html#flatten-org.apache.spark.sql.Column</longdescription>
</pkgmetadata>