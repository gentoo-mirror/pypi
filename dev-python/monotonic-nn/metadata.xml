<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Constrained Monotonic Neural Networks================&lt;!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! --&gt;## Running in Google ColabYou can execute this interactive tutorial in Google Colab by clickingthe button below:&lt;a href=&quot;https://colab.research.google.com/github/airtai/monotonic-nn/blob/main/nbs/index.ipynb&quot; target=”_blank”&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open in Colab&quot; /&gt;&lt;/a&gt;## SummaryThis Python library implements Constrained Monotonic Neural Networks asdescribed in:Davor Runje, Sharath M. Shankaranarayana, “Constrained Monotonic NeuralNetworks”, in Proceedings of the 40th International Conference onMachine Learning, 2023. \[[PDF](https://arxiv.org/pdf/2205.11775.pdf)\].#### AbstractWider adoption of neural networks in many critical domains such asfinance and healthcare is being hindered by the need to explain theirpredictions and to impose additional constraints on them. Monotonicityconstraint is one of the most requested properties in real-worldscenarios and is the focus of this paper. One of the oldest ways toconstruct a monotonic fully connected neural network is to constrainsigns on its weights. Unfortunately, this construction does not workwith popular non-saturated activation functions as it can onlyapproximate convex functions. We show this shortcoming can be fixed byconstructing two additional activation functions from a typicalunsaturated monotonic activation function and employing each of them onthe part of neurons. Our experiments show this approach of buildingmonotonic neural networks has better accuracy when compared to otherstate-of-the-art methods, while being the simplest one in the sense ofhaving the least number of parameters, and not requiring anymodifications to the learning procedure or post-learning steps. Finally,we prove it can approximate any continuous monotone function on acompact subset of $\mathbb{R}^n$.#### CitationIf you use this library, please cite:``` title=&quot;bibtex&quot;@inproceedings{runje2023,  title={Constrained Monotonic Neural Networks},  author={Davor Runje and Sharath M. Shankaranarayana},  booktitle={Proceedings of the 40th {International Conference on Machine Learning}},  year={2023}}```## Python packageThis package contains an implementation of our Monotonic Dense Layer[`MonoDense`](https://monotonic.airt.ai/latest/api/airt/keras/layers/MonoDense/#airt.keras.layers.MonoDense)(Constrained Monotonic Fully Connected Layer). Below is the figure fromthe paper for reference.In the code, the variable `monotonicity_indicator` corresponds to **t**in the figure and parameters `is_convex`, `is_concave` and`activation_weights` are used to calculate the activation selector **s**as follows:- if `is_convex` or `is_concave` is **True**, then the activation  selector **s** will be (`units`, 0, 0) and (0, `units`, 0),  respecively.- if both `is_convex` or `is_concave` is **False**, then the  `activation_weights` represent ratios between $\breve{s}$, $\hat{s}$  and $\tilde{s}$, respecively. E.g. if `activation_weights = (2, 2, 1)`  and `units = 10`, then$$(\breve{s}, \hat{s}, \tilde{s}) = (4, 4, 2)$$![mono-dense-layer-diagram](https://github.com/airtai/monotonic-nn/raw/main/nbs/images/mono-dense-layer-diagram.png)### Install``` shpip install monotonic-nn```### How to useIn this example, we’ll assume we have a simple dataset with three inputsvalues $x_1$, $x_2$ and $x_3$ sampled from the normal distribution,while the output value $y$ is calculated according to the followingformula before adding Gaussian noise to it:$y = x_1^3 + \sin\left(\frac{x_2}{2 \pi}\right) + e^{-x_3}$&lt;table id=&quot;T_37b51&quot;&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th id=&quot;T_37b51_level0_col0&quot; class=&quot;col_heading level0 col0&quot; &gt;x0&lt;/th&gt;      &lt;th id=&quot;T_37b51_level0_col1&quot; class=&quot;col_heading level0 col1&quot; &gt;x1&lt;/th&gt;      &lt;th id=&quot;T_37b51_level0_col2&quot; class=&quot;col_heading level0 col2&quot; &gt;x2&lt;/th&gt;      &lt;th id=&quot;T_37b51_level0_col3&quot; class=&quot;col_heading level0 col3&quot; &gt;y&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td id=&quot;T_37b51_row0_col0&quot; class=&quot;data row0 col0&quot; &gt;0.304717&lt;/td&gt;      &lt;td id=&quot;T_37b51_row0_col1&quot; class=&quot;data row0 col1&quot; &gt;-1.039984&lt;/td&gt;      &lt;td id=&quot;T_37b51_row0_col2&quot; class=&quot;data row0 col2&quot; &gt;0.750451&lt;/td&gt;      &lt;td id=&quot;T_37b51_row0_col3&quot; class=&quot;data row0 col3&quot; &gt;0.234541&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td id=&quot;T_37b51_row1_col0&quot; class=&quot;data row1 col0&quot; &gt;0.940565&lt;/td&gt;      &lt;td id=&quot;T_37b51_row1_col1&quot; class=&quot;data row1 col1&quot; &gt;-1.951035&lt;/td&gt;      &lt;td id=&quot;T_37b51_row1_col2&quot; class=&quot;data row1 col2&quot; &gt;-1.302180&lt;/td&gt;      &lt;td id=&quot;T_37b51_row1_col3&quot; class=&quot;data row1 col3&quot; &gt;4.199094&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td id=&quot;T_37b51_row2_col0&quot; class=&quot;data row2 col0&quot; &gt;0.127840&lt;/td&gt;      &lt;td id=&quot;T_37b51_row2_col1&quot; class=&quot;data row2 col1&quot; &gt;-0.316243&lt;/td&gt;      &lt;td id=&quot;T_37b51_row2_col2&quot; class=&quot;data row2 col2&quot; &gt;-0.016801&lt;/td&gt;      &lt;td id=&quot;T_37b51_row2_col3&quot; class=&quot;data row2 col3&quot; &gt;0.834086&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td id=&quot;T_37b51_row3_col0&quot; class=&quot;data row3 col0&quot; &gt;-0.853044&lt;/td&gt;      &lt;td id=&quot;T_37b51_row3_col1&quot; class=&quot;data row3 col1&quot; &gt;0.879398&lt;/td&gt;      &lt;td id=&quot;T_37b51_row3_col2&quot; class=&quot;data row3 col2&quot; &gt;0.777792&lt;/td&gt;      &lt;td id=&quot;T_37b51_row3_col3&quot; class=&quot;data row3 col3&quot; &gt;-0.093359&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;td id=&quot;T_37b51_row4_col0&quot; class=&quot;data row4 col0&quot; &gt;0.066031&lt;/td&gt;      &lt;td id=&quot;T_37b51_row4_col1&quot; class=&quot;data row4 col1&quot; &gt;1.127241&lt;/td&gt;      &lt;td id=&quot;T_37b51_row4_col2&quot; class=&quot;data row4 col2&quot; &gt;0.467509&lt;/td&gt;      &lt;td id=&quot;T_37b51_row4_col3&quot; class=&quot;data row4 col3&quot; &gt;0.780875&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;Now, we’ll use the[`MonoDense`](https://monotonic.airt.ai/latest/api/airt/keras/layers/MonoDense/#airt.keras.layers.MonoDense)layer instead of `Dense` layer to build a simple monotonic network. Bydefault, the[`MonoDense`](https://monotonic.airt.ai/latest/api/airt/keras/layers/MonoDense/#airt.keras.layers.MonoDense)layer assumes the output of the layer is monotonically increasing withall inputs. This assumtion is always true for all layers except possiblythe first one. For the first layer, we use `monotonicity_indicator` tospecify which input parameters are monotonic and to specify are theyincreasingly or decreasingly monotonic:- set 1 for increasingly monotonic parameter,- set -1 for decreasingly monotonic parameter, and- set 0 otherwise.In our case, the `monotonicity_indicator` is `[1, 0, -1]` because $y$is:- monotonically increasing w.r.t. $x_1$  $\left(\frac{\partial y}{x_1} = 3 {x_1}^2 \geq 0\right)$, and- monotonically decreasing w.r.t. $x_3$  $\left(\frac{\partial y}{x_3} = - e^{-x_2} \leq 0\right)$.``` pythonfrom tensorflow.keras import Sequentialfrom tensorflow.keras.layers import Dense, Inputfrom airt.keras.layers import MonoDensemodel = Sequential()model.add(Input(shape=(3,)))monotonicity_indicator = [1, 0, -1]model.add(    MonoDense(128, activation=&quot;elu&quot;, monotonicity_indicator=monotonicity_indicator))model.add(MonoDense(128, activation=&quot;elu&quot;))model.add(MonoDense(1))model.summary()```    Model: &quot;sequential&quot;    _________________________________________________________________     Layer (type)                Output Shape              Param #       =================================================================     mono_dense (MonoDense)      (None, 128)               512                                                                                 mono_dense_1 (MonoDense)    (None, 128)               16512                                                                               mono_dense_2 (MonoDense)    (None, 1)                 129                                                                                =================================================================    Total params: 17,153    Trainable params: 17,153    Non-trainable params: 0    _________________________________________________________________Now we can train the model as usual using `Model.fit`:``` pythonfrom tensorflow.keras.optimizers import Adamfrom tensorflow.keras.optimizers.schedules import ExponentialDecaylr_schedule = ExponentialDecay(    initial_learning_rate=0.01,    decay_steps=10_000 // 32,    decay_rate=0.9,)optimizer = Adam(learning_rate=lr_schedule)model.compile(optimizer=optimizer, loss=&quot;mse&quot;)model.fit(    x=x_train, y=y_train, batch_size=32, validation_data=(x_val, y_val), epochs=10)```    Epoch 1/10    313/313 [==============================] - 3s 5ms/step - loss: 9.4221 - val_loss: 6.1277    Epoch 2/10    313/313 [==============================] - 1s 4ms/step - loss: 4.6001 - val_loss: 2.7813    Epoch 3/10    313/313 [==============================] - 1s 4ms/step - loss: 1.6221 - val_loss: 2.1111    Epoch 4/10    313/313 [==============================] - 1s 4ms/step - loss: 0.9479 - val_loss: 0.2976    Epoch 5/10    313/313 [==============================] - 1s 4ms/step - loss: 0.9008 - val_loss: 0.3240    Epoch 6/10    313/313 [==============================] - 1s 4ms/step - loss: 0.5027 - val_loss: 0.1455    Epoch 7/10    313/313 [==============================] - 1s 4ms/step - loss: 0.4360 - val_loss: 0.1144    Epoch 8/10    313/313 [==============================] - 1s 4ms/step - loss: 0.4993 - val_loss: 0.1211    Epoch 9/10    313/313 [==============================] - 1s 4ms/step - loss: 0.3162 - val_loss: 1.0021    Epoch 10/10    313/313 [==============================] - 1s 4ms/step - loss: 0.2640 - val_loss: 0.2522    &lt;keras.callbacks.History&gt;## License&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;&lt;img alt=&quot;Creative Commons Licence&quot; style=&quot;border-width:0&quot; src=&quot;https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png&quot; /&gt;&lt;/a&gt;&lt;br /&gt;Thiswork is licensed under a&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;CreativeCommons Attribution-NonCommercial-ShareAlike 4.0 InternationalLicense&lt;/a&gt;.You are free to:- Share — copy and redistribute the material in anymedium or format- Adapt — remix, transform, and build upon the materialThe licensor cannot revoke these freedoms as long as you follow thelicense terms.Under the following terms: - Attribution — You must give appropriatecredit, provide a link to the license, and indicate if changes weremade. You may do so in any reasonable manner, but not in any way thatsuggests the licensor endorses you or your use.- NonCommercial — You may not use the material for commercial purposes.- ShareAlike — If you remix, transform, or build upon the material, you  must distribute your contributions under the same license as the  original.- No additional restrictions — You may not apply legal terms or  technological measures that legally restrict others from doing  anything the license permits.</longdescription>
</pkgmetadata>