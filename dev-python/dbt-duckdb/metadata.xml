<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>## dbt-duckdb[DuckDB](http://duckdb.org) is an embedded database, similar to SQLite, but designed for OLAP-style analytics.It is crazy fast and allows you to read and write data stored in CSV, JSON, and Parquet files directly, without requiring you to loadthem into the database first.[dbt](http://getdbt.com) is the best way to manage a collection of data transformations written in SQL or Python for analyticsand data science. `dbt-duckdb` is the project that ties DuckDB and dbt together, allowing you to create a [Modern Data Stack InA Box](https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html) or a simple and powerful data lakehouse with Python.### InstallationThis project is hosted on PyPI, so you should be able to install it and the necessary dependencies via:`pip3 install dbt-duckdb`The latest supported version targets `dbt-core` 1.5.x and `duckdb` version 0.7.x, but we work hard to ensure that newerversions of DuckDB will continue to work with the adapter as they are released. If you would like to use our new (and experimental!)support for persisting the tables that DuckDB creates to the [AWS Glue Catalog](https://aws.amazon.com/glue/), you should install`dbt-duckdb[glue]` in order to get the AWS dependencies as well.### Configuring Your ProfileA super-minimal dbt-duckdb profile only needs *one* setting:````default:  outputs:   dev:     type: duckdb  target: dev````This will run your dbt-duckdb pipeline against an in-memory DuckDB database that will not be persisted after your run completes. This maynot seem very useful at first, but it turns out to be a powerful tool for a) testing out data pipelines, either locally or in CI jobs andb) running data pipelines that operate purely on external CSV, Parquet, or JSON files. More details on how to work with external data filesin dbt-duckdb are provided in the docs on [reading and writing external files](#reading-and-writing-external-files).To have your dbt pipeline persist relations in a DuckDB file, set the `path` field in your profile to the pathof the DuckDB file that you would like to read and write on your local filesystem. (For in-memory pipelines, the `path`is automatically set to the special value `:memory:`).`dbt-duckdb` also supports common profile fields like `schema` and `threads`, but the `database` property is special: it's value is automatically setto the basename of the file in the `path` argument with the suffix removed. For example, if the `path` is `/tmp/a/dbfile.duckdb`, the `database`field will be set to `dbfile`. If you are running in in-memory mode, then the `database` property will be automatically set to `memory`.#### Using MotherDuckAs of `dbt-duckdb` 1.5.2, you can connect to a DuckDB instance running on [MotherDuck](http://www.motherduck.com) by setting your `path` to use a [md:&lt;database&gt; connection string](https://motherduck.com/docs/getting-started/connect-query-from-python/installation-authentication), just as you would with the DuckDB CLIor the Python API.MotherDuck databases generally work the same way as local DuckDB databases from the perspective of dbt, butthere are a [few differences to be aware of](https://motherduck.com/docs/architecture-and-capabilities#considerations-and-limitations):1. For the moment, MotherDuck _requires_ DuckDB version `0.8.1`.1. MotherDuck databases do not suppport transactions, so there is a new `disable_transactions` profileoption that will be automatically enabled if you are connecting to a MotherDuck database in your `path`.1. MotherDuck preloads a set of the most common DuckDB extensions for you, but does not support loading custom extensions or user-defined functions.1. A small subset of advanced SQL features are currently unsupported; the only impact of this on the dbt adapter is that the [dbt.listagg](https://docs.getdbt.com/reference/dbt-jinja-functions/cross-database-macros#listagg) macro will work against a local DuckDB database, but willnot work against MotherDuck.#### DuckDB Extensions, Settings, and FilesystemsYou can load any supported [DuckDB extensions](https://duckdb.org/docs/extensions/overview) by listing them inthe `extensions` field in your profile. You can also set any additional [DuckDB configuration options](https://duckdb.org/docs/sql/configuration)via the `settings` field, including options that are supported in any loaded extensions. For example, to be able to connect to S3 and read/writeParquet files using an AWS access key and secret, your profile would look something like this:```default:  outputs:    dev:      type: duckdb      path: /tmp/dbt.duckdb      extensions:        - httpfs        - parquet      settings:        s3_region: my-aws-region        s3_access_key_id: &quot;{{ env_var('S3_ACCESS_KEY_ID') }}&quot;        s3_secret_access_key: &quot;{{ env_var('S3_SECRET_ACCESS_KEY') }}&quot;  target: dev```As of verion `1.4.1`, we have added (experimental!) support for DuckDB's (experimental!) support for filesystemsimplemented via [fsspec](https://duckdb.org/docs/guides/python/filesystems.html). The `fsspec` library providessupport for reading and writing files from a [variety of cloud data storage systems](https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations)including S3, GCS, and Azure Blob Storage. You can configure a list of fsspec-compatible implementations for use with your dbt-duckdb project by installing the relevant Python modulesand configuring your profile like so:```default:  outputs:    dev:      type: duckdb      path: /tmp/dbt.duckdb      filesystems:        - fs: s3          anon: false          key: &quot;{{ env_var('S3_ACCESS_KEY_ID') }}&quot;          secret: &quot;{{ env_var('S3_SECRET_ACCESS_KEY') }}&quot;          client_kwargs:            endpoint_url: &quot;http://localhost:4566&quot;  target: dev```Here, the `filesystems` property takes a list of configurations, where each entry must have a property named `fs` that indicates which `fsspec` protocolto load (so `s3`, `gcs`, `abfs`, etc.) and then an arbitrary set of other key-value pairs that are used to configure the `fsspec` implementation. You can see a simple example project thatillustrates the usage of this feature to connect to a Localstack instance running S3 from dbt-duckdb [here](https://github.com/jwills/s3-demo).#### Fetching credentials from contextInstead of specifying the credentials through the settings block, you can also use the use_credential_provider property. If you set this to `aws` (currently the only supported implementation) and you have `boto3` installed in your python environment, we will fetch your AWS credentials using the credential provider chain as described [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html). This means that you can use any supported mechanism from AWS to obtain credentials (e.g., web identity tokens).#### Attaching Additional DatabasesDuckDB version `0.7.0` added support for [attaching additional databases](https://duckdb.org/docs/sql/statements/attach.html) to your dbt-duckdb run so that you can readand write from multiple databases. Additional databases may be configured using [dbt run hooks](https://docs.getdbt.com/docs/build/hooks-operations) or via the `attach` argumentin your profile that was added in dbt-duckdb `1.4.0`:```default:  outputs:    dev:      type: duckdb      path: /tmp/dbt.duckdb      attach:        - path: /tmp/other.duckdb        - path: ./yet/another.duckdb          alias: yet_another        - path: s3://yep/even/this/works.duckdb          read_only: true        - path: sqlite.db          type: sqlite```The attached databases may be referred to in your dbt sources and models by either the basename of the database file minus its suffix (e.g., `/tmp/other.duckdb` is the `other` databaseand `s3://yep/even/this/works.duckdb` is the `works` database) or by an alias that you specify (so the `./yet/another.duckdb` database in the above configuration is referred toas `yet_another` instead of `another`.) Note that these additional databases do not necessarily have to be DuckDB files: DuckDB's storage and catalog engines are pluggable, andDuckDB `0.7.0` ships with support for reading and writing from attached SQLite databases. You can indicate the type of the database you are connecting to via the `type` argument,which currently supports `duckdb` and `sqlite`.#### Configuring dbt-duckdb Pluginsdbt-duckdb has its own [plugin](dbt/adapters/duckdb/plugins/__init__.py) system to enable advanced users to extenddbt-duckdb with additional functionality, including:* Defining [custom Python UDFs](https://duckdb.org/docs/api/python/function.html) on the DuckDB database connectionso that they can be used in your SQL models* Loading source data from [Excel](dbt/adapters/duckdb/plugins/excel.py), [Google Sheets](dbt/adapters/duckdb/plugins/gsheet.py), or [SQLAlchemy](dbt/adapters/duckdb/plugins/sqlalchemy.py) tablesYou can find more details on [how to write your own plugins here](#writing-your-own-plugins). To configure a plugin for usein your dbt project, use the `plugins` property on the profile:```default:  outputs:    dev:      type: duckdb      path: /tmp/dbt.duckdb      plugins:        - module: gsheet          config:            method: oauth        - module: sqlalchemy          alias: sql          config:            connection_url: &quot;{{ env_var('DBT_ENV_SECRET_SQLALCHEMY_URI') }}&quot;        - module: path.to.custom_udf_module```Every plugin must have a `module` property that indicates where the `Plugin` class to load is defined. Values ofthe `module` property that do not contain a &quot;.&quot; are assumed to be one of the built-in plugins that are defined in[dbt.adapters.duckdb.plugins](dbt/adapters/duckdb/plugins/), while any values that do contain a &quot;.&quot; are assumed tobe user-defined modules that are visible on the Python path. Each plugin instance has a name for logging and referencepurposes that defaults to the name of the module, but that may be overridden by the user by setting the `alias`property. Finally, modules may be initialized using an arbitrary set of key-value pairs that are defined in the`config` dictionary. In this example, we initialize the `gsheet` plugin with the setting `method: oauth` and weinitialize the `sqlalchemy` plugin (aliased as &quot;sql&quot;) with a `connection_url` that is set via an environment variable.Please remember that using plugins may require you to add additional dependencies to the Python environment that your dbt-duckdb pipeline runs in:* `excel` depends on `pandas`* `gsheet` depends on `gspread` and `pandas`*  `iceberg` depends on `pyiceberg` and Python &gt;= 3.8* `sqlalchemy` depends on `pandas`, `sqlalchemy`, and the driver(s) you need### Reading and Writing External FilesOne of DuckDB's most powerful features is its ability to read and write CSV, JSON, and Parquet files directly, without needing to import/exportthem from the database first.#### Reading from external filesYou may reference external files in your dbt model's either directly or as dbt `source`s by configuring the `external_location`meta option on the source:```sources:  - name: external_source    meta:      external_location: &quot;s3://my-bucket/my-sources/{name}.parquet&quot;    tables:      - name: source1      - name: source2```Here, the `meta` options on `external_source` defines `external_location` as an [f-string](https://peps.python.org/pep-0498/) thatallows us to express a pattern that indicates the location of any of the tables defined for that source. So a dbt model like:```SELECT *FROM {{ source('external_source', 'source1') }}```will be compiled as:```SELECT *FROM 's3://my-bucket/my-sources/source1.parquet'```If one of the source tables deviates from the pattern or needs some other special handling, then the `external_location` can also be set on the `meta`options for the table itself, for example:```sources:  - name: external_source    meta:      external_location: &quot;s3://my-bucket/my-sources/{name}.parquet&quot;    tables:      - name: source1      - name: source2        meta:          external_location: &quot;read_parquet(['s3://my-bucket/my-sources/source2a.parquet', 's3://my-bucket/my-sources/source2b.parquet'])&quot;```In this situation, the `external_location` setting on the `source2` table will take precedence, so a dbt model like:```SELECT *FROM {{ source('external_source', 'source2') }}```will be compiled to the SQL query:```SELECT *FROM read_parquet(['s3://my-bucket/my-sources/source2a.parquet', 's3://my-bucket/my-sources/source2b.parquet'])```Note that the value of the `external_location` property does not need to be a path-like string; it can also be a functioncall, which is helpful in the case that you have an external source that is a CSV file which requires special handling for DuckDB to load it correctly:```sources:  - name: flights_source    tables:      - name: flights        meta:          external_location: &quot;read_csv('flights.csv', types={'FlightDate': 'DATE'}, names=['FlightDate', 'UniqueCarrier'])&quot;          formatter: oldstyle```Note that we need to override the default `str.format` string formatting strategy for this examplebecause the `types={'FlightDate': 'DATE'}` argument to the `read_csv` function will be interpreted by`str.format` as a template to be matched on, which will cause a `KeyError: &quot;'FlightDate'&quot;` when we attemptto parse the source in a dbt model. The `formatter` configuration option for the source indicates whetherwe should use `newstyle` string formatting (the default), `oldstyle` string formatting, or `template` stringformatting. You can read up on the strategies the various string formatting techniques use at this[Stack Overflow answer](https://stackoverflow.com/questions/13451989/pythons-many-ways-of-string-formatting-are-the-older-ones-going-to-be-depre) and see examples of their usein this [dbt-duckdb integration test](https://github.com/jwills/dbt-duckdb/blob/master/tests/functional/adapter/test_sources.py).#### Writing to external filesWe support creating dbt models that are backed by external files via the `external` materialization strategy:```{{ config(materialized='external', location='local/directory/file.parquet') }}SELECT m.*, s.id IS NOT NULL as has_source_idFROM {{ ref('upstream_model') }} mLEFT JOIN {{ source('upstream', 'source') }} s USING (id)```| Option | Default | Description| :---:    |  :---:    | ---| location | `{{ name }}.{{ format }}` | The path to write the external materialization to. See below for more details.| format | parquet | The format of the external file (parquet, csv, or json)| delimiter | ,    | For CSV files, the delimiter to use for fields.| options | None | Any other options to pass to DuckDB's `COPY` operation (e.g., `partition_by`, `codec`, etc.)| glue_register | false | If true, try to register the file created by this model with the AWS Glue Catalog.| glue_database | default | The name of the AWS Glue database to register the model with.If the `location` argument is specified, it must be a filename (or S3 bucket/path), and dbt-duckdb will attempt to inferthe `format` argument from the file extension of the `location` if the `format` argument is unspecified (this functionality wasadded in version 1.4.1.)If the `location` argument is _not_ specified, then the external file will be named after the model.sql (or model.py) file that defined itwith an extension that matches the `format` argument (`parquet`, `csv`, or `json`). By default, the external files are createdrelative to the current working directory, but you can change the default directory (or S3 bucket/prefix) by specifying the`external_root` setting in your DuckDB profile.#### Re-running external models with an in-memory version of dbt-duckdbWhen using `:memory:` as the DuckDB database, subsequent dbt runs can fail when selecting a subset of models that depend on external tables. This is because external files are only registered as  DuckDB views when they are created, not when they are referenced. To overcome this issue we have provided the `register_upstream_external_models` macro that can be triggered at the beginning of a run. To enable this automatic registration, place the following in your `dbt_project.yml` file:```yamlon-run-start:  - &quot;{{ register_upstream_external_models() }}&quot;```### Python Supportdbt added support for [Python models in version 1.3.0](https://docs.getdbt.com/docs/build/python-models). For most data platforms,dbt will package up the Python code defined in a `.py` file and ship it off to be executed in whatever Python environment thatdata platform supports (e.g., Snowpark for Snowflake or Dataproc for BigQuery.) In dbt-duckdb, we execute Python models in the sameprocess that owns the connection to the DuckDB database, which by default, is the Python process that is created when you run dbt.To execute the Python model, we treat the `.py` file that your model is defined in as a Python module and load it into therunning process using [importlib](https://docs.python.org/3/library/importlib.html). We then construct the arguments to the `model`function that you defined (a `dbt` object that contains the names of any `ref` and `source` information your model needs and a`DuckDBPyConnection` object for you to interact with the underlying DuckDB database), call the `model` function, and then materializethe returned object as a table in DuckDB.The value of the `dbt.ref` and `dbt.source` functions inside of a Python model will be a [DuckDB Relation](https://duckdb.org/docs/api/python/reference/)object that can be easily converted into a Pandas/Polars DataFrame or an Arrow table. The return value of the `model` function can beany Python object that DuckDB knows how to turn into a table, including a Pandas/Polars `DataFrame`, a DuckDB `Relation`, or an Arrow `Table`,`Dataset`, `RecordBatchReader`, or `Scanner`.### Writing Your Own PluginsDefining your own dbt-duckdb plugin is as simple as creating a python module that defines a class named `Plugin` thatinherits from [dbt.adapters.duckdb.plugins.BasePlugin](dbt/adapters/duckdb/plugins/__init__.py). There are currentlyfour methods that may be implemented in your Plugin class:1. `initialize`: Takes in the `config` dictionary for the plugin that is defined in the profile to enable anyadditional configuration for the module based on the project; this method is called once when an instance of the`Plugin` class is created.1. `configure_connection`: Takes an instance of the `DuckDBPyConnection` object used to connect to the DuckDBdatabase and may perform any additional configuration of that object that is needed by the plugin, like definingcustom user-defined functions.1. `load`: Takes a [SourceConfig](dbt/adapters/duckdb/utils.py) instance, which encapsulates the configuration for aa dbt source and can optionally return a DataFrame-like object that DuckDB knows how to turn into a table (this issimilar to a dbt-duckdb Python model, but without the ability to `ref` any models or access any information beyondthe source config.)1. `store`: Takes a [TargetConfig](dbt/adapters/duckdb/utils.py) instance, which encapsulates the configuration foran `external` materialization and can perform additional operations once the CSV/Parquet/JSON file is written. The[glue](dbt/adapters/duckdb/plugins/glue.py) and [sqlalchemy](dbt/adapters/duckdb/plugins/sqlalchemy.py) are examplesthat demonstrate how to use the `store` operation to register an AWS Glue database table or upload a DataFrame toan external database, respectively.dbt-duckdb ships with a number of [built-in plugins](dbt/adapters/duckdb/plugins/) that can be used as examplesfor implementing your own.### RoadmapThings that we would like to add in the near future:* Support for Delta and Iceberg external table formats (both as sources and destinations)* Make dbt's incremental models and snapshots work with external materializations</longdescription>
</pkgmetadata>