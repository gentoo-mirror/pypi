<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>- [API Reference](#api-reference)  - [AWS](#aws)    - [SQS](#sqs)- [Overview](#overview)- [High-level Example](#high-level-example)  - [Inheriting the Orchestrator](#inheriting-the-orchestrator)  - [Using the Orchestrator](#using-the-orchestrator)- [Low-level Example](#low-level-example)# API Reference## AWSHelper classes and functions to interact with and manipulate AWS services.### SQSMake sure to provide type hint `SQSQueue[Patient]` to enable type hints for the queuemethods.Send messages:```pythonfrom kicksaw_integration_utils.aws import SQSQueuefrom pydantic import BaseModelclass Patient(BaseModel):    first_name: str    last_name: str    age: intmessages = [    Patient(first_name=&quot;John&quot;, last_name=&quot;Doe&quot;, age=40),    Patient(first_name=&quot;Jane&quot;, last_name=&quot;Doe&quot;, age=30),]queue: SQSQueue[Patient] = SQSQueue(name=&quot;my-queue-name&quot;, message_model=Patient)queue.send_messages(messages)```Receive and delete messages:```pythonhandles, messages = queue.receive_messages()queue.delete_messages(handles)```# OverviewA set of helper functions for CSV to Salesforce procedures, with reporting in AWS S3.The use case is extremely specific, but the helpers should be modular so they can be cherry-picked.Typical use case:- Receive an S3 event- Download the S3 object- Serialize the file into JSON- Bulk upsert the JSON data to Salesforce- Parse the results of the upsert for errors- Construct a CSV error report- Move the triggering S3 object to an archive folder- Push the error report to an error folder in the same bucket- Push an object to Salesforce that details information about the above execution2nd typical use case:- Start an AWS Step Function- Pass the payload to the KicksawSalesforce client and create an execution object, recording this payload- upsert a bunch of data, parsing the responses- if any errors, push error objects into salesforce, chidling them to the execution object above# High-level ExampleUsing the `Orchestrator` class, you can skip manually setting up a lot of the abovesteps. This class is intended to be subclassed, and should provide plenty of optionsfor overriding methods to better suit your use-case.## Inheriting the Orchestrator```python# orchestrator.pyfrom kicksaw_integration_utils.classes import Orchestrator as BaseOrchestratorclass Orchestrator(BaseOrchestrator):    def __init__(self, *args, **kwargs) -&gt; None:        super().__init__(*args, **kwargs)        # This must be defined in the child class because your Salesforce object could be named anything        self.execution_object_name = &quot;Integration_Execution__c&quot;    @property    def execution_sfdc_hash(self):        # And it could have any number of fields        return {            &quot;Number_of_Errors__c&quot;: self.error_count,            &quot;Error_Report__c&quot;: self.error_report_link,            &quot;Data_File__c&quot;: self.s3_object_key,        }    @property    def error_report_link(self):        return f&quot;https://{self.bucket_name}.{config.AWS_REGION}.amazonaws.com/{self.error_file_s3_key}&quot;```## Using the Orchestrator```python# biz_logic.pyfrom kicksaw_integration_utils.classes import SfClient# import the custom Orchestrator defined abovefrom .orchestrator import Orchestratorsalesforce = SfClient()orchestrator = Orchestrator(&quot;some/s3/key/file.csv&quot;, config.S3_BUCKET, sf_client=salesforce)upsert_key = &quot;My_External_ID__c&quot;accounts_data = [{&quot;Name&quot;: &quot;A name&quot;, upsert_key: &quot;123&quot;}]results = salesforce.bulk.Account.upsert(results, upsert_key)# You'll call log_batch for each batch you upload. This method# will parse the results in search of errorsorchestrator.log_batch(results, accounts_data, &quot;Account&quot;, upsert_key)# This will create the error report, archive the source s3 file, and push# the integration object to Salesforce. You'll definitely want to customize# this by overriding this method or the methods it invokesorchestrator.automagically_finish_up()```# Low-level Example```pythonfrom kicksaw_integration_utils.csv_helpers import create_error_reportfrom kicksaw_integration_utils.s3_helpers import download_file, respond_to_s3_event, upload_filefrom kicksaw_integration_utils.sfdc_helpers import extract_errors_from_results# handler for listening to s3 eventsdef handler(event, context):    respond_to_s3_event(event, download_and_process)def download_and_process(s3_object_key, bucket_name):    download_path = download_file(s3_object_key, bucket_name)    # This function contains your own biz logic; does not come from this library    results = serialize_and_push_to_sfdc(download_path)    sucesses, errors = parse_bulk_upsert_results(results)    report_path, errors_count = create_error_report([errors])    upload_file(report_path, bucket_name)```Just take what'cha need!</longdescription>
</pkgmetadata>