<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Fast Transformers=================Transformers are very successful models that achieve state of the artperformance in many natural language tasks. However, it is very difficult toscale them to long sequences due to the quadratic scaling of self-attention.This library was developed for our research on fast attention for transformers.You can find a list of our papers `in the docs&lt;https://fast-transformers.github.io&gt;`_ as well as related papers and papersthat we have implemented.Quick-start-----------The following code builds a transformer with softmax attention and one withlinear attention and compares the time required by each to encode a sequencewith 1000 elements... code:: python    import torch    from fast_transformers.builders import TransformerEncoderBuilder    # Create the builder for our transformers    builder = TransformerEncoderBuilder.from_kwargs(        n_layers=8,        n_heads=8,        query_dimensions=64,        value_dimensions=64,        feed_forward_dimensions=1024    )    # Build a transformer with softmax attention    builder.attention_type = &quot;full&quot;    softmax_model = builder.get()    # Build a transformer with linear attention    builder.attention_type = &quot;linear&quot;    linear_model = builder.get()    # Construct the dummy input    X = torch.rand(10, 1000, 8*64)    # Prepare everythin for CUDA    X = X.cuda()    softmax_model.cuda()    softmax_model.eval()    linear_model.cuda()    linear_model.eval()    # Warmup the GPU    with torch.no_grad():        softmax_model(X)        linear_model(X)    torch.cuda.synchronize()    # Measure the execution time    softmax_start = torch.cuda.Event(enable_timing=True)    softmax_end = torch.cuda.Event(enable_timing=True)    linear_start = torch.cuda.Event(enable_timing=True)    linear_end = torch.cuda.Event(enable_timing=True)    with torch.no_grad():        softmax_start.record()        y = softmax_model(X)        softmax_end.record()        torch.cuda.synchronize()        print(&quot;Softmax: &quot;, softmax_start.elapsed_time(softmax_end), &quot;ms&quot;)        # Softmax: 144 ms (on a GTX1080Ti)    with torch.no_grad():        linear_start.record()        y = linear_model(X)        linear_end.record()        torch.cuda.synchronize()        print(&quot;Linear: &quot;, linear_start.elapsed_time(linear_end), &quot;ms&quot;)        # Linear: 68 ms (on a GTX1080Ti)Dependencies &amp; Installation---------------------------The fast transformers library has the following dependencies:* PyTorch* C++ toolchain* CUDA toolchain (if you want to compile for GPUs)For most machines installation should be as simple as:.. code:: bash    pip install --user pytorch-fast-transformersNote: macOS users should ensure they have `llvm` and `libomp` installed.Using the `homebrew &lt;https://brew.sh&gt;`_ package manager, this can beaccomplished by running `brew install llvm libomp`.Documentation-------------There exists a dedicated `documentation site&lt;https://fast-transformers.github.io/&gt;`_ but you are also encouraged to readthe `source code &lt;https://github.com/idiap/fast-transformers&gt;`_.Research--------Ours~~~~To read about the theory behind some attention implementations in this librarywe encourage you to follow our research.* Transformers are RNNs: Fast Autoregressive Transformers with  Linear Attention (`2006.16236 &lt;https://arxiv.org/abs/2006.16236&gt;`_)* Fast Transformers with Clustered Attention  (`2007.04825 &lt;https://arxiv.org/abs/2007.04825&gt;`_)If you found our research helpful or influential please consider citing.. code::    @inproceedings{katharopoulos_et_al_2020,        author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},        title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},        booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},        year = {2020}    }    @article{vyas_et_al_2020,        author={Vyas, A. and Katharopoulos, A. and Fleuret, F.},        title={Fast Transformers with Clustered Attention},        booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS)},        year={2020}    }By others~~~~~~~~~* Efficient Attention: Attention with Linear Complexities (`1812.01243  &lt;https://arxiv.org/abs/1812.01243&gt;`_)* Linformer: Self-Attention with Linear Complexity (`2006.04768  &lt;https://arxiv.org/abs/2006.04768&gt;`_)* Reformer: The Efficient Transformer (`2001.04451  &lt;https://arxiv.org/abs/2001.04451&gt;`_)Support, License and Copyright------------------------------This software is distributed with the **MIT** license which pretty much means thatyou can use it however you want and for whatever reason you want. All theinformation regarding support, copyright and the license can be found in the`LICENSE &lt;https://github.com/idiap/fast-transformers/blob/master/LICENSE&gt;`_file in the repository.</longdescription>
</pkgmetadata>