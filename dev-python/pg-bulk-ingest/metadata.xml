<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pg-bulk-ingest[![PyPI package](https://img.shields.io/pypi/v/pg-bulk-ingest?label=PyPI%20package&amp;color=%234c1)](https://pypi.org/project/pg-bulk-ingest/) [![Test suite](https://img.shields.io/github/actions/workflow/status/uktrade/pg-bulk-ingest/test.yml?label=Test%20suite)](https://github.com/uktrade/pg-bulk-ingest/actions/workflows/test.yml) [![Code coverage](https://img.shields.io/codecov/c/github/uktrade/pg-bulk-ingest?label=Code%20coverage)](https://app.codecov.io/gh/uktrade/pg-bulk-ingest)A Python utility function for ingesting data into a SQLAlchemy-defined PostgreSQL table, automatically migrating it as needed, allowing concurrent reads as much as possible.Allowing concurrent writes is not an aim of pg-bulk-ingest. It is designed for use in ETL pipelines where PostgreSQL is used as a data warehouse, and the only writes to the table are from pg-bulk-ingest. It is assumed that there is only one pg-bulk-ingest running against a given table at any one time.## Featurespg-bulk-ingest exposes a single function as its API that:- Creates the table if necessary- Migrates any existing table if necessary, minimising locking- Ingests data in batches, where each batch is ingested in its own transaction- Handles &quot;high-watermarking&quot; to carry on from where a previous ingest finished or errored- Optionally performs an &quot;upsert&quot;, matching rows on primary key- Optionally deletes all existing rows before ingestion- Optionally calls a callback just before each batch is visible to other database clients---Visit the [pg-bulk-ingest documentation](https://pg-bulk-ingest.docs.trade.gov.uk/) for usage instructions.</longdescription>
</pkgmetadata>