<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pyg-sql## Introduction* pip install from https://pypi.org/project/pyg-sql/* conda install: I no longer maintain a conda environment so an old version is available here: https://anaconda.org/yoavgit/pyg-sql. The project itself will happily conda-build.pyg-sql creates sql_cursor (and its constructor, sql_table), a thin wrapper on sql-alchemy (sa.Table), providing three different functionailities:* simplified create/filter/sort/access/join of a sql table* creation of a full no-sql like document-store* full &quot;in-the-background&quot; maintainance of indexed unique records are per specified primary keys while we auto-archive old data* supports both a transactional and an ad-hoc approachpyg-sql supports very light joins but makes playing with a single table MUCH easier than traditional sqlalchemy.## access simplificationsqlalchemy use-pattern makes Table create the &quot;statement&quot; and then let the engine session/connection execute. Conversely, the sql_cursor keeps tabs internally of:    - the table    - the engine    - the &quot;select&quot;, the &quot;order by&quot; and the &quot;where&quot; expressionsThis allows us to    - &quot;query and execute&quot; in one go    - build statements interactively, each time adding select/where/sort to previous where/select        :Example: table creation    ------------------------    &gt;&gt;&gt; from pyg_base import *     &gt;&gt;&gt; from pyg_sql import *     &gt;&gt;&gt; import datetime        &gt;&gt;&gt; t = sql_table(db = 'test_db', table = 'students', non_null = ['name', 'surname'], server = 'DESKTOP-LU5C5QF',                          _id = dict(_id = int, created = datetime.datetime),                           nullable =  dict(doc = str, details = str, dob = datetime.date, age = int, grade = float))    &gt;&gt;&gt; t = t.delete()    :Example: table insertion    -------------------------    &gt;&gt;&gt; t = t.insert(name = 'yoav', surname = 'git', age = 48)    &gt;&gt;&gt; t = t.insert(name = 'anna', surname = 'git', age = 37)    &gt;&gt;&gt; assert len(t) == 2    &gt;&gt;&gt; t = t.insert(name = ['ayala', 'itamar', 'opher'], surname = 'gate', age = [17, 11, 16])    &gt;&gt;&gt; assert len(t) == 5    :Example: simple access    -----------------------    You can access rows either as dicts or as pd.Series        &gt;&gt;&gt; t[0]        {'_id': 1,     'created': datetime.datetime(2023, 1, 23, 17, 0, 6, 553000),     'name': 'yoav',     'surname': 'git',     'doc': None,     'details': None,     'dob': None,     'age': 48,     'grade': None}    &gt;&gt;&gt; t.df[0]    _id                                 1    created    2023-01-23 17:00:06.553000    name                             yoav    surname                           git    doc                              None    details                          None    dob                              None    age                                48    grade                            None    dtype: object        &gt;&gt;&gt; assert t.sort('age')[0].name == 'itamar'                                                     # youngest    &gt;&gt;&gt; assert t.sort('age')[-1].name == 'yoav'                                                      # access of last record    &gt;&gt;&gt; assert t.sort(dict(age=-1))[0].name == 'yoav'                                                # sort in descending order    &gt;&gt;&gt; assert t.sort('name')[::].name == ['anna', 'ayala', 'itamar', 'opher', 'yoav']    &gt;&gt;&gt; assert t.sort('name')[['name', 'surname']][::].shape == (5, 2)                              ## access of specific column(s)    &gt;&gt;&gt; assert t.surname == ['gate', 'git']    &gt;&gt;&gt; assert t['surname'] == ['gate', 'git']    &gt;&gt;&gt; assert t[dict(name = 'yoav')] == t.inc(name = 'yoav')[0]    &gt;&gt;&gt; names = [doc.name for doc in t.sort(dict(age=-1))]    &gt;&gt;&gt; assert names == ['yoav', 'anna', 'ayala', 'opher', 'itamar']    :Example: DataFrame access:    ---------------------------    &gt;&gt;&gt; t.df() ## get all the data as a dataframe    &gt;&gt;&gt; t.sort('age')[['age', 'name']].df[2:4]       age   name    0   17  ayala    1   37   anna        :Example: simple filtering    --------------------------    &gt;&gt;&gt; assert len(t.inc(surname = 'gate')) == 3    &gt;&gt;&gt; assert len(t.inc(surname = 'gate').inc(name = 'ayala')) == 1    # you can build filter in stages    &gt;&gt;&gt; assert len(t.inc(surname = 'gate', name = 'ayala')) == 1        # or build in one step    &gt;&gt;&gt; assert len(t.inc(surname = 'gate').exc(name = 'ayala')) == 2    &gt;&gt;&gt; assert len(t &gt; dict(age = 30)) == 2    &gt;&gt;&gt; assert len(t &lt;= dict(age = 37)) == 4    &gt;&gt;&gt; assert len(t.inc(t.c.age &gt; 30)) == 2  # can filter using the standard sql-alchemy .c.column objects    &gt;&gt;&gt; assert len(t.where(t.c.age &gt; 30)) == 2  # can filter using the standard sql-alchemy &quot;where&quot; statement ## insertion of &quot;documents&quot; into string columns...It is important to realise that we already have much flexibility behind the scene in using &quot;documents&quot; inside string columns:    &gt;&gt;&gt; t = t.delete()    &gt;&gt;&gt; assert len(t) == 0; assert t.count() == 0    &gt;&gt;&gt; import numpy as np    &gt;&gt;&gt; t.insert(name = 'yoav', surname = 'git', details = dict(kids = {'ayala' : dict(age = 17, gender = 'f'), 'opher' : dict(age = 16, gender = 'f'), 'itamar': dict(age = 11, gender = 'm')}, salary = np.array([100,200,300]), ))    &gt;&gt;&gt; t[0] # we can grab the full data back!    {'_id': 81,     'created': datetime.datetime(2022, 6, 30, 0, 10, 33, 900000),     'name': 'yoav',     'surname': 'git',     'doc': None,     'details': {'kids': {'ayala':  {'age': 17, 'gender': 'f'},                          'opher':  {'age': 16, 'gender': 'f'},                          'itamar': {'age': 11, 'gender': 'm'}},                 'salary': array([100, 200, 300])},     'dob': None,     'age': None,     'grade': None}    &gt;&gt;&gt; class Temp():            pass                &gt;&gt;&gt; t.insert(name = 'anna', surname = 'git', details = dict(temp = Temp())) ## yep, we can store actual objects...    &gt;&gt;&gt; t[1]  # and get them back as proper objects on loading    {'_id': 83,     'created': datetime.datetime(2022, 6, 30, 0, 16, 10, 340000),     'name': 'anna',     'surname': 'git',     'doc': None,     'details': {'temp': &lt;__main__.Temp at 0x1a91d9fd3a0&gt;},     'dob': None,     'age': None,     'grade': None}## primary keys and auto-archivePrimary Keys are applied if the primary keys (pk) are specified. Now, when we insert into a table, if another record with same pk exists, the record will be replaced.Rather than simply delete old records, we create automatically a parallel database.archived_schema.table to auto-archive these replaced records.This ensure a full audit and roll-back of records is possible.    :Example: primary keys and deleted records    ------------------------------------------    The table as set up can have multiple items so:        &gt;&gt;&gt; from pyg import *     &gt;&gt;&gt; t = t.delete()    &gt;&gt;&gt; t = t.insert(name = 'yoav', surname = 'git', age = 46)    &gt;&gt;&gt; t = t.insert(name = 'yoav', surname = 'git', age = 47)    &gt;&gt;&gt; t = t.insert(name = 'yoav', surname = 'git', age = 48)    &gt;&gt;&gt; assert len(t) == 3        &gt;&gt;&gt; t = t.delete()     &gt;&gt;&gt; t = sql_table(db = 'test', table = 'students', non_null = ['name', 'surname'],                           _id = dict(_id = int, created = datetime.datetime),                           nullable =  dict(doc = str, details = str, dob = datetime.date, age = int, grade = float),                           pk = ['name', 'surname'])         ## &lt;&lt;&lt;------- We set primary keys    &gt;&gt;&gt; t = t.delete()    &gt;&gt;&gt; t = t.insert(name = 'yoav', surname = 'git', age = 46)    &gt;&gt;&gt; t = t.insert(name = 'yoav', surname = 'git', age = 47)    &gt;&gt;&gt; t = t.insert(name = 'yoav', surname = 'git', age = 48)    &gt;&gt;&gt; assert len(t) == 1     &gt;&gt;&gt; assert t[0].age == 48    Where did the data go to? We automatically archive the deleted old records for dict(name = 'yoav', surname = 'git') here:    &gt;&gt;&gt; t.archived()        t.archived() is a table by same name,        - exists on same database, schema name changed prefixed with 'archived_'    - same table structure with added 'deleted' column into the primary keys        &gt;&gt;&gt; assert len(t.archived().inc(name = 'yoav', age = 46)) &gt; 0    &gt;&gt;&gt; t.archived().delete() ## sql_cursor as a document store    If we set doc = True, the table will be viewed internally as a no-sql-like document store.     - the nullable columns supplied are the columns on which querying will be possible    - the primary keys are still used to ensure we have one document per unique pk    - the document is jsonified (handling non-json stuff like dates, np.array and pd.DataFrames) and put into the 'doc' column in the table, but this is invisible to the user.    :Example: doc management    ------------------------        We now suppose that we are not sure what records we want to keep for each student    &gt;&gt;&gt; from pyg import *    &gt;&gt;&gt; import datetime    &gt;&gt;&gt; t = sql_table(db = 'test', table = 'unstructured_students', non_null = ['name', 'surname'],                           _id = dict(_id = int, created = datetime.datetime),                           nullable =  dict(doc = str, details = str, dob = datetime.date, age = int, grade = float),                           pk = ['name', 'surname'],                          doc = True)   ##&lt;---- The table will actually be a document store    We are now able to keep varied structure per each record.     &gt;&gt;&gt; t = t.delete()        &gt;&gt;&gt; doc = dict(name = 'yoav', surname = 'git', age = 30, profession = 'coder', children = ['ayala', 'opher', 'itamar'])    &gt;&gt;&gt; inserted_doc = t.insert_one(doc)    &gt;&gt;&gt; assert t.inc(name = 'yoav', surname = 'git')[0].children == ['ayala', 'opher', 'itamar']    &gt;&gt;&gt; doc2 = dict(name = 'anna', surname = 'git', age = 28, employer = 'Cambridge University', hobbies = ['chess', 'music', 'swimming'])    &gt;&gt;&gt; _ = t.insert_one(doc2)    &gt;&gt;&gt; assert t[dict(age = 28)].hobbies == ['chess', 'music', 'swimming']  # Note that we can filter or search easily using the column 'age' that was specified in table. We cannot do this on 'employer'        :Example: document store containing pd.DataFrames.    ----------        &gt;&gt;&gt; from pyg import *    &gt;&gt;&gt; doc = dict(name = 'yoav', surname = 'git', age = 35,                    salary = pd.Series([100,200,300], drange(2)),                   costs = dict(fixed_cost     = 100,                                 variable_costs = pd.DataFrame(dict(transport = [0,1,2], food = [4,5,6], education = [10,20,30]), drange(2))))  # pandas object is in doc.costs.variable_costs        &gt;&gt;&gt; t = sql_table(db = 'test', table = 'unstructured_students', non_null = ['name', 'surname'],                           _id = dict(_id = int, created = datetime.datetime),                           nullable =  dict(doc = str, details = str, dob = datetime.date, age = int, grade = float),                           pk = ['name', 'surname'],                          writer = 'c:/temp/%name/%surname.parquet', ##&lt;---- The location where pd.DataFrame/Series are to be stored                          doc = True)       &gt;&gt;&gt; inserted = t.insert_one(doc)    &gt;&gt;&gt; import os    &gt;&gt;&gt; assert 'salary.parquet' in os.listdir('c:/temp/yoav/git')    &gt;&gt;&gt; assert 'variable_costs.parquet' in os.listdir('c:/temp/yoav/git/costs') ## yes, dicts of dicts, or dicts of lists are all fine...        We can now access the data seemlessly:    &gt;&gt;&gt; read_from_db = t.inc(name = 'yoav')[0]         &gt;&gt;&gt; read_from_file = pd_read_parquet('c:/temp/yoav/git/salary.parquet')    &gt;&gt;&gt; assert list(read_from_db.salary.values) == [100, 200, 300]    &gt;&gt;&gt; assert list(read_from_file.values) == [100, 200, 300]    </longdescription>
</pkgmetadata>