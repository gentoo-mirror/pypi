<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># PyLLaMACppOfficial supported Python bindings for [llama.cpp](https://github.com/ggerganov/llama.cpp) + gpt4all[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)[![PyPi version](https://badgen.net/pypi/v/pyllamacpp)](https://pypi.org/project/pyllamacpp/)[//]: # ([![Wheels]&amp;#40;https://github.com/abdeladim-s/pyllamacpp/actions/workflows/wheels.yml/badge.svg?branch=main&amp;event=push&amp;#41;]&amp;#40;https://github.com/abdeladim-s/pyllamacpp/actions/workflows/wheels.yml&amp;#41;)[//]: # ([![Wheels-windows-mac]&amp;#40;https://github.com/abdeladim-s/pyllamacpp/actions/workflows/wheels-windows_mac.yml/badge.svg&amp;#41;]&amp;#40;https://github.com/abdeladim-s/pyllamacpp/actions/workflows/wheels-windows_mac.yml&amp;#41;)[//]: # (&lt;br/&gt;)[//]: # (&lt;p align=&quot;center&quot;&gt;)[//]: # (  &lt;img src=&quot;https://github.com/abdeladim-s/pyllamacpp/blob/main/docs/demo.gif?raw=true&quot;&gt;)[//]: # (&lt;/p&gt;)For those who don't know, `llama.cpp` is a port of Facebook's LLaMA model in pure C/C++:&lt;blockquote&gt;- Without dependencies- Apple silicon first-class citizen - optimized via ARM NEON- AVX2 support for x86 architectures- Mixed F16 / F32 precision- 4-bit quantization support- Runs on the CPU&lt;/blockquote&gt;# Table of contents&lt;!-- TOC --&gt;* [Installation](#installation)* [Usage](#usage)* [Supported model](#supported-model)    * [GPT4All](#gpt4all)* [Discussions and contributions](#discussions-and-contributions)* [License](#license)&lt;!-- TOC --&gt;# Installation1. The easy way is to use the prebuilt wheels```bashpip install pyllamacpp```However, the compilation process of `llama.cpp` is taking into account the architecture of the target `CPU`, so you might need to build it from source:```shellgit clone --recursive https://github.com/abdeladim-s/pyllamacpp &amp;&amp; cd pyllamacpppip install .```# Usage[//]: # ()[//]: # (### Web UI)[//]: # (The package contains a simple web UI to test the bindings:)[//]: # ()[//]: # (- Lightweight, and easy to use.)[//]: # (- Only needs Python.)[//]: # (- Has the option to convert the models to `ggml` format.)[//]: # (- A code like editor.)[//]: # (- Different options to tweak the `llama.cpp` parameters.)[//]: # (- Ability to export the generated text.)[//]: # ()[//]: # (From the command line, run:)[//]: # (```shell)[//]: # (pyllamacpp-webui)[//]: # (```)[//]: # ()[//]: # (That's it!&lt;br&gt;)[//]: # (A web page will be opened on your default browser, otherwise navigate to the links provided by the command.)[//]: # ()[//]: # ()[//]: # (### Python bindings)A simple `Pythonic` API is built on top of `llama.cpp` C/C++ functions. You can call it from Python as follows:```pythonfrom pyllamacpp.model import Modeldef new_text_callback(text: str):    print(text, end=&quot;&quot;)model = Model(ggml_model='./models/gpt4all-model.bin', n_ctx=512)model.generate(&quot;Once upon a time, &quot;, n_predict=55, new_text_callback=new_text_callback, n_threads=8)```If you don't want to use the `callback`, you can get the results from the `generate` method once the inference is finished:```pythongenerated_text = model.generate(&quot;Once upon a time, &quot;, n_predict=55)print(generated_text)```* You can pass any `llama context` [parameter](https://nomic-ai.github.io/pyllamacpp/#pyllamacpp.constants.LLAMA_CONTEXT_PARAMS_SCHEMA) as a keyword argument to the `Model` class* You can pass any `gpt` [parameter](https://nomic-ai.github.io/pyllamacpp/#pyllamacpp.constants.GPT_PARAMS_SCHEMA) as a keyword argument to the `generarte` method* You can always refer to the [short documentation](https://nomic-ai.github.io/pyllamacpp/) for more details.# Supported model### GPT4All1. First [Get](https://github.com/nomic-ai/gpt4all#try-it-yourself) the gpt4all model.2. Convert it to the new `ggml` formatOn your terminal run: ```shellpyllamacpp-convert-gpt4all path/to/gpt4all_model.bin path/to/llama_tokenizer path/to/gpt4all-converted.bin```# FAQs* Where to find the llama tokenizer? [#5](https://github.com/nomic-ai/pyllamacpp/issues/5)# Discussions and contributionsIf you find any bug, please open an [issue](https://github.com/nomic-ai/pyllamacpp/issues).If you have any feedback, or you want to share how you are using this project, feel free to use the [Discussions](https://github.com/nomic-ai/pyllamacpp/discussions) and open a new topic.# LicenseThis project is licensed under the same license as [llama.cpp](https://github.com/ggerganov/llama.cpp/blob/master/LICENSE) (MIT  [License](./LICENSE)).</longdescription>
</pkgmetadata>