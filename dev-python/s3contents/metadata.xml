<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://raw.githubusercontent.com/danielfrg/s3contents/main/docs/logo.png&quot; width=&quot;450px&quot;&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://pypi.org/project/s3contents/&quot;&gt;        &lt;img src=&quot;https://img.shields.io/pypi/v/mkdocs-jupyter.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/danielfrg/s3contents/actions/workflows/test.yml&quot;&gt;        &lt;img src=&quot;https://github.com/danielfrg/s3contents/workflows/test/badge.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://codecov.io/gh/danielfrg/s3contents?branch=main&quot;&gt;        &lt;img src=&quot;https://codecov.io/gh/danielfrg/s3contents/branch/main/graph/badge.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;http://github.com/danielfrg/s3contents/blob/main/LICENSE.txt&quot;&gt;        &lt;img src=&quot;https://img.shields.io/:license-Apache%202-blue.svg&quot;&gt;    &lt;/a&gt;&lt;/p&gt;# S3Contents - Jupyter Notebooks in S3A transparent, drop-in replacement for Jupyter standard filesystem-backed storage system.With this implementation of a[Jupyter Contents Manager](https://jupyter-server.readthedocs.io/en/latest/developers/contents.html)you can save all your notebooks, files and directory structure directly to aS3/GCS bucket on AWS/GCP or a self hosted S3 API compatible like [MinIO](http://minio.io).## Installation```shellpip install s3contents```Install with GCS dependencies:```shellpip install s3contents[gcs]```## s3contents vs XWhile there are some implementations of an S3 Jupyter Content Manager such as[s3nb](https://github.com/monetate/s3nb) or [s3drive](https://github.com/stitchfix/s3drive)s3contents is the only one tested against new versions of Jupyter.It also supports more authentication methods and Google Cloud Storage.This aims to be a fully tested implementation and it's based on [PGContents](https://github.com/quantopian/pgcontents).## ConfigurationCreate a `jupyter_notebook_config.py` file in one of the[Jupyter config directories](https://jupyter.readthedocs.io/en/latest/use/jupyter-directories.html#id1)for example: `~/.jupyter/jupyter_notebook_config.py`.**Jupyter Notebook Classic**: If you plan to use the Classic Jupyter Notebookinterface you need to change `ServerApp` to `NotebookApp` for all the examples on this page.## AWS S3```pythonfrom s3contents import S3ContentsManagerc = get_config()# Tell Jupyter to use S3ContentsManagerc.ServerApp.contents_manager_class = S3ContentsManagerc.S3ContentsManager.bucket = &quot;&lt;S3 bucket name&gt;&quot;# Fix JupyterLab dialog issuesc.ServerApp.root_dir = &quot;&quot;```### AuthenticationAdditionally you can configure multiple authentication methods:Access and secret keys:```pythonc.S3ContentsManager.access_key_id = &quot;&lt;AWS Access Key ID / IAM Access Key ID&gt;&quot;c.S3ContentsManager.secret_access_key = &quot;&lt;AWS Secret Access Key / IAM Secret Access Key&gt;&quot;```Session token:```pythonc.S3ContentsManager.session_token = &quot;&lt;AWS Session Token / IAM Session Token&gt;&quot;```### AWS EC2 role auth setupIt also possible to use IAM Role-based access to the S3 bucket from an Amazon EC2 instance or AWS resource.To do that just leave any authentication options (`access_key_id`, `secret_access_key`) to their default of `None`and ensure that the EC2 instance has an IAM role which provides sufficient permissions (read and write) for the bucket.### Optional settings```python# A prefix in the S3 buckets to use as the root of the Jupyter file systemc.S3ContentsManager.prefix = &quot;this/is/a/prefix/on/the/s3/bucket&quot;# Server-Side Encryptionc.S3ContentsManager.sse = &quot;AES256&quot;# Authentication signature versionc.S3ContentsManager.signature_version = &quot;s3v4&quot;# See AWS key refreshc.S3ContentsManager.init_s3_hook = init_function```### AWS key refreshThe optional `init_s3_hook` configuration can be used to enable AWS key rotation (described [here](https://dev.to/li_chastina/auto-refresh-aws-tokens-using-iam-role-and-boto3-2cjf) and [here](https://www.owenrumney.co.uk/2019/01/15/implementing-refreshingawscredentials-python/)) as follows:```pythonfrom aiobotocore.credentials import AioRefreshableCredentialsfrom aiobotocore.session import get_sessionfrom configparser import ConfigParserfrom s3contents import S3ContentsManagerdef refresh_external_credentials():    config = ConfigParser()    config.read('/home/jovyan/.aws/credentials')    return {        &quot;access_key&quot;: config['default']['aws_access_key_id'],        &quot;secret_key&quot;: config['default']['aws_secret_access_key'],        &quot;token&quot;: config['default']['aws_session_token'],        &quot;expiry_time&quot;: config['default']['aws_expiration']    }async def async_refresh_credentials():    return refresh_external_credentials()def make_key_refresh_boto3(this_s3contents_instance):    session_credentials = AioRefreshableCredentials.create_from_metadata(        metadata = refresh_external_credentials(),        refresh_using = async_refresh_credentials,        method = 'custom-refreshing-key-file-reader'    )    refresh_session =  get_session() # from aibotocore.session    refresh_session._credentials = session_credentials    this_s3contents_instance.boto3_session = refresh_session# Tell Jupyter to use S3ContentsManagerc.ServerApp.contents_manager_class = S3ContentsManagerc.S3ContentsManager.init_s3_hook = make_key_refresh_boto3```### MinIO playground exampleYou can test this using the [`play.minio.io:9000`](https://play.minio.io:9000) playground:Just be sure to create the bucket first.```pythonfrom s3contents import S3ContentsManagerc = get_config()# Tell Jupyter to use S3ContentsManagerc.ServerApp.contents_manager_class = S3ContentsManagerc.S3ContentsManager.access_key_id = &quot;Q3AM3UQ867SPQQA43P2F&quot;c.S3ContentsManager.secret_access_key = &quot;zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG&quot;c.S3ContentsManager.endpoint_url = &quot;https://play.minio.io:9000&quot;c.S3ContentsManager.bucket = &quot;s3contents-demo&quot;c.S3ContentsManager.prefix = &quot;notebooks/test&quot;```## Access local filesTo access local file as well as remote files in S3 you can use [hybridcontents](https://github.com/viaduct-ai/hybridcontents).Install it:```shellpip install hybridcontents```Use a configuration similar to this:```pythonfrom s3contents import S3ContentsManagerfrom hybridcontents import HybridContentsManagerfrom notebook.services.contents.largefilemanager import LargeFileManagerc = get_config()c.ServerApp.contents_manager_class = HybridContentsManagerc.HybridContentsManager.manager_classes = {    # Associate the root directory with an S3ContentsManager.    # This manager will receive all requests that don&quot;t fall under any of the    # other managers.    &quot;&quot;: S3ContentsManager,    # Associate /local_directory with a LargeFileManager.    &quot;local_directory&quot;: LargeFileManager,}c.HybridContentsManager.manager_kwargs = {    # Args for root S3ContentsManager.    &quot;&quot;: {        &quot;access_key_id&quot;: &quot;&lt;AWS Access Key ID / IAM Access Key ID&gt;&quot;,        &quot;secret_access_key&quot;: &quot;&lt;AWS Secret Access Key / IAM Secret Access Key&gt;&quot;,        &quot;bucket&quot;: &quot;&lt;S3 bucket name&gt;&quot;,    },    # Args for the LargeFileManager mapped to /local_directory    &quot;local_directory&quot;: {        &quot;root_dir&quot;: &quot;/Users/danielfrg/Downloads&quot;,    },}```## GCP - Google Cloud StorageInstall the extra dependencies with:```shellpip install s3contents[gcs]``````pythonfrom s3contents.gcs import GCSContentsManagerc = get_config(c.ServerApp.contents_manager_class = GCSContentsManagerc.GCSContentsManager.project = &quot;&lt;your-project&gt;&quot;c.GCSContentsManager.token = &quot;~/.config/gcloud/application_default_credentials.json&quot;c.GCSContentsManager.bucket = &quot;&lt;GCP bucket name&gt;&quot;```Note that the file `~/.config/gcloud/application_default_credentials.json` assumesa POSIX system when you did `gcloud init`.## Other configuration### File Save HooksIf you want to use pre/post file save hooks here are some examples.A `pre_save_hook` is written in the exact same way as normal, operating on thefile in local storage before committing it to the object store.```pythondef scrub_output_pre_save(model, **kwargs):    &quot;&quot;&quot;    Scrub output before saving notebooks    &quot;&quot;&quot;    # only run on notebooks    if model[&quot;type&quot;] != &quot;notebook&quot;:        return    # only run on nbformat v4    if model[&quot;content&quot;][&quot;nbformat&quot;] != 4:        return    for cell in model[&quot;content&quot;][&quot;cells&quot;]:        if cell[&quot;cell_type&quot;] != &quot;code&quot;:            continue        cell[&quot;outputs&quot;] = []        cell[&quot;execution_count&quot;] = Nonec.S3ContentsManager.pre_save_hook = scrub_output_pre_save```A `post_save_hook` instead operates on the file in object storage,because of this it is useful to use the file methods on the `contents_manager`for data manipulation.In addition, one must use the following function signature (unique to `s3contents`):```pythondef make_html_post_save(model, s3_path, contents_manager, **kwargs):    &quot;&quot;&quot;    Convert notebooks to HTML after saving via nbconvert    &quot;&quot;&quot;    from nbconvert import HTMLExporter    if model[&quot;type&quot;] != &quot;notebook&quot;:        return    content, _format = contents_manager.fs.read(s3_path, format=&quot;text&quot;)    my_notebook = nbformat.reads(content, as_version=4)    html_exporter = HTMLExporter()    html_exporter.template_name = &quot;classic&quot;    (body, resources) = html_exporter.from_notebook_node(my_notebook)    base, ext = os.path.splitext(s3_path)    contents_manager.fs.write(path=(base + &quot;.html&quot;), content=body, format=_format)c.S3ContentsManager.post_save_hook = make_html_post_save```</longdescription>
</pkgmetadata>