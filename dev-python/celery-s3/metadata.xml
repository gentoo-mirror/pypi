<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Celery-S3[![Build Status](https://travis-ci.org/robgolding/celery-s3.svg?branch=master)](https://travis-ci.org/robgolding/celery-s3)[![Coverage Status](https://coveralls.io/repos/github/robgolding/celery-s3/badge.svg?branch=master)](https://coveralls.io/github/robgolding/celery-s3?branch=master)Celery-S3 is a simple S3 result backend for Celery.If used in conjunction with the SQS broker, it allows for Celery deploymentsthat use only distributed AWS services -- with no dependency on individualmachines within your infrastructure.This backend probably isn't suitable for particularly high-traffic Celerydeployments, but it works just fine in general -- and imposes no limits on thenumber of workers in the pool.## InstallationInstall via pip:`pip install celery-s3`Then configure Celery to use the `S3Backend`:    CELERY_RESULT_BACKEND = 'celery_s3.backends.S3Backend'    CELERY_S3_BACKEND_SETTINGS = {        'aws_access_key_id': '&lt;your_aws_access_key_id&gt;',        'aws_secret_access_key': '&lt;your_aws_secret_access_key&gt;',        'bucket': '&lt;your_bucket_name&gt;',    }## ConfigurationTo use a folder within the specified bucket, set the `base_path` in your`CELERY_S3_BACKEND_SETTINGS`:    CELERY_S3_BACKEND_SETTINGS = {        ...        'base_path': '/celery/',        ...    }To use a region other than the default (`us-east-1`), set the `aws_region`parameter:    CELERY_S3_BACKEND_SETTINGS = {        ...        'aws_region': 'us-east-1',        ...    }To use [reduced redundancy storage](https://aws.amazon.com/s3/reduced-redundancy/),set the `reduced_redundancy` parameter:    CELERY_S3_BACKEND_SETTINGS = {        ...        'reduced_redundancy': True,        ...    }To use [server-side encryption](http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html),set the `encrypt_key` parameter:    CELERY_S3_BACKEND_SETTINGS = {        ...        'encrypt_key': True,        ...    }## NotesStoring Celery results with this backend will obviously result in API callsbeing made to Amazon S3.  For each result, at least one `PUT` request will bemade (priced at $0.01 per 1,000 requests at the time of writing).  Also, thedata contained within the result object will be stored indefinitely, unlessotherwise specified.To fetch a result for a task that has already finished, at least two requestswill be made (one `HEAD` and one `GET`).  If you use Celery's `result.get()` towait for a task to finish, S3 will be polled continuously until the task hasfinished.By default, the poll interval is set to 0.5 seconds, which could result ina lot of requests (two `HEAD` requests per second until the task has finished,then one `GET` request to fetch the result).  If you need to use`result.get()`, consider increasing the interval and using a timeout to preventpolling forever: `result.get(interval=5, timeout=600)`.Also, for tasks whose result you don't need, be sure to use `ignore_result`:    @celery.task(ignore_result=True)    def process_data(obj):        obj.do_processing()Once task results have been used and are no longer needed, be sure to call`result.forget()` to delete the corresponding S3 key.  Otherwise, old resultswill remain forever and contribute to storage costs (storage is priced at$0.095 per GB per month at the time of writing).Also, the S3 lifecycle can be used to archive or delete old keys aftera certain period of time.</longdescription>
</pkgmetadata>