<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;# Modelz LLM&lt;/div&gt;&lt;p align=center&gt;&lt;a href=&quot;https://discord.gg/KqswhpVgdU&quot;&gt;&lt;img alt=&quot;discord invitation link&quot; src=&quot;https://dcbadge.vercel.app/api/server/KqswhpVgdU?style=flat&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://twitter.com/TensorChord&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/tensorchord?style=social&quot; alt=&quot;trackgit-views&quot; /&gt;&lt;/a&gt;&lt;/p&gt;Modelz LLM is an inference server that facilitates the utilization of open source large language models (LLMs), such as FastChat, LLaMA, and ChatGLM, on either **local or cloud-based** environments with **OpenAI compatible API**.## Features- **OpenAI compatible API**: Modelz LLM provides an OpenAI compatible API for LLMs, which means you can use the OpenAI python SDK or LangChain to interact with the model.- **Self-hosted**: Modelz LLM can be easily deployed on either local or cloud-based environments.- **Open source LLMs**: Modelz LLM supports open source LLMs, such as FastChat, LLaMA, and ChatGLM.- **Cloud native**: We provide docker images for different LLMs, which can be easily deployed on Kubernetes, or other cloud-based environments (e.g. [Modelz](https://modelz.ai))## Quick Start### Install```bashpip install modelz-llm# or install from sourcepip install git+https://github.com/tensorchord/modelz-llm.git[gpu]```### Run the self-hosted API serverPlease first start the self-hosted API server by following the instructions:```bashmodelz-llm -m bigscience/bloomz-560m --device cpu```Currently, we support the following models:| Model Name | Huggingface Model | Docker Image | Recommended GPU| ---------- | ----------- | ---------------- | -- || FastChat T5 | `lmsys/fastchat-t5-3b-v1.0` | [modelzai/llm-fastchat-t5-3b](https://hub.docker.com/repository/docker/modelzai/llm-fastchat-t5-3b/general) | Nvidia L4(24GB) || Vicuna 7B Delta V1.1  | `lmsys/vicuna-7b-delta-v1.1` | [modelzai/llm-vicuna-7b](https://hub.docker.com/repository/docker/modelzai/llm-vicuna-7b/general) | Nvidia A100(40GB) || LLaMA 7B    | `decapoda-research/llama-7b-hf` | [modelzai/llm-llama-7b](https://hub.docker.com/repository/docker/modelzai/llm-llama-7b/general) | Nvidia A100(40GB) || ChatGLM 6B INT4    | `THUDM/chatglm-6b-int4` | [modelzai/llm-chatglm-6b-int4](https://hub.docker.com/repository/docker/modelzai/llm-chatglm-6b-int4/general) | Nvidia T4(16GB) || ChatGLM 6B  | `THUDM/chatglm-6b` | [modelzai/llm-chatglm-6b](https://hub.docker.com/repository/docker/modelzai/llm-chatglm-6b/general) | Nvidia L4(24GB) || Bloomz 560M | `bigscience/bloomz-560m` | [modelzai/llm-bloomz-560m](https://hub.docker.com/repository/docker/modelzai/llm-bloomz-560m/general) | CPU || Bloomz 1.7B | `bigscience/bloomz-1b7` | | CPU || Bloomz 3B | `bigscience/bloomz-3b` |  | Nvidia L4(24GB) || Bloomz 7.1B | `bigscience/bloomz-7b1` | | Nvidia A100(40GB) |### Use OpenAI python SDKThen you can use the OpenAI python SDK to interact with the model:```pythonimport openaiopenai.api_base=&quot;http://localhost:8000&quot;openai.api_key=&quot;any&quot;# create a chat completionchat_completion = openai.ChatCompletion.create(model=&quot;any&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello world&quot;}])```### Integrate with LangchainYou could also integrate modelz-llm with langchain:```pythonimport openaiopenai.api_base=&quot;http://localhost:8000&quot;openai.api_key=&quot;any&quot;from langchain.llms import OpenAIllm = OpenAI()llm.generate(prompts=[&quot;Could you please recommend some movies?&quot;])```## Deploy on ModelzYou could also deploy the modelz-llm directly on [Modelz](https://docs.modelz.ai):[![](./docs/images/deploy.svg)](https://cloud.modelz.ai/deployment/template?templateId=5e884bb3-6c32-468e-bc62-95cee55c17d4)## Supported APIsModelz LLM supports the following APIs for interacting with open source large language models:- `/completions`- `/chat/completions`- `/embeddings`- `/engines/&lt;any&gt;/embeddings`- `/v1/completions`- `/v1/chat/completions`- `/v1/embeddings`## Acknowledgements- [FastChat](https://github.com/lm-sys/FastChat) for the prompt generation logic.- [Mosec](https://github.com/mosecorg/mosec) for the inference engine.</longdescription>
</pkgmetadata>