<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># mlplatformutils&lt;br /&gt; **mlplatformutils package for observability and ML Pipeline Processing** &lt;br /&gt; &lt;br /&gt; **This framework supports Azure Machine Learning training Pipeline supporting across computes such as Azure Synapse Spark, Virtual Machines Clusters, Azure Kubernetes Cluster, Azure Databricks. It supports reading/writing data from Azure Data Lake Gen2 in parquet and DELTA format, Azure Data Explorer (Kusto), Azure Sql DB instnces. The framework suports Python and Spark scalably. Writes with Spark with capabilties such a dynamic partitition overwrites, repartitioning are fully supported. In operating data reads and writes from such sources, The framework integrates built-in lineage framework providing column level lineage across the systems on a scalable Graph leveraging Azure Cosmos Gremlin Graph DB service. This enables a robust upstream dependency tracking and proactive alerting &amp; eventing.  All operations are suported over Service Principal (Client Id, Client Secrets) for applications and processing. The package also provides creating and managing computes, PIP dependecies for Azure Machine Learning Workspace and the training definitions.**## Description&lt;br /&gt;**app_insights_logger** - Contains **telemetrylogger** Class with Functions to Manage and Log Telemetry into Azure Application Insights &lt;br /&gt;&lt;br /&gt;* trackEvent* trackTrace* trackException* logEvent* gather_event_details&lt;br /&gt;**lineagegraph** - Contains **LineageGraph** Class with functions to manage Graph on Azure Cosmos DB enabled with Gremlin &lt;br /&gt;&lt;br /&gt;* add_vertex* get_vertices* is_vertex* update_vertex* insert_edges* drop_vertex* drop_edge* query_graph* update_lineage_graph* connect_lineage_graph**platformutils** - Contains platform utility functions to check, install depedencies, check Azure ML Compute * is_package_installed* install_pip* get_environment* set_environment* assert_amlcompute* read_setup_ini**sparkutils** - Contains functions to read data from sources such as (Azure Data Lake Gen2, Azure Data Explorer (Kusto), Azure Sql Server) and write (Azure Data Lake Gen2)while ensuring integrated Lineage Graph Logging.* read_from_adls_gen2* write_to_adls_gen2* read_from_kusto* synapseread_from_kusto* read_from_azsql* read_sstream_from_adls_gen1**sparkcoreutils** - Contains functions to read data from sources such as (Azure Data Lake Gen2, Azure Data Explorer (Kusto), Azure Sql Server, Azure Data Lake Gen1 Structured Streams) and write (Azure Data Lake Gen2) **without** integrated Lineage Graph Logging.* read_from_adls_gen2* write_to_adls_gen2* read_from_kusto* read_from_azsql* read_sstream_from_adls_gen1**pandasutils** - Contains functions to read data from Azure Data Lake Gen2 (from Delta Format or Parquet Format) into Pandas Dataframe without Spark while ensuring integrated Lineage Graph Logging.* read_from_delta_as_pandas* read_parquet_file_from_adlsgen2_as_pandas* read_parquet_directory_from_adlsgen2_as_pandas* write_pandas_as_parquet_file_to_adlsgen2**pandascoreutils** - Contains functions to read data from Azure Data Lake Gen2 (from Delta Format or Parquet Format) into Pandas Dataframe without Spark **without** integrated Lineage Graph Logging.* read_from_delta_as_pandas* read_parquet_file_from_adlsgen2_as_pandas* read_parquet_directory_from_adlsgen2_as_pandas* write_pandas_as_parquet_file_to_adlsgen2**freshnessutils** - Contains functions to add freshness details into Azure Cosmos (NoSQL) document db. This helps with the details on the freshness metrics on evaluating the SLA, and downstream processing. It captures and provides details on model, training dataset freshness for the most recent and historical processing.* add_freshness* upsert_freshness* query_freshness### Examples&lt;br /&gt;**from mlplatformutils.core.platformutils import is_package_installed** &lt;br /&gt;**print(is_package_installed(&quot;pandas&quot;))** &lt;br /&gt;**from mlplatformutils.core.app_insights_logger import telemetrylogger** &lt;br /&gt;**from mlplatformutils.core.lineagegraph import LineageGraph** &lt;br /&gt;**from mlplatformutils.core.sparkutils import write_to_adls_gen2, read_from_adls_gen2, read_sstream_from_adls_gen1** &lt;br /&gt;**from mlplatformutils.core.pandasutils import write_pandas_as_parquet_file_to_adlsgen2, read_parquet_directory_from_adlsgen2_as_pandas** &lt;br /&gt;**from mlplatformutils.core.sparkcoreutils import write_to_adls_gen2, read_from_adls_gen2, read_sstream_from_adls_gen1** &lt;br /&gt;**from mlplatformutils.core.pandascoreutils import write_pandas_as_parquet_file_to_adlsgen2, read_parquet_directory_from_adlsgen2_as_pandas** &lt;br /&gt;**from mlplatformutils.core.freshnessutils import add_freshness, upsert_freshness, query_freshness** &lt;br /&gt;**import mlplatformutils.core.version as vr** &lt;br /&gt;**print(vr.\_\_version\_\_)** &lt;br /&gt;### Notes&lt;br /&gt;When Running this Lineage Package from Jupyter Nootebook, the below 3 Lines Help overcome JupyterNotebook **RuntimeError: Cannot run the event loop while another loop is running** &lt;br /&gt;**import asyncio** &lt;br /&gt;**import nest_asyncio** &lt;br /&gt;**nest_asyncio.apply()** &lt;br /&gt;## Structure&lt;br /&gt;.&lt;br /&gt;|-- LICENSE.txt&lt;br /&gt;|-- README.rst&lt;br /&gt;|-- setup.cfg&lt;br /&gt;|-- setup.py&lt;br /&gt;|-- src&lt;br /&gt;|   |-- mlplatformutils&lt;br /&gt;|   |   |-- __init__.py&lt;br /&gt;|   |   |-- core&lt;br /&gt;|   |   |-- |-- __init__.py&lt;br /&gt;|   |   |-- |-- sparkcoreutils.py&lt;br /&gt;|   |   |-- |-- sparkutils.py&lt;br /&gt;|   |   |-- |-- platformutils.py&lt;br /&gt;|   |   |-- |-- pandascoreutils.py&lt;br /&gt;|   |   |-- |-- pandasutils.py&lt;br /&gt;|   |   |-- |-- lineagegraph.py&lt;br /&gt;|   |   |-- |-- freshnessutils.py&lt;br /&gt;|   |   |-- |-- app_insights_logger.py&lt;br /&gt;|-- tests&lt;br /&gt;|   |-- __init__.py&lt;br /&gt;|   |-- core&lt;br /&gt;|   |-- |--__init__.py&lt;br /&gt;|   |-- |-- test_sparkcoreutils.py&lt;br /&gt;|   |-- |-- test_sparkutils.py&lt;br /&gt;|   |-- |-- test_platformutils.py&lt;br /&gt;|   |-- |-- test_pandascoreutils.py&lt;br /&gt;|   |-- |-- test_pandasutils.py&lt;br /&gt;|   |-- |-- test_lineagegraph.py&lt;br /&gt;|   |-- |-- test_freshnessutils.py&lt;br /&gt;|   |-- |-- test_app_insights_logger.py&lt;br /&gt;&lt;br /&gt;## Instructions&lt;br /&gt; install twine - twine is a utility package that is used for publishing Python packages on PyPI &lt;br /&gt;  **python -m pip install twine** &lt;br /&gt;  Build Package - create the source distribution of the package &lt;br /&gt;  **python setup.py sdist** &lt;br /&gt;  Upload Package to PyPI &lt;br /&gt; ***python -m twine upload dist/* *** &lt;br /&gt;</longdescription>
</pkgmetadata>