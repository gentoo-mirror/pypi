<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;h1&gt;bio-attention&lt;/h1&gt;Simple implementations of attention modules adapted for the biological data domain.[![PyPi Version](https://img.shields.io/pypi/v/bio-attention.svg)](https://pypi.python.org/pypi/bio-attention/)[![GitHub license](https://img.shields.io/github/license/gdewael/bio-attention)](https://github.com/gdewael/bio-attention/blob/main/LICENSE)&lt;/div&gt;# :construction: THIS CODE IS BEING ACTIVELY DEVELOPED :construction:Don't look for stability here (yet).## Why use this package?There are already plenty of excellent implementations out there that allow you to test out the countless variants of transformers [[1]](https://github.com/facebookresearch/xformers), [[2]](https://github.com/lucidrains/x-transformers).This repository primarily separates itself from the previous in that it contains positional encodings schemes adapted to allow for irregularly-spaced positions in sequences.## InstallSince PyTorch is a dependency of `bio-attention`, we recommend [installing PyTorch](https://pytorch.org/get-started/locally/) independently first, as your system may require a specific version (e.g. CUDA drivers).After PyTorch installation, `bio-attention` can be installed using `pip````bashpip install bio-attention```## NoteThis package used the be a 2D sliding window attention package. The current formulation of the package does not allow for this type of attention anymore (instead, I recommend to perform axial attention with alternating sliding window attention across one axis and full self-attention across the other). If you want to use 2D sliding window attention, check out the [old version of this repo](https://github.com/gdewael/bio-attention/tree/ac3cb87906a2ff7adf9de393a5d2bbd3bf11eef3).## Usage## Package roadmap- [x] Embedding layers  - [x] Continuous  - [x] Discrete  - [x] Binary  - [x] Bin- [~] Positional encoding schemes  - [x] Sinusoidal  - [x] Embedding  - [x] Continuous  - [x] Rotary  - [x] AliBi  - [x] DPB  - [x] XL  - [x] Test support for multi-dimensional inputs- [~] Attention modules  - [x] Vanilla  - [x] Windowed  - [x] Random  - [x] Performer  - [x] Encoder  - [x] Decoder  - [ ] Cross  - [x] Support for multi-dim inputs- [ ] Add a warning if non-increasing positional indices are used with a decoder attention- [ ] Add docs clarifying that clf tokens are automatically accounted for if no pos is provided for them- [ ] Tests- [ ] Typing- [ ] Docs</longdescription>
</pkgmetadata>