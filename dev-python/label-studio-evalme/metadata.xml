<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># label-studio-evalmeEvaluation metrics package## InstallationSimple installation from PyPI```bashpip install label-studio-evalme```&lt;details&gt;  &lt;summary&gt;Other installation methods&lt;/summary&gt;  Pip from source```bash# with gitpip install git+https://github.com/heartexlabs/label-studio-evalme.git@master```&lt;/details&gt;## What is Evalme?Evalme is a collection of Label Studio evaluation metric implementations and an easy-to-use API to create custom metrics. It offers:* A standardized interface to increase reproducibility* Reduced boilerplate* Optimized metrics for Label Studio## Get started with EvalmeYou can use Evalme with any Label Studio versions or with Label Studio Enterprise.### Load existing data from Label StudioUse the Label Studio REST API to load existing data from your instance of Label Studio or Label Studio Enterprise.Specify your Label Studio URL, access token and project ID in the parameters:``` pythonfrom evalme.matcher import Matcherloader = Matcher(url=&quot;http://127.0.0.1:8000&quot;,                 token=&quot;ACCESS_TOKEN&quot;,                 project='1')loader.refresh()```You can also load data from exported annotation files from Label Studio, exported using [the API](https://labelstud.io/guide/api.html#Export-annotations) or the [Label Studio UI](https://labelstud.io/guide/export.html):``` pythonfrom evalme.matcher import Matcherloader = Matcher()loader.load('your_filename')```After you load data, it is available in the `_raw_data` field. ### Built-in metricsBy default there is a naive metric object. It evaluates annotation differences with a naive approach:if an object is fully equal to another one, the evaluation method returns 1,otherwise it returns 0.To use the built-in metrics, do the following:``` pythonfrom evalme.matcher import Matcherloader = Matcher()loader.load('your_filename')# Run agreement_matrix method to get matrix for all your annotationsmatrix = loader.agreement_matrix()# print resultprint(matrix)```### Implement your own metricYou can implement your own metric by creating an evaluation function and registering it in Metrics class. For example, create an evaluation function with 2 parameters for compared objects:```pythonfrom evalme.matcher import Matcher# write your own evaluation function or use existing onedef naive(x, y):&quot;&quot;&quot;    Naive comparison of annotations    &quot;&quot;&quot;    if len(x) != len(y):        result = 0    else:        for i in range(len(x)):            if x[i]['value'] != y[i]['value']:                result = 0                break        else:            result = 1    return result# Register it in Metrics objectMetrics.register(    name='naive',    form=None,    tag='all',    func=naive,    desc='Naive comparison of result dict')# create Matcher object from previous exampleloader = Matcher()loader.load('your_filename')matrix = loader.agreement_matrix(metric_name='naive')# print resultprint(matrix)```## Contribute!The Label Studio team is hard at work adding even more metrics, but we're looking for incredible contributors like you to submit new metrics and improve existing ones!Join our [Slack community](https://join.slack.com/t/label-studio/shared_invite/zt-cr8b7ygm-6L45z7biEBw4HXa5A2b5pw)to get help becoming a contributor!## CommunityFor help or questions, join our huge community on [Slack](https://join.slack.com/t/label-studio/shared_invite/zt-cr8b7ygm-6L45z7biEBw4HXa5A2b5pw)!## LicensePlease observe the MIT License that is listed in this repository. </longdescription>
</pkgmetadata>