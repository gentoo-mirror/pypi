<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># visionai-data-format`VisionAI` format is Dataverse[&quot;url&quot;] standardized annotation format to label objects and sequences in the context of Autonomous Driving System(ADS). `VisionAI` provides consistent and effective driving environment description and categorization in the real-world case.This tool provides validator of `VisionAI` format schema. Currently, the library supports:  - Validate created `VisionAI` data format  - Validate `VisionAI` data attributes with given `Ontology` information.[Package (PyPi)](https://pypi.org/project/visionai-data-format/)    |   [Source code](https://github.com/linkernetworks/visionai-data-format)## Getting started(WIP)### Install the package```pip install visionai-data-format```**Prerequisites**: You must have [Python 3.7](https://www.python.org/downloads/) and above to use this package.## ExampleThe following sections provide examples for the following:* [Validate VisionAI schema](###validate-visionai-schema)* [Validate VisionAI data with given Ontology](###validate-visionai-data-with-given-ontology)### Validate VisionAI schemaTo validate `VisionAI` data structure, could follow the example below:```Pythonfrom visionai_data_format.schemas.visionai_schema import VisionAIModel# your custom visionai datacustom_visionai_data = {    &quot;visionai&quot;: {        &quot;frame_intervals&quot;: [            {                &quot;frame_start&quot;: 0,                &quot;frame_end&quot;: 0            }        ],        &quot;frames&quot;: {            &quot;000000000000&quot;: {                &quot;objects&quot;: {                    &quot;893ac389-7782-4bc3-8f61-09a8e48c819f&quot;: {                        &quot;object_data&quot;: {                            &quot;bbox&quot;: [                                {                                    &quot;name&quot;: &quot;bbox_shape&quot;,                                    &quot;stream&quot;:&quot;camera1&quot;,                                    &quot;val&quot;: [761.565,225.46,98.33000000000004, 164.92000000000002]                                }                            ],                            &quot;cuboid&quot;: [                                {                                    &quot;name&quot;: &quot;cuboid_shape&quot;,                                    &quot;stream&quot;: &quot;lidar1&quot;,                                    &quot;val&quot;: [                                        8.727633224700037,-1.8557590122690717,-0.6544039394148177, 0.0,                                        0.0,-1.5807963267948966,1.2,0.48,1.89                                    ]                                }                            ]                        }                    }                },                &quot;frame_properties&quot;: {                    &quot;streams&quot;: {                        &quot;camera1&quot;: {                            &quot;uri&quot;: &quot;https://helenmlopsstorageqatest.blob.core.windows.net/vainewformat/kitti/kitti_small/data/000000000000/data/camera1/000000000000.png&quot;                        },                        &quot;lidar1&quot;: {                            &quot;uri&quot;: &quot;https://helenmlopsstorageqatest.blob.core.windows.net/vainewformat/kitti/kitti_small/data/000000000000/data/lidar1/000000000000.pcd&quot;                        }                    }                }            }        },        &quot;objects&quot;: {            &quot;893ac389-7782-4bc3-8f61-09a8e48c819f&quot;: {                &quot;frame_intervals&quot;: [                    {                        &quot;frame_start&quot;: 0,                        &quot;frame_end&quot;: 0                    }                ],                &quot;name&quot;: &quot;pedestrian&quot;,                &quot;object_data_pointers&quot;: {                    &quot;bbox_shape&quot;: {                        &quot;frame_intervals&quot;: [                            {                                &quot;frame_start&quot;: 0,                                &quot;frame_end&quot;: 0                            }                        ],                        &quot;type&quot;: &quot;bbox&quot;                    },                    &quot;cuboid_shape&quot;: {                        &quot;frame_intervals&quot;: [                            {                                &quot;frame_start&quot;: 0,                                &quot;frame_end&quot;: 0                            }                        ],                        &quot;type&quot;: &quot;cuboid&quot;                    }                },                &quot;type&quot;: &quot;pedestrian&quot;            }        },        &quot;coordinate_systems&quot;: {            &quot;lidar1&quot;: {                &quot;type&quot;: &quot;sensor_cs&quot;,                &quot;parent&quot;: &quot;&quot;,                &quot;children&quot;: [                    &quot;camera1&quot;                ]            },            &quot;camera1&quot;: {                &quot;type&quot;: &quot;sensor_cs&quot;,                &quot;parent&quot;: &quot;lidar1&quot;,                &quot;children&quot;: [],                &quot;pose_wrt_parent&quot;: {                    &quot;matrix4x4&quot;: [                        -0.00159609942076306,                        -0.005270645688933059,                        0.999984790046273,                        0.3321936949138632,                        -0.9999162467477257,                        0.012848695454066989,                        -0.0015282672486530082,                        -0.022106263278130818,                        -0.012840436309973332,                        -0.9999035522454274,                        -0.0052907123281999745,                        -0.06171977032225582,                        0.0,                        0.0,                        0.0,                        1.0                    ]                }            }        },        &quot;streams&quot;: {            &quot;camera1&quot;: {                &quot;type&quot;: &quot;camera&quot;,                &quot;uri&quot;: &quot;https://helenmlopsstorageqatest.blob.core.windows.net/vainewformat/kitti/kitti_small/data/000000000000/data/camera1/000000000000.png&quot;,                &quot;description&quot;: &quot;Frontal camera&quot;,                &quot;stream_properties&quot;: {                    &quot;intrinsics_pinhole&quot;: {                        &quot;camera_matrix_3x4&quot;: [                            -1.1285209781809271,                            -706.9900823216068,                            -181.46849639413674,                            0.2499212908887926,                            -3.726606344908137,                            9.084661126711246,                            -1.8645282480709864,                            -0.31027342289053916,                            707.0385458128643,                            -1.0805602883730354,                            603.7910589125847,                            45.42556655376811                        ],                        &quot;height_px&quot;: 370,                        &quot;width_px&quot;: 1224                    }                }            },            &quot;lidar1&quot;: {                &quot;type&quot;: &quot;lidar&quot;,                &quot;uri&quot;: &quot;https://helenmlopsstorageqatest.blob.core.windows.net/vainewformat/kitti/kitti_small/data/000000000000/data/lidar1/000000000000.pcd&quot;,                &quot;description&quot;: &quot;Central lidar&quot;            }        },        &quot;metadata&quot;: {            &quot;schema_version&quot;: &quot;1.0.0&quot;        }    }}# validate custom data# If the data structure doesn't meets the VisionAI requirements, it would raise BaseModel error message# otherwise, it will returns dictionary of validated visionai datavalidated_visionai = VisionAIModel(**custom_visionai_data).dict()```First, we declare our custom `VisionAI` data, then call `VisionAI(**custom_visionai_data).dict()` to validate our custom data visionai schema. It will raise error if any of required fields is missing or the value type doesn't meet with defined data type ( `BaseModel` error message). Otherwise, it will return dictionary of validated `VisionAI` data### Validate VisionAI data with given OntologyBefore upload dataset into `Dataverse` platform, we could try to validate a `VisionAI` annotation with `Ontology` schema. `Ontology` schema works as a predefined `Project Ontology` data in `Dataverse`.`Ontology` contains `contexts`, `objects`, `streams`, and `tags` four main elements similar to `VisioniAI` schema. The difference is that `Ontology` is the union of all categories and attributes that will be compared with a `VisionAI` data.1. `contexts`    need to be filled if only the project ontology is `classification` type.2. `objects`    need to be filled for other project ontologies instead of `classification`, such as `bounding_box` or `semantic_segmentation`, etc.3. `streams`    required to be filled, since it is the project sensor related information.4. `tags`    need to be filled in case of `semantic_segmentation` project ontology.Following is the example of `Ontology` Schema and how to validate `VisionAI` data with it:```Pythonfrom visionai_data_format.schemas.ontology import Ontologycustom_ontology = {    &quot;objects&quot;: {        &quot;pedestrian&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                },                &quot;activity&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                }            }        },        &quot;truck&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                },                &quot;color&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                },                &quot;new&quot;: {                    &quot;type&quot;: &quot;boolean&quot;,                    &quot;value&quot;: []                },                &quot;year&quot;: {                    &quot;type&quot;: &quot;num&quot;,                    &quot;value&quot;: []                },                &quot;status&quot;: {                    &quot;type&quot;: &quot;vec&quot;,                    &quot;value&quot;: [                        &quot;stop&quot;,                        &quot;run&quot;,                        &quot;small&quot;,                        &quot;large&quot;                    ]                }            }        },        &quot;car&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                },                &quot;color&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                },                &quot;new&quot;: {                    &quot;type&quot;: &quot;boolean&quot;,                    &quot;value&quot;: []                },                &quot;year&quot;: {                    &quot;type&quot;: &quot;num&quot;,                    &quot;value&quot;: []                },                &quot;status&quot;: {                    &quot;type&quot;: &quot;vec&quot;,                    &quot;value&quot;: [                        &quot;stop&quot;,                        &quot;run&quot;,                        &quot;small&quot;,                        &quot;large&quot;                    ]                }            }        },        &quot;cyclist&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                }            }        },        &quot;dontcare&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                }            }        },        &quot;misc&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                },                &quot;color&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                },                &quot;info&quot;: {                    &quot;type&quot;: &quot;vec&quot;,                    &quot;value&quot;: [                        &quot;toyota&quot;,                        &quot;new&quot;                    ]                }            }        },        &quot;van&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                }            }        },        &quot;tram&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                }            }        },        &quot;person_sitting&quot;: {            &quot;attributes&quot;: {                &quot;bbox_shape&quot;: {                    &quot;type&quot;: &quot;bbox&quot;,                    &quot;value&quot;: None                },                &quot;cuboid_shape&quot;: {                    &quot;type&quot;: &quot;cuboid&quot;,                    &quot;value&quot;: None                }            }        }    },    &quot;contexts&quot;:{        &quot;*tagging&quot;: {            &quot;attributes&quot;:{                &quot;profession&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                },                &quot;roadname&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                },                &quot;name&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                },                &quot;unknown_object&quot;: {                    &quot;type&quot;: &quot;vec&quot;,                    &quot;value&quot;: [                        &quot;sky&quot;,                        &quot;leaves&quot;,                        &quot;wheel_vehicle&quot;,                        &quot;fire&quot;,                        &quot;water&quot;                    ]                },                &quot;static_status&quot;: {                    &quot;type&quot;: &quot;boolean&quot;,                    &quot;value&quot;: [                        &quot;true&quot;,                        &quot;false&quot;                    ]                },                &quot;year&quot;: {                    &quot;type&quot;: &quot;num&quot;,                    &quot;value&quot;: []                },                &quot;weather&quot;: {                    &quot;type&quot;: &quot;text&quot;,                    &quot;value&quot;: []                }            }        }    },    &quot;streams&quot;: {        &quot;camera1&quot;: {            &quot;type&quot;: &quot;camera&quot;        },        &quot;lidar1&quot;: {            &quot;type&quot;: &quot;lidar&quot;        }    },    &quot;tags&quot;: None}# Validate your custom ontologyvalidated_ontology = Ontology(**custom_ontology).dict()# Validate VisionAI data with our ontology, custom_visionai_data is the custom data from upper exampleerrors = VisionAIModel(**custom_visionai_data).validate_with_ontology(ontology=validated_ontology)# Shows the errors# If there is any error occurred, it will returns list of error messages# Otherwise, it will return empty list# example of errors :# &gt;[&quot;validate objects error: Missing attributes from data pointers : {('893ac389-7782-4bc3-8f61-09a8e48c819f', 'bbox_shape'), ('893ac389-7782-4bc3-8f61-09a8e48c819f', 'cuboid_shape')} \n&quot;]print(errors)```First, create a new `Ontology` that contains the project ontology. Then, call `validate_with_ontology(ontology=validated_ontology)` to validate whether current `VisionAI` data meets the `Ontology` data information. It will returns list of error messages if any error occured, otherwise it returns empty list.## Tools### Convert `BDD+` format data to `VisionAI` format#### (Only support box2D and camera sensor data only for now)``` python visionai_data_format/convert_dataset.py -input_format bddp -output_format vision_ai -image_annotation_type 2d_bounding_box -input_annotation_path ./bdd_test.json -source_data_root ./data_root -output_dest_folder ~/visionai_output_dir -uri_root http://storage_test -n_frame 5 -sequence_idx_start 0 -camera_sensor_name camera1 -annotation_name groundtruth -img_extension .jpg --copy_sensor_data```Arguments :- `-input_format`  : input format (use bddp for BDD+)- `-output_format`  : output format (vision_ai)- `-image_annotation_type`  : label annotation type for image (2d_bounding_box for box2D)- `-input_annotation_path`  : source annotation path (BDD+ format json file)- `-source_data_root`  : source data root for sensor data and calibration data (will find and copy image from this root)- `-output_dest_folder` : output root folder (VisionAI local root folder)- `-uri_root` : uri root for target upload VAI storage i.e: https://azuresorate/vai_dataset- `-n_frame`  : number of frame to be converted (-1 means all), by default -1- `-sequence_idx_start `  : sequence start id, by default 0- `-camera_sensor_name`  : camera sensor name (default: &quot;&quot;, specified it if need to convert camera data)- `-lidar_sensor_name`  : lidar sensor name (default: &quot;&quot;, specified it if need to convert lidar data)- `-annotation_name` : annotation folder name (default: &quot;groundtruth&quot;)- `-img_extension` :image file extention (default: &quot;.jpg&quot;)- `--copy_sensor_data` :enable to copy image/lidar data### Convert `VisionAI` format data to `BDD+` format#### (Only support box2D for now)The script below could help convert `VisionAI` annotation data to `BDD+` json file```python visionai_data_format/vai_to_bdd.py -vai_src_folder /path_for_visionai_root_folder -bdd_dest_file /dest_path/bdd.json -company_code 99 -storage_name storge1 -container_name dataset1 -annotation_name groundtruth```Arguments :- `-vai_src_folder` : VAI root folder contains VAI format json file- `-bdd_dest_file`  : BDD+ format file save destination- `-company_code`  : company code- `-storage_name`  : storage name- `-container_name`  : container name (dataset name)- `-annotation_name` : annotation folder name (default: &quot;groundtruth&quot;)### Convert `Kitti` format data to `VisionAI` format#### (Only support KITTI with one camera and one lidar sensor)Important:- image type is not restricted, could be &quot;.jpg&quot; or &quot;.png&quot;, but we will convert it into &quot;.jpg&quot; in `VisionAI` format- only support for `P2` projection matrix calibration informationCurrently,only support `KITTI` dataset with structure folder :```bash.kitti_folder├── calib│   ├── 000000.txt│   ├── 000001.txt│   ├── 000002.txt│   ├── 000003.txt│   └── 000004.txt├── data│   ├── 000000.png│   ├── 000001.png│   ├── 000002.png│   ├── 000003.png│   └── 000004.png├── labels│   ├── 000000.txt│   ├── 000001.txt│   ├── 000002.txt│   ├── 000003.txt│   └── 000004.txt└── pcd    ├── 000000.pcd    ├── 000001.pcd    ├── 000002.pcd    ├── 000003.pcd    └── 000004.pcd```Command :``` python visionai_data_format/convert_dataset.py -input_format kitti -output_format vision_ai -image_annotation_type 2d_bounding_box -source_data_root ./data_root -output_dest_folder ~/visionai_output_dir -uri_root http://storage_test -n_frame 5 -sequence_idx_start 0 -camera_sensor_name camera1 -lidar_sensor_name lidar1 -annotation_name groundtruth -img_extension .jpg --copy_sensor_data```Arguments :- `-input_format`  : input format (use kitti for KITTI)- `-output_format`  : output format (vision_ai)- `-image_annotation_type`  : label annotation type for image (2d_bounding_box for box2D)- `-source_data_root`  : source data root for sensor data and calibration data (will find and copy image from this root)- `-output_dest_folder` : output root folder (VisionAI local root folder)- `-uri_root` : uri root for target upload VAI storage i.e: https://azuresorate/vai_dataset- `-n_frame`  : number of frame to be converted (-1 means all), by default -1- `-sequence_idx_start `  : sequence start id, by default 0- `-camera_sensor_name`  : camera sensor name (default: &quot;&quot;, specified it if need to convert camera data)- `-lidar_sensor_name`  : lidar sensor name (default: &quot;&quot;, specified it if need to convert lidar data)- `-annotation_name` : annotation folder name (default: &quot;groundtruth&quot;)- `-img_extension` :image file extention (default: &quot;.jpg&quot;)- `--copy_sensor_data` :enable to copy image/lidar data### Convert `COCO` format data to `VisionAI` format``` python visionai_data_format/convert_dataset.py -input_format coco -output_format vision_ai -image_annotation_type 2d_bounding_box -input_annotation_path ./coco_instance.json -source_data_root ./coco_images/ -output_dest_folder ~/visionai_output_dir -uri_root http://storage_test -n_frame 5 -sequence_idx_start 0 -camera_sensor_name camera1 -annotation_name groundtruth -img_extension .jpg --copy_sensor_data```Arguments :- `-input_format`  : input format (use coco for COCO format)- `-output_format`  : output format (vision_ai)- `-image_annotation_type`  : label annotation type for image (2d_bounding_box for box2D)- `-source_data_root`  : image data folder- `-output_dest_folder` : output root folder (VisionAI local root folder)- `-uri_root` : uri root for target upload VAI storage i.e: https://azuresorate/vai_dataset- `-n_frame`  : number of frame to be converted (-1 means all), by default -1- `-sequence_idx_start `  : sequence start id, by default 0- `-camera_sensor_name`  : camera sensor name (default: &quot;&quot;, specified it if need to convert camera data)- `-annotation_name` : annotation folder name (default: &quot;groundtruth&quot;)- `-img_extension` :image file extention (default: &quot;.jpg&quot;)- `--copy_sensor_data` :enable to copy image/lidar data## Troubleshooting(WIP)## Next steps(WIP)## Contributing(WIP)## Links to language repos(WIP)[Python Readme](https://github.com/linkernetworks/visionai-data-format/tree/develop/README.md)</longdescription>
</pkgmetadata>