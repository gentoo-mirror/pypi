<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># llm-rs-python: Python Bindings for Rust's llm Library[![PyPI](https://img.shields.io/pypi/v/llm-rs)](https://pypi.org/project/llm-rs/)[![PyPI - License](https://img.shields.io/pypi/l/llm-rs)](https://pypi.org/project/llm-rs/)[![Downloads](https://static.pepy.tech/badge/llm-rs)](https://pepy.tech/project/llm-rs)Welcome to `llm-rs`, an unofficial Python interface for the Rust-based [llm](https://github.com/rustformers/llm) library, made possible through [PyO3](https://github.com/PyO3/pyo3). Our package combines the convenience of Python with the performance of Rust to offer an efficient tool for your machine learning projects. üêç‚ù§Ô∏èü¶ÄWith `llm-rs`, you can operate a variety of Large Language Models (LLMs) including LLama and GPT-NeoX directly on your CPU. For a detailed overview of all the supported architectures, visit the [llm](https://github.com/rustformers/llm) project page. ### Integrations:* ü¶úÔ∏èüîó [LangChain](https://github.com/hwchase17/langchain)* üåæüî± [Haystack](https://github.com/deepset-ai/haystack)## InstallationSimply install it via pip: `pip install llm-rs`### Installation with GPU Acceleration Support&gt; ‚ö†Ô∏è Please note that GPU Acceleration support is currently in its experimental phase.`llm-rs` incorporates support for various GPU-accelerated backends to facilitate enhanced inference times. To enable GPU-acceleration the `use_gpu` parameter of your `SessionConfig` must be set to `True`. We distribute prebuilt binaries for the following operating systems and graphics APIs:#### MacOS (Using Metal)For MacOS users, the Metal-supported version of `llm-rs` can be easily installed via pip:`pip install llm-rs-metal`#### Windows/Linux (Using CUDA for Nvidia GPUs)Due to the significant file size, CUDA-supported packages cannot be directly uploaded to `pip`. To install them, download the appropriate `*.whl` file from the latest [Release](https://github.com/LLukas22/llm-rs-python/releases/latest) and install it using pip as follows:`pip install [wheelname].whl`#### Windows/Linux (Using OpenCL for All GPUs)&gt; ‚ö†Ô∏è OpenCL support is highly experimental and may not provide stable results.For universal GPU support on Windows and Linux, we offer an OpenCL-supported version. It can be installed via pip:`pip install llm-rs-opencl`## Usage### Running local GGML models:Models can be loaded via the `AutoModel` interface.```python from llm_rs import AutoModel, KnownModels#load the modelmodel = AutoModel.from_pretrained(&quot;path/to/model.bin&quot;,model_type=KnownModels.Llama)#generateprint(model.generate(&quot;The meaning of life is&quot;))```### Streaming TextText can be yielded from a generator via the `stream` function:```python from llm_rs import AutoModel, KnownModels#load the modelmodel = AutoModel.from_pretrained(&quot;path/to/model.bin&quot;,model_type=KnownModels.Llama)#generatefor token in model.stream(&quot;The meaning of life is&quot;):    print(token)```### Running GGML models from the Hugging Face HubGGML converted models can be directly downloaded and run from the hub.```python from llm_rs import AutoModelmodel = AutoModel.from_pretrained(&quot;rustformers/mpt-7b-ggml&quot;,model_file=&quot;mpt-7b-q4_0-ggjt.bin&quot;)```If there are multiple models in a repo the `model_file` has to be specified.If you want to load repositories which were not created throught this library, you have to specify the `model_type` parameter as the metadata files needed to infer the architecture are missing.### Running Pytorch Transfomer models from the Hugging Face Hub`llm-rs` supports automatic conversion of all supported transformer architectures on the Huggingface Hub. To run covnersions additional dependencies are needed which can be installed via `pip install llm-rs[convert]`.The models can then be loaded and automatically converted via the `from_pretrained` function.```pythonfrom llm_rs import AutoModelmodel = AutoModel.from_pretrained(&quot;mosaicml/mpt-7b&quot;)```### Convert Huggingface Hub ModelsThe following example shows how a [Pythia](https://huggingface.co/EleutherAI/pythia-410m) model can be covnverted, quantized and run.```pythonfrom llm_rs.convert import AutoConverterfrom llm_rs import AutoModel, AutoQuantizerimport sys#define the model which should be converted and an output directoryexport_directory = &quot;path/to/directory&quot; base_model = &quot;EleutherAI/pythia-410m&quot;#convert the modelconverted_model = AutoConverter.convert(base_model, export_directory)#quantize the model (this step is optional)quantized_model = AutoQuantizer.quantize(converted_model)#load the quantized modelmodel = AutoModel.load(quantized_model,verbose=True)#generate textdef callback(text):    print(text,end=&quot;&quot;)    sys.stdout.flush()model.generate(&quot;The meaning of life is&quot;,callback=callback)```## ü¶úÔ∏èüîó LangChain UsageUtilizing `llm-rs-python` through langchain requires additional dependencies. You can install these using `pip install llm-rs[langchain]`. Once installed, you gain access to the `RustformersLLM` model through the `llm_rs.langchain` module. This particular model offers features for text generation and embeddings.Consider the example below, demonstrating a straightforward LLMchain implementation with MPT-Instruct:```pythonfrom llm_rs.langchain import RustformersLLMfrom langchain import PromptTemplatefrom langchain.chains import LLMChainfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlertemplate=&quot;&quot;&quot;Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction:{instruction}### Response:Answer:&quot;&quot;&quot;prompt = PromptTemplate(input_variables=[&quot;instruction&quot;],template=template,)llm = RustformersLLM(model_path_or_repo_id=&quot;rustformers/mpt-7b-ggml&quot;,model_file=&quot;mpt-7b-instruct-q5_1-ggjt.bin&quot;,callbacks=[StreamingStdOutCallbackHandler()])chain = LLMChain(llm=llm, prompt=prompt)chain.run(&quot;Write a short post congratulating rustformers on their new release of their langchain integration.&quot;)```## üåæüî± Haystack UsageUtilizing `llm-rs-python` through haystack requires additional dependencies. You can install these using `pip install llm-rs[haystack]`. Once installed, you gain access to the `RustformersInvocationLayer` model through the `llm_rs.haystack` module. This particular model offers features for text generation.Consider the example below, demonstrating a straightforward Haystack-Pipeline implementation with OpenLLama-3B:```pythonfrom haystack.nodes import PromptNode, PromptModelfrom llm_rs.haystack import RustformersInvocationLayermodel = PromptModel(&quot;rustformers/open-llama-ggml&quot;,                    max_length=1024,                    invocation_layer_class=RustformersInvocationLayer,                    model_kwargs={&quot;model_file&quot;:&quot;open_llama_3b-q5_1-ggjt.bin&quot;})pn = PromptNode(    model,    max_length=1024)pn(&quot;Write me a short story about a lama riding a crab.&quot;,stream=True)```## DocumentationFor in-depth information on customizing the loading and generation processes, refer to our detailed [documentation](https://llukas22.github.io/llm-rs-python/).</longdescription>
</pkgmetadata>