<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># DTU HPCDTU HPC is a collection of scripts and tools for running jobs on the DTU HPC cluster.It should help you to get started with running jobs on the cluster, and to make your life easier.## InstallationTo install just run:```bashpip install dtuhpc```## Getting startedTo get started you first need to run:```bashdtuhpc auth```It will ask you for your username and password for DTU, and it will thenask you for an encryption password. This password is used to encrypt yourDTU password, so that it can be stored on your computer. You will need toremember this password, as it is used to decrypt your password when yourun commands.Afterwards, you should create a configuration file for your project. Thisshould be named `.dtuhpc.toml` and should be placed in the root of your project.You can use the following template:```toml[ssh]user = &quot;&lt;username&gt;&quot;host = &quot;&lt;login-host&gt;&quot;default_cwd = &quot;&lt;default working directory&gt;&quot;key_filename = &quot;&lt;path to ssh key&gt;&quot;[github]access_token = &quot;&lt;github access token&gt;&quot;[project]name = &quot;&lt;project name&gt;&quot;path = &quot;&lt;path to project on cluster&gt;&quot;default_deploy_branch = &quot;master&quot;```The `ssh` section is used to configure the ssh connection to the cluster.The GitHub access token can be generated from the following [page](https://github.com/settings/tokens).### Setup projectTo set up a project, you can run:```bashdtuhpc init [--poetry] [--custom-job=&lt;path to job script&gt;]```This will dispatch a job to the cluster, which will clone your project, create avirtual environment, and install the dependencies.You can choose to use either poetry, pip, or a custom job script. How to define jobswill be explained in the next section.### Writing jobsJobs are defined as toml files. It contains numerous options:```name = &quot;&lt;name of job&gt;&quot;queue = &quot;&lt;queue name&gt;&quot;single_host = &lt;true/false&gt;walltime = { hours = &lt;hours&gt;, minutes = &lt;minutes&gt; }standard_output = &quot;&lt;path to standard output file&gt;&quot;error_output = &quot;&lt;path to error output file&gt;&quot;memory = &lt;memory to allocate&gt;memory_kill_limit = &lt;memory kill limit&gt;cores = &lt;number of cores to allocate&gt;email = &quot;&lt;email address&gt;&quot;notification_start = &lt;true/false&gt;notification_end = &lt;true/false&gt;core_block_size = &lt;core block size&gt;core_p_tile_size = &lt;core p tile size&gt;use_gpu = { num_of_gpus = &lt;number of gpus&gt;, per_task = &lt;true/false&gt; }commands = [    &quot;&lt;bash command 1&gt;&quot;,    &quot;&lt;bash command 2&gt;&quot;,    ...]```An example of a script can be seen here:```tomlqueue = &quot;hpc&quot;name = &quot;init_${{ project_name }}&quot;walltime = { hours = 0, minutes = 15 }single_host = truecpu = 2memory = 4standard_output = &quot;init_${{ project_name }}.out&quot;error_output = &quot;init_${{ project_name }}.err&quot;commands = [    &quot;git clone ${{ git_url }} ${{ project_path }}&quot;,    &quot;module load python3/3.10.7&quot;,    &quot;cd ${{ project_path }}&quot;,    &quot;python3 -m venv ${{ project_path }}/venv&quot;,    &quot;source ${{ project_path }}/venv/bin/activate&quot;,    &quot;pip3 install 'poetry==1.3.2'&quot;,    &quot;poetry install&quot;,]```In this script, we can see that we can use variables in the script. These variablesare some default ones that are only available for the `init` job.### Deploying jobsTo deploy a job you just run the following command:```bashdtuhpc deploy &lt;job_path&gt;```It will then ask you to pick from branches or PR's. It will then dispatch the jobto the cluster.### Other commandsSome other commands:#### Exec commands on clusterTo execute commands on the cluster, you can run:```bashdtuhpc exec '&lt;command to run&gt;'```It will run in the default working directory, which is defined in the configuration file.#### SSH into clusterTo ssh into the cluster, you can run:```bashdtuhpc ssh```It will then open an ssh connection to the cluster. From here you can run commandsas you would normally.#### Predefined subcommandsThere are also some predefined subcommands, which are just wrappers around thecluster commands. They are all prefixed by `dtuhpc c &lt;command_name&gt;`. To get thefull documentation for the commands, you can run:```bashdtuhpc c &lt;command_name&gt; --help```##### bkillKill a job on the cluster.```bashdtuhpc c bkill &lt;job_id&gt;```##### bqueuesList all queues on the cluster.```bashdtuhpc c bqueues```##### bstatGet the status of a job on the cluster.```bashdtuhpc c bstat &lt;optional job_id&gt;```##### bsubSubmit a job to the cluster.```bashdtuhpc c bsub &lt;path to job script&gt;```##### nodestatGet the status of the nodes on the cluster.```bashdtuhpc c nodestat```##### showstartShow the start time of a job on the cluster.```bashdtuhpc c showstart &lt;job_id&gt;```</longdescription>
</pkgmetadata>