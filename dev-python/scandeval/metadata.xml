<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align='center'&gt;&lt;img src=&quot;https://raw.githubusercontent.com/saattrupdan/ScandEval/main/gfx/scandeval.png&quot; width=&quot;517&quot; height=&quot;217&quot;&gt;&lt;/div&gt;### Evaluation of pretrained language models on mono- or multilingual Scandinavian language tasks.______________________________________________________________________[![PyPI Status](https://badge.fury.io/py/scandeval.svg)](https://pypi.org/project/scandeval/)[![Documentation](https://img.shields.io/badge/docs-passing-green)](https://saattrupdan.github.io/ScandEval/scandeval.html)[![License](https://img.shields.io/github/license/saattrupdan/ScandEval)](https://github.com/saattrupdan/ScandEval/blob/main/LICENSE)[![LastCommit](https://img.shields.io/github/last-commit/saattrupdan/ScandEval)](https://github.com/saattrupdan/ScandEval/commits/main)[![Code Coverage](https://img.shields.io/badge/Coverage-73%25-yellow.svg)](https://github.com/saattrupdan/ScandEval/tree/main/tests)[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg)](https://github.com/saattrupdan/ScandEval/blob/main/CODE_OF_CONDUCT.md)## InstallationTo install the package simply write the following command in your favorite terminal:```$ pip install scandeval```## Quickstart### Benchmarking from the Command LineThe easiest way to benchmark pretrained models is via the command line interface. Afterhaving installed the package, you can benchmark your favorite model like so:```$ scandeval --model-id &lt;model-id&gt;```Here `model_id` is the HuggingFace model ID, which can be found on the [HuggingFaceHub](https://huggingface.co/models). By default this will benchmark the model on allthe datasets eligible. If you want to benchmark on a specific dataset, this can be donevia the `--dataset` flag. This will for instance evaluate the model on the`AngryTweets` dataset:```$ scandeval --model-id &lt;model-id&gt; --dataset angry-tweets```We can also separate by language. To benchmark all Danish models on all Danishdatasets, say, this can be done using the `language` tag, like so:```$ scandeval --language da```Multiple models, datasets and/or languages can be specified by just attaching multiplearguments. Here is an example with two models:```$ scandeval --model-id &lt;model-id1&gt; --model-id &lt;model-id2&gt; --dataset angry-tweets```The specific model version to use can also be added after the suffix '@':```$ scandeval --model-id &lt;model-id&gt;@&lt;commit&gt;```It can be a branch name, a tag name, or a commit id. It defaults to 'main' for latest.See all the arguments and options available for the `scandeval` command by typing```$ scandeval --help```### Benchmarking from a ScriptIn a script, the syntax is similar to the command line interface. You simply initialisean object of the `Benchmarker` class, and call this benchmark object with your favoritemodels and/or datasets:```&gt;&gt;&gt; from scandeval import Benchmarker&gt;&gt;&gt; benchmark = Benchmarker()&gt;&gt;&gt; benchmark('&lt;model-id&gt;')```To benchmark on a specific dataset, you simply specify the second argument, shown herewith the `AngryTweets` dataset again:```&gt;&gt;&gt; benchmark('&lt;model_id&gt;', 'angry-tweets')```If you want to benchmark a subset of all the models on the Hugging Face Hub, you canspecify several parameters in the `Benchmarker` initializer to narrow down the list ofmodels to the ones you care about. As a simple example, the following would benchmarkall the Nynorsk models on Nynorsk datasets:```&gt;&gt;&gt; benchmark = Benchmarker(language='nn')&gt;&gt;&gt; benchmark()```## DocumentationSee the full documentation [here](https://saattrupdan.github.io/ScandEval/scandeval.html).## Citing ScandEvalIf you want to cite the framework then feel free to use this:```@article{nielsen2022scandeval,  title={ScandEval: Evaluation of language models on mono- or multilingual Scandinavian language tasks.},  author={Nielsen, Dan Saattrup},  journal={GitHub. Note: https://github.com/saattrupdan/ScandEval},  year={2022}}```## RemarksThe image used in the logo has been created by the amazing [Scandinavia and theWorld](https://satwcomic.com/) team. Go check them out!## Project structure```.├── .flake8├── .github│   └── workflows│       ├── ci.yaml│       └── docs.yaml├── .gitignore├── .pre-commit-config.yaml├── CHANGELOG.md├── LICENSE├── README.md├── gfx│   └── scandeval.png├── makefile├── notebooks├── poetry.toml├── pyproject.toml├── src│   ├── scandeval│   │   ├── __init__.py│   │   ├── benchmark_config_factory.py│   │   ├── benchmark_dataset.py│   │   ├── benchmarker.py│   │   ├── callbacks.py│   │   ├── cli.py│   │   ├── config.py│   │   ├── dataset_configs.py│   │   ├── dataset_factory.py│   │   ├── dataset_tasks.py│   │   ├── exceptions.py│   │   ├── hf_hub.py│   │   ├── languages.py│   │   ├── model_loading.py│   │   ├── named_entity_recognition.py│   │   ├── question_answering.py│   │   ├── question_answering_trainer.py│   │   ├── scores.py│   │   ├── sequence_classification.py│   │   ├── speed_benchmark.py│   │   ├── types.py│   │   └── utils.py│   └── scripts│       ├── create_angry_tweets.py│       ├── create_dane.py│       ├── create_mim_gold_ner.py│       ├── create_norec.py│       ├── create_norne.py│       ├── create_scala.py│       ├── create_scandiqa.py│       ├── create_suc3.py│       ├── create_swerec.py│       ├── create_wikiann_fo.py│       ├── fill_in_missing_model_metadata.py│       ├── fix_dot_env_file.py│       ├── load_ud_pos.py│       └── versioning.py└── tests    ├── __init__.py    ├── conftest.py    ├── test_benchmark_config_factory.py    ├── test_benchmark_dataset.py    ├── test_benchmarker.py    ├── test_callbacks.py    ├── test_cli.py    ├── test_config.py    ├── test_dataset_configs.py    ├── test_dataset_factory.py    ├── test_dataset_tasks.py    ├── test_exceptions.py    ├── test_hf_hub.py    ├── test_languages.py    ├── test_model_loading.py    ├── test_named_entity_recognition.py    ├── test_question_answering.py    ├── test_question_answering_trainer.py    ├── test_scores.py    ├── test_sequence_classification.py    ├── test_speed_benchmark.py    ├── test_types.py    └── test_utils.py```</longdescription>
</pkgmetadata>