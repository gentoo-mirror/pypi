<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![ci_status](https://github.com/landing-ai/landingai-python/actions/workflows/ci_cd.yml/badge.svg)[![PyPI version](https://badge.fury.io/py/landingai.svg?)](https://badge.fury.io/py/landingai)![version](https://img.shields.io/pypi/pyversions/landingai)![license](https://img.shields.io/github/license/landing-ai/landingai-python)[![downloads](https://static.pepy.tech/badge/landingai/month)](https://pepy.tech/project/landingai)&lt;br&gt;&lt;p align=&quot;center&quot;&gt;  &lt;img width=&quot;100&quot; height=&quot;100&quot; src=&quot;https://github.com/landing-ai/landingai-python/raw/main/assets/avi-logo.png&quot;&gt;&lt;/p&gt;# LandingLens Python SDKThe LandingLens Python SDK contains the LandingLens development library and examples that show how to integrate your app with LandingLens in a variety of scenarios. The examples cover different model types, image acquisition sources, and post-procesing techniques. We've provided some examples in Jupyter Notebooks to focus on ease of use, and some examples in Python apps to provide a more robust and complete experience.&lt;!-- Generated using https://www.tablesgenerator.com/markdown_tables --&gt;| Example | Description | Type ||---|---|---|| [Poker Card Suit Identification](https://github.com/landing-ai/landingai-python/blob/main/examples/webcam-collab-notebook/webcam-collab-notebook.ipynb) | This notebook shows how to use an object detection model from LandingLens to detect suits on playing cards. A webcam is used to take photos of playing cards. | Jupyter Notebook [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/landing-ai/landingai-python/blob/main/examples/webcam-collab-notebook/webcam-collab-notebook.ipynb)|| [Door Monitoring for Home Automation](https://github.com/landing-ai/landingai-python/blob/main/examples/rtsp-capture-notebook/rtsp-capture.ipynb) | This notebook shows how to use an object detection model from LandingLens to detect whether a door is open or closed. An RTSP camera is used to acquire images. | Jupyter Notebook [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/landing-ai/landingai-python/blob/main/examples/rtsp-capture-notebook/rtsp-capture.ipynb) || [Satellite Images and Post-Processing](https://github.com/landing-ai/landingai-python/tree/main/examples/post-processings/farmland-coverage/farmland-coverage.ipynb) | This notebook shows how to use a Visual Prompting model from LandingLens to identify different objects in satellite images. The notebook includes post-processing scripts that calculate the percentage of ground cover that each object takes up. | Jupyter Notebook [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/landing-ai/landingai-python/blob/main/examples/post-processings/farmland-coverage/farmland-coverage.ipynb) || [License Plate Detection and Recognition](https://github.com/landing-ai/landingai-python/tree/main/examples/license-plate-ocr-notebook/license_plate_ocr.ipynb) | This notebook shows how to extract frames from a video file and use a object detection model and OCR from LandingLens to identify and recognize different license plates. | Jupyter Notebook [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/landing-ai/landingai-python/blob/main/examples/license-plate-ocr-notebook/license_plate_ocr.ipynb) || [Streaming Video](https://github.com/landing-ai/landingai-python/tree/main/examples/capture-service) | This application shows how to continuously run inference on images extracted from a streaming RTSP video camera feed. | Python application |## Documentation-  [Landing AI Python Library Quick Start Guide](https://landing-ai.github.io/landingai-python/)-  [Landing AI Python Library API Reference](https://landing-ai.github.io/landingai-python/api/common/)-  [Landing AI Python Library Changelog](https://landing-ai.github.io/landingai-python/changelog/)-  [Landing AI Support Center](https://support.landing.ai/)-  [LandingLens Walk-Through Video](https://www.youtube.com/watch?v=779kvo2dxb4)## Install the Library```bashpip install landingai```## Quick Start### PrerequisitesThis library needs to communicate with the LandingLens platform to perform certain functions. For example, the `Predictor` API calls the HTTP endpoint of your deployed model. To enable communication with LandingLens, you will need the following information:1. The **Endpoint ID** of your deployed model in LandingLens. You can find this on the Deploy page in LandingLens.2. The **API Key** for the LandingLens organization that has the model you want to deploy. To learn how to generate these credentials, go [here](https://support.landing.ai/docs/api-key-and-api-secret).### Run InferenceRun inference using the endpoint you created in LandingLens:1. Install the Python library.2. Create a `Predictor` class with your Endpoint ID and API Key.3. Load your image into a PIL Image (below the image is &quot;image.png&quot;)4. Call the `predict()` function with an Image.```pythonfrom PIL import Imagefrom landingai.predict import Predictor# Enter your API Key and Secretendpoint_id = &quot;FILL_YOUR_INFERENCE_ENDPOINT_ID&quot;api_key = &quot;FILL_YOUR_API_KEY&quot;# Load your imageimage = Image.open(&quot;image.png&quot;)# Run inferencepredictor = Predictor(endpoint_id, api_key=api_key)predictions = predictor.predict(image)```See an end to end **working example** [here](https://colab.research.google.com/github/landing-ai/landingai-python/blob/main/examples/webcam-collab-notebook/webcam-collab-notebook.ipynb).### Visualize and Save PredictionsVisualize your inference results by overlaying the predictions on the input image and saving the updated image:```pythonfrom landingai.visualize import overlay_predictions# continue the above examplepredictions = predictor.predict(image)image_with_preds = overlay_predictions(predictions, image)image_with_preds.save(&quot;image.jpg&quot;)```### Create a Vision PipelineAll the modules shown above and others can be chained together using the `landingai.pipeline` abstraction. At its core, a pipeline is a sequence of chained calls that operate on a `landingai.pipeline.Frame`.The following example shows how the previous sections come together on a pipeline. For more details, go to the [*Vision Pipelines User Guide*](https://landing-ai.github.io/landingai-python/landingai.html#vision-pipelines) ```pythonfrom landingai.predict import Predictorimport landingai.pipeline as plcloud_sky_model = Predictor(&quot;FILL_YOUR_INFERENCE_ENDPOINT_ID&quot;                            , api_key=&quot;FILL_YOUR_API_KEY&quot;) Camera = pl.image_source.NetworkedCamera(stream_url)for frame in Camera:    (        frame.downsize(width=1024)        .run_predict(predictor=cloud_sky_model)        .overlay_predictions()        .show_image()        .save_image(filename_prefix=&quot;./capture&quot;)    )    ```## Run Examples LocallyAll the examples in this repo can be run locally.To give you some guidance, here's how you can run the [`rtsp-capture`](https://github.com/landing-ai/landingai-python/tree/main/examples/capture-service) example locally in a shell environment:1. Clone the repo to local: `git clone https://github.com/landing-ai/landingai-python.git`2. Install the library: `poetry install --with examples` (See the [poetry docs](https://python-poetry.org/docs/#installation) for how to install `poetry`)3. Activate the virtual environment: `poetry shell`4. Run: `python landingai-python/examples/capture-service/run.py`</longdescription>
</pkgmetadata>