<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># BlueCast[![codecov](https://codecov.io/gh/ThomasMeissnerDS/BlueCast/branch/main/graph/badge.svg?token=XRIS04O097)](https://codecov.io/gh/ThomasMeissnerDS/BlueCast)[![Codecov workflow](https://github.com/ThomasMeissnerDS/BlueCast/actions/workflows/workflow.yaml/badge.svg)](https://github.com/ThomasMeissnerDS/BlueCast/actions/workflows/workflow.yaml)[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white)](https://github.com/pre-commit/pre-commit)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)[![pydocstyle](https://img.shields.io/badge/pydocstyle-enabled-AD4CD3)](http://www.pydocstyle.org/en/stable/)[![Documentation Status](https://readthedocs.org/projects/bluecast/badge/?version=latest)](https://bluecast.readthedocs.io/en/latest/?badge=latest)[![PyPI version](https://badge.fury.io/py/bluecast.svg)](https://pypi.python.org/pypi/bluecast/)[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)[![python](https://img.shields.io/badge/Python-3.9-3776AB.svg?style=flat&amp;logo=python&amp;logoColor=white)](https://www.python.org)[![python](https://img.shields.io/badge/Python-3.10-3776AB.svg?style=flat&amp;logo=python&amp;logoColor=white)](https://www.python.org)[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)A lightweight and fast auto-ml library. This is the successor of thee2eml automl library. While e2eml tried to cover many modelarchitectures and a lot of different preprocessing options,BlueCast focuses on a few model architectures (on default Xgboostonly) and a few preprocessing options (only what isneeded for Xgboost). This allows for a much faster developmentcycle and a much more stable codebase while also having as few dependenciesas possible for the library. Despite being lightweight in its core BlueCastoffers high customization options for advanced users. Findthe full documentation [here](https://bluecast.readthedocs.io/en/latest/).&lt;!-- toc --&gt;* [Installation](#installation)  * [Installation for end users](#installation-for-end-users)  * [Installation for developers](#installation-for-developers)* [General usage](#general-usage)  * [Basic usage](#basic-usage)  * [Advanced usage](#advanced-usage)    * [Explanatory analysis](#explanatory-analysis)    * [Enable cross-validation](#enable-cross-validation)    * [Categorical encoding](#categorical-encoding)    * [Custom preprocessing](#custom-preprocessing)    * [Custom feature selection](#custom-feature-selection)    * [Custom ML model](#custom-ml-model)* [Convenience features](#convenience-features)* [Code quality](#code-quality)* [Documentation](#documentation)* [How to contribute](#how-to-contribute)* [Meta](#meta)&lt;!-- tocstop --&gt;## Installation### Installation for end usersFrom PyPI:```shpip install bluecast```Using a fresh environment with Python 3.9 or higher is recommended. We consciouslydo not support Python 3.8 or lower to prevent the usage of outdated Python versionsand issues connected to it.### Installation for developers* Clone the repository:* Create a new conda environment with Python 3.9 or higher* run `pip install poetry` to install poetry as dependency manager* run `poetry install` to install all dependencies## General usage### Basic usageThe module blueprints contains the main functionality of the library. The mainentry point is the `Blueprint` class. It already includes needed preprocessing(including some convenience functionality like feature type detection)and model hyperparameter tuning.```shfrom bluecast.blueprints.cast import BlueCastautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```### Advanced usage#### Explanatory analysisBlueCast offers a simple way to get a first overview of the data:```shfrom bluecast.eda.analyse import (    bi_variate_plots,    correlation_heatmap,    correlation_to_target,    univariate_plots,)from bluecast.preprocessing.feature_types import FeatureTypeDetector# Here we automatically detect the numeric columnsfeat_type_detector = FeatureTypeDetector()train_data = feat_type_detector.fit_transform_feature_types(train_data)# show univariate plotsunivariate_plots(        train_data.loc[            :, feat_type_detector.num_columns  # here the target column EC1 is already included        ],        &quot;EC1&quot;,    )# show bi-variate plotsbi_variate_plots(train_data.loc[            :, feat_type_detector.num_columns        ],        &quot;EC1&quot;)# show correlation heatmapcorrelation_heatmap(train_data.loc[            :, feat_type_detector.num_columns])# show correlation to targetcorrelation_to_target(train_data.loc[            :, feat_type_detector.num_columns        ],        &quot;EC1&quot;,)```#### Enable cross-validationWhile the default behaviour of BlueCast is to use a simpletrain-test-split, cross-validation can be enabled easily:```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.config.training_config import TrainingConfig, XgboostTuneParamsConfig# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hypertuning_cv_folds = 5 # default is 1# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```#### Categorical encodingBy default, BlueCast uses target encoding.This behaviour can be changed in the TrainingConfig by setting `cat_encoding_via_ml_algorithm`to True. This will change the expectations of `custom_last_mile_computation` though.If `cat_encoding_via_ml_algorithm` is set to False, `custom_last_mile_computation`will receive numerical features only as target encoding will apply before. If `cat_encoding_via_ml_algorithm`is True (default setting) `custom_last_mile_computation` will receive categoricalfeatures as well, because Xgboost#s inbuilt categorical encoding will be used.```sh#### Custom  training configurationDespite e2eml, BlueCast allows easy customization. Users can adjust theconfiguration and just pass it to the `BlueCast` class. Here is an example:```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.config.training_config import TrainingConfig, XgboostTuneParamsConfig# Create a custom tuning config and adjust hyperparameter search spacexgboost_param_config = XgboostTuneParamsConfig()xgboost_param_config.steps_max = 100xgboost_param_config.num_leaves_max = 16# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hyperparameter_tuning_rounds = 10train_config.autotune_model = False # we want to run just normal training, no hyperparameter tuning# We could even just overwrite the final Xgboost params using the XgboostFinalParamConfig class# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,        conf_xgboost=xgboost_param_config,    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```#### Custom preprocessingThe `BlueCast` class also allows for custom preprocessing. This is done byan abstract class that can be inherited and passed into the `BlueCast` class.BlueCast provides two entry points to inject custom preprocessing. Theattribute `custom_preprocessor` is called right after the train_test_split.The attribute `custom_last_mile_computation` will be called before the modeltraining or prediction starts (when only numerical features are present anymore)and allows users to execute last computations (i.e. sub sampling or final calculations).```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.preprocessing.custom import CustomPreprocessing# Create a custom tuning config and adjust hyperparameter search spacexgboost_param_config = XgboostTuneParamsConfig()xgboost_param_config.steps_max = 100xgboost_param_config.num_leaves_max = 16# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hyperparameter_tuning_rounds = 10train_config.autotune_model = False # we want to run just normal training, no hyperparameter tuning# We could even just overwrite the final Xgboost params using the XgboostFinalParamConfig classclass MyCustomPreprocessing(CustomPreprocessing):    def __init__(self):        self.trained_patterns = {}    def fit_transform(        self, df: pd.DataFrame, target: pd.Series    ) -&gt; Tuple[pd.DataFrame, pd.Series]:        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        cat_df = df[['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ']].copy()        zscores = Zscores()        zscores.fit_all(df, ['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'])        df = zscores.transform_all(df, ['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'])        self.trained_patterns[&quot;zscores&quot;] = zscores        imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        imp_mean.fit(df.loc[:, num_columns])        df = imp_mean.transform(df.loc[:, num_columns])        self.trained_patterns[&quot;imputation&quot;] = imp_mean        df = pd.DataFrame(df, columns=num_columns).merge(cat_df, left_index=True, right_index=True, how=&quot;left&quot;)        df = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha'], axis=1)        return df, target    def transform(        self,        df: pd.DataFrame,        target: Optional[pd.Series] = None,        predicton_mode: bool = False,    ) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        cat_df = df[['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ']].copy()        df = self.trained_patterns[&quot;zscores&quot;].transform_all(df, ['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'])        imp_mean = self.trained_patterns[&quot;imputation&quot;]        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        df.loc[:, num_columns] = df.loc[:, num_columns].replace([np.inf, -np.inf], np.nan)        df = imp_mean.transform(df.loc[:, num_columns])        df = pd.DataFrame(df, columns=num_columns).merge(cat_df, left_index=True, right_index=True, how=&quot;left&quot;)        df = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha'], axis=1)        return df, target# add custom last mile computationclass MyCustomLastMilePreprocessing(CustomPreprocessing):    def custom_function(self, df: pd.DataFrame) -&gt; pd.DataFrame:        df = df / 2        df[&quot;custom_col&quot;] = 5        return df    # Please note: The base class enforces that the fit_transform method is implemented    def fit_transform(        self, df: pd.DataFrame, target: pd.Series    ) -&gt; Tuple[pd.DataFrame, pd.Series]:        df = self.custom_function(df)        df = df.head(1000)        target = target.head(1000)        return df, target    # Please note: The base class enforces that the fit_transform method is implemented    def transform(        self,        df: pd.DataFrame,        target: Optional[pd.Series] = None,        predicton_mode: bool = False,    ) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        df = self.custom_function(df)        if not predicton_mode and isinstance(target, pd.Series):            df = df.head(100)            target = target.head(100)        return df, targecustom_last_mile_computation = MyCustomLastMilePreprocessing()custom_preprocessor = MyCustomPreprocessing()# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,        conf_xgboost=xgboost_param_config,        custom_preprocessor=custom_preprocessor, # this takes place right after test_train_split        custom_last_mile_computation=custom_last_mile_computation, # last step before model training/prediction    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```#### Custom feature selectionBlueCast offers automated feature selection. On default the featureselection is disabled, but BlueCast raises a warning to inform theuser about this option. The behaviour can be controlled via the`TrainingConfig`.```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.preprocessing.custom import CustomPreprocessingfrom bluecast.config.training_config import TrainingConfig# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hyperparameter_tuning_rounds = 10train_config.autotune_model = False # we want to run just normal training, no hyperparameter tuningtrain_config.enable_feature_selection = True# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```Also this step can be customized. The following example shows how to:```shfrom bluecast.config.training_config import FeatureSelectionConfigfrom bluecast.config.training_config import TrainingConfigfrom bluecast.preprocessing.custom import CustomPreprocessingfrom sklearn.feature_selection import RFECVfrom sklearn.metrics import make_scorer, matthews_corrcoeffrom sklearn.model_selection import StratifiedKFoldfrom typing import Optional, Tuple# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.enable_feature_selection = True# add custom feature selectionclass RFECVSelector(CustomPreprocessing):    def __init__(self, random_state: int = 0):        super().__init__()        self.selected_features = None        self.random_state = random_state        self.selection_strategy: RFECV = RFECV(            estimator=xgb.XGBClassifier(),            step=1,            cv=StratifiedKFold(5, random_state=random_state, shuffle=True),            min_features_to_select=1,            scoring=make_scorer(matthews_corrcoef),            n_jobs=2,        )    def fit_transform(self, df: pd.DataFrame, target: pd.Series) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        self.selection_strategy.fit(df, target)        self.selected_features = self.selection_strategy.support_        df = df.loc[:, self.selected_features]        return df, target    def transform(self,                  df: pd.DataFrame,                  target: Optional[pd.Series] = None,                  predicton_mode: bool = False) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        df = df.loc[:, self.selected_features]        return df, targetcustom_feature_selector = RFECVSelector()# Create an instance of the BlueCast class with the custom modelbluecast = BlueCast(    class_problem=&quot;binary&quot;,    target_column=&quot;target&quot;,    conf_feature_selection=custom_feat_sel,    conf_training=train_config,    custom_feature_selector=custom_feature_selector,# Create some sample data for testingx_train = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]})y_train = pd.Series([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])x_test = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]}x_train[&quot;target&quot;] = y_trai# Fit the BlueCast model using the custom modelbluecast.fit(x_train, &quot;target&quot;# Predict on the test data using the custom modelpredicted_probas, predicted_classes = bluecast.predict(x_test)```#### Custom ML modelFor some users it might just be convenient to use the BlueCast class toenjoy convenience features (details see below), but use a custom ML model.This is possible by passing a custom model to the BlueCast class. The needed propertiesare defined via the BaseClassMlModel class. Here is an example:```shfrom bluecast.ml_modelling.base_classes import (    BaseClassMlModel,    PredictedClasses,  # just for linting checks    PredictedProbas,  # just for linting checks)class CustomModel(BaseClassMlModel):    def __init__(self):        self.model = None    def fit(        self,        x_train: pd.DataFrame,        x_test: pd.DataFrame,        y_train: pd.Series,        y_test: pd.Series,    ) -&gt; None:        self.model = LogisticRegression()        self.model.fit(x_train, y_train)    def predict(self, df: pd.DataFrame) -&gt; Tuple[PredictedProbas, PredictedClasses]:        predicted_probas = self.model.predict_proba(df)        predicted_classes = self.model.predict(df)        return predicted_probas, predicted_classescustom_model = CustomModel()# Create an instance of the BlueCast class with the custom modelbluecast = BlueCast(    class_problem=&quot;binary&quot;,    target_column=&quot;target&quot;,    ml_model=custom_model,# Create some sample data for testingx_train = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]})y_train = pd.Series([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])x_test = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]}x_train[&quot;target&quot;] = y_trai# Fit the BlueCast model using the custom modelbluecast.fit(x_train, &quot;target&quot;# Predict on the test data using the custom modelpredicted_probas, predicted_classes = bluecast.predict(x_test)```Please note that custom ML models require user defined hyperparameter tuning. Pre-definedconfigurations are not available for custom models.## Convenience featuresDespite being a lightweight library, BlueCast also includes some conveniencewith the following features:* automatic feature type detection and casting* automatic DataFrame schema detection: checks if unseen data has new or  missing columns* categorical feature encoding (target encoding or directly in Xgboost)* datetime feature encoding* automated GPU availability check and usage for Xgboost  a fit_eval method to fit a model and evaluate it on a validation set  to mimic production environment reality* functions to save and load a trained pipeline* shapley values* warnings for potential misconfigurationsThe fit_eval method can be used like this:```shfrom bluecast.blueprints.cast import BlueCastautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;    )automl.fit_eval(df_train, df_eval, y_eval, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```It is important to note that df_train contains the target column whiledf_eval does not. The target column is passed separately as y_eval.## Code qualityTo ensure code quality, we use the following tools:* various pre-commit libraries* strong type hinting in the code base* unit tests using PytestFor contributors, it is expected that all pre-commit and unit tests pass.For new features it is expected that unit tests are added.## DocumentationDocumentation is provided via [Read the Docs](https://bluecast.readthedocs.io/en/latest/)## How to contributeContributions are welcome. Please follow the following steps:* Create a new branch from develop branch* Add your feature or fix* Add unit tests for new features* Run pre-commit checks and unit tests (using Pytest)* Adjust the `docs/source/index.md` file* Copy paste the content of the `docs/source/index.md` file into the  `README.md` file* Push your changes and create a pull requestIf library or dev dependencies have to be changed, adjust the pyproject.toml.For readthedocs it is also requited to update the`docs/srtd_requirements.txt` file. Simply run:```shpoetry export --with dev -f requirements.txt --output docs/rtd_requirements.txt```If readthedocs will be able to create the documentation can be tested via:```shpoetry run sphinx-autobuild docs/source docs/build/html```This will show a localhost link containing the documentation.## MetaCreator: Thomas Meißner – [LinkedIn](https://www.linkedin.com/in/thomas-mei%C3%9Fner-m-a-3808b346)</longdescription>
</pkgmetadata>