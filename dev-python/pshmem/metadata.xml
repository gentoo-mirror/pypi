<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># MPI design patterns with shared memoryThis is a small package that implements parallel design patterns using MPI one-sided andshared memory constructs.## Installation and RequirementsThis package needs a recent version of the `mpi4py` package in order to be useful.However, the classes also accept a value of `None` for the communicator, in which case atrivial local implementation is used.  The code uses other widely available packages(like numpy) and requires a recent Python3 installation.  You can install the code froma git checkout with:    pip install .Or:    python3 setup.py installOr directly from github.## MPIShared ClassThis class implements a pattern where a shared array is allocated on each node.Processes can update pieces of the shared array with the synchronous &quot;set()&quot; method.During this call, the data from the desired process is first replicated to all nodes,and then one process on each node copies that piece into the shared array.All processes on all nodes can freely read data from the node-local copy of the sharedarray.### ExampleYou can use `MPIShared` as a context manager or by explicitly creating and freeingmemory.  Here is an example of creating a shared memory object that is replicated acrossnodes:```pythonimport numpy as npfrom mpi4py import MPIfrom pshmem import MPISharedcomm = MPI.COMM_WORLDwith MPIShared((3, 5), np.float64, comm) as shm:    # A copy of the data exists on every node and is initialized to zero.    # There is a numpy array &quot;view&quot; of that memory available with slice notation    # or by accessing the &quot;data&quot; member:    if comm.rank == 0:        # You can get a summary of the data by printing it:        print(&quot;String representation:\n&quot;)        print(shm)        print(&quot;\n===== Initialized Data =====&quot;)    for p in range(comm.size):        if p == comm.rank:            print(&quot;rank {}:\n&quot;.format(p), shm.data, flush=True)        comm.barrier()    set_data = None    set_offset = None    if comm.rank == 0:        set_data = np.arange(6, dtype=np.float64).reshape((2, 3))        set_offset = (1, 1)    # The set() method is collective, but the inputs only matter on one rank    shm.set(set_data, offset=set_offset, fromrank=0)    # You can also use the usual '[]' notation.  However, this call must do an    # additional pre-communication to detect which process the data is coming from.    # And this line is still collective and must be called on all processes:    shm[set_offset] = set_data    # This updated data has now been replicated to the shared memory on all nodes.    if comm.rank == 0:        print(&quot;======= Updated Data =======&quot;)    for p in range(comm.size):        if p == comm.rank:            print(&quot;rank {}:\n&quot;.format(p), shm.data, flush=True)        comm.barrier()    # You can read from the node-local copy of the data from all processes,    # using either the &quot;data&quot; member or slice access:    if comm.rank == comm.size - 1:        print(&quot;==== Read-only access ======&quot;)        print(&quot;rank {}: shm[2, 3] = {}&quot;.format(comm.rank, shm[2, 3]), flush=True)        print(&quot;rank {}: shm.data = \n{}&quot;.format(comm.rank, shm.data), flush=True)```Putting the above code into a file `test.py` and running this on 4 processes gives:```mpirun -np 4 python3 test.pyString representation:&lt;MPIShared  replicated on 1 nodes, each with 4 processes (4 total)  shape = (3, 5), dtype = float64  [ [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] ]&gt;===== Initialized Data =====rank 0: [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]rank 1: [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]rank 2: [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]rank 3: [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]======= Updated Data =======rank 0: [[0. 0. 0. 0. 0.] [0. 0. 1. 2. 0.] [0. 3. 4. 5. 0.]]rank 1: [[0. 0. 0. 0. 0.] [0. 0. 1. 2. 0.] [0. 3. 4. 5. 0.]]rank 2: [[0. 0. 0. 0. 0.] [0. 0. 1. 2. 0.] [0. 3. 4. 5. 0.]]rank 3: [[0. 0. 0. 0. 0.] [0. 0. 1. 2. 0.] [0. 3. 4. 5. 0.]]==== Read-only access ======rank 3: shm[2, 3] = 5.0rank 3: shm.data =[[0. 0. 0. 0. 0.] [0. 0. 1. 2. 0.] [0. 3. 4. 5. 0.]] ```Note that if you are not using a context manager, then you should be careful to closeand delete the object like this:```pythonshm = MPIShared((3, 5), np.float64, comm=comm)# Do stuffshm.close()del shm```## MPILock ClassThis implements a MUTEX lock across an arbitrary communicator.  A memory buffer on asingle process acts as a waiting list where processes can add themselves (usingone-sided calls).  The processes pass a token to transfer ownership of the lock.  Thetoken is passed in order of request.### ExampleA typical use case is where we want to serialize some operation across a large number ofprocesses that reside on different nodes.  For example, perhaps we are making requeststo the external network from a computing center and we do not want to saturate that withall processes simultaneously.  Or perhaps we are writing to a shared data file whichdoes not support parallel writes and we have a sub-communicator of writing processeswhich take turns updating the filesystem.  We can instantiate a lock on anycommunicator, so it is possible to split the world communicator into groups and havesome operation serialized just within that group:```pythonwith MPILock(MPI.COMM_WORLD) as mpilock:    mpilock.lock()    # Do something here.  Only one process at a time will do this.    mpilock.unlock()```## TestsAfter installation, you can run some tests with:    mpirun -np 4 python3 -c 'import pshmem.test; pshmem.test.run()'If you have mpi4py available but would like to explicitly disable the use of MPI in thetests, you can set an environment variable:    MPI_DISABLE=1 python3 -c 'import pshmem.test; pshmem.test.run()'</longdescription>
</pkgmetadata>