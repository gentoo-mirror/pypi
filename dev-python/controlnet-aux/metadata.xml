<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># ControlNet auxiliary modelsThis is a PyPi installable package of [lllyasviel's ControlNet Annotators](https://github.com/lllyasviel/ControlNet/tree/main/annotator)The code is copy-pasted from the respective folders in https://github.com/lllyasviel/ControlNet/tree/main/annotator and connected to [the ðŸ¤— Hub](https://huggingface.co/lllyasviel/Annotators).All credit &amp; copyright goes to https://github.com/lllyasviel .## Install```pip install controlnet-aux==0.0.6```To support DWPose which is dependent on MMDetection, MMCV and MMPose```pip install -U openmimmim install mmenginemim install &quot;mmcv&gt;=2.0.1&quot;mim install &quot;mmdet&gt;=3.1.0&quot;mim install &quot;mmpose&gt;=1.1.0&quot;```## UsageYou can use the processor class, which can load each of the auxiliary models with the following code```pythonimport requestsfrom PIL import Imagefrom io import BytesIOfrom controlnet_aux.processor import Processor# load imageurl = &quot;https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png&quot;response = requests.get(url)img = Image.open(BytesIO(response.content)).convert(&quot;RGB&quot;).resize((512, 512))# load processor from processor_id# options are:# [&quot;canny&quot;, &quot;depth_leres&quot;, &quot;depth_leres++&quot;, &quot;depth_midas&quot;, &quot;depth_zoe&quot;, &quot;lineart_anime&quot;,#  &quot;lineart_coarse&quot;, &quot;lineart_realistic&quot;, &quot;mediapipe_face&quot;, &quot;mlsd&quot;, &quot;normal_bae&quot;, &quot;normal_midas&quot;,#  &quot;openpose&quot;, &quot;openpose_face&quot;, &quot;openpose_faceonly&quot;, &quot;openpose_full&quot;, &quot;openpose_hand&quot;,#  &quot;scribble_hed, &quot;scribble_pidinet&quot;, &quot;shuffle&quot;, &quot;softedge_hed&quot;, &quot;softedge_hedsafe&quot;,#  &quot;softedge_pidinet&quot;, &quot;softedge_pidsafe&quot;, &quot;dwpose&quot;]processor_id = 'scribble_hed'processor = Processor(processor_id)processed_image = processor(img, to_pil=True)```Each model can be loaded individually by importing and instantiating them as follows```pythonfrom PIL import Imageimport requestsfrom io import BytesIOfrom controlnet_aux import HEDdetector, MidasDetector, MLSDdetector, OpenposeDetector, PidiNetDetector, NormalBaeDetector, LineartDetector, LineartAnimeDetector, CannyDetector, ContentShuffleDetector, ZoeDetector, MediapipeFaceDetector, SamDetector, LeresDetector, DWposeDetector# load imageurl = &quot;https://huggingface.co/lllyasviel/sd-controlnet-openpose/resolve/main/images/pose.png&quot;response = requests.get(url)img = Image.open(BytesIO(response.content)).convert(&quot;RGB&quot;).resize((512, 512))# load checkpointshed = HEDdetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)midas = MidasDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)mlsd = MLSDdetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)open_pose = OpenposeDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)pidi = PidiNetDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)normal_bae = NormalBaeDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)lineart = LineartDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)lineart_anime = LineartAnimeDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)zoe = ZoeDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)sam = SamDetector.from_pretrained(&quot;ybelkada/segment-anything&quot;, subfolder=&quot;checkpoints&quot;)mobile_sam = SamDetector.from_pretrained(&quot;dhkim2810/MobileSAM&quot;, model_type=&quot;vit_t&quot;, filename=&quot;mobile_sam.pt&quot;)leres = LeresDetector.from_pretrained(&quot;lllyasviel/Annotators&quot;)# specify configs, ckpts and device, or it will be downloaded automatically and use cpu by default# det_config: ./src/controlnet_aux/dwpose/yolox_config/yolox_l_8xb8-300e_coco.py# det_ckpt: https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_l_8x8_300e_coco/yolox_l_8x8_300e_coco_20211126_140236-d3bd2b23.pth# pose_config: ./src/controlnet_aux/dwpose/dwpose_config/dwpose-l_384x288.py# pose_ckpt: https://huggingface.co/wanghaofan/dw-ll_ucoco_384/resolve/main/dw-ll_ucoco_384.pthimport torchdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')dwpose = DWposeDetector(det_config=det_config, det_ckpt=det_ckpt, pose_config=pose_config, pose_ckpt=pose_ckpt, device=device)# instantiatecanny = CannyDetector()content = ContentShuffleDetector()face_detector = MediapipeFaceDetector()# processprocessed_image_hed = hed(img)processed_image_midas = midas(img)processed_image_mlsd = mlsd(img)processed_image_open_pose = open_pose(img, hand_and_face=True)processed_image_pidi = pidi(img, safe=True)processed_image_normal_bae = normal_bae(img)processed_image_lineart = lineart(img, coarse=True)processed_image_lineart_anime = lineart_anime(img)processed_image_zoe = zoe(img)processed_image_sam = sam(img)processed_image_leres = leres(img)processed_image_canny = canny(img)processed_image_content = content(img)processed_image_mediapipe_face = face_detector(img)processed_image_dwpose = dwpose(img)```</longdescription>
</pkgmetadata>