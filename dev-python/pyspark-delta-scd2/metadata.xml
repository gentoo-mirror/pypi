<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Demo PySpark Delta Table SCD2 implementation[![Python package](https://github.com/spsoni/pyspark-delta-scd2/actions/workflows/python-package.yml/badge.svg)](https://github.com/spsoni/pyspark-delta-scd2/actions/workflows/python-package.yml)[![CodeQL](https://github.com/spsoni/pyspark-delta-scd2/actions/workflows/codeql.yml/badge.svg)](https://github.com/spsoni/pyspark-delta-scd2/actions/workflows/codeql.yml)This project utilizes `faker-pyspark` to generate random schema and dataframes to mimic data table snapshots.Using these snapshots to process and apply SCD2 pattern into delta table as the destination. Source of Inspiration for SCD2 pattern: https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/BDB-2547/glue/scd-deltalake-employee-etl-job.py ## InstallationInstall with pip:``` bashpip install pyspark-delta-scd2 delta-spark faker-pyspark```Please note, this package do not enforce version of delta-spark, PySpark and faker-pyspark.When you want to use this example in AWS Glue environment, enforced versions conflict with the target environment.### Generate incremental updates to dataframe and apply scd2``` python&gt;&gt;&gt; from pyspark_delta_scd2 import get_spark, PySparkDeltaScd2&gt;&gt;&gt; spark = get_spark()&gt;&gt;&gt; demo  = PySparkDeltaScd2(spark=spark)&gt;&gt;&gt; # initial load&gt;&gt;&gt; df1   = demo.process()&gt;&gt;&gt; # incremental update&gt;&gt;&gt; df2   = demo.process()&gt;&gt;&gt; # df2 should have some deletes, updates and inserts```</longdescription>
</pkgmetadata>