<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># `vbjax``vbjax` is a Jax-based package for working with virtual brain style models.## InstallationInstalls with `pip install vbjax`, but you can use the source,```bashgit clone https://github.com/ins-amu/vbjaxcd vbjaxpip install .[dev]```The primary additional dependency of vbjax is[JAX](github.com/google/jax), which itself depends only onNumPy, SciPy &amp; opt-einsum, so it should be safe to add to yourexisting projects.If you have a CUDA-enabled GPU, you install the requisite dependencies like so```bashpip install --upgrade &quot;jax[cuda11_pip]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html```*BUT* because GPU software stack versions make aligning stars look like child's play,container images are available and auto-built w/[GitHub Actions](.github/workflows/docker-image.yml), so you can use w/ Docker```bashdocker run --rm -it ghcr.io/ins-amu/vbjax:main python3 -c 'import vbjax; print(vbjax.__version__)'```The images are built on Nvidia runtime images, so `--gpus all` is enoughfor Jax to discover the GPU(s).## ExamplesHere are some examples of simulations which show what you cando with the library.  Because they are implemented atop Jax, itis easy to take gradients for optimization or MCMC, or do efficientGPU parallel batching. Or both ಠ_ರೃ  ### Simple networkHere's the smallest simulation you might want to do: an all-to-all connected network with Montbrio-Pazo-Roxinmass model dynamics,```pythonimport vbjax as vbimport jax.numpy as npdef network(x, p):    c = 0.03*x.sum(axis=1)    return vb.mpr_dfun(x, c, p)_, loop = vb.make_sde(dt=0.01, dfun=network, gfun=0.1)zs = vb.randn(500, 2, 32)xs = loop(zs[0], zs[1:], vb.mpr_default_theta)vb.plot_states(xs, 'rV', jpg='example1', show=True)```![](example1.jpg)While integrators and mass models tend to be the same across publications, butthe network model itself varies (regions vs surface, stimulus etc), vbjax allowsuser to focus on defining the `network` and then getting time series.### Simplest neural field Here's a neural field,```pythonimport jax.numpy as npimport vbjax as vb# setup local connectivitylmax, nlat, nlon = 16, 32, 64lc = vb.make_shtdiff(lmax=lmax, nlat=nlat, nlon=nlon)# network dynamicsdef net(x, p):    c = lc(x[0]), 0.0    return vb.mpr_dfun(x, c, p)# solution + plotx0 = vb.randn(2, nlat, nlon)*0.5 + np.r_[0.2,-2.0][:,None,None]_, loop = vb.make_sde(0.1, net, 0.2)zs = vb.randn(500, 2, nlat, nlon)xt = loop(x0, zs, vb.mpr_default_theta._replace(eta=-3.9, cr=5.0))vb.make_field_gif(xt[::10], 'example2.gif')```![](example2.gif)This example shows how the field forms patterns gradually despite thenoise in the simulation, due to the effect of local connectivity### MCMC estimation of neural field activityFor MCMC estimates with NumPyro we define a function to computeposterior log probability `p(theta | x)`,  ```python  def logp(xt=None):      x0h = numpyro.sample('x0h', dist.Normal(jnp.zeros((nlat, nlon)), 1))      xth_mu = loop(x0h, ts, k)      numpyro.sample('xth', dist.Normal(xth_mu, 1), obs=xt)```run MCMC w/ NUTS,```python  mcmc = MCMC(NUTS(logp), num_warmup=500, num_samples=500)  mcmc.run(jax.random.PRNGKey(0), xt=xt)  x0h = mcmc.get_samples()['x0h']```check diagnostics like estimated sample size, shrinkage and z-score,```python  ess = numpyro.diagnostics.effective_sample_size(x0h.reshape((1, 500, -1)))  assert ess.min() &gt; 100  shrinkage, zscore = vbjax.shrinkage_zscore(x0, x0h, 1)  assert shrinkage.min() &gt; 0.7  assert zscore.max() &lt; 1.5```Full code is in the [test suite](vbjax/tests/test_field_inference.py), canbe run `pytest -m slow`, since it takes about 5 minutes to run on a GPU, 5 min on m1 CPU core and12 minutes on an x86_64 CPU core.### Fitting an autoregressive processHere's a 1-lag MVAR```pythonimport jaximport jax.numpy as npimport vbjax as vbnn = 8true_A = vb.randn(nn,nn)_, loop = vb.make_sde(1, lambda x,A: -x+(A*x).mean(axis=1), 1)x0 = vb.randn(nn)zs = vb.randn(1000, nn)xt = loop(x0, zs, true_A)````xt` and `true_A` are the simulated time series and ground truthinteraction matrices. To fit anything we need a loss function &amp; gradient descent,```pythondef loss(est_A):    return np.sum(np.square(xt - loop(x0, zs, est_A)))grad_loss = jax.grad(loss)est_A = np.ones((nn, nn))*0.3  # wrongfor i in range(51):    est_A = est_A - 0.01*grad_loss(est_A)    if i % 10 == 0:        print('step', i, 'log loss', np.log(loss(est_A)))print('mean sq err', np.square(est_A - true_A).mean())```which prints```step 0 log loss 5.8016257step 10 log loss 3.687574step 20 log loss 1.7174681step 30 log loss -0.15798996step 40 log loss -1.9851608step 50 log loss -3.7805486mean sq err 8.422789e-05```This is a pretty simple example but it's meant to show that any modelyou build with vbjax like this is usable with optimization or NumPyro'sMCMC algorithms.## HPC usageWe use this on HPC systems, most easily with container images.  Open an issue if it doesn't work.&lt;details&gt;&lt;summary&gt;CSCS Piz Daint&lt;/summary&gt;Useful modules```bashmodule load daint-gpumodule load cudatoolkit/11.2.0_3.39-2.1__gf93aa1cmodule load TensorFlow```then install in some Python environment; the default works fine```bashpip3 install &quot;jax[cuda]==0.3.8&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.htmlpip3 install &quot;jaxlib==0.3.8+cuda11.cudnn805&quot; -U -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html```This provides an older version of JAX unfortunately. The Sarus runtime can be used to make use of latest versions of vbjax and jax:```bash$ module load daint-gpu$ module load sarus$ sarus pull ghcr.io/ins-amu/vbjax:main...$ srun -p debug -A ich042 -C gpu --pty sarus run ghcr.io/ins-amu/vbjax:main python3 -c 'import jax; print(jax.numpy.zeros(32).device())'...gpu:0```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;JSC JUSUF&lt;/summary&gt;A nice module is available to get CUDA libs```bashmodule load cuDNN/8.6.0.163-CUDA-11.7```then you might set up a conda env,```bashwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.sh -b -p ~/conda. ~/conda/bin/activateconda create -n jax python=3.9 numpy scipysource activate jax```once you have an env, install the CUDA-enabled JAX```bashpip3 install --upgrade &quot;jax[cuda]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html```and check it works```bash(jax) [woodman1@jsfl02 ~]$ srun -A icei-hbp-2021-0002 -p develgpus --pty python3 -c 'import jax.numpy as np ; print(np.zeros(32).device())'gpu:0```JSC also makes Singularity available, so the prebuilt image can be used```TODO```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;CEA&lt;/summary&gt;The prebuilt image is the best route:```TODO```&lt;/details&gt;## DevelopmentNew ideas or even documenting tricks (like how Jax works) should gointo the test suite, and there are some ideas floating there beforemaking it into the library itself.```git clone https://github.com/ins-amu/vbjaxcd vbjaxpip install '.[dev]'pytest```### Installing SHTnsThis library is used for some testing.  It is impossible to install on Windows natively, so WSLx is required.  On macOS,```bashbrew install fftwgit clone https://bitbucket.org/nschaeff/shtns./configure --enable-python --disable-simd --prefix=/opt/homebrewmake -j &amp;&amp; make install &amp;&amp; python setup.py install```### Releasesa release of version `v1.2.3` requires following steps- [ ] `git checkout main`: tag releases from main for now- [ ] edit `_version.py` to have correct release number- [ ] `python -m vbjax._version tag` to create and push new tag  - [GitHub tests, builds and pushes tag release to PyPI](.github/workflows/publish-tags.yml)- [ ] use GitHub UI to create new release</longdescription>
</pkgmetadata>