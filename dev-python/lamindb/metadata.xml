<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Stars](https://img.shields.io/github/stars/laminlabs/lamindb?logo=GitHub&amp;color=yellow)](https://github.com/laminlabs/lamindb)[![codecov](https://codecov.io/gh/laminlabs/lamindb/branch/main/graph/badge.svg?token=VKMRJ7OWR3)](https://codecov.io/gh/laminlabs/lamindb)[![pypi](https://img.shields.io/pypi/v/lamindb?color=blue&amp;label=pypi%20package)](https://pypi.org/project/lamindb)[![Documentation](https://img.shields.io/badge/Documentation-green)](https://lamin.ai/docs/guide/)# LaminDBOpen-source data platform for biology.```{warning}Public beta: Close to having converged a stable API, but some breaking changes might still occur.```## IntroductionLaminDB is an open-source Python library to manage files &amp; datasets while- tracking provenance across pipelines, notebooks &amp; app uploads- validating &amp; linking data batches using biological registries &amp; ontologiesYou can- Manage features &amp; labels schema-less or schema-full.- Query, search, look up, save, load and stream with one API.- Collaborate across a mesh of LaminDB instances.- Enjoy [idempotent](https://lamin.ai/docs/faq/idempotency) &amp; [ACID](https://lamin.ai/docs/faq/acid) operations.LaminApp is a data management app built on LaminDB. If LaminDB ~ git, LaminApp ~ GitHub.LaminApp, support, code templates &amp; auto-dispatched integration tests for a BioTech data &amp; analytics platform are currently only available on an enterprise plan. LaminApp is available for your cloud infrastructure or hosted by us.## Quickstart[Installation and sign-up](#setup) take no time: Run `pip install lamindb` and `lamin signup &lt;email&gt;` on the command line.Then, init a LaminDB instance with local or cloud default storage like you'd init a git repository:```shell$ lamin init --storage ./mydata   # or s3://my-bucket, gs://my-bucket```Import `lamindb`:```pythonimport lamindb as ln# import lnschema_bionty as lb # optional, for bionty schema```### Manage files and data objects```python# Store and register a fileln.File(&quot;s3://my-bucket/images/image001.jpg&quot;).save()  # or a local path# Store and register a DataFrame objectdf = pd.DataFrame({&quot;feat1&quot;: [1, 2], &quot;feat2&quot;: [3, 4]})  # AnnData works, tooln.File(df, description=&quot;Data batch 1&quot;).save()  # create a File object and save/upload it# To find it, if you don't have specific metadata in mind, run a searchln.File.search(&quot;batch 1&quot;)# Or query (under-the-hood, you have the full power of SQL to query)file = ln.File.filter(description=&quot;Data batch 1&quot;).one()  # get exactly one result# Query by `key` (the relative path within your storage)ln.File.filter(key__startswith=&quot;images/&quot;).df()  # get query results as a DataFrame# Load a file back into memorydf = file.load()# Or get a backed accessor to stream its content from the cloud:backed = file.backed()  # currently works for AnnData, zarr, HDF5, not yet for DataFrame```### Auto-complete categoricals and search```python# When you're unsure about spellings, use a lookup objectusers = ln.User.lookup() # by default uses handle fieldln.File.filter(created_by=users.lizlemon)users = ln.User.lookup(&quot;name&quot;) # a lookup object of the name fieldusers.liz_lemon# Or searchln.User.search(&quot;liz lemon&quot;) # by default searches the handle fielduser = ln.User.search(&quot;liz lemon&quot;, return_queryset=True).first() # grab the top search result as a recordln.User.search(&quot;liz lemon&quot;, field=[&quot;name&quot;, &quot;handle&quot;]) # search against multiple fields```### Track &amp; query data lineageIn addition to basic provenance information (`created_by`, `created_at`,`created_by`), you can track which notebooks &amp; pipelinestransformed files.View all parent transforms and files in a lineage graph:```pythonfile.view_lineage()```&lt;img src=&quot;https://raw.githubusercontent.com/laminlabs/lamindb/main/docs/img/readme/view_lineage.svg&quot; width=&quot;800&quot;&gt;#### NotebooksTrack a Jupyter Notebook:```python# my-analysis.ipynbln.track()  # auto-detect &amp; save notebook metadataln.File(&quot;my_artifact.parquet&quot;).save()  # this file is now aware that it was saved in this notebook```When you query the file, later on, you'll know which notebook it came from:```pythonfile = ln.File.filter(description=&quot;my_artifact.parquet&quot;).one()  # query for a filefile.transform  # the notebook with id, title, filename, version, etc.file.run  # the specific run of the notebook that created the file# Alternatively, you can query for notebooks and find the files written by themtransforms = ln.Transform.filter(type=&quot;notebook&quot;, created_at__year=2022).search(&quot;T cell&quot;).all()ln.File.filter(transform__in=transforms).df()  # the files created by these notebooks```#### PipelinesThis works like for notebooks just that you need to provide pipeline metadata yourself.```python# To save a pipeline to the `Transform` registry, callln.Transform(name=&quot;Awesom-O&quot;, version=&quot;0.41.2&quot;).save()  # save a pipeline, optionally with metadata# Track a pipeline runtransform = ln.Transform.filter(name=&quot;Awesom-O&quot;, version=&quot;0.41.2&quot;).one()  # filter pipeline from the registryln.track(transform)  # create a new global run contextln.File(&quot;s3://my_samples01/my_artifact.fastq.gz&quot;).save()  # file gets auto-linked against run &amp; transform# Now, you can query for the latest pipeline runsln.Run.filter(transform=transform).order_by(&quot;-created_at&quot;).df()  # get the latest pipeline runs```### Load your instance from anywhereIf provided with access, others can load your instance via:```shell$ lamin load myaccount/mydata```### Manage biological registries```shell$ lamin init --storage ./bioartifacts --schema bionty``````python# create an ontology-coupled record and save itlb.CellType.from_bionty(name=&quot;neuron&quot;).save()# bulk create knowledge-coupled recordsadata = ln.dev.datasets.anndata_with_obs()cell_types = lb.CellType.from_values(adata.obs.cell_type, field=lb.CellType.name)ln.save(cell_types) # bulk save cell types# standardize synonymslb.CellType.map_synonyms([&quot;T cell&quot;, &quot;T-cell&quot;, &quot;T lymphocyte&quot;])# construct and view ontological hierarchy of a recordneuron = lb.CellType.lookup().neuronmy_cell_type = lb.CellType(name=&quot;my neuron cell&quot;)my_cell_type.parents.add(neuron)my_cell_type.view_parents(distance=3)```&lt;img src=&quot;https://raw.githubusercontent.com/laminlabs/lamindb/main/docs/img/readme/neuron_view_parents_dist%3D2.svg&quot; width=&quot;500&quot;&gt;### Track biological features```python# track features present in var(X) and obsadata = ln.dev.datasets.anndata_with_obs()file = ln.File.from_anndata(    adata, description=&quot;my RNA-seq dataset&quot;, var_ref=lb.Gene.ensembl_gene_id)file.save()# view a summary of tracked features# you have registered two feature sets: 'obs' and 'var'file.features# add labels to featurestissues = lb.Tissue.from_values(adata.obs[&quot;tissue&quot;], field=lb.Tissue.name)diseases = lb.Disease.from_values(adata.obs[&quot;disease&quot;], field=lb.Disease.name)file.features.add_labels(tissues + diseases)# fetch labels of a featurefile.features[&quot;obs&quot;].get(name=&quot;tissue&quot;).df()# display rich metadata of a file (provenance and features)file.describe()```### Manage custom schemas1. Create a GitHub repository with Django ORMs similar to [github.com/laminlabs/lnschema-lamin1](https://github.com/laminlabs/lnschema-lamin1)2. Create &amp; deploy migrations via `lamin migrate create` and `lamin migrate deploy`It's fastest if we do this for you based on our templates within an enterprise plan, but you can fully manage the process yourself.## Setup### Installation![pyversions](https://img.shields.io/pypi/pyversions/lamindb)```shellpip install lamindb  # basic data management```You can configure the installation using `extras`, e.g.,```shellpip install 'lamindb[jupyter,bionty,fcs,aws]'```Supported `extras` are:```yamljupyter  # Track Jupyter notebooksbionty   # Manage basic biological entitiesfcs      # Manage FCS files (flow cytometry)zarr     # Store &amp; stream arrays with zarraws      # AWS (s3fs, etc.)gcp      # Google Cloud (gcfs, etc.)postgres # Postgres server```### DockerHere is a way of running LaminDB in a docker: [github.com/laminlabs/lamindb-docker](https://github.com/laminlabs/lamindb-docker).### Sign upWhy do I have to sign up?- Data lineage requires a user identity (who modified which data when?).- Collaboration requires a user identity (who shares this with me?).Signing up takes 1 min.We do _not_ store any of your data, but only basic metadata about you (email address, etc.) &amp; your LaminDB instances (S3 bucket names, etc.).- Sign up: `lamin signup &lt;email&gt;`- Log in: `lamin login &lt;handle&gt;`## How does it work?### DependenciesLaminDB builds semantics of R&amp;D and biology onto well-established tools:- SQLite &amp; Postgres for SQL databases using Django Registry (previously: SQLModel)- S3, GCP &amp; local storage for object storage using fsspec- Configurable storage formats: pyarrow, anndata, zarr, etc.- Biological knowledge sources &amp; ontologies: see [Bionty](https://lamin.ai/docs/bionty)LaminDB is open source.### ArchitectureLaminDB consists of the `lamindb` Python package (repository [here](https://github.com/laminlabs/lamindb)) with its components:- [bionty](https://github.com/laminlabs/bionty): Basic biological entities (usable standalone).- [lamindb-setup](https://github.com/laminlabs/lamindb-setup): Setup &amp; configure LaminDB, client for Lamin Hub.- [lnschema-core](https://github.com/laminlabs/lnschema-core): Core schema, ORMs to model data objects &amp; data lineage.- [lnschema-bionty](https://github.com/laminlabs/lnschema-bionty): Bionty schema, ORMs that are coupled to Bionty's entities.- [lnschema-lamin1](https://github.com/laminlabs/lnschema-lamin1): Exemplary configured schema to track samples, treatments, etc.- [nbproject](https://github.com/laminlabs/nbproject): Parse metadata from Jupyter notebooks.- [lamin-utils](https://github.com/laminlabs/lamin-utils): Utilities for LaminDB and Bionty.- [readfcs](https://github.com/laminlabs/readfcs): FCS file reader.LaminHub &amp; LaminApp are not open-sourced, and neither are templates that model lab operations.## Notebooks- Find all guide notebooks [here](https://github.com/laminlabs/lamindb/tree/main/docs/guide).- You can run these notebooks in hosted versions of JupyterLab, e.g., [Saturn Cloud](https://github.com/laminlabs/run-lamin-on-saturn), Google Vertex AI, Google Colab, and others.- Jupyter Lab &amp; Notebook offer a fully interactive experience, VS Code &amp; others require using the CLI to track notebooks: `lamin track my-notebook.ipynb`## DocumentationRead the [docs](https://lamin.ai/docs/guide/).</longdescription>
</pkgmetadata>