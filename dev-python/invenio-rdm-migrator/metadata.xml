<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>..    Copyright (C) 2022 CERN.    Invenio-RDM-Migrator is free software; you can redistribute it and/or    modify it under the terms of the MIT License; see LICENSE file for more    details.===================== Invenio-RDM-Migrator=====================.. image:: https://github.com/inveniosoftware/invenio-rdm-migrator/workflows/CI/badge.svg        :target: https://github.com/inveniosoftware/invenio-rdm-migrator/actions?query=workflow%3ACI+branch%3Amaster.. image:: https://img.shields.io/github/tag/inveniosoftware/invenio-rdm-migrator.svg        :target: https://github.com/inveniosoftware/invenio-rdm-migrator/releases.. image:: https://img.shields.io/pypi/dm/invenio-rdm-migrator.svg        :target: https://pypi.python.org/pypi/invenio-rdm-migrator.. image:: https://img.shields.io/github/license/inveniosoftware/invenio-rdm-migrator.svg        :target: https://github.com/inveniosoftware/invenio-rdm-migrator/blob/master/LICENSEDataCite-based data model for Invenio.Development===========Install-------Make sure that you have `libpq-dev` installed in your system. See`psycopg installation instructions &lt;https://www.psycopg.org/install/&gt;`_for more information.Choose a version of search and database, then run:.. code-block:: console    pip install -e .Tests-----.. code-block:: console    ./run-tests.shHow to run it=============To run the migration you need:- A running InvenioRDM instance.- If your data contains references to other records (e.g. vocabularies),  then it is also required to run the setup step... code-block:: console    invenio-cli services setup --force --no-demo-data- Install Invenio-RDM-Migrator, any other dependencies must be handled  in the Pipfile of your instance... code-block:: console    $ pip install invenio-rdm-migrator- Create/edit the configuration file on your instance, for example  `streams.yaml`:.. code-block:: yaml    records:        extract:            filename: /path/to/records.json        load:            db_uri: postgresql+psycopg2://inveniordm:inveniordm@localhost:5432/inveniordm            tmp_dir: /tmp/migrator- You will need to create a small python script  putting together the different blocks of the ETL. You can find an eample  at `my-site/site/my_site/migrator/__main__.py`... code-block:: python    from invenio_rdm_migrator.streams import StreamDefinition    from invenio_rdm_migrator.streams.records import RDMRecordCopyLoad    if __name__ == &quot;__main__&quot;:        RecordStreamDefinition = StreamDefinition(            name=&quot;records&quot;,            extract_cls=JSONLExtract,            transform_cls=ZenodoToRDMRecordTransform,            load_cls=RDMRecordCopyLoad,        )        runner = Runner(            stream_definitions=[                RecordStreamDefinition,            ],            config_filepath=&quot;path/to/your/streams.yaml&quot;,        )        runner.run()- Finally, you can execute the above code. Since it is in the `__main__` file  of the python package, you can run it as a module:.. code-block:: console    $ python -m my_site.migrator- Once the migration has completed, in your instance you can reindex the data.  For example, for users and records it would look like:.. code-block:: console    $ invenio-cli pyshell    In [1]: from invenio_access.permissions import system_identity    In [2]: from invenio_rdm_records.proxies import current_rdm_records_service    In [3]: from invenio_users_resources.proxies import current_users_service    In [4]: current_users_service.rebuild_index(identity=system_identity)    In [5]: current_rdm_records_service.rebuild_index(identity=system_identity)Implement your {Extract/Transform/Load}=======================================There are for packages in this module `extract`, `transform`, `load`, and`streams`. The first three correspond to the three steps of an ETL process.The `streams` package contains the logic to run the process and differentstream-specific implementations of ETL classes (e.g. `records`).Extract-------The extract is the first part of the data processing stream. It'sfunctionality is quite simple: return an iterator (e.g. of records), where eachyielded value is a dictionary. Note that the data in this step is _transformed_to an extent, but only in format (e.g. JSON, XML), not in content. For example,to implement a `XMLExtract` class:.. code-block:: python    class XMLExtract(Extract):    ...        def run(self):            with open(&quot;file.xml&quot;) as file:                for entry in file:                    yield xml.loads(entry)Transform---------The transformer is in charge of modifying the content to suit, in this case,the InvenioRDM data model (e.g. for records) so it can be imported in the DB.It will loop through the entries (i.e. the iterator returned by the extractclass), transform and yield (e.g. the record). Diving more in the example ofa record:To transform something to an RDM record, you need to implement`streams/records/transform.py:RDMRecordTransform`. For each record it willyield what is considered a semantically &quot;full&quot; record: the record itself,its parent, its draft in case it exists and the files related them... code-block:: python    {        &quot;record&quot;: self._record(entry),        &quot;draft&quot;: self._draft(entry),        &quot;parent&quot;: self._parent(entry),    }This means that you will need to implement the functions for each key. Notethat, only `_record` and `_parent` should return content, the others can return`None`. In this case we will need to re-think which methods should be`abstractmethod` and which ones be defaulted to `None/{}/some other default` inthe base. You can find an example implementation at`zenodo-rdm/site/zenodo_rdm/migrator/transform.py:ZenodoToRDMRecordTransform`.Some of these functions can themselves use a `transform/base:Entry`transformer. An _entry_ transformer is a one layer deeper abstraction, toprovide an interface with the methods needed to generate valid data for part ofthe `Transform` class. In the record example, you can implement`transform.base:RDMRecordEntry`, which can be used in the`RDMRecordTransform._record` function mentioned in the code snippet above. Notethat implementing this interface will produce valid _data_ for a record.However, the _metadata_ is not interfaced. It is an open question how much weshould define these interfaces and avoid duplicating already existingMarshmallow schemas.At this point you might be wondering &quot;Why not Marshmallow then?&quot;. The answer is&quot;separation of responsibilities, performance and simplicity&quot;. The later layswith the fact that most of the data transformation is custom, so we would endup with a schema full of `Method` fields, which does not differ much from whatwe have but would have an impact on performance (Marshmallow is slow...).Regarding the responsibilities part, validating - mostly referential, likevocabularies - can only be done on _load_ where RDM instance knowledge/appctxis available.Note that no validation (not even structural) is done (at the moment) in thisstep.Load----The final step to have the records available in the RDM instance is to loadthem. The available `load/postgresql:PostgreSQLCopyLoad` will carry out 2 steps:- 1. Prepare the data, writing one DB row per line in a csv file:.. code-block:: console    $ /path/to/data/tables1668697280.943311        |        | - pidstore_pid.csv        | - rdm_parents_metadata.csv        | - rdm_records_metadata.csv        | - rdm_versions_state.csv2. Perform the actual loading, using `COPY`. Inserting all rows at once is more   efficient than performing one `INSERT` per row.Internally what is happening is that the `prepare` function makes use of`TableGenerator` implementations and then yields the list of csv files.So the `load` only iterates through the filenames, not the actual entries.A `TableGenerator` will, for each value in the received iterator, yield oneor more rows (lines to be written to the a csv file). For example for a recordit will yield: recid, DOI and OAI (PersistentIdentifiers), record and parentmetadata, etc.Notes=====**Shared cache between streams**During a migration run, there is a need to share information across different streams,e.g populate communities before records and keep the map between community slug namesand autogenerated community ids, or at the same stream across different `TableGenerator`instances, e.g on records stream we keep the &quot;seen&quot; parent ids so we can update theinformation of the parent for different record versions.For that reason, we pass a cache dict, that can change in the future in a type ofpersistent storage e.g redis, in each `stream.load` step so streams can populate/consumethe cache.The cache for each stream can also be populated in each stream configuration like belowin your `streams.yaml`:.. code-block:: yaml    records:        extract:            filename: /path/to/records.json        load:            cache:                communities:                    community_slug: &lt;community_id&gt;            db_uri: postgresql+psycopg2://inveniordm:inveniordm@localhost:5432/inveniordm            tmp_dir: /tmp/migratorWhen the runner will instantiate each stream will merge the existing state of the cachewith whatever is provided in the stream configuration. That means, that the streamconfiguration takes precedence and can override the whole cache before the stream runs!Any cache state that exists before is overridden for the rest of the migration run.**Infrastructure**While now we are chaining the iterator from one step into the other in thestreams, the idea is that all three steps will pull/push to/from queues sothey can be deployed in different parts of the system (e.g. the load partin the worker nodes).**Others**- Using generators instead of lists, allows us to iterate through the data  only once and perform the E-T-L steps on them. Instead of loop for E, loop  for T, loop for L. In addition, this allows us to have the csv files open  during the writing and closing them at the end (open/close is an expensive  op when done 3M times)...    Copyright (C) 2022 CERN.    Invenio-RDM-Migrator is free software; you can redistribute it and/or    modify it under the terms of the MIT License; see LICENSE file for more    details.Changes=======Version 1.0.0- Initial public release.</longdescription>
</pkgmetadata>