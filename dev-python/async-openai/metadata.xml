<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># async-openai Unofficial Async Python client library for the [OpenAI](https://openai.com) API based on [Documented Specs](https://beta.openai.com/docs/api-reference/making-requests) **Latest Version**: [![PyPI version](https://badge.fury.io/py/async-openai.svg)](https://badge.fury.io/py/async-openai) **[Official Client](https://github.com/openai/openai-python)**## Features- [x] Asyncio based with Sync and Async Support with `httpx`- [ ] Supports all API endpoints    - [x] `Completions`: [Docs](https://beta.openai.com/docs/api-reference/completions)        - [x] `Edits`: [Docs](https://beta.openai.com/docs/api-reference/edits)        - [x] `Embeddings`: [Docs](https://beta.openai.com/docs/api-reference/embeddings)    - [ ] `Images`: [Docs](https://beta.openai.com/docs/api-reference/images)    - [ ] `Files`: [Docs](https://beta.openai.com/docs/api-reference/files)    - [ ] `Finetuning`: [Docs](https://beta.openai.com/docs/api-reference/fine-tunes)    - [x] `Models`: [Docs](https://beta.openai.com/docs/api-reference/models)    - [ ] `Moderations`: [Docs](https://beta.openai.com/docs/api-reference/moderations)    - [ ] `Search`: [Docs](#)- [x] Strongly typed validation of requests and responses with `Pydantic` Models with transparent     access to the raw response and object-based results.- [x] Handles Retries automatically through `backoff`- [x] Supports Local and Remote Cloud Object Storage File Handling Asyncronously through `file-io`    - [x] Supports `S3`: `s3://bucket/path/to/file.txt`        - [x] Supports `GCS`: `gs://bucket/path/to/file.txt`    - [x] Supports `Minio`: `minio://bucket/path/to/file.txt`- [x] Supports `limited` cost tracking for `Completions` and `Edits` requests (when stream is not enabled)--- ## Installation```bash# Install from stablepip install async-openai# Install from dev/latestpip install git+https://github.com/GrowthEngineAI/async-openai.git```### Quick Usage```pythonimport asynciofrom async_openai import OpenAI, settings, CompletionResponse# Environment variables should pick up the defaults# however, you can also set them explicitly.# `api_key` - Your OpenAI API key.                  Env: [`OPENAI_API_KEY`]# `url` - The URL of the OpenAI API.                Env: [`OPENAI_URL`]# `api_type` - The OpenAI API type.                 Env: [`OPENAI_API_TYPE`]# `api_version` - The OpenAI API version.           Env: [`OPENAI_API_VERSION`]# `organization` - The OpenAI organization.         Env: [`OPENAI_ORGANIZATION`]# `proxies` - A dictionary of proxies to be used.   Env: [`OPENAI_PROXIES`]# `timeout` - The timeout in seconds to be used.    Env: [`OPENAI_TIMEOUT`]# `max_retries` - The number of retries to be used. Env: [`OPENAI_MAX_RETRIES`]OpenAI.configure(    api_key = &quot;sk-XXXX&quot;,    organization = &quot;org-XXXX&quot;,    debug_enabled = False,)# Alternatively you can configure the settings through environment variables# settings.configure(#    api_key = &quot;sk-XXXX&quot;,#     organization = &quot;org-XXXX&quot;,# )# [Sync] create a completion# Results return a CompletionResult objectresult: CompletionResponse = OpenAI.completions.create(    prompt = 'say this is a test',    max_tokens = 4,    stream = True)# print the completion text# which are concatenated together from the result['choices'][n]['text']print(result.text)# print the number of choices returnedprint(len(result))# get the cost consumption for the requestprint(result.consumption)# [Async] create a completion# All async methods are generally prefixed with `async_`result: CompletionResponse = asyncio.run(    OpenAI.completions.async_create(        prompt = 'say this is a test',        max_tokens = 4,        stream = True    ))```### Initialize Clients Manually, and working with multiple clients```pythonfrom async_openai import OpenAI# Configure your primary client (default)OpenAI.configure(    api_key = &quot;sk-XXXX&quot;,    organization = &quot;org-XXXX&quot;,    debug_enabled = False,    # Azure Configuration    azure_api_base = 'https://....openai.azure.com/',    azure_api_version = '2023-07-01-preview',    azure_api_key = '....',)# Returns the default client (openai)oai = OpenAI.init_api_client()# Configure your secondary client (azure) and use it directlyaz = OpenAI.init_api_client('az', set_as_default = False, debug_enabled = True)result = az.completions.create(    prompt = 'say this is a test',    max_tokens = 4,    stream = True)# Use the default client (openai)result = OpenAI.completions.create(    prompt = 'say this is a test',    max_tokens = 4,    stream = True)# Or result = oai.completions.create(    prompt = 'say this is a test',    max_tokens = 4,    stream = True)# You can select the different clients by name or indexresult = OpenAI['az'].completions.create(    prompt = 'say this is a test',    max_tokens = 4,    stream = True)# Use the default client (openai)result = OpenAI['default'].completions.create(    prompt = 'say this is a test',    max_tokens = 4,    stream = True)# Will use the `default` client since it was initialized firstresult = OpenAI[0].completions.create(    prompt = 'say this is a test',    max_tokens = 4,    stream = True)```### Handling Errors, Retries, and RotationsThe below will show you how to rotate between multiple clients when you hit an error.**Important** Auto-rotation is only supported with `chat_create` and `async_chat_create` methods. Otherwise, you should handle the rotation manually.```pythonimport asynciofrom async_openai import OpenAI, ChatResponsefrom async_openai.utils import loggerOpenAI.configure(    api_key = &quot;sk-XXXX&quot;,    organization = &quot;org-XXXX&quot;,    debug_enabled = False,    # Azure Configuration    azure_api_base = 'https://....openai.azure.com/',    azure_api_version = '2023-07-01-preview',    azure_api_key = '....',    # This will allow you to auto rotate clients when you hit an error.    # But only if you have multiple clients configured and are using `OpenAI.chat_create`    enable_rotating_clients = True,     # This will prioritize Azure over OpenAI when using `OpenAI.chat_create`    prioritize = &quot;azure&quot;,)# Display the current clientOpenAI.get_current_client_info(verbose = True)# Rotate to the next client# OpenAI.rotate_client(verbose = True)#### [Sync] create a completion with auto-rotation and auto-retry###result: ChatResponse = OpenAI.chat_create(    model = &quot;gpt-3.5-turbo-16k&quot;,    messages = [        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Translate the following English text to French: “Multiple models, each with different capabilities and price points. Prices are per 1,000 tokens. You can think of tokens as pieces of words, where 1,000 tokens is about 750 words. This paragraph is 35 tokens”&quot;}    ],    auto_retry = True,)logger.info(f'Result Chat Message: {result.messages}')logger.info(f'Result Usage: {result.usage}')logger.info(f'Result Consumption: {result.consumption}')#### [Async] create a completion with auto-rotation and auto-retry###result: ChatResponse = asyncio.run(    OpenAI.async_chat_create(        model = &quot;gpt-3.5-turbo-16k&quot;,        messages = [            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Translate the following English text to French: “Multiple models, each with different capabilities and price points. Prices are per 1,000 tokens. You can think of tokens as pieces of words, where 1,000 tokens is about 750 words. This paragraph is 35 tokens”&quot;}        ],        auto_retry = True,    ))```### Function CallsThe latest version of the API allows for function calls to be made. This is currently only supported in `Chat` and requires api version: `2023-07-01-preview` for `azure`.Function calls support using `pydantic` models to auto-generate the schemas```pythonimport asynciofrom enum import Enumfrom client_rotate import OpenAIfrom async_openai.utils import loggerfrom pydantic import BaseModel, Fieldclass Unit(str, Enum):    celsius = &quot;celsius&quot;    fahrenheit = &quot;fahrenheit&quot;class Weather(BaseModel):    location: str = Field(..., description=&quot;The city and state, e.g. San Francisco, CA.&quot;)    unit: Unit = Field(Unit.fahrenheit)functions = [   {    &quot;name&quot;: &quot;get_current_weather&quot;,    &quot;description&quot;: &quot;Get the current weather in a given location&quot;,    &quot;parameters&quot;: Weather,  }]result: ChatResponse = OpenAI.chat_create(    model = &quot;gpt-3.5-turbo-16k&quot;,    messages = [        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What's the weather like in Boston today?&quot;}    ],    functions = functions,    auto_retry = True,)logger.info(f'Result Chat Message: {result.messages}')logger.info(f'Result Chat Function: {result.function_results}')logger.info(f'Result Usage: {result.usage}')logger.info(f'Result Consumption: {result.consumption}')&quot;&quot;&quot;Result:&gt; Result Chat Message: [ChatMessage(content='', role='assistant', function_call=FunctionCall(name='get_current_weather', arguments={'location': 'Boston, MA'}), name=None)]&gt; Result Chat Function: [FunctionCall(name='get_current_weather', arguments={'location': 'Boston, MA'})]&gt; Result Usage: prompt_tokens=16 completion_tokens=19 total_tokens=35&gt; Result Consumption: 0.00012399999999999998&quot;&quot;&quot;```### Configure Azure Model MappingYour azure models may be named differently than the default mapping. By configuring the mapping, you can automatically map the models to the correct azure model (when using openai model names).```pythonfrom async_openai import OpenAI&quot;&quot;&quot;Default Azure Model Mapping{    'gpt-3.5-turbo': 'gpt-35-turbo',    'gpt-3.5-turbo-16k': 'gpt-35-turbo-16k',    'gpt-3.5-turbo-instruct': 'gpt-35-turbo-instruct',    'gpt-3.5-turbo-0301': 'gpt-35-turbo-0301',    'gpt-3.5-turbo-0613': 'gpt-35-turbo-0613',}&quot;&quot;&quot;AzureModelMapping = {    'gpt-3.5-turbo': 'azure-gpt-35-turbo',    'gpt-3.5-turbo-16k': 'azure-gpt-35-turbo-16k',    'gpt-3.5-turbo-instruct': 'azure-gpt-35-turbo-instruct',    'gpt-3.5-turbo-0301': 'azure-gpt-35-turbo-0301',    'gpt-3.5-turbo-0613': 'azure-gpt-35-turbo-0613',}OpenAI.configure(    api_key = &quot;sk-XXXX&quot;,    organization = &quot;org-XXXX&quot;,    debug_enabled = False,    # Azure Configuration    azure_api_base = 'https://....openai.azure.com/',    azure_api_version = '2023-07-01-preview',    azure_api_key = '....',    azure_model_mapping = AzureModelMapping,)# This will now use the azure endpoint as the default clientOpenAI.init_api_client('az', set_as_default = True, debug_enabled = True)# This will automatically map &quot;gpt-3.5-turbo-16k&quot; -&gt; &quot;azure-gpt-35-turbo-16k&quot;result: ChatResponse = OpenAI.chat.create(    model = &quot;gpt-3.5-turbo-16k&quot;,    messages = [        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Translate the following English text to French: “Multiple models, each with different capabilities and price points. Prices are per 1,000 tokens. You can think of tokens as pieces of words, where 1,000 tokens is about 750 words. This paragraph is 35 tokens”&quot;}    ],    auto_retry = True,)```---### DependenciesThe aim of this library is to be as lightweight as possible. It is built on top of the following libraries:- [aiohttpx](https://github.com/GrowthEngineAI/aiohttpx): Unified Async / Sync HTTP Client that wraps around `httpx`    - [httpx](https://www.python-httpx.org/): Async / Sync HTTP Requests    - [lazyops](https://github.com/trisongz/lazyops): Provides numerous utility functions for working with Async / Sync code and data structures- [pydantic](https://pydantic-docs.helpmanual.io/): Type Support- [file-io](https://github.com/trisongz/file-io): Async Cloud-based File Storage I/O- [backoff](https://github.com/litl/backoff): Retries with Exponential Backoff</longdescription>
</pkgmetadata>