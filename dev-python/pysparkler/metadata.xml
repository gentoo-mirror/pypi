<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># PySparkler## AboutPySparkler is a tool that upgrades your PySpark scripts to Spark 3.3. It is a command line tool that takes a PySparkscript as input and outputs a Spark 3.3 compatible script. It is written in Python and uses the[LibCST](https://github.com/Instagram/LibCST) module to parse the input script and generate the output script.## Basic UsageInstall from PyPI:```bashpip install pysparkler```Provide the path to the script you want to upgrade:```bashpysparkler upgrade --input-file /path/to/script.py```## PySpark Upgrades SupportedThis tool follows the [Apache Spark Migration guide for PySpark](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html)to upgrade your PySpark scripts. In the latest stable version it supports the following upgrades from the migration guide:| Migration                                       | Supported | Details                                                                                                                                      ||-------------------------------------------------|-----------|----------------------------------------------------------------------------------------------------------------------------------------------|| Upgrading from PySpark 3.3 to 3.4               | ❌         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-3-3-to-3-4)               || Upgrading from PySpark 3.2 to 3.3               | ✅         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-3-2-to-3-3)               || Upgrading from PySpark 3.1 to 3.2               | ✅         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-3-1-to-3-2)               || Upgrading from PySpark 2.4 to 3.0               | ✅         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-2-4-to-3-0)               || Upgrading from PySpark 2.3 to 2.4               | ❌         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-2-3-to-2-4)               || Upgrading from PySpark 2.3.0 to 2.3.1 and above | ❌         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-2-3-0-to-2-3-1-and-above) || Upgrading from PySpark 2.2 to 2.3               | ❌         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-2-2-to-2-3)               || Upgrading from PySpark 1.4 to 1.5               | ❌         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-1-4-to-1-5)               || Upgrading from PySpark 1.0-1.2 to 1.3           | ❌         | [Link](https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_upgrade.html#upgrading-from-pyspark-1-0-1-2-to-1-3)           |## Features SupportedThe tool supports the following features:| Feature                          | Supported ||----------------------------------|-----------|| Upgrade PySpark Python script    | ✅         || Upgrade PySpark Jupyter Notebook | ✅         || Dry-run Mode                     | ✅         || Verbose Mode                     | ✅         |### Upgrade PySpark Python scriptThe tool can upgrade a PySpark Python script. It takes the path to the script as input and upgrades it in place:```bashpysparkler upgrade --input-file /path/to/script.py```If you want to output the upgraded script to a different directory, you can use the `--output-file` flag:```bashpysparkler upgrade --input-file /path/to/script.py --output-file /path/to/output.py```### Upgrade PySpark Jupyter NotebookThe tool can upgrade a PySpark Jupyter Notebook to Spark 3.3. It takes the path to the notebook as input and upgradesit in place:```bashpysparkler upgrade --input-file /path/to/notebook.ipynb```Similar to upgrading python scripts, if you want to output the upgraded notebook to a different directory, you can usethe `--output-file` flag:```bashpysparkler upgrade --input-file /path/to/notebook.ipynb --output-file /path/to/output.ipynb```To change the output kernel name in the output Jupyter notebook, you can use the `--output-kernel` flag:```bashpysparkler upgrade --input-file /path/to/notebook.ipynb --output-kernel spark33-python3```### Dry-Run ModeFor both the above upgrade options, to run in dry mode, you can use the `--dry-run` flag. This will not write theupgraded script but will print a unified diff of the input and output scripts for you to inspect the changes:```bashpysparkler upgrade --input-file /path/to/script.py --dry-run```### Verbose ModeFor both the above upgrade options, to run in verbose mode, you can use the `--verbose` flag. This will print tool'sinput variables, the input file content, the output content, and a unified diff of the input and output content:```bashpysparkler --verbose upgrade --input-file /path/to/script.py```## ContributingFor the development, Poetry is used for packing and dependency management. You can install this using:```bashpip install poetry```If you have an older version of pip and virtualenv you need to update these:```bashpip install --upgrade virtualenv pip```### InstallationTo get started, you can run `make install`, which installs Poetry and all the dependencies of the PySparkler library.This also installs the development dependencies.```bashmake install```If you don't want to install the development dependencies, you need to install using `poetry install --only main`.If you want to install the library on the host, you can simply run `pip3 install -e .`. If you wish to use a virtualenvironment, you can run `poetry shell`. Poetry will open up a virtual environment with all the dependencies set.### IDE SetupTo set up IDEA with Poetry:- Open up the Python project in IntelliJ- Make sure that you're on latest master (that includes Poetry)- Go to File -&gt; Project Structure (⌘;)- Go to Platform Settings -&gt; SDKs- Click the + sign -&gt; Add Python SDK- Select Poetry Environment from the left hand side bar and hit OK- It can take some time to download all the dependencies based on your internet- Go to Project Settings -&gt; Project- Select the Poetry SDK from the SDK dropdown, and click OKFor IDEA ≤2021 you need to install the[Poetry integration as a plugin](https://plugins.jetbrains.com/plugin/14307-poetry/).Now you're set using Poetry, and all the tests will run in Poetry, and you'll have syntax highlighting in thepyproject.toml to indicate stale dependencies.### Linting`pre-commit` is used for autoformatting and linting:```bashmake lint```Pre-commit will automatically fix the violations such as import orders, formatting etc. Pylint errors you need to fixyourself.In contrast to the name suggest, it doesn't run the checks on the commit. If this is something that you like, you canset this up by running `pre-commit install`.You can bump the integrations to the latest version using `pre-commit autoupdate`. This will check if there is a newerversion of `{black,mypy,isort,...}` and update the yaml.### TestingFor Python, `pytest` is used a testing framework in combination with `coverage` to enforce 90%+ code coverage.```bashmake test```To pass additional arguments to pytest, you can use `PYTEST_ARGS`. For example, to run pytest in verbose mode:```bashmake test PYTEST_ARGS=&quot;-v&quot;```## Architecture### Why LibCST?LibCST is a Python library that provides a concrete syntax tree (CST) for Python code. CST preserves even the whitespacesof the source code which is very important since we only want to modify the code and not the formatting.### How does it work?Using the codemod module of LibCST can simplify the process of writing a PySpark migration script, as it allows us towrite small, reusable transformers and chain them together to perform a sequence of transformations.### Why Transformer Codemod? Why not Visitor?The main advantage of using a Transformer is that it allows for more fine-grained control over the transformationprocess. Transformer classes can be defined to apply specific transformations to specific parts of the codebase, andmultiple Transformer classes can be combined to form a chain of transformations. This can be useful when dealing withcomplex codebases where different parts of the code require different transformations.More on this can be found [here](https://libcst.readthedocs.io/en/latest/tutorial.html#Build-Visitor-or-Transformer).</longdescription>
</pkgmetadata>