<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>.. image:: https://raw.githubusercontent.com/scrapinghub/scrapyrt/master/artwork/logo.gif   :width: 400px   :align: center==========================ScrapyRT (Scrapy realtime)==========================.. image:: https://github.com/scrapinghub/scrapyrt/workflows/CI/badge.svg   :target: https://github.com/scrapinghub/scrapyrt/actions.. image:: https://img.shields.io/pypi/pyversions/scrapyrt.svg    :target: https://pypi.python.org/pypi/scrapyrt.. image:: https://img.shields.io/pypi/v/scrapyrt.svg    :target: https://pypi.python.org/pypi/scrapyrt.. image:: https://img.shields.io/pypi/l/scrapyrt.svg    :target: https://pypi.python.org/pypi/scrapyrt.. image:: https://img.shields.io/pypi/dm/scrapyrt.svg   :target: https://pypistats.org/packages/scrapyrt   :alt: Downloads count.. image:: https://readthedocs.org/projects/scrapyrt/badge/?version=latest   :target: https://scrapyrt.readthedocs.io/en/latest/api.htmlAdd HTTP API for your `Scrapy &lt;https://scrapy.org/&gt;`_ project in minutes.You send a request to ScrapyRT with spider name and URL, and in response, you get items collected by a spidervisiting this URL. * All Scrapy project components (e.g. middleware, pipelines, extensions) are supported* You run Scrapyrt in Scrapy project directory. It starts HTTP server allowing you to schedule spiders and get spider output in JSON.Quickstart===============**1. install**.. code-block:: shell    &gt; pip install scrapyrt**2. switch to Scrapy project (e.g. quotesbot project)**.. code-block:: shell    &gt; cd my/project_path/is/quotesbot**3. launch ScrapyRT**.. code-block:: shell    &gt; scrapyrt**4. run your spiders**.. code-block:: shell    &gt; curl &quot;localhost:9080/crawl.json?spider_name=toscrape-css&amp;url=http://quotes.toscrape.com/&quot;**5. run more complex query, e.g. specify callback for Scrapy request and zipcode argument for spider**.. code-block:: shell    &gt;  curl --data '{&quot;request&quot;: {&quot;url&quot;: &quot;http://quotes.toscrape.com/page/2/&quot;, &quot;callback&quot;:&quot;some_callback&quot;}, &quot;spider_name&quot;: &quot;toscrape-css&quot;, &quot;crawl_args&quot;: {&quot;zipcode&quot;:&quot;14000&quot;}}' http://localhost:9080/crawl.json -vScrapyrt will look for ``scrapy.cfg`` file to determine your project settings,and will raise error if it won't find one.  Note that you need to have allyour project requirements installed.Note====* Project is not a replacement for `Scrapyd &lt;https://scrapyd.readthedocs.io/en/stable/&gt;`_ or `Scrapy Cloud &lt;https://www.zyte.com/scrapy-cloud/&gt;`_ or other infrastructure to run long running crawls* Not suitable for long running spiders, good for spiders that will fetch one response from some website and return items quicklyDocumentation=============`Documentation is available on readthedocs &lt;http://scrapyrt.readthedocs.org/en/latest/index.html&gt;`_.Support=======Open source support is provided here in Github. Please `create a questionissue`_ (ie. issue with &quot;question&quot; label).Commercial support is also available by `Zyte`_... _create a question issue: https://github.com/scrapinghub/scrapyrt/issues/new?labels=question.. _Zyte: http://zyte.comLicense=======ScrapyRT is offered under `BSD 3-Clause license &lt;https://en.wikipedia.org/wiki/BSD_licenses#3-clause_license_(%22BSD_License_2.0%22,_%22Revised_BSD_License%22,_%22New_BSD_License%22,_or_%22Modified_BSD_License%22)&gt;`_.Development===========Development taking place on `Github &lt;https://github.com/scrapinghub/scrapyrt&gt;`_.</longdescription>
</pkgmetadata>