<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Amos and JEstimator[![Unittests &amp; Auto-publish](https://github.com/google-research/jestimator/actions/workflows/pytest_and_autopublish.yml/badge.svg)](https://github.com/google-research/jestimator/actions/workflows/pytest_and_autopublish.yml)[![PyPI version](https://badge.fury.io/py/jestimator.svg)](https://badge.fury.io/py/jestimator)*This is not an officially supported Google product.*This is the source code for the paper &quot;[Amos: An Adam-style Optimizer withAdaptive Weight Decay towards Model-OrientedScale](https://arxiv.org/abs/2210.11693)&quot;.It implements **Amos**, an optimizer compatible with the[optax](https://github.com/deepmind/optax) library, and **JEstimator**, alight-weight library with a `tf.Estimator`-like interface to manage[T5X](https://github.com/google-research/t5x)-compatible checkpoints for machinelearning programs in [Jax](https://github.com/google/jax), which we use to runexperiments in the paper.## Quick Start```pip install jestimator```It will install the Amos optimizer implemented in the jestimator lib.## Usage of AmosThis implementation of Amos is used with [Jax](https://github.com/google/jax), ahigh-performance numerical computing library with automatic differentiation, formachine learning research. The API of Amos is compatible with[optax](https://github.com/deepmind/optax), a library of Jax optimizers(hopefully Amos will be integrated into optax in the near future).In order to demonstrate the usage, we will apply Amos to MNIST. It is based onFlax's official[MNIST Example](https://github.com/google/flax/tree/main/examples/mnist), andyou can find the code in a jupyter notebook[here](https://github.com/google-research/jestimator/tree/main/jestimator/models/mnist/mnist.ipynb).### 1. Imports```import jaximport jax.numpy as jnp                # JAX NumPyfrom jestimator import amos            # The Amos optimizer implementationfrom jestimator import amos_helper     # Helper module for Amosfrom flax import linen as nn           # The Linen APIfrom flax.training import train_state  # Useful dataclass to keep train stateimport mathimport tensorflow_datasets as tfds     # TFDS for MNISTfrom sklearn.metrics import accuracy_score```### 2. Load data```def get_datasets():  &quot;&quot;&quot;Load MNIST train and test datasets into memory.&quot;&quot;&quot;  ds_builder = tfds.builder('mnist')  ds_builder.download_and_prepare()  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))  train_ds['image'] = jnp.float32(train_ds['image']) / 255.  test_ds['image'] = jnp.float32(test_ds['image']) / 255.  return train_ds, test_ds```### 3. Build model```class CNN(nn.Module):  &quot;&quot;&quot;A simple CNN model.&quot;&quot;&quot;  @nn.compact  def __call__(self, x):    x = nn.Conv(features=32, kernel_size=(3, 3))(x)    x = nn.relu(x)    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))    x = nn.Conv(features=64, kernel_size=(3, 3))(x)    x = nn.relu(x)    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))    x = x.reshape((x.shape[0], -1))  # flatten    x = nn.Dense(features=256)(x)    x = nn.relu(x)    x = nn.Dense(features=10)(x)    return x  def classify_xe_loss(self, x, labels):    # Labels read from the tfds MNIST are integers from 0 to 9.    # Logits are arrays of size 10.    logits = self(x)    logits = jax.nn.log_softmax(logits)    labels_ = jnp.expand_dims(labels, -1)    llh_ = jnp.take_along_axis(logits, labels_, axis=-1)    loss = -jnp.sum(llh_)    return loss```### 4. Create train stateA `TrainState` object keeps the model parameters and optimizer states, and canbe checkpointed into files.We create the model and optimizer in this function.**For the optimizer, we use Amos here.** The following hyper-parameters are set:*   *learning_rate*:       The global learning rate.*   *eta_fn*:              The model-specific 'eta'.*   *shape_fn*:            Memory reduction setting.*   *beta*:                Rate for running average of gradient squares.*   *clip_value*:          Gradient clipping for stable training.The global learning rate is usually set to the 1/sqrt(N), where N is the numberof batches in the training data. For MNIST, we have 60k training examples andbatch size is 32. So learning_rate=1/sqrt(60000/32).The model-specific 'eta_fn' requires a function that, given a variable name andshape, returns a float indicating the expected scale of that variable. Hopefullyin the near future we will have libraries that can automatically calculate this'eta_fn' from the modeling code; but for now we have to specify it manually.One can use the amos_helper.params_fn_from_assign_map() helper function tocreate 'eta_fn' from an assign_map. An assign_map is a dict which maps regexrules to a value or simple Python expression. It will find the first regex rulewhich matches the name of a variable, and evaluate the Python expression ifnecessary to return the value. See our example below.The 'shape_fn' similarly requires a function that, given a variable name andshape, returns a reduced shape for the corresponding slot variables. We can usethe amos_helper.params_fn_from_assign_map() helper function to create 'shape_fn'from an assign_map as well.'beta' is the exponential decay rate for running average of gradient squares. Weset it to 0.98 here.'clip_value' is the gradient clipping value, which should match the magnitude ofthe loss function. If the loss function is a sum of cross-entropy, then weshould set 'clip_value' to the sqrt of the number of labels.Please refer to our [paper](https://arxiv.org/abs/2210.11693) for more detailsof the hyper-parameters.```def get_train_state(rng):  model = CNN()  dummy_x = jnp.ones([1, 28, 28, 1])  params = model.init(rng, dummy_x)  eta_fn = amos_helper.params_fn_from_assign_map(      {          '.*/bias': 0.5,          '.*Conv_0/kernel': 'sqrt(8/prod(SHAPE[:-1]))',          '.*Conv_1/kernel': 'sqrt(2/prod(SHAPE[:-1]))',          '.*Dense_0/kernel': 'sqrt(2/SHAPE[0])',          '.*Dense_1/kernel': 'sqrt(1/SHAPE[0])',      },      eval_str_value=True,  )  shape_fn = amos_helper.params_fn_from_assign_map(      {          '.*Conv_[01]/kernel': '(1, 1, 1, SHAPE[-1])',          '.*Dense_0/kernel': '(1, SHAPE[1])',          '.*': (),      },      eval_str_value=True,  )  optimizer = amos.amos(      learning_rate=1/math.sqrt(60000/32),      eta_fn=eta_fn,      shape_fn=shape_fn,      beta=0.98,      clip_value=math.sqrt(32),  )  return train_state.TrainState.create(      apply_fn=model.apply, params=params, tx=optimizer)```### 5. Train stepUse JAX’s @jit decorator to just-in-time compile the function for betterperformance.```@jax.jitdef train_step(batch, state):  grad_fn = jax.grad(state.apply_fn)  grads = grad_fn(      state.params,      batch['image'],      batch['label'],      method=CNN.classify_xe_loss)  return state.apply_gradients(grads=grads)```### 6. Infer stepUse JAX’s @jit decorator to just-in-time compile the function for betterperformance.```@jax.jitdef infer_step(batch, state):  logits = state.apply_fn(state.params, batch['image'])  return jnp.argmax(logits, -1)```### 7. MainRun the training loop and evaluate on test set.```train_ds, test_ds = get_datasets()rng = jax.random.PRNGKey(0)rng, init_rng = jax.random.split(rng)state = get_train_state(init_rng)del init_rng  # Must not be used anymore.num_epochs = 9for epoch in range(1, num_epochs + 1):  # Use a separate PRNG key to permute image data during shuffling  rng, input_rng = jax.random.split(rng)  perms = jax.random.permutation(input_rng, 60000)  del input_rng  perms = perms.reshape((60000 // 32, 32))  for perm in perms:    batch = {k: v[perm, ...] for k, v in train_ds.items()}    state = train_step(batch, state)  pred = jax.device_get(infer_step(test_ds, state))  accuracy = accuracy_score(test_ds['label'], pred)  print('epoch: %d, test accuracy: %.2f' % (epoch, accuracy * 100))```After 9 epochs, we should get 99.26 test accuracy. If you made it, congrats!## JEstimatorWith JEstimator, you can build your model mostly similar to the MNIST exampleabove, but without writing code for the &quot;Main&quot; section; JEstimator will serve asthe entry point for your model, automatically handle checkpointing in atrain/eval-once/eval-while-training-and-save-the-best/predict mode, and set upprofiling, tensorboard, and logging.In addition, JEstimator supports model partitioning which is required fortraining very large models across multiple TPU pods. It supports a[T5X](https://github.com/google-research/t5x)-compatible checkpoint format thatsaves and restores checkpoints in a distributed manner, which is suitable forlarge multi-pod models.In order to run models with JEstimator, we need to install[T5X](https://github.com/google-research/t5x#installation) and[FlaxFormer](https://github.com/google/flaxformer):```git clone --branch=main https://github.com/google-research/t5xcd t5xpython3 -m pip install -e .cd ..git clone --branch=main https://github.com/google/flaxformercd flaxformerpip3 install .cd ..```Then, clone this repo to get the JEstimator code:```git clone --branch=main https://github.com/google-research/jestimatorcd jestimator```Now, we can test a toy linear regression model:```PYTHONPATH=. python3 jestimator/models/linear_regression/linear_regression_test.py```## MNIST Example in JEstimatorWe provide this[MNIST Example](https://github.com/google-research/jestimator/tree/main/jestimator/models/mnist/mnist.py)to demonstrate how to write modeling code with JEstimator. It is much like theexample above, but with a big advantage that, a config object is passed aroundto collect information from global flags and the dataset, in order todynamically setup modeling. This makes it easier to apply the model to different datasets; for example, one can immediately try the [emnist](https://www.tensorflow.org/datasets/catalog/emnist) or [eurosat](https://www.tensorflow.org/datasets/catalog/eurosat) datasets simply by changing a command-line argument, without modifying the code.With the following command, we can start a job to train on MNIST, log every 100steps, and save the checkpoints to $HOME/experiments/mnist/models:```PYTHONPATH=. python3 jestimator/estimator.py \  --module_imp=&quot;jestimator.models.mnist.mnist&quot; \  --module_config=&quot;jestimator/models/mnist/mnist.py&quot; \  --train_pattern=&quot;tfds://mnist/split=train&quot; \  --model_dir=&quot;$HOME/experiments/mnist/models&quot; \  --train_batch_size=32 \  --train_shuffle_buf=4096 \  --train_epochs=9 \  --check_every_steps=100 \  --max_ckpt=20 \  --save_every_steps=1000 \  --module_config.warmup=2000 \  --module_config.amos_beta=0.98```Meanwhile, we can start a job to monitor the $HOME/experiments/mnist/modelsfolder, evaluate on MNIST test set, and save the model with the highestaccuracy:```PYTHONPATH=. python3 jestimator/estimator.py \  --module_imp=&quot;jestimator.models.mnist.mnist&quot; \  --module_config=&quot;jestimator/models/mnist/mnist.py&quot; \  --eval_pattern=&quot;tfds://mnist/split=test&quot; \  --model_dir=&quot;$HOME/experiments/mnist/models&quot; \  --eval_batch_size=32 \  --mode=&quot;eval_wait&quot; \  --check_ckpt_every_secs=1 \  --save_high=&quot;test_accuracy&quot;```At the same time, we can start a tensorboard to monitor the process:```tensorboard --logdir $HOME/experiments/mnist/models```## More JEstimator ModelsHere are the recipes to run several models in JEstimator.### LSTM on PTBTo train a single layer LSTM model on PTB:```PYTHONPATH=. python3 jestimator/estimator.py \  --module_imp=&quot;jestimator.models.lstm.lm&quot; \  --module_config=&quot;jestimator/models/lstm/lm.py&quot; \  --module_config.vocab_path=&quot;jestimator/models/lstm/ptb/vocab.txt&quot; \  --train_pattern=&quot;jestimator/models/lstm/ptb/ptb.train.txt&quot; \  --model_dir=&quot;$HOME/models/ptb_lstm&quot; \  --train_batch_size=64 \  --train_consecutive=113 \  --train_shuffle_buf=4096 \  --max_train_steps=200000 \  --check_every_steps=1000 \  --max_ckpt=20 \  --module_config.opt_config.optimizer=&quot;amos&quot; \  --module_config.opt_config.learning_rate=0.01 \  --module_config.opt_config.beta=0.98 \  --module_config.opt_config.momentum=0.0```To evaluate the model on validation set:```PYTHONPATH=. python3 jestimator/estimator.py \  --module_imp=&quot;jestimator.models.lstm.lm&quot; \  --module_config=&quot;jestimator/models/lstm/lm.py&quot; \  --module_config.vocab_path=&quot;jestimator/models/lstm/ptb/vocab.txt&quot; \  --eval_pattern=&quot;jestimator/models/lstm/ptb/ptb.valid.txt&quot; \  --model_dir=&quot;$HOME/models/ptb_lstm&quot; \  --eval_batch_size=1```</longdescription>
</pkgmetadata>