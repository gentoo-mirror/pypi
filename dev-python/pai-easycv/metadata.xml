<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;[![PyPI](https://img.shields.io/pypi/v/pai-easycv)](https://pypi.org/project/pai-easycv/)[![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/)[![license](https://img.shields.io/github/license/alibaba/EasyCV.svg)](https://github.com/open-mmlab/mmdetection/blob/master/LICENSE)[![open issues](https://isitmaintained.com/badge/open/alibaba/EasyCV.svg)](https://github.com/alibaba/EasyCV/issues)[![GitHub pull-requests](https://img.shields.io/github/issues-pr/alibaba/EasyCV.svg)](https://GitHub.com/alibaba/EasyCV/pull/)[![GitHub latest commit](https://badgen.net/github/last-commit/alibaba/EasyCV)](https://GitHub.com/alibaba/EasyCV/commit/)&lt;!-- [![GitHub contributors](https://img.shields.io/github/contributors/alibaba/EasyCV.svg)](https://GitHub.com/alibaba/EasyCV/graphs/contributors/) --&gt;&lt;!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) --&gt;&lt;/div&gt;# EasyCVEnglish | [简体中文](README_zh-CN.md)## IntroductionEasyCV is an all-in-one computer vision toolbox based on PyTorch, mainly focuses on self-supervised learning, transformer based models, and major CV tasks including image classification, metric-learning, object detection, pose estimation, and so on.### Major features- **SOTA SSL Algorithms**  EasyCV provides state-of-the-art algorithms in self-supervised learning based on contrastive learning such as SimCLR, MoCO V2, Swav, DINO, and also MAE based on masked image modeling. We also provide standard benchmarking tools for ssl model evaluation.- **Vision Transformers**  EasyCV aims to provide an easy way to use the off-the-shelf SOTA transformer models trained either using supervised learning or self-supervised learning, such as ViT, Swin Transformer, and DETR Series. More models will be added in the future. In addition, we support all the pretrained models from [timm](https://github.com/rwightman/pytorch-image-models).- **Functionality &amp; Extensibility**  In addition to SSL, EasyCV also supports image classification, object detection, metric learning, and more areas will be supported in the future. Although covering different areas,  EasyCV decomposes the framework into different components such as dataset, model and running hook, making it easy to add new components and combining it with existing modules.  EasyCV provides simple and comprehensive interface for inference. Additionally, all models are supported on [PAI-EAS](https://help.aliyun.com/document_detail/113696.html), which can be easily deployed as online service and support automatic scaling and service monitoring.- **Efficiency**  EasyCV supports multi-gpu and multi-worker training. EasyCV uses [DALI](https://github.com/NVIDIA/DALI) to accelerate data io and preprocessing process, and uses [TorchAccelerator](https://github.com/alibaba/EasyCV/tree/master/docs/source/tutorials/torchacc.md) and fp16 to accelerate training process. For inference optimization, EasyCV exports model using jit script, which can be optimized by [PAI-Blade](https://help.aliyun.com/document_detail/205134.html)## What's New[🔥 2023.03.06]* 06/03/2023 EasyCV v0.10.0 was released.- Add segmentation model STDC- Add skeleton based video recognition model STGCN- Support ReID and Multi-len MOT[🔥 2023.01.17]* 17/01/2023 EasyCV v0.9.0 was released.- Support Single-lens MOT- Support video recognition (X3D, SWIN-video)[🔥 2022.12.02]* 02/12/2022 EasyCV v0.8.0 was released.- bevformer-base NDS increased by 0.8 on nuscenes val, training speed increased by 10%, and inference speed increased by 40%.- Support Objects365 pretrain and Adding the DINO++ model can achieve an accuracy of 63.4mAP at a model scale of 200M(Under the same scale, the accuracy is the best).[🔥 2022.08.31] We have released our YOLOX-PAI that achieves SOTA results within 40~50 mAP (less than 1ms). And we also provide a convenient and fast export/predictor api for end2end object detection. To get a quick start of YOLOX-PAI, click [here](docs/source/tutorials/yolox.md)!* 31/08/2022 EasyCV v0.6.0 was released.  -  Release YOLOX-PAI which achieves SOTA results within 40~50 mAP (less than 1ms)  -  Add detection algo DINO which achieves 58.5 mAP on COCO  -  Add mask2former algo  -  Releases imagenet1k, imagenet22k, coco, lvis, voc2012 data with BaiduDisk to accelerate downloadingPlease refer to [change_log.md](docs/source/change_log.md) for more details and history.## Technical ArticlesWe have a series of technical articles on the functionalities of EasyCV.* [EasyCV开源｜开箱即用的视觉自监督+Transformer算法库](https://zhuanlan.zhihu.com/p/505219993)* [MAE自监督算法介绍和基于EasyCV的复现](https://zhuanlan.zhihu.com/p/515859470)* [基于EasyCV复现ViTDet：单层特征超越FPN](https://zhuanlan.zhihu.com/p/528733299)* [基于EasyCV复现DETR和DAB-DETR，Object Query的正确打开方式](https://zhuanlan.zhihu.com/p/543129581)* [YOLOX-PAI: 加速YOLOX, 比YOLOv6更快更强](https://zhuanlan.zhihu.com/p/560597953)* [EasyCV带你复现更好更快的自监督算法-FastConvMAE](https://zhuanlan.zhihu.com/p/566988235)* [EasyCV DataHub 提供多领域视觉数据集下载，助力模型生产](https://zhuanlan.zhihu.com/p/572593950)* [使用EasyCV Mask2Former轻松实现图像分割](https://zhuanlan.zhihu.com/p/583831421)## InstallationPlease refer to the installation section in [quick_start.md](docs/source/quick_start.md) for installation.## Get StartedPlease refer to [quick_start.md](docs/source/quick_start.md) for quick start. We also provides tutorials for more usages.* [self-supervised learning](docs/source/tutorials/ssl.md)* [image classification](docs/source/tutorials/cls.md)* [metric learning](docs/source/tutorials/metric_learning.md)* [object detection with yolox-pai](docs/source/tutorials/yolox.md)* [model compression with yolox](docs/source/tutorials/compression.md)* [using torchacc](docs/source/tutorials/torchacc.md)* [file io for local and oss files](docs/source/tutorials/file.md)* [using mmdetection model in EasyCV](docs/source/tutorials/mmdet_models_usage_guide.md)* [batch prediction tools](docs/source/tutorials/predict.md)notebook* [self-supervised learning](docs/source/tutorials/EasyCV图像自监督训练-MAE.ipynb)* [image classification](docs/source/tutorials/EasyCV图像分类resnet50.ipynb)* [object detection with yolox-pai](docs/source/tutorials/EasyCV图像检测YoloX.ipynb)* [metric learning](docs/source/tutorials/EasyCV度量学习resnet50.ipynb)## Model Zoo&lt;div align=&quot;center&quot;&gt;  &lt;b&gt;Architectures&lt;/b&gt;&lt;/div&gt;&lt;table align=&quot;center&quot;&gt;  &lt;tbody&gt;    &lt;tr align=&quot;center&quot;&gt;      &lt;td&gt;        &lt;b&gt;Self-Supervised Learning&lt;/b&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;b&gt;Image Classification&lt;/b&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;b&gt;Object Detection&lt;/b&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;b&gt;Segmentation&lt;/b&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;b&gt;Object Detection 3D&lt;/b&gt;      &lt;/td&gt;    &lt;/tr&gt;    &lt;tr valign=&quot;top&quot;&gt;      &lt;td&gt;        &lt;ul&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/byol&quot;&gt;BYOL (NeurIPS'2020)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/dino&quot;&gt;DINO (ICCV'2021)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/mixco&quot;&gt;MiXCo (NeurIPS'2020)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/moby&quot;&gt;MoBY (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/mocov2&quot;&gt;MoCov2 (ArXiv'2020)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/simclr&quot;&gt;SimCLR (ICML'2020)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/swav&quot;&gt;SwAV (NeurIPS'2020)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/mae&quot;&gt;MAE (CVPR'2022)&lt;/a&gt;&lt;/li&gt;            &lt;li&gt;&lt;a href=&quot;configs/selfsup/fast_convmae&quot;&gt;FastConvMAE (ArXiv'2022)&lt;/a&gt;&lt;/li&gt;      &lt;/ul&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;ul&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/resnet&quot;&gt;ResNet (CVPR'2016)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/resnext&quot;&gt;ResNeXt (CVPR'2017)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/hrnet&quot;&gt;HRNet (CVPR'2019)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/vit&quot;&gt;ViT (ICLR'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/swint&quot;&gt;SwinT (ICCV'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/efficientformer&quot;&gt;EfficientFormer (ArXiv'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/deit&quot;&gt;DeiT (ICML'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/xcit&quot;&gt;XCiT (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/tnt&quot;&gt;TNT (NeurIPS'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/convit&quot;&gt;ConViT (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/cait&quot;&gt;CaiT (ICCV'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/levit&quot;&gt;LeViT (ICCV'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/convnext&quot;&gt;ConvNeXt (CVPR'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/resmlp&quot;&gt;ResMLP (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/coat&quot;&gt;CoaT (ICCV'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/convmixer&quot;&gt;ConvMixer (ICLR'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/mlp-mixer&quot;&gt;MLP-Mixer (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/nest&quot;&gt;NesT (AAAI'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/pit&quot;&gt;PiT (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/twins&quot;&gt;Twins (NeurIPS'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/timm/shuffle_transformer&quot;&gt;Shuffle Transformer (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/deitiii&quot;&gt;DeiT III (ECCV'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/classification/imagenet/deit&quot;&gt;Hydra Attention (2022)&lt;/a&gt;&lt;/li&gt;        &lt;/ul&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;ul&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/fcos&quot;&gt;FCOS (ICCV'2019)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/yolox&quot;&gt;YOLOX (ArXiv'2021)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/yolox&quot;&gt;YOLOX-PAI (ArXiv'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/detr&quot;&gt;DETR (ECCV'2020)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/dab_detr&quot;&gt;DAB-DETR (ICLR'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/dab_detr&quot;&gt;DN-DETR (CVPR'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/dino&quot;&gt;DINO (ArXiv'2022)&lt;/a&gt;&lt;/li&gt;        &lt;/ul&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;/ul&gt;          &lt;li&gt;&lt;b&gt;Instance Segmentation&lt;/b&gt;&lt;/li&gt;        &lt;ul&gt;        &lt;ul&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/mask_rcnn&quot;&gt;Mask R-CNN (ICCV'2017)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/detection/vitdet&quot;&gt;ViTDet (ArXiv'2022)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/segmentation/mask2former&quot;&gt;Mask2Former (CVPR'2022)&lt;/a&gt;&lt;/li&gt;        &lt;/ul&gt;        &lt;/ul&gt;        &lt;/ul&gt;          &lt;li&gt;&lt;b&gt;Semantic Segmentation&lt;/b&gt;&lt;/li&gt;        &lt;ul&gt;        &lt;ul&gt;          &lt;li&gt;&lt;a href=&quot;configs/segmentation/fcn&quot;&gt;FCN (CVPR'2015)&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href=&quot;configs/segmentation/upernet&quot;&gt;UperNet (ECCV'2018)&lt;/a&gt;&lt;/li&gt;        &lt;/ul&gt;        &lt;/ul&gt;        &lt;/ul&gt;          &lt;li&gt;&lt;b&gt;Panoptic Segmentation&lt;/b&gt;&lt;/li&gt;        &lt;ul&gt;        &lt;ul&gt;          &lt;li&gt;&lt;a href=&quot;configs/segmentation/mask2former&quot;&gt;Mask2Former (CVPR'2022)&lt;/a&gt;&lt;/li&gt;        &lt;/ul&gt;        &lt;/ul&gt;      &lt;/ul&gt;      &lt;/td&gt;      &lt;td&gt;        &lt;ul&gt;            &lt;li&gt;&lt;a href=&quot;configs/detection3d/bevformer&quot;&gt;BEVFormer (ECCV'2022)&lt;/a&gt;&lt;/li&gt;      &lt;/ul&gt;      &lt;/td&gt;    &lt;/tr&gt;&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;Please refer to the following model zoo for more details.- [self-supervised learning model zoo](docs/source/model_zoo_ssl.md)- [classification model zoo](docs/source/model_zoo_cls.md)- [detection model zoo](docs/source/model_zoo_det.md)- [detection3d model zoo](docs/source/model_zoo_det3d.md)- [segmentation model zoo](docs/source/model_zoo_seg.md)- [pose model zoo](docs/source/model_zoo_pose.md)## Data HubEasyCV have collected dataset info for different senarios, making it easy for users to finetune or evaluate models in EasyCV model zoo.Please refer to [data_hub.md](docs/source/data_hub.md).## LicenseThis project is licensed under the [Apache License (Version 2.0)](LICENSE). This toolkit also contains various third-party components and some code modified from other repos under other open source licenses. See the [NOTICE](NOTICE) file for more information.## ContactThis repo is currently maintained by PAI-CV team, you can contact us by* Dingding group number: 41783266* Email: easycv@list.alibaba-inc.com### Enterprise ServiceIf you need EasyCV enterprise service support, or purchase cloud product services, you can contact us by DingDing Group.![dingding_qrcode](https://user-images.githubusercontent.com/4771825/165244727-b5d69628-97a6-4e2a-a23f-0c38a8d29341.jpg)</longdescription>
</pkgmetadata>