<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>GCSFS=====A Python filesystem abstraction of Google Cloud Storage (GCS) implemented as a `PyFilesystem2 &lt;https://github.com/PyFilesystem/pyfilesystem2&gt;`__ extension... image:: https://img.shields.io/pypi/v/fs-gcsfs.svg    :target: https://pypi.org/project/fs-gcsfs/.. image:: https://img.shields.io/pypi/pyversions/fs-gcsfs.svg    :target: https://pypi.org/project/fs-gcsfs/.. image:: https://travis-ci.org/Othoz/gcsfs.svg?branch=master    :target: https://travis-ci.org/Othoz/gcsfs.. image:: https://readthedocs.org/projects/fs-gcsfs/badge/?version=latest    :target: https://fs-gcsfs.readthedocs.io/en/latest/?badge=latestWith GCSFS, you can interact with `Google Cloud Storage &lt;https://cloud.google.com/storage/&gt;`__ as if it was a regular filesystem.Apart from the nicer interface, this will highly decouple your code from the underlying storage mechanism: Exchanging the storage backend with an`in-memory filesystem &lt;https://pyfilesystem2.readthedocs.io/en/latest/reference/memoryfs.html&gt;`__ for testing or any otherfilesystem like `S3FS &lt;https://github.com/pyfilesystem/s3fs&gt;`__ becomes as easy as replacing ``gs://bucket_name`` with ``mem://`` or ``s3://bucket_name``.For a full reference on all the PyFilesystem possibilities, take a look at the`PyFilesystem Docs &lt;https://pyfilesystem2.readthedocs.io/en/latest/index.html&gt;`__!Documentation--------------  `GCSFS Documentation &lt;http://fs-gcsfs.readthedocs.io/en/latest/&gt;`__-  `PyFilesystem Wiki &lt;https://www.pyfilesystem.org&gt;`__-  `PyFilesystem Reference &lt;https://docs.pyfilesystem.org/en/latest/reference/base.html&gt;`__Installing----------Install the latest GCSFS version by running::    $ pip install fs-gcsfsOr in case you are using conda::    $ conda install -c conda-forge fs-gcsfsExamples--------Instantiating a filesystem on Google Cloud Storage (for a full reference visit the`Documentation &lt;http://fs-gcsfs.readthedocs.io/en/latest/index.html#reference&gt;`__):.. code-block:: python    from fs_gcsfs import GCSFS    gcsfs = GCSFS(bucket_name=&quot;mybucket&quot;)Alternatively you can use a `FS URL &lt;https://pyfilesystem2.readthedocs.io/en/latest/openers.html&gt;`__ to open up a filesystem:.. code-block:: python    from fs import open_fs    gcsfs = open_fs(&quot;gs://mybucket/root_path?project=test&amp;api_endpoint=http%3A//localhost%3A8888&amp;strict=False&quot;)Supported query parameters are:- `project` (str): Google Cloud project to use- `api_endpoint` (str): URL-encoded endpoint that will be passed to the GCS client's `client_options &lt;https://googleapis.dev/python/google-api-core/latest/client_options.html#google.api_core.client_options.ClientOptions&gt;`__- `strict` (&quot;True&quot; or &quot;False&quot;): Whether GCSFS will be opened in strict modeYou can use GCSFS like your local filesystem:.. code-block:: python    &gt;&gt;&gt; from fs_gcsfs import GCSFS    &gt;&gt;&gt; gcsfs = GCSFS(bucket_name=&quot;mybucket&quot;)    &gt;&gt;&gt; gcsfs.tree()    ├── foo    │   ├── bar    │   │   ├── file1.txt    │   │   └── file2.csv    │   └── baz    │       └── file3.txt    └── file4.json    &gt;&gt;&gt; gcsfs.listdir(&quot;foo&quot;)    [&quot;bar&quot;, &quot;baz&quot;]    &gt;&gt;&gt; gcsfs.isdir(&quot;foo/bar&quot;)    TrueUploading a file is as easy as:.. code-block:: python    from fs_gcsfs import GCSFS    gcsfs = GCSFS(bucket_name=&quot;mybucket&quot;)    with open(&quot;local/path/image.jpg&quot;, &quot;rb&quot;) as local_file:        with gcsfs.open(&quot;path/on/bucket/image.jpg&quot;, &quot;wb&quot;) as gcs_file:            gcs_file.write(local_file.read())You can even sync an entire bucket on your local filesystem by using PyFilesystem's utility methods:.. code-block:: python    from fs_gcsfs import GCSFS    from fs.osfs import OSFS    from fs.copy import copy_fs    gcsfs = GCSFS(bucket_name=&quot;mybucket&quot;)    local_fs = OSFS(&quot;local/path&quot;)    copy_fs(gcsfs, local_fs)For exploring all the possibilities of GCSFS and other filesystems implementing the PyFilesystem interface, we recommend visiting the official`PyFilesystem Docs &lt;https://pyfilesystem2.readthedocs.io/en/latest/index.html&gt;`__!Development-----------To develop on this project make sure you have `pipenv &lt;https://pipenv.readthedocs.io/en/latest/&gt;`__ installedand run the following from the root directory of the project::    $ pipenv install --dev --threeThis will create a virtualenv with all packages and dev-packages installed.Tests-----All CI tests run against an actual GCS bucket provided by `Othoz &lt;http://othoz.com/&gt;`__.In order to run the tests against your own bucket,make sure to set up a `Service Account &lt;https://cloud.google.com/iam/docs/service-accounts&gt;`__ with all necessary permissions:- storage.objects.get- storage.objects.list- storage.objects.create- storage.objects.update- storage.objects.deleteAll five permissions listed above are e.g. included in the `predefined Cloud Storage IAM Role &lt;https://cloud.google.com/storage/docs/access-control/iam-roles&gt;`__ ``roles/storage.objectAdmin``.Expose your bucket name as an environment variable ``$TEST_BUCKET`` and run the tests via::    $ pipenv run pytestNote that the tests mostly wait for I/O, therefore it makes sense to highly parallelize them with `xdist &lt;https://github.com/pytest-dev/pytest-xdist&gt;`__, e.g. by running the tests with::    $ pipenv run pytest -n 10Credits-------Credits go to `S3FS &lt;https://github.com/PyFilesystem/s3fs&gt;`__ which was the main source of inspiration and shares a lot of code with GCSFS.</longdescription>
</pkgmetadata>