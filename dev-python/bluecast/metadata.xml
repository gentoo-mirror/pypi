<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># BlueCast[![codecov](https://codecov.io/gh/ThomasMeissnerDS/BlueCast/branch/main/graph/badge.svg?token=XRIS04O097)](https://codecov.io/gh/ThomasMeissnerDS/BlueCast)[![Codecov workflow](https://github.com/ThomasMeissnerDS/BlueCast/actions/workflows/workflow.yaml/badge.svg)](https://github.com/ThomasMeissnerDS/BlueCast/actions/workflows/workflow.yaml)[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white)](https://github.com/pre-commit/pre-commit)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)[![pydocstyle](https://img.shields.io/badge/pydocstyle-enabled-AD4CD3)](http://www.pydocstyle.org/en/stable/)[![Documentation Status](https://readthedocs.org/projects/bluecast/badge/?version=latest)](https://bluecast.readthedocs.io/en/latest/?badge=latest)[![PyPI version](https://badge.fury.io/py/bluecast.svg)](https://pypi.python.org/pypi/bluecast/)[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)[![python](https://img.shields.io/badge/Python-3.9-3776AB.svg?style=flat&amp;logo=python&amp;logoColor=white)](https://www.python.org)[![python](https://img.shields.io/badge/Python-3.10-3776AB.svg?style=flat&amp;logo=python&amp;logoColor=white)](https://www.python.org)[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)A lightweight and fast auto-ml library.BlueCast focuses on a few model architectures (on default Xgboostonly) and a few preprocessing options (only what isneeded for Xgboost). This allows for a much faster developmentcycle and a much more stable codebase while also having as few dependenciesas possible for the library. Despite being lightweight in its core BlueCastoffers high customization options for advanced users. Findthe full documentation [here](https://bluecast.readthedocs.io/en/latest/).&lt;!-- toc --&gt;* [Installation](#installation)  * [Installation for end users](#installation-for-end-users)  * [Installation for developers](#installation-for-developers)* [General usage](#general-usage)  * [Basic usage](#basic-usage)  * [Advanced usage](#advanced-usage)    * [Explanatory analysis](#explanatory-analysis)    * [Leakage detection](#leakage-detection)    * [Enable cross-validation](#enable-cross-validation)    * [Enable even more overfitting-robust cross-validation](#enable-even-more-overfitting-robust-cross-validation)    * [Gaining extra performance](#gaining-extra-performance)    * [Use multi-model blended pipeline](#use-multi-model-blended-pipeline)    * [Categorical encoding](#categorical-encoding)    * [Custom training configuration](#custom--training-configuration)    * [Custom preprocessing](#custom-preprocessing)    * [Custom feature selection](#custom-feature-selection)    * [Custom ML model](#custom-ml-model)    * [Using the inbuilt ExperientTracker](#using-the-inbuilt-experienttracker)* [Convenience features](#convenience-features)* [Code quality](#code-quality)* [Documentation](#documentation)* [Kaggle competition results and example notebooks](#kaggle-competition-results-and-example-notebooks)* [How to contribute](#how-to-contribute)* [Meta](#meta)&lt;!-- tocstop --&gt;## Installation### Installation for end usersFrom PyPI:```shpip install bluecast```Using a fresh environment with Python 3.9 or higher is recommended. We consciouslydo not support Python 3.8 or lower to prevent the usage of outdated Python versionsand issues connected to it.### Installation for developers* Clone the repository:* Create a new conda environment with Python 3.9 or higher* run `pip install poetry` to install poetry as dependency manager* run `poetry install` to install all dependencies## General usage### Basic usageThe module blueprints contains the main functionality of the library. The mainentry point is the `Blueprint` class. It already includes needed preprocessing(including some convenience functionality like feature type detection)and model hyperparameter tuning.```shfrom bluecast.blueprints.cast import BlueCastautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```BlueCast has simple utilities to save and load your pipeline:```shfrom bluecast.general_utils.general_utils import save_to_production, load_for_production# save pipeline including trackersave_to_production(automl, &quot;/kaggle/working/&quot;, &quot;bluecast_cv_pipeline&quot;)# in production or for further experiments this can be loaded againautoml = load_for_production(&quot;/kaggle/working/&quot;, &quot;bluecast_cv_pipeline&quot;)```### Advanced usage#### Explanatory analysisBlueCast offers a simple way to get a first overview of the data:```shfrom bluecast.eda.analyse import (    bi_variate_plots,    correlation_heatmap,    correlation_to_target,    plot_pca,    plot_theil_u_heatmap,    plot_tsne,    univariate_plots,    check_unique_values,    plot_null_percentage,    mutual_info_to_target.)from bluecast.preprocessing.feature_types import FeatureTypeDetector# Here we automatically detect the numeric columnsfeat_type_detector = FeatureTypeDetector()train_data = feat_type_detector.fit_transform_feature_types(train_data)# show univariate plotsunivariate_plots(        train_data.loc[:, feat_type_detector.num_columns],  # here the target column EC1 is already included        &quot;EC1&quot;,    )# show bi-variate plotsbi_variate_plots(    train_data.loc[:, feat_type_detector.num_columns],      &quot;EC1&quot;      )# show correlation heatmapcorrelation_heatmap(train_data.loc[:, feat_type_detector.num_columns])# show mutual information of categorical features to target# features are expected to be numerical formatextra_params = {&quot;random_state&quot;: 30}mutual_info_to_target(train_data.loc[:, feat_type_detector.num_columns], &quot;EC1&quot;, **extra_params)# show correlation to targetcorrelation_to_target(    train_data.loc[:, feat_type_detector.num_columns],      &quot;EC1&quot;,      )# show feature space after principal component analysisplot_pca(    train_data.loc[:, feat_type_detector.num_columns],    &quot;target&quot;    )# show feature space after t-SNEplot_tsne(    train_data.loc[:, feat_type_detector.num_columns],    &quot;target&quot;,    perplexity=30,    random_state=0    )# show a heatmap of assocations between categorical variablestheil_matrix = plot_theil_u_heatmap(train_data, feat_type_detector.cat_columns)# plot the percentage of Nulls for all featuresplot_null_percentage(    train_data.loc[:, feat_type_detector.num_columns],    )# detect columns with a very high share of unique valuesmany_unique_cols = check_unique_values(train_data, feat_type_detector.cat_columns)```#### Leakage detectionWith big data and complex pipelines data leakage can easily sneak in.To detect leakage BlueCast offers two functions:```shfrom bluecast.eda.data_leakage_checks import (    detect_categorical_leakage,    detect_leakage_via_correlation,)# Detect leakage of numeric columns based on correlationresult = detect_leakage_via_correlation(        train_data.loc[:, feat_type_detector.num_columns], &quot;target&quot;, threshold=0.9    )# Detect leakage of categorical columns based on Theil's Uresult = detect_categorical_leakage(        train_data.loc[:, feat_type_detector.cat_columns], &quot;target&quot;, threshold=0.9    )```#### Enable cross-validationWhile the default behaviour of BlueCast is to use a simpletrain-test-split, cross-validation can be enabled easily:```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.config.training_config import TrainingConfig# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hypertuning_cv_folds = 5 # default is 1# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```This will use Xgboost's inbuilt cross validation routine which allows BlueCastto execute early pruning on not promising hyperparameter sets. This way BlueCastcan test many more hyperparameters than usual cross validation.#### Enable even more overfitting-robust cross-validationThere might be situations where a preprocessing step has a high risk of overfittingand  needs even more careful evaluation (i.e. oversampling techniques). For suchscenarios BlueCast offers a solution as well.```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.config.training_config import TrainingConfig# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hypertuning_cv_folds = 5 # default is 1train_config.precise_cv_tuning = True # this enables the better routine# this only makes sense if we have an overfitting risky stepcustom_preprocessor = MyCustomPreprocessing() # see section Custom Preprocessing for details# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,        custom_in_fold_preprocessor=custom_preprocessor # this happens during each fold    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```The custom in fold preprocessing takes place within the cross validation andexecutes the step on each fold. The evaluation metric is special here:Instead of calculating matthews correlation coefficient reversed only,it applied increasingly random noise to the eval dataset to find an evenmore robust hyperparameter set.This is much more robust, but does not offerearly pruning and is much slower. BlueCastCV supports this as well.Please note that this is an experimental feature.#### Gaining extra performanceBy default BlueCast uses Optuna's Bayesian hyperparameter optimization,however Bayesian methods give an estimate and do not necessarly findthe ideal spot, thus BlueCast has an optional GridSearch settingthat allows BlueCast to refine some of the parameters Optuna has found.This can be enabled by setting `enable_grid_search_fine_tuning` to True.This fine-tuning step uses a different random seed than the autotuningroutine (seed from the settings + 1000).```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.config.training_config import TrainingConfig# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hypertuning_cv_folds = 5 # default is 1train_config.enable_grid_search_fine_tuning = True # default is Falsetrain_config.gridsearch_tuning_max_runtime_secs = 3600 # max runtime in secstrain_config.gridsearch_nb_parameters_per_grid = 5 # increasing this means X^3 trials atm# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```This comes with a tradeoff of longer runtime. This behaviour can be furthercontrolled with two parameters:* `gridsearch_nb_parameters_per_grid`: Decides how  many steps the grid shall have per parameter* `gridsearch_tuning_max_runtime_secs`: Sets the maximum time in seconds  the tuning shall run. This will finish the latest trial nd will exceed  this limit though.#### Use multi-model blended pipelineBy default, BlueCast trains a single model. However, it is possible totrain multiple models with one call for extra robustness. `BlueCastCV`has a `fit` and a `fit_eval` method. The `fit_eval` method trains themodels, but also provides out-of-fold validation. Also `BlueCastCV`allows to pass custom configurations.```shfrom bluecast.blueprints.cast import BlueCastCVfrom bluecast.config.training_config import TrainingConfig, XgboostTuneParamsConfig# Pass the custom configs to the BlueCast classautoml = BlueCastCV(        class_problem=&quot;binary&quot;,        #conf_training=train_config,        #conf_xgboost=xgboost_param_config,        #custom_preprocessor=custom_preprocessor, # this takes place right after test_train_split        #custom_last_mile_computation=custom_last_mile_computation, # last step before model training/prediction        #custom_feature_selector=custom_feature_selector,    )# this class has a train method:# automl.fit(df_train, target_col=&quot;target&quot;)automl.fit_eval(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```#### Categorical encodingBy default, BlueCast uses target encoding.This behaviour can be changed in the TrainingConfig by setting `cat_encoding_via_ml_algorithm`to True. This will change the expectations of `custom_last_mile_computation` though.If `cat_encoding_via_ml_algorithm` is set to False, `custom_last_mile_computation`will receive numerical features only as target encoding will apply before. If `cat_encoding_via_ml_algorithm`is True (default setting) `custom_last_mile_computation` will receive categoricalfeatures as well, because Xgboost's inbuilt categorical encoding will be used.#### Custom  training configurationDespite e2eml, BlueCast allows easy customization. Users can adjust theconfiguration and just pass it to the `BlueCast` class. Here is an example:```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.config.training_config import TrainingConfig, XgboostTuneParamsConfig# Create a custom tuning config and adjust hyperparameter search spacexgboost_param_config = XgboostTuneParamsConfig()xgboost_param_config.steps_max = 100xgboost_param_config.max_leaves_max = 16# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hyperparameter_tuning_rounds = 10train_config.autotune_model = False # we want to run just normal training, no hyperparameter tuning# We could even just overwrite the final Xgboost params using the XgboostFinalParamConfig class# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,        conf_xgboost=xgboost_param_config,    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```#### Custom preprocessingThe `BlueCast` class also allows for custom preprocessing. This is done byan abstract class that can be inherited and passed into the `BlueCast` class.BlueCast provides two entry points to inject custom preprocessing. Theattribute `custom_preprocessor` is called right after the train_test_split.The attribute `custom_last_mile_computation` will be called before the modeltraining or prediction starts (when only numerical features are present anymore)and allows users to execute last computations (i.e. sub sampling or final calculations).```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.preprocessing.custom import CustomPreprocessing# Create a custom tuning config and adjust hyperparameter search spacexgboost_param_config = XgboostTuneParamsConfig()xgboost_param_config.steps_max = 100xgboost_param_config.max_leaves_max = 16# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hyperparameter_tuning_rounds = 10train_config.autotune_model = False # we want to run just normal training, no hyperparameter tuning# We could even just overwrite the final Xgboost params using the XgboostFinalParamConfig classclass MyCustomPreprocessing(CustomPreprocessing):    def __init__(self):        self.trained_patterns = {}    def fit_transform(        self, df: pd.DataFrame, target: pd.Series    ) -&gt; Tuple[pd.DataFrame, pd.Series]:        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        cat_df = df[['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ']].copy()        zscores = Zscores()        zscores.fit_all(df, ['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'])        df = zscores.transform_all(df, ['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'])        self.trained_patterns[&quot;zscores&quot;] = zscores        imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        imp_mean.fit(df.loc[:, num_columns])        df = imp_mean.transform(df.loc[:, num_columns])        self.trained_patterns[&quot;imputation&quot;] = imp_mean        df = pd.DataFrame(df, columns=num_columns).merge(cat_df, left_index=True, right_index=True, how=&quot;left&quot;)        df = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha'], axis=1)        return df, target    def transform(        self,        df: pd.DataFrame,        target: Optional[pd.Series] = None,        predicton_mode: bool = False,    ) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        cat_df = df[['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ']].copy()        df = self.trained_patterns[&quot;zscores&quot;].transform_all(df, ['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'])        imp_mean = self.trained_patterns[&quot;imputation&quot;]        num_columns = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha', 'EJ'], axis=1).columns        df.loc[:, num_columns] = df.loc[:, num_columns].replace([np.inf, -np.inf], np.nan)        df = imp_mean.transform(df.loc[:, num_columns])        df = pd.DataFrame(df, columns=num_columns).merge(cat_df, left_index=True, right_index=True, how=&quot;left&quot;)        df = df.drop(['Beta', 'Gamma', 'Delta', 'Alpha'], axis=1)        return df, target# add custom last mile computationclass MyCustomLastMilePreprocessing(CustomPreprocessing):    def custom_function(self, df: pd.DataFrame) -&gt; pd.DataFrame:        df = df / 2        df[&quot;custom_col&quot;] = 5        return df    # Please note: The base class enforces that the fit_transform method is implemented    def fit_transform(        self, df: pd.DataFrame, target: pd.Series    ) -&gt; Tuple[pd.DataFrame, pd.Series]:        df = self.custom_function(df)        df = df.head(1000)        target = target.head(1000)        return df, target    # Please note: The base class enforces that the fit_transform method is implemented    def transform(        self,        df: pd.DataFrame,        target: Optional[pd.Series] = None,        predicton_mode: bool = False,    ) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        df = self.custom_function(df)        if not predicton_mode and isinstance(target, pd.Series):            df = df.head(100)            target = target.head(100)        return df, targecustom_last_mile_computation = MyCustomLastMilePreprocessing()custom_preprocessor = MyCustomPreprocessing()# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,        conf_xgboost=xgboost_param_config,        custom_preprocessor=custom_preprocessor, # this takes place right after test_train_split        custom_last_mile_computation=custom_last_mile_computation, # last step before model training/prediction    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```#### Custom feature selectionBlueCast offers automated feature selection. On default the featureselection is disabled, but BlueCast raises a warning to inform theuser about this option. The behaviour can be controlled via the`TrainingConfig`.```shfrom bluecast.blueprints.cast import BlueCastfrom bluecast.preprocessing.custom import CustomPreprocessingfrom bluecast.config.training_config import TrainingConfig# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.hyperparameter_tuning_rounds = 10train_config.autotune_model = False # we want to run just normal training, no hyperparameter tuningtrain_config.enable_feature_selection = True# Pass the custom configs to the BlueCast classautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;        conf_training=train_config,    )automl.fit(df_train, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```Also this step can be customized. The following example shows how to:```shfrom bluecast.config.training_config import TrainingConfigfrom bluecast.preprocessing.custom import CustomPreprocessingfrom sklearn.feature_selection import RFECVfrom sklearn.metrics import make_scorer, matthews_corrcoeffrom sklearn.model_selection import StratifiedKFoldfrom typing import Optional, Tuple# Create a custom training config and adjust general training parameterstrain_config = TrainingConfig()train_config.enable_feature_selection = True# add custom feature selectionclass RFECVSelector(CustomPreprocessing):    def __init__(self, random_state: int = 0):        super().__init__()        self.selected_features = None        self.random_state = random_state        self.selection_strategy: RFECV = RFECV(            estimator=xgb.XGBClassifier(),            step=1,            cv=StratifiedKFold(5, random_state=random_state, shuffle=True),            min_features_to_select=1,            scoring=make_scorer(matthews_corrcoef),            n_jobs=2,        )    def fit_transform(self, df: pd.DataFrame, target: pd.Series) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        self.selection_strategy.fit(df, target)        self.selected_features = self.selection_strategy.support_        df = df.loc[:, self.selected_features]        return df, target    def transform(self,                  df: pd.DataFrame,                  target: Optional[pd.Series] = None,                  predicton_mode: bool = False) -&gt; Tuple[pd.DataFrame, Optional[pd.Series]]:        df = df.loc[:, self.selected_features]        return df, targetcustom_feature_selector = RFECVSelector()# Create an instance of the BlueCast class with the custom modelbluecast = BlueCast(    class_problem=&quot;binary&quot;,    target_column=&quot;target&quot;,    conf_feature_selection=custom_feat_sel,    conf_training=train_config,    custom_feature_selector=custom_feature_selector,# Create some sample data for testingx_train = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]})y_train = pd.Series([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])x_test = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]}x_train[&quot;target&quot;] = y_trai# Fit the BlueCast model using the custom modelbluecast.fit(x_train, &quot;target&quot;# Predict on the test data using the custom modelpredicted_probas, predicted_classes = bluecast.predict(x_test)```#### Custom ML modelFor some users it might just be convenient to use the BlueCast class toenjoy convenience features (details see below), but use a custom ML model.This is possible by passing a custom model to the BlueCast class. The needed propertiesare defined via the BaseClassMlModel class. Here is an example:```shfrom bluecast.ml_modelling.base_classes import (    BaseClassMlModel,    PredictedClasses,  # just for linting checks    PredictedProbas,  # just for linting checks)class CustomModel(BaseClassMlModel):    def __init__(self):        self.model = None    def fit(        self,        x_train: pd.DataFrame,        x_test: pd.DataFrame,        y_train: pd.Series,        y_test: pd.Series,    ) -&gt; None:        self.model = LogisticRegression()        self.model.fit(x_train, y_train)        # if you wih to track experiments using an own ExperimentTracker add it here        # or in the fit method itself    def predict(self, df: pd.DataFrame) -&gt; Tuple[PredictedProbas, PredictedClasses]:        predicted_probas = self.model.predict_proba(df)        predicted_classes = self.model.predict(df)        return predicted_probas, predicted_classescustom_model = CustomModel()# Create an instance of the BlueCast class with the custom modelbluecast = BlueCast(    class_problem=&quot;binary&quot;,    target_column=&quot;target&quot;,    ml_model=custom_model,# Create some sample data for testingx_train = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]})y_train = pd.Series([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])x_test = pd.DataFrame(    {&quot;feature1&quot;: [i for i in range(10)], &quot;feature2&quot;: [i for i in range(10)]}x_train[&quot;target&quot;] = y_trai# Fit the BlueCast model using the custom modelbluecast.fit(x_train, &quot;target&quot;# Predict on the test data using the custom modelpredicted_probas, predicted_classes = bluecast.predict(x_test)```Please note that custom ML models require user defined hyperparameter tuning. Pre-definedconfigurations are not available for custom models.Also note that the calculation of SHAP values only works with tree based models bydefault. For other model architectures disable SHAP values in the TrainingConfigvia:`train_config.calculate_shap_values = True`Just instantiate a new instance of the TrainingConfig, update the param as aboveand pass the config as an argument to the BlueCast instance during instantiation.Feature importance can be added in the custom model definition.#### Using the inbuilt ExperientTrackerFor experimentation environments it can be useful to store all variablesand results from model runs.BlueCast has an inbuilt experiment tracker to enhance the provided insights.No setup is required. BlueCast will automatically store all necessary dataafter each hyperparameter tuning trial.```sh# instantiate and train BlueCastfrom bluecast.blueprints.cast import BlueCastautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;    )automl.fit_eval(df_train, df_eval, y_eval, target_col=&quot;target&quot;)# access the experiment trackertracker = automl.experiment_tracker# see all stored information as a Pandas DataFrametracker_df = tracker.retrieve_results_as_df()```Now from here you could even feed selected columns back into a BlueCastinstance and try to predict the eval_score to check the get the featureimportance of your experiment data! Maybe you uncover hidden patternsfor your model training.Please note that the number of stored experiments will probably be lowerthan the number of started hyperparameter tuning trials. The experiment trackeris skipped whenever Optuna prunes a trial.The experiment triggers whenever the `fit` or `fit_eval` methods of a BlueCastclass instance are called (also within BlueCastCV). This means for custommodels the tracker will not trigger automatically and has to be added manually.## Convenience featuresDespite being a lightweight library, BlueCast also includes some conveniencewith the following features:* automatic feature type detection and casting* automatic DataFrame schema detection: checks if unseen data has new or  missing columns* categorical feature encoding (target encoding or directly in Xgboost)* datetime feature encoding* automated GPU availability check and usage for Xgboost  a fit_eval method to fit a model and evaluate it on a validation set  to mimic production environment reality* functions to save and load a trained pipeline* shapley values* ROC AUC curve &amp; lift chart* warnings for potential misconfigurationsThe fit_eval method can be used like this:```shfrom bluecast.blueprints.cast import BlueCastautoml = BlueCast(        class_problem=&quot;binary&quot;,        target_column=&quot;target&quot;    )automl.fit_eval(df_train, df_eval, y_eval, target_col=&quot;target&quot;)y_probs, y_classes = automl.predict(df_val)```It is important to note that df_train contains the target column whiledf_eval does not. The target column is passed separately as y_eval.## Code qualityTo ensure code quality, we use the following tools:* various pre-commit libraries* strong type hinting in the code base* unit tests using PytestFor contributors, it is expected that all pre-commit and unit tests pass.For new features it is expected that unit tests are added.## DocumentationDocumentation is provided via [Read the Docs](https://bluecast.readthedocs.io/en/latest/)## Kaggle competition results and example notebooksEven though BlueCast has been designed to be a lightweightautoml framework, it still offers the possibilities toreach very good performance. We tested BlueCast in Kagglecompetitions to showcase the libraries capabilitiesfeature- and performance-wise.* ICR top 20% finish with over 6000 participants ([notebook](https://www.kaggle.com/code/thomasmeiner/icr-bluecast-automl-almost-bronze-ranks))* An advanced example covering lots of functionalities ([notebook](https://www.kaggle.com/code/thomasmeiner/ps3e23-automl-eda-outlier-detection/notebook))* PS3E23: Predict software defects top 12% finish ([notebook](https://www.kaggle.com/code/thomasmeiner/ps3e23-automl-eda-outlier-detection?scriptVersionId=145650820))## How to contributeContributions are welcome. Please follow the following steps:* Create a new branch from develop branch* Add your feature or fix* Add unit tests for new features* Run pre-commit checks and unit tests (using Pytest)* Adjust the `docs/source/index.md` file* Copy paste the content of the `docs/source/index.md` file into the  `README.md` file* Push your changes and create a pull requestIf library or dev dependencies have to be changed, adjust the pyproject.toml.For readthedocs it is also requited to update the`docs/srtd_requirements.txt` file. Simply run:```shpoetry export --with dev -f requirements.txt --output docs/rtd_requirements.txt```If readthedocs will be able to create the documentation can be tested via:```shpoetry run sphinx-autobuild docs/source docs/build/html```This will show a localhost link containing the documentation.## MetaCreator: Thomas Meißner – [LinkedIn](https://www.linkedin.com/in/thomas-mei%C3%9Fner-m-a-3808b346)</longdescription>
</pkgmetadata>