<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>fastText |CircleCI|===================`fastText &lt;https://fasttext.cc/&gt;`__ is a library for efficient learningof word representations and sentence classification.In this document we present how to use fastText in python.Table of contents------------------  `Requirements &lt;#requirements&gt;`__-  `Installation &lt;#installation&gt;`__-  `Usage overview &lt;#usage-overview&gt;`__-  `Word representation model &lt;#word-representation-model&gt;`__-  `Text classification model &lt;#text-classification-model&gt;`__-  `IMPORTANT: Preprocessing data / encoding   conventions &lt;#important-preprocessing-data-encoding-conventions&gt;`__-  `More examples &lt;#more-examples&gt;`__-  `API &lt;#api&gt;`__-  `train_unsupervised parameters &lt;#train_unsupervised-parameters&gt;`__-  `train_supervised parameters &lt;#train_supervised-parameters&gt;`__-  `model object &lt;#model-object&gt;`__Requirements============`fastText &lt;https://fasttext.cc/&gt;`__ builds on modern Mac OS and Linuxdistributions. Since it uses C++11 features, it requires a compiler withgood C++11 support. You will need `Python &lt;https://www.python.org/&gt;`__(version 2.7 or â‰¥ 3.4), `NumPy &lt;http://www.numpy.org/&gt;`__ &amp;`SciPy &lt;https://www.scipy.org/&gt;`__ and`pybind11 &lt;https://github.com/pybind/pybind11&gt;`__.Installation============To install the latest release, you can do :.. code:: bash    $ pip install fasttextor, to get the latest development version of fasttext, you can installfrom our github repository :.. code:: bash    $ git clone https://github.com/facebookresearch/fastText.git    $ cd fastText    $ sudo pip install .    $ # or :    $ sudo python setup.py installUsage overview==============Word representation model-------------------------In order to learn word vectors, as `describedhere &lt;https://fasttext.cc/docs/en/references.html#enriching-word-vectors-with-subword-information&gt;`__,we can use ``fasttext.train_unsupervised`` function like this:.. code:: py    import fasttext    # Skipgram model :    model = fasttext.train_unsupervised('data.txt', model='skipgram')    # or, cbow model :    model = fasttext.train_unsupervised('data.txt', model='cbow')where ``data.txt`` is a training file containing utf-8 encoded text.The returned ``model`` object represents your learned model, and you canuse it to retrieve information... code:: py    print(model.words)   # list of words in dictionary    print(model['king']) # get the vector of the word 'king'Saving and loading a model object~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~You can save your trained model object by calling the function``save_model``... code:: py    model.save_model(&quot;model_filename.bin&quot;)and retrieve it later thanks to the function ``load_model`` :.. code:: py    model = fasttext.load_model(&quot;model_filename.bin&quot;)For more information about word representation usage of fasttext, youcan refer to our `word representationstutorial &lt;https://fasttext.cc/docs/en/unsupervised-tutorial.html&gt;`__.Text classification model-------------------------In order to train a text classifier using the method `describedhere &lt;https://fasttext.cc/docs/en/references.html#bag-of-tricks-for-efficient-text-classification&gt;`__,we can use ``fasttext.train_supervised`` function like this:.. code:: py    import fasttext    model = fasttext.train_supervised('data.train.txt')where ``data.train.txt`` is a text file containing a training sentenceper line along with the labels. By default, we assume that labels arewords that are prefixed by the string ``__label__``Once the model is trained, we can retrieve the list of words and labels:.. code:: py    print(model.words)    print(model.labels)To evaluate our model by computing the precision at 1 (P@1) and therecall on a test set, we use the ``test`` function:.. code:: py    def print_results(N, p, r):        print(&quot;N\t&quot; + str(N))        print(&quot;P@{}\t{:.3f}&quot;.format(1, p))        print(&quot;R@{}\t{:.3f}&quot;.format(1, r))    print_results(*model.test('test.txt'))We can also predict labels for a specific text :.. code:: py    model.predict(&quot;Which baking dish is best to bake a banana bread ?&quot;)By default, ``predict`` returns only one label : the one with thehighest probability. You can also predict more than one label byspecifying the parameter ``k``:.. code:: py    model.predict(&quot;Which baking dish is best to bake a banana bread ?&quot;, k=3)If you want to predict more than one sentence you can pass an array ofstrings :.. code:: py    model.predict([&quot;Which baking dish is best to bake a banana bread ?&quot;, &quot;Why not put knives in the dishwasher?&quot;], k=3)Of course, you can also save and load a model to/from a file as `in theword representation usage &lt;#saving-and-loading-a-model-object&gt;`__.For more information about text classification usage of fasttext, youcan refer to our `text classificationtutorial &lt;https://fasttext.cc/docs/en/supervised-tutorial.html&gt;`__.Compress model files with quantization~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~When you want to save a supervised model file, fastText can compress itin order to have a much smaller model file by sacrificing only a littlebit performance... code:: py    # with the previously trained `model` object, call :    model.quantize(input='data.train.txt', retrain=True)    # then display results and save the new model :    print_results(*model.test(valid_data))    model.save_model(&quot;model_filename.ftz&quot;)``model_filename.ftz`` will have a much smaller size than``model_filename.bin``.For further reading on quantization, you can refer to `this paragraphfrom our blogpost &lt;https://fasttext.cc/blog/2017/10/02/blog-post.html#model-compression&gt;`__.IMPORTANT: Preprocessing data / encoding conventions----------------------------------------------------In general it is important to properly preprocess your data. Inparticular our example scripts in the `rootfolder &lt;https://github.com/facebookresearch/fastText&gt;`__ do this.fastText assumes UTF-8 encoded text. All text must be `unicode forPython2 &lt;https://docs.python.org/2/library/functions.html#unicode&gt;`__and `str forPython3 &lt;https://docs.python.org/3.5/library/stdtypes.html#textseq&gt;`__.The passed text will be `encoded as UTF-8 bypybind11 &lt;https://pybind11.readthedocs.io/en/master/advanced/cast/strings.html?highlight=utf-8#strings-bytes-and-unicode-conversions&gt;`__before passed to the fastText C++ library. This means it is important touse UTF-8 encoded text when building a model. On Unix-like systems youcan convert text using `iconv &lt;https://en.wikipedia.org/wiki/Iconv&gt;`__.fastText will tokenize (split text into pieces) based on the followingASCII characters (bytes). In particular, it is not aware of UTF-8whitespace. We advice the user to convert UTF-8 whitespace / wordboundaries into one of the following symbols as appropiate.-  space-  tab-  vertical tab-  carriage return-  formfeed-  the null characterThe newline character is used to delimit lines of text. In particular,the EOS token is appended to a line of text if a newline character isencountered. The only exception is if the number of tokens exceeds theMAX\_LINE\_SIZE constant as defined in the `Dictionaryheader &lt;https://github.com/facebookresearch/fastText/blob/master/src/dictionary.h&gt;`__.This means if you have text that is not separate by newlines, such asthe `fil9 dataset &lt;http://mattmahoney.net/dc/textdata&gt;`__, it will bebroken into chunks with MAX\_LINE\_SIZE of tokens and the EOS token isnot appended.The length of a token is the number of UTF-8 characters by consideringthe `leading two bits of abyte &lt;https://en.wikipedia.org/wiki/UTF-8#Description&gt;`__ to identify`subsequent bytes of a multi-bytesequence &lt;https://github.com/facebookresearch/fastText/blob/master/src/dictionary.cc&gt;`__.Knowing this is especially important when choosing the minimum andmaximum length of subwords. Further, the EOS token (as specified in the`Dictionaryheader &lt;https://github.com/facebookresearch/fastText/blob/master/src/dictionary.h&gt;`__)is considered a character and will not be broken into subwords.More examples-------------In order to have a better knowledge of fastText models, please considerthe main`README &lt;https://github.com/facebookresearch/fastText/blob/master/README.md&gt;`__and in particular `the tutorials on ourwebsite &lt;https://fasttext.cc/docs/en/supervised-tutorial.html&gt;`__.You can find further python examples in `the docfolder &lt;https://github.com/facebookresearch/fastText/tree/master/python/doc/examples&gt;`__.As with any package you can get help on any Python function using thehelp function.For example::    +&gt;&gt;&gt; import fasttext    +&gt;&gt;&gt; help(fasttext.FastText)    Help on module fasttext.FastText in fasttext:    NAME        fasttext.FastText    DESCRIPTION        # Copyright (c) 2017-present, Facebook, Inc.        # All rights reserved.        #        # This source code is licensed under the MIT license found in the        # LICENSE file in the root directory of this source tree.    FUNCTIONS        load_model(path)            Load a model given a filepath and return a model object.        tokenize(text)            Given a string of text, tokenize it and return a list of tokens    [...]API===``train_unsupervised`` parameters---------------------------------.. code:: python        input             # training file path (required)        model             # unsupervised fasttext model {cbow, skipgram} [skipgram]        lr                # learning rate [0.05]        dim               # size of word vectors [100]        ws                # size of the context window [5]        epoch             # number of epochs [5]        minCount          # minimal number of word occurences [5]        minn              # min length of char ngram [3]        maxn              # max length of char ngram [6]        neg               # number of negatives sampled [5]        wordNgrams        # max length of word ngram [1]        loss              # loss function {ns, hs, softmax, ova} [ns]        bucket            # number of buckets [2000000]        thread            # number of threads [number of cpus]        lrUpdateRate      # change the rate of updates for the learning rate [100]        t                 # sampling threshold [0.0001]        verbose           # verbose [2]``train_supervised`` parameters-------------------------------.. code:: python        input             # training file path (required)        lr                # learning rate [0.1]        dim               # size of word vectors [100]        ws                # size of the context window [5]        epoch             # number of epochs [5]        minCount          # minimal number of word occurences [1]        minCountLabel     # minimal number of label occurences [1]        minn              # min length of char ngram [0]        maxn              # max length of char ngram [0]        neg               # number of negatives sampled [5]        wordNgrams        # max length of word ngram [1]        loss              # loss function {ns, hs, softmax, ova} [softmax]        bucket            # number of buckets [2000000]        thread            # number of threads [number of cpus]        lrUpdateRate      # change the rate of updates for the learning rate [100]        t                 # sampling threshold [0.0001]        label             # label prefix ['__label__']        verbose           # verbose [2]        pretrainedVectors # pretrained word vectors (.vec file) for supervised learning []``model`` object----------------``train_supervised``, ``train_unsupervised`` and ``load_model``functions return an instance of ``_FastText`` class, that we generalyname ``model`` object.This object exposes those training arguments as properties : ``lr``,``dim``, ``ws``, ``epoch``, ``minCount``, ``minCountLabel``, ``minn``,``maxn``, ``neg``, ``wordNgrams``, ``loss``, ``bucket``, ``thread``,``lrUpdateRate``, ``t``, ``label``, ``verbose``, ``pretrainedVectors``.So ``model.wordNgrams`` will give you the max length of word ngram usedfor training this model.In addition, the object exposes several functions :.. code:: python        get_dimension           # Get the dimension (size) of a lookup vector (hidden layer).                                # This is equivalent to `dim` property.        get_input_vector        # Given an index, get the corresponding vector of the Input Matrix.        get_input_matrix        # Get a copy of the full input matrix of a Model.        get_labels              # Get the entire list of labels of the dictionary                                # This is equivalent to `labels` property.        get_line                # Split a line of text into words and labels.        get_output_matrix       # Get a copy of the full output matrix of a Model.        get_sentence_vector     # Given a string, get a single vector represenation. This function                                # assumes to be given a single line of text. We split words on                                # whitespace (space, newline, tab, vertical tab) and the control                                # characters carriage return, formfeed and the null character.        get_subword_id          # Given a subword, return the index (within input matrix) it hashes to.        get_subwords            # Given a word, get the subwords and their indicies.        get_word_id             # Given a word, get the word id within the dictionary.        get_word_vector         # Get the vector representation of word.        get_words               # Get the entire list of words of the dictionary                                # This is equivalent to `words` property.        is_quantized            # whether the model has been quantized        predict                 # Given a string, get a list of labels and a list of corresponding probabilities.        quantize                # Quantize the model reducing the size of the model and it's memory footprint.        save_model              # Save the model to the given path        test                    # Evaluate supervised model using file given by path        test_label              # Return the precision and recall score for each label.The properties ``words``, ``labels`` return the words and labels fromthe dictionary :.. code:: py    model.words         # equivalent to model.get_words()    model.labels        # equivalent to model.get_labels()The object overrides ``__getitem__`` and ``__contains__`` functions inorder to return the representation of a word and to check if a word isin the vocabulary... code:: py    model['king']       # equivalent to model.get_word_vector('king')    'king' in model     # equivalent to `'king' in model.get_words()`Join the fastText community----------------------------  `Facebook page &lt;https://www.facebook.com/groups/1174547215919768&gt;`__-  `Stack   overflow &lt;https://stackoverflow.com/questions/tagged/fasttext&gt;`__-  `Google   group &lt;https://groups.google.com/forum/#!forum/fasttext-library&gt;`__-  `GitHub &lt;https://github.com/facebookresearch/fastText&gt;`__.. |CircleCI| image:: https://circleci.com/gh/facebookresearch/fastText/tree/master.svg?style=svg   :target: https://circleci.com/gh/facebookresearch/fastText/tree/master</longdescription>
</pkgmetadata>