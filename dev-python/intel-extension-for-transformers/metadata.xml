<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;  Intel® Extension for Transformers===========================&lt;h3&gt; An innovative toolkit to accelerate Transformer-based models on Intel platforms&lt;/h3&gt;[Architecture](./docs/architecture.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[NeuralChat](./examples/optimization/pytorch/huggingface/language-modeling/chatbot)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Examples](./docs/examples.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Documentations](https://intel.github.io/intel-extension-for-transformers/latest/docs/Welcome.html)&lt;/div&gt;---&lt;div align=&quot;left&quot;&gt;Intel® Extension for Transformers is an innovative toolkit to accelerate Transformer-based models on Intel platforms, in particular effective on 4th Intel Xeon Scalable processor Sapphire Rapids (codenamed [Sapphire Rapids](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html)). The toolkit provides the below key features and examples:*  Seamless user experience of model compressions on Transformer-based models by extending [Hugging Face transformers](https://github.com/huggingface/transformers) APIs and leveraging [Intel® Neural Compressor](https://github.com/intel/neural-compressor)*  Advanced software optimizations and unique compression-aware runtime (released with NeurIPS 2022's paper [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) and [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114), and NeurIPS 2021's paper [Prune Once for All: Sparse Pre-Trained Language Models](https://arxiv.org/abs/2111.05754))*  Optimized Transformer-based model packages such as [Stable Diffusion](examples/deployment/neural_engine/stable_diffusion), [GPT-J-6B](examples/deployment/neural_engine/gpt-j), [GPT-NEOX](examples/optimization/pytorch/huggingface/language-modeling/quantization/inc#2-validated-model-list), [BLOOM-176B](./examples/optimization/pytorch/huggingface/language-modeling/inference/README.md#BLOOM-176B), [T5](examples/optimization/pytorch/huggingface/summarization/quantization#2-validated-model-list), [Flan-T5](examples/optimization/pytorch/huggingface/summarization/quantization#2-validated-model-list) and end-to-end workflows such as [SetFit-based text classification](./docs/tutorials/pytorch/text-classification/SetFit_model_compression_AGNews.ipynb) and [document level sentiment analysis (DLSA)](workflows/dlsa) *  [NeuralChat](examples/optimization/pytorch/huggingface/language-modeling/chatbot), a custom Chatbot trained on Intel CPUs through parameter-efficient fine-tuning [PEFT](https://github.com/huggingface/peft) on domain knowledge## Installation### Install from Pypi```bashpip install intel-extension-for-transformers```&gt; For more installation method, please refer to [Installation Page](docs/installation.md)## Getting Started### Sentiment Analysis with Quantization#### Prepare Dataset```pythonfrom datasets import load_dataset, load_metricfrom transformers import AutoConfig,AutoModelForSequenceClassification,AutoTokenizerraw_datasets = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)raw_datasets = raw_datasets.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)```#### Quantization```pythonfrom intel_extension_for_transformers.optimization import QuantizationConfig, metrics, objectivesfrom intel_extension_for_transformers.optimization.trainer import NLPTrainerconfig = AutoConfig.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;,num_labels=2)model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;,config=config)model.config.label2id = {0: 0, 1: 1}model.config.id2label = {0: 'NEGATIVE', 1: 'POSITIVE'}# Replace transformers.Trainer with NLPTrainer# trainer = transformers.Trainer(...)trainer = NLPTrainer(model=model,     train_dataset=raw_datasets[&quot;train&quot;],     eval_dataset=raw_datasets[&quot;validation&quot;],    tokenizer=tokenizer)q_config = QuantizationConfig(metrics=[metrics.Metric(name=&quot;eval_loss&quot;, greater_is_better=False)])model = trainer.quantize(quant_config=q_config)input = tokenizer(&quot;I like Intel Extension for Transformers&quot;, return_tensors=&quot;pt&quot;)output = model(**input).logits.argmax().item()```&gt; For more quick samples, please refer to [Get Started Page](docs/get_started.md). For more validated examples, please refer to [Support Model Matrix](docs/examples.md)## Documentation&lt;table&gt;&lt;thead&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;OVERVIEW&lt;/th&gt;  &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs&quot;&gt;Model Compression&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;examples/optimization/pytorch/huggingface/language-modeling/chatbot&quot;&gt;NeuralChat&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/docs&quot;&gt;Neural Engine&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/kernels/README.md&quot;&gt;Kernel Libraries&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;MODEL COMPRESSION&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/quantization.md&quot;&gt;Quantization&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/pruning.md&quot;&gt;Pruning&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/distillation.md&quot;&gt;Distillation&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/optimization/pytorch/huggingface/text-classification/orchestrate_optimizations/README.md&quot;&gt;Orchestration&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;https://github.com/intel/intel-extension-for-transformers/tree/main/examples/optimization/pytorch/huggingface/language-modeling/nas&quot;&gt;Neural Architecture Search&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/export.md&quot;&gt;Export&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/metrics.md&quot;&gt;Metrics&lt;/a&gt;/&lt;a href=&quot;docs/objectives.md&quot;&gt;Objectives&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/pipeline.md&quot;&gt;Pipeline&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;NEURAL ENGINE&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/docs/onnx_compile.md&quot;&gt;Model Compilation&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/docs/add_customized_pattern.md&quot;&gt;Custom Pattern&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/docs/deploy_and_integration.md&quot;&gt;Deployment&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/docs/engine_profiling.md&quot;&gt;Profiling&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;KERNEL LIBRARIES&lt;/th&gt;  &lt;/tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/kernels/docs/kernel_desc&quot;&gt;Sparse GEMM Kernels&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/kernels/docs/kernel_desc&quot;&gt;Custom INT8 Kernels&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/kernels/docs/profiling.md&quot;&gt;Profiling&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/test/kernels/benchmark/benchmark.md&quot;&gt;Benchmark&lt;/a&gt;&lt;/td&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;ALGORITHMS&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;4&quot;&gt;&lt;a href=&quot;https://github.com/intel/intel-extension-for-transformers/blob/main/examples/optimization/pytorch/huggingface/question-answering/dynamic/README.md&quot;&gt;Length Adaptive&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;4&quot;&gt;&lt;a href=&quot;docs/data_augmentation.md&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/td&gt;      &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;TUTORIALS AND RESULTS&lt;/a&gt;&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/intel/intel-extension-for-transformers/tree/main/docs/tutorials/pytorch&quot;&gt;Tutorials&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/examples.md&quot;&gt;Supported Models&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/docs/validated_model.md&quot;&gt;Model Performance&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/backends/neural_engine/kernels/docs/validated_data.md&quot;&gt;Kernel Performance&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;## Selected Publications/Events* Blog of Tech-Innovation Artificial-Intelligence(AI): [Intel® Xeon® Processors Are Still the Only CPU With MLPerf Results, Raising the Bar By 5x - Intel Communities](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Xeon-Processors-Are-Still-the-Only-CPU-With-MLPerf-Results/post/1472750) (April 2023)* Blog published on Medium: [MLefficiency — Optimizing transformer models for efficiency](https://medium.com/@kawapanion/mlefficiency-optimizing-transformer-models-for-efficiency-a9e230cff051) (Dec 2022)* NeurIPS'2022: [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) (Nov 2022)* NeurIPS'2022: [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114) (Nov 2022)* Blog published by Cohere: [Top NLP Papers—November 2022](https://txt.cohere.ai/top-nlp-papers-november-2022/) (Nov 2022)* Blog published by Alibaba: [Deep learning inference optimization for Address Purification](https://zhuanlan.zhihu.com/p/552484413) (Aug 2022)* NeurIPS'2021: [Prune Once for All: Sparse Pre-Trained Language Models](https://arxiv.org/abs/2111.05754) (Nov 2021)</longdescription>
</pkgmetadata>