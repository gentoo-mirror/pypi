<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Imaginary Dev OpenAI wrapper[![image](https://img.shields.io/pypi/v/im_openai.svg)](https://pypi.python.org/pypi/im_openai)[![image](https://img.shields.io/travis/alecf/im_openai.svg)](https://travis-ci.com/alecf/im_openai)[![Documentation Status](https://readthedocs.org/projects/im-openai/badge/?version=latest)](https://im-openai.readthedocs.io/en/latest/?version=latest)Wrapper library for openai to send events to the Imaginary Programmingmonitor-   Free software: MIT license-   Documentation: &lt;https://im-openai.readthedocs.io&gt;.## Features-   Patches the openai library to allow user to set an ip_project_key    for each request-   Works out of the box with langchain## Get StartedAt startup, before any openai calls, patch the library with thefollowing code:```pythonfrom im_openai import patch_openaipatch_openai()```Then, set the ip_project_key for each request:```pythonimport openaicompletion = openai.ChatCompletion.create(    engine=&quot;davinci&quot;,    prompt=&quot;This is a test&quot;,    ip_project_key=&quot;my_project_key&quot;)```If you're using langchain, you can set the ip_project_key in the langchain llm setup:```pythonllm = OpenAI(    openai_api_key=os.environ[&quot;OPENAI_API_KEY&quot;],    model_kwargs={&quot;ip_project_key&quot;: &quot;my_project_key&quot;},)```## CreditsThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template... _Cookiecutter: https://github.com/audreyr/cookiecutter.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage======= History =======## 0.1.0 (2023-06-20)-   First release on PyPI.## 0.1.1 (2023-06-23)-   add TemplateString helper and support for data / params## 0.1.2 (2023-06-23)-   add support for original template too## 0.2.0 (2023-06-26)- add explicit support for passing the &quot;prompt template text&quot;## 0.3.0 (2023-06-28)- add support for chat templates (as objects instead of arrays)## 0.4.0 (2023-06-29)- switch event reporting to be async / non-blocking## 0.4.1 (2023-06-29)- add utility for formatting langchain messages## 0.4.2 (2023-06-29)- remove stray breakpoint## 0.4.3 (2023-06-30)- pass along chat_id- attempt to auto-convert langchain prompt templates## 0.4.4 (2023-06-30)- remove stray prints## 0.5.0 (2023-07-06)- Add langchain callbacks handlers## 0.6.0 (2023-07-10)- Handle duplicate callbacks, agents, etc## 0.6.1 (2023-07-12)- Fix prompt retrieval in deep chains## 0.6.2 (2023-07-13)- Handle cases where input values are not strings## 0.6.3 (2023-07-18)- Better support for server-generated event ids   (pre-llm sends event, post-llm re-uses the same id)- more tests for different kinds of templates## 0.6.4- include temporary patched version of loads()## 0.7.0- breaking change: move im_openai.langchain_util to im_openai.langchain- add support for injecting callbacks into all langchain calls using tracing hooks## 0.7.1- Pass along model params to the server</longdescription>
</pkgmetadata>