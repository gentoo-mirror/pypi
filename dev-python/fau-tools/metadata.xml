<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>## IntroductionThis is an individual module, which is mainly for **pytorch CNN** training.Moreover, it also supports some awesome features: saving model, saving training process, plotting figures and so on...## Install`pip install fau-tools`## Usage### importThe following code is recommended.```pythonimport fau_toolsfrom fau_tools import torch_tools```### quick startThe tutor will use a simple example to help you get started quickly!**The following example uses Fau-tools to train a model in MNIST hand-written digits dataset.**```pythonimport torchimport torch.utils.data as tdataimport torchvisionfrom torch import nnimport fau_toolsfrom fau_tools import torch_tools# A simple CNN networkclass CNN(nn.Module):  def __init__(self):    super().__init__()    self.conv = nn.Sequential(      nn.Conv2d(1, 16, 3, 1, 1),  # -&gt; (16, 28, 28)      nn.ReLU(),      nn.MaxPool2d(2),  # -&gt; (16, 14, 14)      nn.Conv2d(16, 32, 3, 1, 1),  # -&gt; (32, 14, 14)      nn.ReLU(),      nn.MaxPool2d(2)  # -&gt; (32, 7, 7)    )    self.output = nn.Linear(32 * 7 * 7, 10)  def forward(self, x):    x = self.conv(x)    x = x.flatten(1)  # same as x = x.view(x.size(0), -1)    return self.output(x)# Hyper Parameters definitiontotal_epoch = 10lr = 1E-3batch_size = 1024# Load datasettrain_data      = torchvision.datasets.MNIST('Datasets', True, torchvision.transforms.ToTensor(), download=True)test_data       = torchvision.datasets.MNIST('Datasets', False, torchvision.transforms.ToTensor())train_data.data = train_data.data[:6000]  # mini datatest_data.data  = test_data.data[:2000]  # mini data# Get data loadertrain_loader = tdata.DataLoader(train_data, batch_size, True)test_loader  = tdata.DataLoader(test_data, batch_size)# Initialize model, optimizer and loss functionmodel = CNN()optimizer = torch.optim.Adam(model.parameters(), lr)loss_function = nn.CrossEntropyLoss()# Train!torch_tools.torch_train(model, train_loader, test_loader, optimizer, loss_function, total_epoch=total_epoch, name=&quot;MNIST&quot;)# the last parameter is the name for saving model and training process.```Now, we can run the python file, and the training process will be visualized, just like the following picture.![training_visualization](github_attachment/training_visualization.png)&gt; Three files named `MNIST_9846.pth`, `MNIST_9846.csv` and `MNIST_9846.txt` will be saved.&gt;&gt; The first file is the trained model.&gt;&gt; The second file records the training process, which you can use matplotlib to visualize it.&gt;&gt; The third file saves some hyper parameters about the training.---The above is the primary usage of this tool, but there are also some other snazzy features, which will be introduced later.## ENDHope you could like it! And welcome issues and pull requests.</longdescription>
</pkgmetadata>