<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># TrueGradPyTorch interface for TrueGrad Optimizers## Getting Started### Installation```BASHpython3 -m pip install truegrad```## ExamplesTrueGrad supports various backends, each with their own tradeoffs:| Name                                               | Advantages                                                                                                                                                                                      | Disadvantages                                                                                                                ||----------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|| [truegrad.nn](#nn)                                 | * What you see is what you get - Modules not in truegrad.nn and truegrad.nn.functional are not supported&lt;br/&gt;* Custom forward/backward for some fused functions&lt;br/&gt;* Optimized backward passes | * Limited applicability - custom modules can't be used&lt;br/&gt;* Requires code modification                                      || [truegrad.utils.patch_torch](#patch-torch)         | * Uses truegrad.nn under the hood&lt;br/&gt;* Works for many (off-the-shelf!) torch models&lt;br/&gt;* No code modification necessary                                                                       | * Uncertainty if model is compatible                                                                                         || [backpack](#backpack)                              | * Highest stability&lt;br/&gt;* Loud warnings and errors&lt;br/&gt;* Battle-tested&lt;br/&gt;* Simple to extend further                                                                                           | * High memory usage&lt;br/&gt;* High compute usage&lt;br/&gt;* Sparse support for torch operations                                       || [truegrad.utils.patch_model](#patch-custom-models) | * Works with custom models                                                                                                                                                                      | * Fails silently on fused functions&lt;br/&gt;* ~50% to 100% slower than truegrad.nn                                               || [patch_torch + patch_model](#Full-Patching)        | * Best compatibility&lt;br/&gt;* Reduced overheads compared to `patch_model` (by falling back to faster pre-patched `patch_torch` where available)                                                    | * Fails silently on fused functions outside of torch.nn&lt;br/&gt; * Slower than truegrad.nn when truegrad.nn would've been enough |Below, you'll find examples for each of these backends, as well as a [general strategy](#partial-truegrad) allowingpartial application of TrueGrad.### nnThe preferred method of using TrueGrad is by replacing `torch.nn` with performant `truegrad.nn` modules. While othermethods add compute and memory overheads, `truegrad.nn` and `truegrad.nn.functional` have hand-crafted gradients. Thisis the most powerful method, although it requires code modifications.```PYTHONimport torchfrom truegrad import nnfrom truegrad.optim import TGAdamW# define model by mixing truegrad.nn and torch.nnmodel = torch.nn.Sequential(nn.Linear(1, 10),                            nn.LayerNorm(10),                            torch.nn.ReLU(),                            nn.Linear(10, 1))optim = TGAdamW(model.parameters())  # truegrad.optim.TGAdamW instead of torch.optim.AdamW# standard training loop while True:    input = torch.randn((16, 1))    model(input).mean().backward()    optim.step()    optim.zero_grad()```### Patch TorchIn some cases, you can't modify the model's source. For example, when importing models from `torchvision`. If that's thecase, or if you simply want to try out TrueGrad, you can use `truegrad.utils.patch_torch()`, toreplace `torch.nn.Module`'s with `truegrad.nn.Module`'s where possible. For example, the code below can be used to traina ResNet-18:```PYTHONimport torchfrom torchvision.models import resnet18from truegrad.optim import TGAdamWfrom truegrad.utils import patch_torchpatch_torch()  # call before model creation, otherwise complete freedommodel = resnet18().cuda()optim = TGAdamW(model.parameters(), lr=1e-7, weight_decay=0)# constant input/output to overfitinp = torch.randn((2, 3, 224, 224)).cuda()tgt = torch.randint(0, 1000, (2,)).cuda()# standard training loopi = 0while True:    loss = torch.nn.functional.cross_entropy(model(inp), tgt)    loss.backward()    optim.step()    optim.zero_grad()    i += 1    if i % 5 == 0:        print(i, loss.item())```Similarly, most huggingface transformers work out of the box:```PYTHONimport torchimport transformersfrom torch.nn import functional as Ffrom truegrad.optim import TGAdamWfrom truegrad.utils import patch_torchpatch_torch()  # only added line to get truegrad statistics for TGAdamWmodel = transformers.BertModel.from_pretrained(&quot;google/bert_uncased_L-2_H-128_A-2&quot;)  # any existing modeltokenizer = transformers.BertTokenizer.from_pretrained(&quot;google/bert_uncased_L-2_H-128_A-2&quot;)optim = TGAdamW(model.parameters())# constant input to overfitinput = tokenizer([&quot;Hello World!&quot;], return_tensors=&quot;pt&quot;)# training loop as normalwhile True:    out = model(**input)    loss = F.l1_loss(out[0], torch.ones_like(out[0]))    loss.backward()    optim.step()    optim.zero_grad()    print(loss.item())```Note that this works even though transformers have custom modules, which could cause issues. The key factor is that allparameters come from `torch.nn.Module`'s, which are patched by `patch_torch()`. Therefore, truegrad handles allparameter usages. Therefore, any composition of `torch.nn.Module`'s makes for a truegrad-compatible model.### BackPackThe most stable although also memory hungry method to compute TrueGrad statistics is to use[BackPack](https://github.com/f-dangel/backpack). BackPack is a third-party library that automatically computes the sumof gradient squares and works for most models by implementing custom backward rules for many `torch.nn.Module`'s.```PYTHONimport backpackimport torchfrom torch.nn import CrossEntropyLossfrom truegrad.optim import TGAdamWfrom torchvision.models import alexnetmodel = alexnet()  # BatchNorm and in-place ops (like ResNet's residual path) aren't supportedoptim = TGAdamW(model.parameters(), lr=1e-7, weight_decay=0)# replace inplace ops like nn.ReLU(inplace=True) where possiblefor mod in model.modules():    if hasattr(mod, &quot;inplace&quot;):        mod.inplace = False# backpack relies on module-level pytorch hooksmodel = backpack.extend(model)lossfunc = backpack.extend(CrossEntropyLoss())# constant input/output to overfitinp = torch.randn((2, 3, 224, 224))tgt = torch.randint(0, 1000, (2,))# standard training loopi = 0while True:    # &quot;SumGradSquared&quot; computes the sum of the squared gradient    with backpack.backpack(backpack.extensions.SumGradSquared()):        loss = lossfunc(model(inp), tgt)        loss.backward()    optim.step()    optim.zero_grad()    i += 1    if i % 5 == 0:        print(i, loss.item())```If you're using custom modules with self-defined parameters, this method will not work. Additionally, note that, ifyour model has any layer called `.output` or you're using PyTorch &gt;= 1.13, you will need to install[BackPack-HF](https://github.com/ClashLuke/backpack-hf) via`python3 -m pip install git+https://github.com/ClashLuke/backpack-hf`.### Patch Custom ModelsAnother option to integrate TrueGrad into existing models is to patch them using `truegrad.utils.patch_model()`.`patch_model()` will go through all `torch.nn.Module`'s in PyTorch model and convert their `torch.nn.Parameter`'s to`truegrad.nn.TrueGradParameter`'s. A `TrueGradParameter` acts largely the same as a `torch.nn.Parameter`, but addsrequired operations into the model's backward pass. Note that this doesn't give the most effective computation graph,but works well for many custom models.\Importantly, be aware that this does not work for fused functions, such as `torch.nn.LayerNorm`and `torch.nn.MultiheadAttention`. However, unfused functions which directly access a parameter, such as multiplication,work well. Therefore, torch.nn.Linear and HuggingFace's attention work as expected.```PYTHONimport torchfrom truegrad.optim import TGAdamWfrom truegrad.utils import patch_modelfrom torchvision.models import alexnetmodel = alexnet()  # patch_model can't handle fused ops like VGG's and ResNet's BatchNormoptim = TGAdamW(model.parameters())# replace inplace ops like nn.ReLU(inplace=True) where possiblefor mod in model.modules():    if hasattr(mod, &quot;inplace&quot;):        mod.inplace = Falsepatch_model(model)  # replace torch.nn.Parameter with truegrad.nn.Parameter# constant input/output to overfitinp = torch.randn((2, 3, 224, 224))tgt = torch.randint(0, 1000, (2,))# standard training loopi = 0while True:    loss = torch.nn.functional.cross_entropy(model(inp), tgt)    loss.backward()    optim.step()    optim.zero_grad()    i += 1    if i % 5 == 0:        print(i, loss.item())```### Full PatchingOne way of avoiding [truegrad.utils.patch_model](#patch-custom-models)'s downsides when working with off-the-shelfmodels containing custom parameters, such as [lucidrains' ViT's](https://github.com/lucidrains/vit-pytorch/) is to also`patch_torch`. This takes care of many fused functions, such as LayerNorm, while still allowing full flexibility inmodel design.```PYTHONimport torchfrom vit_pytorch.levit import LeViTfrom truegrad.utils import patch_torch, patch_modelfrom truegrad.optim import TGAdamWpatch_torch()  # before model instantiationlevit = LeViT(        image_size=224,        num_classes=1000,        stages=3,  # number of stages        dim=(256, 384, 512),  # dimensions at each stage        depth=4,  # transformer of depth 4 at each stage        heads=(4, 6, 8),  # heads at each stage        mlp_mult=2,        dropout=0.1        )opt = TGAdamW(levit.parameters())patch_model(levit)  # replace torch.nn.Parameter with truegrad.nn.TrueGradParameter# constant input to overfitimg = torch.randn(1, 3, 224, 224)# standard training loopwhile True:    loss = levit(img).square().mean()    loss.backward()    opt.step()    opt.zero_grad()    print(loss.item())```### Partial TrueGradUnfortunately, it's not always sensible to apply TrueGrad, as some backward passes are too slow, and sometimes it'simpossible to avoid a fused function.Therefore, it can be an option to use TGAdamW only on specific subsections of the model. To do so, you canspecify `default_to_adam=True` to TGAdamW. Adding this option allows TGAdamW to fall back to AdamW if there isno `sum_grad_squared` attribute available.For example, the code from [#nn](#nn) could be extended in the following way:```PYTHONimport torchfrom truegrad import nnfrom truegrad.optim import TGAdamWmodel = torch.nn.Sequential(nn.Linear(1, 10),  # Weights coming from truegrad.nn                             nn.LayerNorm(10),                            torch.nn.ReLU(),                            torch.nn.Linear(10, 1))  # Weights coming torch.nnoptim = TGAdamW(model.parameters(), default_to_adam=True)# standard training loopi = 0while True:    input = torch.randn((16, 1))    loss = model(input).mean()    loss.backward()    optim.step()    optim.zero_grad()    i += 1    if i % 5 == 0:        print(i, loss.item())```</longdescription>
</pkgmetadata>