<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Bounded Process&amp;Thread Pool ExecutorBoundedSemaphore for [ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor) &amp; [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor) from [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html)## Installation```bashpip install bounded-pool-executor```# What is the main problem?If you use the standard module &quot;**concurrent.futures**&quot; and want to simultaneously process several million data, then a queue of workers will take up all the free memory.If the script is run on a weak VPS, this will lead to a **memory leak**.## BoundedProcessPoolExecutor VS ProcessPoolExecutor# BoundedProcessPoolExecutor**BoundedProcessPoolExecutor** will put a new worker in queue only when another worker has finished his work.```pythonfrom bounded_pool_executor import BoundedProcessPoolExecutorfrom time import sleepfrom random import randintdef do_job(num):    sleep_sec = randint(1, 10)    print('value: %d, sleep: %d sec.' % (num, sleep_sec))    sleep(sleep_sec)with BoundedProcessPoolExecutor(max_workers=5) as worker:    for num in range(10000):        print('#%d Worker initialization' % num)        worker.submit(do_job, num)```### Result:![BoundedProcessPoolExecutor](https://python-scripts.com/wp-content/uploads/2018/12/bounded.gif)# Classic concurrent.futures.ProcessPoolExecutor**ProcessPoolExecutor** inserts all workers into the queue and expects tasks to be performed as the new worker is released, depending on the value of `max_workers`.```pythonimport concurrent.futuresfrom time import sleepfrom random import randintdef do_job(num):    sleep_sec = randint(1, 3)    print('value: %d, sleep: %d sec.' % (num, sleep_sec))    sleep(sleep_sec)with concurrent.futures.ProcessPoolExecutor(max_workers=5) as worker:    for num in range(100000):        print('#%d Worker initialization' % num)        worker.submit(do_job, num)```### Result:![concurrent.futures.ProcessPoolExecutor](https://python-scripts.com/wp-content/uploads/2018/12/future-ProcessPoolExecutor.gif)</longdescription>
</pkgmetadata>