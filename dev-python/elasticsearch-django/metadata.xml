<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>**This project now requires Python 3.8+ and Django 3.2+.For previous versions please refer to the relevant tag or branch.**# Elasticsearch for DjangoThis is a lightweight Django app for people who are using Elasticsearchwith Django, and want to manage their indexes.## CompatibilityThe master branch is now based on `elasticsearch-py` 8. If you areusing older versions, please switch to the relevant branch (released onPyPI as 2.x, 5.x, 6.x).## Search Index LifecycleThe basic lifecycle for a search index is simple:1. Create an index2. Post documents to the index3. Query the indexRelating this to our use of search within a Django project it looks like this:1. Create mapping file for a named index2. Add index configuration to Django settings3. Map models to document types in the index4. Post document representation of objects to the index5. Update the index when an object is updated6. Remove the document when an object is deleted7. Query the index8. Convert search results into a QuerySet (preserving relevance)# Django ImplementationThis section shows how to set up Django to recognise ES indexes, and themodels that should appear in an index. From this setup you should beable to run the management commands that will create and populate eachindex, and keep the indexes in sync with the database.## Create index mapping fileThe prerequisite to configuring Django to work with an index is havingthe mapping for the index available. This is a bit chicken-and-egg, butthe underlying assumption is that you are capable of creating the indexmappings outside of Django itself, as raw JSON. (The easiest way tospoof this is to POST a JSON document representing your document type atURL on your ES instance (`POST http://ELASTICSEARCH_URL/{{index_name}}`)and then retrieving the auto-magic mapping that ES created via `GEThttp://ELASTICSEARCH_URL/{{index_name}}/_mapping`.)Once you have the JSON mapping, you should save it in the root of theDjango project as `search/mappings/{{index_name}}.json`.## Configure Django settingsThe Django settings for search are contained in a dictionary called`SEARCH_SETTINGS`, which should be in the main `django.conf.settings`file. The dictionary has three root nodes, `connections`, `indexes` and`settings`. Below is an example:```python    SEARCH_SETTINGS = {        'connections': {            'default': getenv('ELASTICSEARCH_URL'),            'backup': {                # all Elasticsearch init kwargs can be used here                'cloud_id': '{{ cloud_id }}'            }        },        'indexes': {            'blog': {                'models': [                    'website.BlogPost',                ]            }        },        'settings': {            # batch size for ES bulk api operations            'chunk_size': 500,            # default page size for search results            'page_size': 25,            # set to True to connect post_save/delete signals            'auto_sync': True,            # List of models which will never auto_sync even if auto_sync is True            'never_auto_sync': [],            # if true, then indexes must have mapping files            'strict_validation': False        }    }```The `connections` node is (hopefully) self-explanatory - we supportmultiple connections, but in practice you should only need the one -'default' connection. This is the URL used to connect to your ESinstance. The `settings` node contains site-wide search settings. The`indexes` nodes is where we configure how Django and ES play together,and is where most of the work happens.Note that prior to v8.2 the connection value had to be a connectionstring; since v8.2 this can still be a connection string, but can alsobe a dictionary that contains any kwarg that can be passed to the`Elasticsearch` init method.**Index settings**Inside the index node we have a collection of named indexes - in thiscase just the single index called `blog`. Inside each index we have a`models` key which contains a list of Django models that should appearin the index, denoted in `app.ModelName` format. You can have multiplemodels in an index, and a model can appear in multiple indexes. Howmodels and indexes interact is described in the next section.**Configuration Validation**When the app boots up it validates the settings, which involves thefollowing:1. Do each of the indexes specified have a mapping file?2. Do each of the models implement the required mixins?## Implement search document mixinsSo far we have configured Django to know the names of the indexes wewant, and the models that we want to index. What it doesn't yet know iswhich objects to index, and how to convert an object to its search indexdocument. This is done by implementing two separate mixins -`SearchDocumentMixin` and `SearchDocumentManagerMixin`. Theconfiguration validation routine will tell you if these are notimplemented. **SearchDocumentMixin**This mixin is responsible for the seaerch index document format. We areindexing JSON representations of each object, and we have two methods onthe mixin responsible for outputting the correct format -`as_search_document` and `as_search_document_update`.An aside on the mechanics of the `auto_sync` process, which is hooked upusing Django's `post_save` and `post_delete` model signals. ES supportspartial updates to documents that already exist, and we make afundamental assumption about indexing models - that **if you pass the`update_fields` kwarg to a `model.save` method call, then you areperforming a partial update**, and this will be propagated to ES as apartial update only.To this end, we have two methods for generating the model's JSONrepresentation - `as_search_document`, which should return a dict thatrepresents the entire object; and `as_search_document_update`, whichtakes the `update_fields` kwarg. This method handler two partial update'strategies', defined in the `SEARCH_SETTINGS`, 'full' and 'partial'.The default 'full' strategy simply proxies the `as_search_document`method - i.e. partial updates are treated as a full document update. The'partial' strategy is more intelligent - it will map the update_fieldsspecified to the field names defined in the index mapping files. If afield name is passed into the save method but is not in the mappingfile, it is ignored. In addition, if the underlying Django model fieldis a related object, a `ValueError` will be raised, as we cannotserialize this automatically. In this scenario, you will need tooverride the method in your subclass - see the code for more details.To better understand this, let us say that we have a model (`MyModel`)that is configured to be included in an index called `myindex`. If wesave an object, without passing `update_fields`, then this is considereda full document update, which triggers the object's`index_search_document` method:```pythonobj = MyModel.objects.first()obj.save()...# AUTO_SYNC=true will trigger a re-index of the complete object document:obj.index_search_document(index='myindex')```However, if we only want to update a single field (say the `timestamp`),and we pass this in to the save method, then this will trigger the`update_search_document` method, passing in the names of the fields thatwe want updated.```python# save a single field on the objectobj.save(update_fields=['timestamp'])...# AUTO_SYNC=true will trigger a partial update of the object documentobj.update_search_document(index, update_fields=['timestamp'])```We pass the name of the index being updated as the first arg, as objects may have different representations in different indexes:```python    def as_search_document(self, index):        return {'name': &quot;foo&quot;} if index == 'foo' else {'name': &quot;bar&quot;}```In the case of the second method, the simplest possible implementationwould be a dictionary containing the names of the fields being updatedand their new values, and this is the default implementation. If thefields passed in are simple fields (numbers, dates, strings, etc.) thena simple `{'field_name': getattr(obj, field_name}` is returned. However,if the field name relates to a complex object (e.g. a related object)then this method will raise an `InvalidUpdateFields` exception. In thisscenario you should override the default implementationwith one of yourown.```python    def as_search_document_update(self, index, update_fields):        if 'user' in update_fields:            # remove so that it won't raise a ValueError            update_fields.remove('user')            doc = super().as_search_document_update(index, update_fields)            doc['user'] = self.user.get_full_name()            return doc        return super().as_search_document_update(index, update_fields)```The reason we have split out the update from the full-document indexcomes from a real problem that we ourselves suffered. The full objectrepresentation that we were using was quite DB intensive - we werestoring properties of the model that required walking the ORM tree.However, because we were also touching the objects (see below) to recordactivity timestamps, we ended up flooding the database with queriessimply to update a single field in the output document. Partial updatessolves this issue:```python    def touch(self):        self.timestamp = now()        self.save(update_fields=['timestamp'])    def as_search_document_update(self, index, update_fields):        if list(update_fields) == ['timestamp']:            # only propagate changes if it's +1hr since the last timestamp change            if now() - self.timestamp &lt; timedelta(hours=1):                return {}            else:                return {'timestamp': self.timestamp}        ....```**Processing updates async**If you are generating a lot of index updates you may want to run themasync (via some kind of queueing mechanism). There is no built-in methodto do this, given the range of queueing libraries and patternsavailable, however it is possible using the `pre_index`, `pre_update`and `pre_delete` signals. In this case, you should also turn off`AUTO_SYNC` (as this will run the updates synchronously), and processthe updates yourself. The signals pass in the kwargs required by therelevant model methods, as well as the `instance` involved:```python# ensure that SEARCH_AUTO_SYNC=Falsefrom django.dispatch import receiverimport django_rqfrom elasticsearch_django.signals import (    pre_index,    pre_update,    pre_delete)queue = django_rq.get_queue(&quot;elasticsearch&quot;)@receiver(pre_index, dispatch_uid=&quot;async_index_document&quot;)def index_search_document_async(sender, **kwargs):    &quot;&quot;&quot;Queue up search index document update via RQ.&quot;&quot;&quot;    instance = kwargs.pop(&quot;instance&quot;)    queue.enqueue(        instance.update_search_document,        index=kwargs.pop(&quot;index&quot;),    )@receiver(pre_update, dispatch_uid=&quot;async_update_document&quot;)def update_search_document_async(sender, **kwargs):    &quot;&quot;&quot;Queue up search index document update via RQ.&quot;&quot;&quot;    instance = kwargs.pop(&quot;instance&quot;)    queue.enqueue(        instance.index_search_document,        index=kwargs.pop(&quot;index&quot;),        update_fields=kwargs.pop(&quot;update_fields&quot;),    )@receiver(pre_delete, dispatch_uid=&quot;async_delete_document&quot;)def delete_search_document_async(sender, **kwargs):    &quot;&quot;&quot;Queue up search index document deletion via RQ.&quot;&quot;&quot;    instance = kwargs.pop(&quot;instance&quot;)    queue.enqueue(        instance.delete_search_document,        index=kwargs.pop(&quot;index&quot;),    )```**SearchDocumentManagerMixin**This mixin must be implemented by the model's default manager(`objects`). It also requires a single method implementation -`get_search_queryset()` - which returns a queryset of objects that areto be indexed. This can also use the `index` kwarg to provide differentsets of objects to different indexes.```python    def get_search_queryset(self, index='_all'):        return self.get_queryset().filter(foo='bar')```We now have the bare bones of our search implementation. We can now usethe included management commands to create and populate our searchindex:```shell# create the index 'foo' from the 'foo.json' mapping file$ ./manage.py create_search_index foo# populate foo with all the relevant objects$ ./manage.py update_search_index foo```The next step is to ensure that our models stay in sync with the index.## Add model signal handlers to update indexIf the setting `auto_sync` is True, then on `AppConfig.ready` each modelconfigured for use in an index has its `post_save` and `post_delete`signals connected. This means that they will be kept in sync across allindexes that they appear in whenever the relevant model method iscalled. (There is some very basic caching to prevent too many updates -the object document is cached for one minute, and if there is no changein the document the index update is ignored.)There is a **VERY IMPORTANT** caveat to the signal handling. It will**only** pick up on changes to the model itself, and not on related(`ForeignKey`, `ManyToManyField`) model changes. If the search documentis affected by such a change then you will need to implement additionalsignal handling yourself.In addition to `object.save()`, SeachDocumentMixin also provides the`update_search_index(self, action, index='_all', update_fields=None,force=False)` method. Action should be 'index', 'update' or 'delete'.The difference between 'index' and 'update' is that 'update' is apartial update that only changes the fields specified, rather thanre-updating the entire document. If `action` is 'update' whilst`update_fields` is None, action will be changed to `index`.We now have documents in our search index, kept up to date with theirDjango counterparts. We are ready to start querying ES.---# Search Queries (How to Search)## Running search queries**SearchQuery**The `elasticsearch_django.models.SearchQuery` model wraps thisfunctionality up and provides helper properties, as well as logging thequery:```pythonfrom elasticsearch_django.settings import get_clientfrom elasticsearch_django.models import execute_search# run a default match_all querysq = execute_search(index=&quot;foo&quot;, query={&quot;match_all&quot;: {}})# the raw response is stored on the return object,# but is not stored on the object in the database.print(sq.response)```Calling the `execute_search` function will execute the underlyingsearch, log the query JSON, the number of hits, and the list of hit metainformation for future analysis. The `execute` method also includesthese additional kwargs:* `user` - the user who is making the query, useful for logging* `search_terms` - the search query supplied by the user (as opposed to  the DSL) - not used by ES, but stored in the logs* `reference` - a free text reference field - used for grouping searches  together - could be session id.* `save` - by default the SearchQuery created will be saved, but passing  in False will prevent this.## Converting search hits into Django objectsRunning a search against an index will return a page of results, eachcontaining the `_source` attribute which is the search document itself(as created by the `SearchDocumentMixin.as_search_document` method),together with meta info about the result - most significantly therelevance **score**, which is the magic value used for ranking(ordering) results. However, the search document probably doesn'tcontain all the of the information that you need to display the result,so what you really need is a standard Django QuerySet, containing theobjects in the search results, but maintaining the order. This meansinjecting the ES score into the queryset, and then using it forordering. There is a method on the `SearchDocumentManagerMixin` called`from_search_query` which will do this for you. It uses raw SQL to addthe score as an annotation to each object in the queryset. (It also addsthe 'rank' - so that even if the score is identical for all hits, theordering is preserved.)```pythonfrom models import BlogPost# run a default match_all querysq = execute_search(index=&quot;blog&quot;, query={&quot;match_all&quot;: {}})for obj in BlogPost.objects.from_search_query(sq):    print obj.search_score, obj.search_rank```</longdescription>
</pkgmetadata>