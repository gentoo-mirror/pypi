<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;h1 style=&quot;text-align:center&quot;&gt;Transformers Domain Adaptation&lt;/h1&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://transformers-domain-adaptation.readthedocs.io/en/latest/content/introduction.html&quot;&gt;Documentation&lt;/a&gt; â€¢    &lt;a href=&quot;https://colab.research.google.com/github/georgianpartners/Transformers-Domain-Adaptation/blob/master/notebooks/GuideToTransformersDomainAdaptation.ipynb&quot;&gt;Colab Guide&lt;/a&gt;&lt;/p&gt;[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/transformers-domain-adaptation)](https://pypi.org/project/transformers-domain-adaptation/)[![PyPI version](https://badge.fury.io/py/transformers-domain-adaptation.svg)](https://badge.fury.io/py/transformers-domain-adaptation)![Python package](https://github.com/georgianpartners/Transformers-Domain-Adaptation/workflows/Python%20package/badge.svg)[![Documentation Status](https://readthedocs.org/projects/transformers-domain-adaptation/badge/?version=latest)](https://transformers-domain-adaptation.readthedocs.io/en/latest/?badge=latest)&lt;/div&gt;This toolkit improves the performance of HuggingFace transformer models on downstream NLP tasks,by domain-adapting models to the target domain of said NLP tasks (e.g. BERT -&gt; LawBERT).![](docs/source/domain_adaptation_diagram.png)The overall Domain Adaptation framework can be broken down into three phases:1. Data Selection    &gt; Select a relevant subset of documents from the in-domain corpus that is likely to be beneficial for domain pre-training (see below)2. Vocabulary Augmentation    &gt; Extending the vocabulary of the transformer model with domain specific-terminology3. Domain Pre-Training    &gt; Continued pre-training of transformer model on the in-domain corpus to learn linguistic nuances of the target domainAfter a model is domain-adapted, it can be fine-tuned on the downstream NLP task of choice, like any pre-trained transformer model.### ComponentsThis toolkit provides two classes, `DataSelector` and `VocabAugmentor`, to simplify the Data Selection and Vocabulary Augmentation steps respectively.## InstallationThis package was developed on Python 3.6+ and can be downloaded using `pip`:```pip install transformers-domain-adaptation```## Features- Compatible with the HuggingFace ecosystem:    - `transformers 4.x`    - `tokenizers`    - `datasets`## UsagePlease refer to our Colab guide!&lt;a href=&quot;https://colab.research.google.com/github/georgianpartners/Transformers-Domain-Adaptation/blob/master/notebooks/GuideToTransformersDomainAdaptation.ipynb&quot; target=&quot;_parent&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;&lt;/a&gt;## ResultsTODO</longdescription>
</pkgmetadata>