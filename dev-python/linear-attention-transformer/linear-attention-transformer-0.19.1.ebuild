# automatically generated by g-sorcery
# please do not edit this file

EAPI=8

REALNAME="linear-attention-transformer"
REALVERSION="0.19.1"
DIGEST_SOURCES="yes"
PYTHON_COMPAT=( python{3_9,3_10,3_11} )
DISTUTILS_USE_PEP517=standalone

inherit python-r1 gs-pypi

DESCRIPTION="Linear Attention Transformer"

HOMEPAGE="https://github.com/lucidrains/linear-attention-transformer"
LICENSE="MIT"
SRC_URI="https://files.pythonhosted.org/packages/1a/f4/e7aedb94bc94583983d2371eb987943e000c82907a32a4353ba64881ebb0/linear_attention_transformer-${REALVERSION}.tar.gz"
SOURCEFILE="linear_attention_transformer-${REALVERSION}.tar.gz"
RESTRICT="test"

SLOT="0"
KEYWORDS="~amd64 ~x86"

IUSE=""
DEPENDENCIES="dev-python/axial-positional-embedding[${PYTHON_USEDEP}]
	dev-python/einops[${PYTHON_USEDEP}]
	dev-python/linformer[${PYTHON_USEDEP}]
	dev-python/local-attention[${PYTHON_USEDEP}]
	dev-python/product-key-memory[${PYTHON_USEDEP}]
	dev-python/torch[${PYTHON_USEDEP}]"
BDEPEND="${DEPENDENCIES}"
RDEPEND="${DEPENDENCIES}"
