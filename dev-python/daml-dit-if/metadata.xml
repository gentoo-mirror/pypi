<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>daml-dit-if====Daml Hub [integrations](https://hub.daml.com/docs/quickstart/#integrations)are loadable Python modules that mediate the relationship between a Daml Hubledger and various external systems. Because of their special role withinDaml Hub, integrations have the ability to issue and receive externalnetwork requests in addition to the usual ledger interactions supportedby [bots](https://hub.daml.com/docs/quickstart/#python-bots)and [triggers](https://hub.daml.com/docs/quickstart/#daml-triggers).This gives integrations the ability to issue network requests based onledger activity as well as issue ledger commands based on networkactivity. To allow monitoring and control by Daml Hub, integrations mustalso follow a set of interface conventions. This repositorycontains a framework that builds on Digital Asset's[DAZL Ledger Client](https://github.com/digital-asset/dazl-client)to simplify development of common types of integrations.This framework makes it possible to develop custom integrationtypes, but due to their access to the network, integrations haveprivileged status within Daml Hub and require elevated permissions toinstall. Please contact [Digital Asset](https://discuss.daml.com/) formore information on deploying custom integration types into Daml Hub.For examples of fully constructed integrations built with theframework, there are several open source examples available in GitHubrepositories. These also correspond to some of the default integrationsavailable to all Daml Hub users via the _Browse Integrations_ tab in the[console](https://hub.daml.com/docs/quickstart)'s ledger view.* [Core Pack](https://github.com/digital-asset/daml-dit-integration-core)* [Coindesk](https://github.com/digital-asset/daml-dit-integration-coindesk)* [Slack](https://github.com/digital-asset/daml-dit-integration-slack)* [Exberry](https://github.com/digital-asset/daml-dit-integration-exberry)## Integration Packaging and DeploymentIntegrations are packaged in [DIT files](https://github.com/digital-asset/daml-dit-api),built using the [`ddit` build tool](https://github.com/digital-asset/daml-dit-ddit),and can be deployed into Daml Hub using the same upload mechanism as otherartifacts (Daml Models, bots, etc.). Daml Hub also has an 'arcade' facilitythat uses a public [GitHub repository](https://github.com/digital-asset/daml-dit-arcade-index)to maintain a list of integrations and sample apps that can be deployed througha single click on the [console user interface](https://hub.daml.com/docs/quickstart).Logically speaking, Daml Hub integrations package their Pythonimplementation code alongside metadata describing availableintegration types and any other resources required to make theintegration operate. Integrations usually require Daml models torepresent their data model, and often require external Pythondependencies specified through a `requirements.txt` file. Both ofthese are examples of additional resources that might be bundled intoa DIT file.  Note that while `daml-dit-if` and `dazl` are both Pythondependencies required by integrations, they are exceptions to thisrule.  Daml Hub provides both of these by default, so they should notbe listed in `requirements.txt` and should not be included in the DITfile.## Integration Design GuidelinesBuilding integrations for Daml Hub, we've found the following to be goodguidance for designing reliable and usable integrations:* Rather than require a number of distinct integration types or integration instances, favor designs that reduce the number of types and instances. This can simplify both configuration and deployment.* When possible, consider using contracts on ledger to allow configuration of multiple activities of the same integration rather than multiple instances of the same integration with different sets of configuration parameters.* Store private API tokens and keys on-ledger in contracts. This restricts visibility of secrets to the integration itself.* Try to ensure that as much retry/handshaking logic is managed directly in integration code, rather than in Daml. This makes it easier to ensure that the Daml logic is more purely focused on the business processes being modeled rather than the details of the integration protocol.* Prefer level sensitive logic to edge sensitive logic. Rather than triggering an external interaction based on the occurrence of an event, trigger based on the presence of a contract and archive the contract when the interaction is complete. This can improve reliability and error recovery in the event of failed integrations, and help keep technical integration details out of the business logic in Daml.## The Integration Framework APIThe integration framework API has two parts:* Metadata describing the available integration types.* A Python API for registering event handlers.The metadata for an integration is stored in an additional`integration_types` section within `dabl-meta.yaml`. This sectionlists and describes the integration types defined within the DIT file.This metadata section includes the name of the entry point functionfor the integration type, some descriptive text, and a list of theconfiguration arguments accepted by the integration:The [ledger event log integration](https://github.com/digital-asset/daml-dit-integration-core/blob/master/src/core_int/integration_ledger_event_log.py) is defined like this:```yamlcatalog:    ... elided ...integration_types:    ... elided ...    - id: com.projectdabl.integrations.core.ledger_event_log      name: Ledger Event Log      description: &gt;          Writes a log message for all ledger events.      entrypoint: core_int.integration_ledger_event_log:integration_ledger_event_log_main      env_class: core_int.integration_ledger_event_log:IntegrationLedgerEventLogEnv      fields:          - id: historyBound            name: Transaction History Bound            description: &gt;                Bound on the length of the history maintained by the integration                for the purpose of the log fetch endpoint. -1 can be used to remove                the bound entirely.            field_type: text            default_value: &quot;1024&quot;```* `id` - The symbolic identifier used to select the integration type within the DIT.* `name` - A user friendly name for the integration.* `description` - A description of what the integration does.* `entrypoint` - The package qualified name of the entrypoint function.* `env_class` - The package qualified name of the class used to contain the integration's _environment_.* `fields` - A list of [configuration fields](https://github.com/digital-asset/daml-dit-if#integration-configuration-arguments) that users will be able to enter through the console when configuring an integration. These will be passed into the integration instance at runtime via an instance of `env_class`.The `entrypoint` and `env_class` fields identify by name the twoPython structures that represent the runtime definition of anintegration type's implemenation.The first, `entrypoint`, is required for all integration types andnames a function the framework calls when starting a new instance ofan integration. During the entrypoint function, integrations are ableto register handlers for various sort of events (ledger, web, andotherwise), start coroutines, and access integration configurationarguments specified through the Daml Hub console UI. The syntax for`entrypoint` is `$PYTHON_PACKAGE_NAME:$FUNCTION_NAME`, and the functionitself must have the following signature.```pythonfrom daml_dit_if.api import IntegrationEventsdef integration_ledger_event_log_main(        env: 'IntegrationLedgerEventLogEnv',        events: 'IntegrationEvents'):```The `events` argument is an instance of [`IntegrationEvents`](https://github.com/digital-asset/daml-dit-if/blob/master/daml_dit_if/api/__init__.py#L272)containing a number of [function decorators](https://www.python.org/dev/peps/pep-0318/)that can be used to register [event handlers](https://github.com/digital-asset/daml-dit-if#integration-event-handlers).This represents the bulk of the integration call API, and containsmeans to register handlers for various DAZL ledger events, HTTPSendpoints, timers, and internal message queues. Based on the eventhandlers that the integration registers, the integration framworkconfigures itself appropriately to make those events known to theintegration while it is running.The `env` argument is the environment in which the integration isrunning. This contains named fields with all of the integrationconfiguration parameters, as well as access to various other featuresof the integration framework. These include in-memory queuing andmetadata for the Daml model associated with the integration. Becausethe configuration parameters for an integration can vary fromone integration type to the next, the type of `env` can be specializedto the given integration type. For the example above, the specificenvironment type `IntegrationLedgerEventLogEnv`, is defined asfollows.```pythonfrom daml_dit_if.api import IntegrationEnvironment@dataclassclass IntegrationLedgerEventLogEnv(IntegrationEnvironment):    historyBound: int```The environment class for an integration type is named in`dabl-meta.yaml` with the field `env_class`. This class must derivefrom [`IntegrationEnvironment`](https://github.com/digital-asset/daml-dit-if/blob/master/daml_dit_if/api/__init__.py#L280),and if a specific subclass is not specified, the integration willreceive an instance of `IntegrationEnvironment` as its environmentwhen it starts up.  The syntax for `env_class` is the same as thesyntax for `entrypoint` : `$PYTHON_PACKAGE_NAME:$FUNCTION_NAME`.## LoggingDaml Hub integrations use the default Python logging package, and theframework provides support for controlling log level at runtime. Tointegrate properly with this logic, it is important that integrationsuse the standard mechanism for accessing the integration logger. Thislogger is switched from `INFO` level to `DEBUG` level at a`DABL_LOG_LEVEL` setting of 10 or above.```pythonfrom daml_dit_if.api import getIntegrationLoggerLOG = getIntegrationLogger()```## Integration coroutinesFor integrations that need to maintain ongoing processing independentof event handlers, a coroutine can be returned from theentrypoint. The framework will arrange for this coroutine to bescheduled for execution alongside the other coroutines managed by theframework itself. For an example of this, see the [Exberry integration](https://github.com/digital-asset/daml-dit-integration-exberry/blob/c34e0962631da181a0bcd7ed92ef2aa4fbc8eb46/src/exberry_int/integration_exberry.py#L347).## Integration Event HandlersIntegrations are purely event driven and may only take action inresponse to an event notification from the framework. Integrationsregister their interest in given events by decorating custom functionswith decorators provided to the integration when it is startingup. These decorators are found in the `IntegrationEvents` instancepassed to the entrypoint function. As an example, the [Slack integration](https://github.com/digital-asset/daml-dit-integration-slack)listens for outbound messages as follows.```pythondef integration_slack_main(        env: 'IntegrationEnvironment',        events: 'IntegrationEvents'):... elided ...    @events.ledger.contract_created(        'SlackIntegration.OutboundMessage:OutboundMessage')    async def on_contract_created(event):         ... elided ...```Once the integration has started, `on_contract_created` will be calledfor each DAZL event corresponding to an `OutboundMessage` contractbeing created on the ledger.In addition to ledger events, the framework provides a range of othertypes of integration event handlers:```python@dataclassclass IntegrationEvents:    queue: 'IntegrationQueueEvents'    time: 'IntegrationTimeEvents'    ledger: 'IntegrationLedgerEvents'    webhook: 'IntegrationWebhookRoutes'```* **Ledger** - [DAZL](https://github.com/digital-asset/dazl-client) ledger events. (Contract Archived, Contract Created, Transaction Boundaries, etc.)* **Webhook** - Inbound HTTPS requests from the outside world. (GET or POST)* **Time** - Periodic timer events. (Useful to poll an external system, etc.)* **Queue** - In-memory message queue events. (Useful when none of the other types apply.)### Ledger EventsThe framework provides ledger event handlers for ledger initializtionevents, transaction boundaries, and contract create and archivedevents.  These events are all subject to the Daml ledger visiblitymodel. An integration runs as a specific ledger party with a specificset of rights to the ledger. The integration will only see contractevents visible to that party.All ledger event handlers can return a list of DAZL ledger commands tobe issued by the framework when the event handler returns.```pythonfrom dazl import exercise... elided ...    @events.ledger.contract_created(        'SlackIntegration.OutboundMessage:OutboundMessage')    async def on_contract_created(event):         ... elided ...         return [exercise(event.cid, 'Archive')]```The contract created decorator takes a few of arguments thatcontrol how it presents events to the framework.```python@abc.abstractmethoddef contract_created(self, template: Any, match: 'Optional[ContractMatch]' = None,                     sweep: bool = True, flow: bool = True):````template` is the DAZL template query string for event handler.  Theevent handler will be called only for contracts that match thisquery. It can be `*` to subscribe to all templates, or it can be aqualified template name:```python    @events.ledger.contract_created(        'SlackIntegration.OutboundMessage:OutboundMessage')    async def on_contract_created(eve```If the template name does not specify a full package ID, the frameworkwill assume that the template name refers to a template in theintegration's package and automatically qualify that name with thepackage ID. This eliminates ambiguity if there are multiple templateswith the same symbolic name and eliminates a DAZL error that occurswhen subscribing to contract template that the ledger has not yet seeninstantiated.`sweep` and `flow` control how the event handler sees historical andnewly created contracts on the ledger. If `sweep` is `True`, theframework will sweep the ledger for contracts that already exist whenthe integration is starting up and call the event handler for eachsuch contract when starting up. When `flow` is `True`, the integrationwill receive events corresponding to new contracts that are createdwhile it is running. Note that the framework provides no correspondingcontrol over contract archived events. If an archived event handler isregistered for a contract template, it will receive all visiblearchive events for the template, regardless of whether or not theframework called a created event handler corresponding to thetemplate.### Webhook EventsEach integration instance maintains an `aiohttp` web endpoint that'sused to accept inbound HTTPS requests from external systems. Integrationscan register to receive both `GET` and `POST` requests from externalsystems using the `webhook` event decorators.```python    @events.webhook.post(label='Slack Event Webhook Endpoint')    async def on_webhook_post(request):        body = await request.json()```Each integration is assigned a base Daml Hub URL based on the name ofits enclosing ledger and its integration ID. All webhook URL's for agiven integration are relative to that assigned base URL, and arepresented to the user via the integration status display.Due to their nature, webhook handlers have to have the ability toreturn both a set of ledger commands and an HTTP response. This isaccomodated with the `IntegrationWebhookResponse` class that containsboth a `commands` field and a `response` field.```pythonfrom daml_dit_if.api import IntegrationWebhookResponse    @events.webhook.get(url_suffix='/json', label='JSON Table', auth=AuthorizationLevel.PUBLIC)    async def on_get_table_json(request):        row_data = get_formatted_table_data()        return IntegrationWebhookResponse(            response=json_response({'rows': row_data}))```To populate the `response` of an `IntegrationWebhookResponse`, thereare also several utility functions for generating standard `aiohttp`responses:```pythonfrom daml_dit_if.api import \    json_response, \    empty_success_response, \    blob_success_response, \    unauthorized_response, \    forbidden_response, \    not_found_response, \    bad_request, \    internal_server_error```#### Webhook Event ConfigurationThe full definition for an integration decorator allows severalparameters to control the event handler.```python    @abc.abstractmethod    def get(self, url_suffix: 'Optional[str]' = None, label: 'Optional[str]' = None,             auth: 'Optional[AuthorizationLevel]' = AuthorizationLevel.PUBLIC):````label` is a user friendly description of the event handler URL. It isused to label the status presented to the event handler as it isdisplayed in the console.`url_suffix` is the URL suffix for this event handler relative to theintegration's base URL. It can be used to distinguish multipleendpoints within the same integration if necessary, or left outentirely. `aiohttp` pattern matching works in these suffixes as well.`auth` is the authorization mode of the webhook endpoint. By default,all webhook endpoints are publically visible, but the framework hastwo options for stricter access controls.* `ANY_PARTY` - Requests must be presented with a valid Daml Hub JWT corresponding to the integration's ledger.* `INTEGRATION_PARTY` - Requests must be presented with a valid Daml Hub JWT corresponding to the integration's ledger and party.A JWT is considered to be valid for a given party, only if that partyis listed in both the `readAs` and `actAs` claims.`ANY_PARTY` is intended to be used in scenarios where the integrationmight wish to enforce its own access controls based on anauthenticated user identity. To support this, the framework has twofunctions for extracting the user's identify from an inboundrequest. Note that these functions do not return results on `PUBLIC`endpoints, due to the fact that there is no authentication checkingdone for these endpoints and no notion of request identity.```pythonfrom daml_dit_if.api import    get_request_parties, \    get_single_request_party    ... elided. ..    @events.webhook.get(label='CSV Table', auth=AuthorizationLevel.INTEGRATION_PARTY)    async def on_get_table_csv(request):        row_data = get_formatted_table_data()        LOG.info('&gt;&gt;&gt; %r/%r', get_request_parties(request), get_single_request_party(request))```### Timer EventsTo support time-based activities (polling, etc.), the integrationframework provides support for periodic timer events. These are eventsthat the framework schedules to be invoked at a repeating schedule ata fixed interval. The interval is specified in seconds and thedecorator contains an optional label argument used to populate thedescriptive text on the integration status display.  As with otherevent handlers, the handler for timer events can return a list ofledger commands to be issued by the framework.```python    @events.time.periodic_interval(env.interval, label='Periodic Timer')    async def interval_timer_elapsed():        LOG.debug('Timer elapsed: %r', active_cids)        return [exercise(cid, env.templateChoice, {})                for cid                in active_cids]```The integration framework makes a best effort attempt to call thetimer event handler at the requested periodicity, but no guaranteesare made about exact timing. The requested periodicity should beconsidered a minimum. The event handler for a given timer will not becalled re-entrantly.### Queue EventsThe integration framework also provides in-memory queues and will callqueue event handlers for message placed on those queues. The intent ofthis capability is to provide a way for integrations to respond totypes of events that the other event handlers do not cover.  Anexample of this is the [Symphony integration](https://github.com/digital-asset/daml-dit-integration-symphony),which uses queue events to accept and process incoming messagesreceived from the Symphony client library. Because there is noframework event handler specific to Symphony, the client libraryconnection is opened as part of initialization and is written to placeincoming messages from the socket onto an internal messagingqueue. The queue handler can then take appropriate action on theledger for inbound events. This is also the preferred integrationstrategy for websockets - open the connection when the integrationis initialized and have the connection post events to a framework queuefor integration processing.This is how the Sympnony integration registers the handler:```pythondef integration_symphony_receive_dm_main(        env: 'IntegrationSymphonyReceiveDMEnv',        events: 'IntegrationEvents'):    ... elided ...    @events.queue.message()    async def handle_message(message):        return [create(message['type'], message['payload'])]```Inbound messages are placed into the queue using `env.queue.put(...)`:```python    async def on_im_message(self, im_message):        ... elided ...        await self.env.queue.put(msg_data)```Both the event handler decorator and the `put` call accept an optional`queue_name` that allows messages to be divided into multiple channelsfor separate handling.```pythonclass IntegrationQueueSink:    @abc.abstractmethod    async def put(self, message: 'Any', queue_name: 'str' = 'default'):class IntegrationQueueEvents:    @abc.abstractmethod    def message(self, queue_name: 'str' = 'default'):```There is neither a persistence guarantee nor any retry logic in theinternal queuing mechanism. If a message is placed on an internalqueue and the integration fails or is stopped before the event handleris invoked, the message will not be processed.## Integration Configuration ArgumentsIntegrations can be paramterized using multiple mechanisms, each withpros and cons. By default, every integration (and Daml Hub automationin general) is configured with a ledger party and a label. Theintegration is connected to the ledger as that party, and the label isessentially a comment that can be used to describe the purpose of theautomation.Integrations may also receive configuration information via ledgercontracts. The [CoinDesk Integration](https://github.com/digital-asset/daml-dit-integration-coindesk/blob/master/src/core_int/integration_coindesk_price.py#L99)uses contracts to describe the number of BTC exchange rates tomaintain on the ledger. The integration accepts this configurationvia the ledger directly. This is also the preferred way to communicateAPI keys, tokens etc. to an integration. The Daml ledger privacy modelprevents any private data communicated via contract from exposure tounauthorized parties.For other sorts of configurations, there is also a mechanism by whichintegrations can define configuration fields. These are presented tothe user in the integration configurator and communicated to theintegration via an argument file (`int_args.yaml`) that's parsed bythe framework and passed to the entrypoint function via the `env`argument. These configuration parameters can contain descriptions andbe of data types that are specifically useful integrations. In additonto numbers and strings, there is also support for configuration fieldsthat are enumerations, party identifiers, contract template ID's,contract choice names, etc. They are configured in the `fields` blockof the integration type definition of `dabl_meta.yaml`.Here is an example field list definition from the Ledger Event Logintegration.```yaml      fields:          - id: historyBound            name: Transaction History Bound            description: &gt;                Bound on the length of the history maintained by the integration                for the purpose of the log fetch endpoint. -1 can be used to remove                the bound entirely.            field_type: text            default_value: &quot;1024&quot;```* `id` - The machine readable ID of the field. This corresponds to the field name in the `env_class` class instance used to store the environment.* `name` - The user friendly name of the configuration field.* `description` - Long form text describing the purpose of the configuration field.* `field_type` - The type definition of the configuration field.* `default_value` - An optional default value for the field.* `required` - An optional boolean that can be explicitly set to `false` is the field  is optional| Type Name           | Description ||---------------------|-------------|| _default_ or `text`          | Plain single line text.            || `number`            | A number, decimals allowed.            || `integer`           | An integer. Presented as a field with arrow up/down.            || `party`             | The name of a ledger party. Presented as a dropdown.          || `contract_template` | The name of a contract template on the ledger. Presented as a dropdown            || `contract_choice`   | The name of a choice on a contract template specified by another `contract_template` field. Specified with a JSON reference to the template field: `contract_choice:{&quot;templateNameField&quot;: &quot;targetTemplate&quot;}`  || `enum`              | An enumeration. Specified with a JSON list of choices: `enum:[&quot;Create And Execute&quot;, &quot;Trigger Contract&quot;]`            || `clob`              | Long form text, presented as multiple lines.            |</longdescription>
</pkgmetadata>