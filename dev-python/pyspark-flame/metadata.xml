<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pyspark-flameA low-overhead profiler for Spark on PythonPyspark-flame hooks into Pyspark's existing profiling capabilities to provide alow-overhead stack-sampling profiler, that outputs performance data in aformat compatible with[Brendan Gregg's FlameGraph Visualizer](https://github.com/brendangregg/FlameGraph).Because pyspark-flame hooks into Pyspark's profiling capabilities, it can profilethe entire execution of an RDD, across the whole of the cluster, and providesRDD-level visibility of performance.Unlike the cProfile-based profiler included with Pyspark, pyspark-flame usesstack sampling. It takes stack traces at regular (configurable) intervals,which allows its overhead to be low and tunable, and doesn't skew results,making it suitable for use in performance test environments at high volumes.## Installation```bashpip install pyspark-flame```## Usage```pythonfrom pyspark_flame import FlameProfilerfrom pyspark import SparkConf, SparkContextconf = SparkConf().set(&quot;spark.python.profile&quot;, &quot;true&quot;)conf = conf.set(&quot;spark.python.profile.dump&quot;, &quot;.&quot;)  # Optional - if not, dumps to stdout at exitsc = SparkContext(    'local', 'test', conf=conf, profiler_cls=FlameProfiler,    environment={'pyspark_flame.interval': 0.25}  # Optional - default is 0.2 seconds)# Do stuff with Spark context...sc.show_profiles()# Or maybesc.dump_profiles('.')```For convenience, flamegraph.pl is vendored in, so you can produce a flame graphwith:```bashflamegraph.pl rdd-1.flame &gt; rdd-1.svg```</longdescription>
</pkgmetadata>