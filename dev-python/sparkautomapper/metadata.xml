<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![Build and Test](https://github.com/imranq2/SparkAutoMapper/actions/workflows/build_and_test.yml/badge.svg)](https://github.com/imranq2/SparkAutoMapper/actions/workflows/build_and_test.yml)[![Upload Python Package](https://github.com/imranq2/SparkAutoMapper/actions/workflows/python-publish.yml/badge.svg)](https://github.com/imranq2/SparkAutoMapper/actions/workflows/python-publish.yml)[![Known Vulnerabilities](https://snyk.io/test/github/imranq2/SparkAutoMapper/badge.svg?targetFile=requirements.txt)](https://snyk.io/test/github/imranq2/SparkAutoMapper?targetFile=requirements.txt)# SparkAutoMapperFluent API to map data from one view to another in Spark.  Uses native Spark functions underneath so it is just as fast as hand writing the transformations.Since this is just Python, you can use any Python editor.  Since everything is typed using Python typings, most editors will auto-complete and warn you when you do something wrong## Usage```shell scriptpip install sparkautomapper```## Documentationhttps://icanbwell.github.io/SparkAutoMapper/## SparkAutoMapper input and outputYou can pass either a dataframe to SparkAutoMapper or specify the name of a Spark view to read from.You can receive the result as a dataframe or (optionally) pass in the name of a view where you want the result.## Dynamic Typing Examples#### Set a column in destination to a text value (read from pass in data frame and return the result in a new dataframe)Set a column in destination to a text value```pythonfrom spark_auto_mapper.automappers.automapper import AutoMappermapper = AutoMapper(    keys=[&quot;member_id&quot;]).columns(    dst1=&quot;hello&quot;)```#### Set a column in destination to a text value (read from a Spark view and put result in another Spark view)Set a column in destination to a text value```pythonfrom spark_auto_mapper.automappers.automapper import AutoMappermapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    dst1=&quot;hello&quot;)```#### Set a column in destination to an int valueSet a column in destination to a text value```pythonfrom spark_auto_mapper.automappers.automapper import AutoMappermapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    dst1=1050)```#### Copy a column (src1) from source_view to destination view column (dst1)```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    dst1=A.column(&quot;src1&quot;))```Or you can use the shortcut for specifying a column (wrap column name in [])```pythonfrom spark_auto_mapper.automappers.automapper import AutoMappermapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    dst1=&quot;[src1]&quot;)```#### Convert data type for a column (or string literal)```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    birthDate=A.date(A.column(&quot;date_of_birth&quot;)))```#### Use a Spark SQL Expression (Any valid Spark SQL expression can be used)```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    gender=A.expression(    &quot;&quot;&quot;    CASE        WHEN `Member Sex` = 'F' THEN 'female'        WHEN `Member Sex` = 'M' THEN 'male'        ELSE 'other'    END    &quot;&quot;&quot;    ))```#### Specify multiple transformations```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    dst1=&quot;[src1]&quot;,    birthDate=A.date(&quot;[date_of_birth]&quot;),    gender=A.expression(                &quot;&quot;&quot;    CASE        WHEN `Member Sex` = 'F' THEN 'female'        WHEN `Member Sex` = 'M' THEN 'male'        ELSE 'other'    END    &quot;&quot;&quot;    ))```#### Use variables or parameters```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Adef mapping(parameters: dict):    mapper = AutoMapper(        view=&quot;members&quot;,        source_view=&quot;patients&quot;,        keys=[&quot;member_id&quot;]    ).columns(        dst1=A.column(parameters[&quot;my_column_name&quot;])    )```#### Use conditional logic```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Adef mapping(parameters: dict):    mapper = AutoMapper(        view=&quot;members&quot;,        source_view=&quot;patients&quot;,        keys=[&quot;member_id&quot;]    ).columns(        dst1=A.column(parameters[&quot;my_column_name&quot;])    )        if parameters[&quot;customer&quot;] == &quot;Microsoft&quot;:        mapper = mapper.columns(            important_customer=1,            customer_name=parameters[&quot;customer&quot;]        )    return mapper```#### Using nested array columns```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).withColumn(    dst2=A.list(        [            &quot;address1&quot;,            &quot;address2&quot;        ]    ))```#### Using nested struct columns```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    dst2=A.complex(        use=&quot;usual&quot;,        family=&quot;imran&quot;    ))```#### Using lists of structs```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapper(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).columns(    dst2=A.list(        [            A.complex(                use=&quot;usual&quot;,                family=&quot;imran&quot;            ),            A.complex(                use=&quot;usual&quot;,                family=&quot;[last_name]&quot;            )        ]    ))```## Executing the AutoMapper ```pythonspark.createDataFrame(    [        (1, 'Qureshi', 'Imran'),        (2, 'Vidal', 'Michael'),    ],    ['member_id', 'last_name', 'first_name']).createOrReplaceTempView(&quot;patients&quot;)source_df: DataFrame = spark.table(&quot;patients&quot;)df = source_df.select(&quot;member_id&quot;)df.createOrReplaceTempView(&quot;members&quot;)result_df: DataFrame = mapper.transform(df=df)```## Statically Typed ExamplesTo improve the auto-complete and syntax checking even more, you can define Complex types:Define a custom data type:```pythonfrom spark_auto_mapper.type_definitions.automapper_defined_types import AutoMapperTextInputTypefrom spark_auto_mapper.helpers.automapper_value_parser import AutoMapperValueParserfrom spark_auto_mapper.data_types.date import AutoMapperDateDataTypefrom spark_auto_mapper.data_types.list import AutoMapperListfrom spark_auto_mapper_fhir.fhir_types.automapper_fhir_data_type_complex_base import AutoMapperFhirDataTypeComplexBaseclass AutoMapperFhirDataTypePatient(AutoMapperFhirDataTypeComplexBase):    # noinspection PyPep8Naming    def __init__(self,                 id_: AutoMapperTextInputType,                 birthDate: AutoMapperDateDataType,                 name: AutoMapperList,                 gender: AutoMapperTextInputType                 ) -&gt; None:        super().__init__()        self.value = dict(            id=AutoMapperValueParser.parse_value(id_),            birthDate=AutoMapperValueParser.parse_value(birthDate),            name=AutoMapperValueParser.parse_value(name),            gender=AutoMapperValueParser.parse_value(gender)        )```Now you get auto-complete and syntax checking:```pythonfrom spark_auto_mapper.automappers.automapper import AutoMapperfrom spark_auto_mapper.helpers.automapper_helpers import AutoMapperHelpers as Amapper = AutoMapperFhir(    view=&quot;members&quot;,    source_view=&quot;patients&quot;,    keys=[&quot;member_id&quot;]).withResource(    resource=F.patient(        id_=A.column(&quot;a.member_id&quot;),        birthDate=A.date(            A.column(&quot;date_of_birth&quot;)        ),        name=A.list(            F.human_name(                use=&quot;usual&quot;,                family=A.column(&quot;last_name&quot;)            )        ),        gender=&quot;female&quot;    ))```# Publishing a new package1. Edit VERSION to increment the version2. Create a new release3. The GitHub Action should automatically kick in and publish the package4. You can see the status in the Actions tab</longdescription>
</pkgmetadata>