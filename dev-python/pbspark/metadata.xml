<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># pbsparkThis package provides a way to convert protobuf messages into pyspark dataframes and vice versa using pyspark `udf`s.## InstallationTo install:```bashpip install pbspark```## UsageSuppose we have a pyspark DataFrame which contains a column `value` which has protobuf encoded messages of our `SimpleMessage`:```protobufsyntax = &quot;proto3&quot;;package example;message SimpleMessage {  string name = 1;  int64 quantity = 2;  float measure = 3;}```### Basic conversion functionsThere are two functions for operating on columns, `to_protobuf` and `from_protobuf`. These operations convert to/from an encoded protobuf column to a column of a struct representing the inferred message structure. `MessageConverter` instances (discussed below) can optionally be passed to these functions.```pythonfrom pyspark.sql.session import SparkSessionfrom example.example_pb2 import SimpleMessagefrom pbspark import from_protobuffrom pbspark import to_protobufspark = SparkSession.builder.getOrCreate()example = SimpleMessage(name=&quot;hello&quot;, quantity=5, measure=12.3)data = [{&quot;value&quot;: example.SerializeToString()}]df_encoded = spark.createDataFrame(data)df_decoded = df_encoded.select(from_protobuf(df_encoded.value, SimpleMessage).alias(&quot;value&quot;))df_expanded = df_decoded.select(&quot;value.*&quot;)df_expanded.show()# +-----+--------+-------+# | name|quantity|measure|# +-----+--------+-------+# |hello|       5|   12.3|# +-----+--------+-------+df_reencoded = df_decoded.select(to_protobuf(df_decoded.value, SimpleMessage).alias(&quot;value&quot;))```There are two helper functions, `df_to_protobuf` and `df_from_protobuf` for use on dataframes. They have a kwarg `expanded`, which will also take care of expanding/contracting the data between the single `value` column used in these examples and a dataframe which contains a column for each message field. `MessageConverter` instances (discussed below) can optionally be passed to these functions.```pythonfrom pyspark.sql.session import SparkSessionfrom example.example_pb2 import SimpleMessagefrom pbspark import df_from_protobuffrom pbspark import df_to_protobufspark = SparkSession.builder.getOrCreate()example = SimpleMessage(name=&quot;hello&quot;, quantity=5, measure=12.3)data = [{&quot;value&quot;: example.SerializeToString()}]df_encoded = spark.createDataFrame(data)# expanded=True will perform a `.select(&quot;value.*&quot;)` after converting,# resulting in each protobuf field having its own columndf_expanded = df_from_protobuf(df_encoded, SimpleMessage, expanded=True)df_expanded.show()# +-----+--------+-------+# | name|quantity|measure|# +-----+--------+-------+# |hello|       5|   12.3|# +-----+--------+-------+# expanded=True will first pack data using `struct([df[c] for c in df.columns])`,# use this if the passed dataframe is already expandeddf_reencoded = df_to_protobuf(df_expanded, SimpleMessage, expanded=True)```### Column conversion using the `MessageConverter`The four helper functions above are also available as methods on the `MessageConverter` class. Using an instance of `MessageConverter` we can decode the column of encoded messages into a column of spark `StructType` and then expand the fields.```pythonfrom pyspark.sql.session import SparkSessionfrom pbspark import MessageConverterfrom example.example_pb2 import SimpleMessagespark = SparkSession.builder.getOrCreate()example = SimpleMessage(name=&quot;hello&quot;, quantity=5, measure=12.3)data = [{&quot;value&quot;: example.SerializeToString()}]df_encoded = spark.createDataFrame(data)mc = MessageConverter()df_decoded = df_encoded.select(mc.from_protobuf(df_encoded.value, SimpleMessage).alias(&quot;value&quot;))df_expanded = df_decoded.select(&quot;value.*&quot;)df_expanded.show()# +-----+--------+-------+# | name|quantity|measure|# +-----+--------+-------+# |hello|       5|   12.3|# +-----+--------+-------+df_expanded.schema# StructType(List(StructField(name,StringType,true),StructField(quantity,IntegerType,true),StructField(measure,FloatType,true))```We can also re-encode them into protobuf.```pythondf_reencoded = df_decoded.select(mc.to_protobuf(df_decoded.value, SimpleMessage).alias(&quot;value&quot;))```For expanded data, we can also encode after packing into a struct column:```pythonfrom pyspark.sql.functions import structdf_unexpanded = df_expanded.select(    struct([df_expanded[c] for c in df_expanded.columns]).alias(&quot;value&quot;))df_reencoded = df_unexpanded.select(    mc.to_protobuf(df_unexpanded.value, SimpleMessage).alias(&quot;value&quot;))```### Conversion detailsInternally, `pbspark` uses protobuf's `MessageToDict`, which deserializes everything into JSON compatible objects by default. The exceptions are* protobuf's bytes type, which `MessageToDict` would decode to a base64-encoded string; `pbspark` will decode any bytes fields directly to a spark `BinaryType`.* protobuf's well known type, Timestamp type, which `MessageToDict` would decode to a string; `pbspark` will decode any Timestamp messages directly to a spark `TimestampType` (via python datetime objects).* protobuf's int64 types, which `MessageToDict` would decode to a string for compatibility reasons; `pbspark` will decode these to `LongType`.### Custom conversion of message typesCustom serde is also supported. Suppose we use our `NestedMessage` from the repository's example and we want to serialize the key and value together into a single string.```protobufmessage NestedMessage {  string key = 1;  string value = 2;}```We can create and register a custom serializer with the `MessageConverter`.```pythonfrom pbspark import MessageConverterfrom example.example_pb2 import ExampleMessagefrom example.example_pb2 import NestedMessagefrom pyspark.sql.types import StringTypemc = MessageConverter()# register a custom serializer# this will serialize the NestedMessages into a string rather than a# struct with `key` and `value` fieldsencode_nested = lambda message:  message.key + &quot;:&quot; + message.valuemc.register_serializer(NestedMessage, encode_nested, StringType())# ...from pyspark.sql.session import SparkSessionfrom pyspark import SparkContextfrom pyspark.serializers import CloudPickleSerializersc = SparkContext(serializer=CloudPickleSerializer())spark = SparkSession(sc).builder.getOrCreate()message = ExampleMessage(nested=NestedMessage(key=&quot;hello&quot;, value=&quot;world&quot;))data = [{&quot;value&quot;: message.SerializeToString()}]df_encoded = spark.createDataFrame(data)df_decoded = df_encoded.select(mc.from_protobuf(df_encoded.value, ExampleMessage).alias(&quot;value&quot;))# rather than a struct the value of `nested` is a stringdf_decoded.select(&quot;value.nested&quot;).show()# +-----------+# |     nested|# +-----------+# |hello:world|# +-----------+```### How to write conversion functionsMore generally, custom serde functions should be written in the following format.```python# Encoding takes a message instance and returns the result# of the custom transformation.def encode_nested(message: NestedMessage) -&gt; str:    return message.key + &quot;:&quot; + message.value# Decoding takes the encoded value, a message instance, and path string# and populates the fields of the message instance. It returns `None`.# The path str is used in the protobuf parser to log parse error info.# Note that the first argument type should match the return type of the# encoder if using both.def decode_nested(s: str, message: NestedMessage, path: str):    key, value = s.split(&quot;:&quot;)    message.key = key    message.value = value```### Avoiding PicklingErrorsA seemingly common issue with protobuf and distributed processing is when a `PicklingError` is encountered when transmitting (pickling) protobuf message types from a main process to a fork. To avoid this, you need to ensure that the fully qualified module name in your protoc-generated python file is the same as the module path from which the message type is imported. In other words, for the example here, the descriptor module passed to the builder is `example.example_pb2````python# from example/example_pb2.py_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, &quot;example.example_pb2&quot;, globals())                                                     ^^^^^^^^^^^^^^^^^^^```And to import the message type we would call the same module path:```pythonfrom example.example_pb2 import ExampleMessage     ^^^^^^^^^^^^^^^^^^^```Note that the import module is the same as the one passed to the builder from the protoc-generated python. If these do not match, then you will encounter a `PicklingError`. From the pickle documentation: *pickle can save and restore class instances transparently, however the class definition must be importable and live in the same module as when the object was stored.*To ensure that the module path is correct, you should run `protoc` from the relative root path of your proto files. For example, in this project, in the `Makefile` under the `gen` command, we call `protoc` from the project root rather than from within the `example` directory.```makefileexport PROTO_PATH=.gen:poetry run protoc -I $$PROTO_PATH --python_out=$$PROTO_PATH --mypy_out=$$PROTO_PATH --proto_path=$$PROTO_PATH $$PROTO_PATH/example/*.proto```### Known issues`RecursionError` when using self-referencing protobuf messages. Spark schemas do not allow for arbitrary depth, so protobuf messages which are circular- or self-referencing will result in infinite recursion errors when inferring the schema. If you have message structures like this you should resort to creating custom conversion functions, which forcibly limit the structural depth when converting these messages.## DevelopmentEnsure that [asdf](https://asdf-vm.com/) is installed, then run `make setup`.* To format code `make fmt`* To test code `make test`* To run protoc `make gen`</longdescription>
</pkgmetadata>