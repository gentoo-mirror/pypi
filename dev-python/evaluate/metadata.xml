<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;    &lt;br&gt;    &lt;img src=&quot;https://huggingface.co/datasets/evaluate/media/resolve/main/evaluate-banner.png&quot; width=&quot;400&quot;/&gt;    &lt;br&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;a href=&quot;https://github.com/huggingface/evaluate/actions/workflows/ci.yml?query=branch%3Amain&quot;&gt;        &lt;img alt=&quot;Build&quot; src=&quot;https://github.com/huggingface/evaluate/actions/workflows/ci.yml/badge.svg?branch=main&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/huggingface/evaluate/blob/master/LICENSE&quot;&gt;        &lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/license/huggingface/evaluate.svg?color=blue&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://huggingface.co/docs/evaluate/index&quot;&gt;        &lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/website/http/huggingface.co/docs/evaluate/index.svg?down_color=red&amp;down_message=offline&amp;up_message=online&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/huggingface/evaluate/releases&quot;&gt;        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/huggingface/evaluate.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;CODE_OF_CONDUCT.md&quot;&gt;        &lt;img alt=&quot;Contributor Covenant&quot; src=&quot;https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg&quot;&gt;    &lt;/a&gt;&lt;/p&gt;ðŸ¤— Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized. It currently contains:- **implementations of dozens of popular metrics**: the existing metrics cover a variety of tasks spanning from NLP to Computer Vision, and include dataset-specific metrics for datasets. With a simple command like `accuracy = load(&quot;accuracy&quot;)`, get any of these metrics ready to use for evaluating a ML model in any framework (Numpy/Pandas/PyTorch/TensorFlow/JAX).- **comparisons and measurements**: comparisons are used to measure the difference between models and measurements are tools to evaluate datasets.- **an easy way of adding new evaluation modules to the ðŸ¤— Hub**: you can create new evaluation modules and push them to a dedicated Space in the ðŸ¤— Hub with `evaluate-cli create [metric name]`, which allows you to see easily compare different metrics and their outputs for the same sets of references and predictions.[ðŸŽ“ **Documentation**](https://huggingface.co/docs/evaluate/)ðŸ”Ž **Find a [metric](https://huggingface.co/evaluate-metric), [comparison](https://huggingface.co/evaluate-comparison), [measurement](https://huggingface.co/evaluate-measurement) on the Hub**[ðŸŒŸ **Add a new evaluation module**](https://huggingface.co/docs/evaluate/)ðŸ¤— Evaluate also has lots of useful features like:- **Type checking**: the input types are checked to make sure that you are using the right input formats for each metric- **Metric cards**: each metrics comes with a card that describes the values, limitations and their ranges, as well as providing examples of their usage and usefulness.- **Community metrics:** Metrics live on the Hugging Face Hub and you can easily add your own metrics for your project or to collaborate with others.# Installation## With pipðŸ¤— Evaluate can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)```bashpip install evaluate```# UsageðŸ¤— Evaluate's main methods are:- `evaluate.list_evaluation_modules()` to list the available metrics, comparisons and measurements- `evaluate.load(module_name, **kwargs)` to instantiate an evaluation module- `results = module.compute(*kwargs)` to compute the result of an evaluation module# Adding a new evaluation moduleFirst install the necessary dependencies to create a new metric with the following command:```bashpip install evaluate[template]```Then you can get started with the following command which will create a new folder for your metric and display the necessary steps:```bashevaluate-cli create &quot;Awesome Metric&quot;```See this [step-by-step guide](https://huggingface.co/docs/evaluate/creating_and_sharing) in the documentation for detailed instructions.## CreditsThanks to [@marella](https://github.com/marella) for letting us use the `evaluate` namespace on PyPi previously used by his [library](https://github.com/marella/evaluate).</longdescription>
</pkgmetadata>