<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;  &lt;img alt=&quot;logo&quot; src=&quot;https://www.zypp.io/static/assets/img/logos/zypp/white/500px.png&quot;  width=&quot;200&quot;/&gt;&lt;/p&gt;&lt;br&gt;[![Downloads](https://pepy.tech/badge/df_to_azure)](https://pepy.tech/project/keyvault)[![Open Source](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)](https://opensource.org/)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![PyPI](https://img.shields.io/pypi/v/df_to_azure)](https://pypi.org/project/df-to-azure/)[![Latest release](https://badgen.net/github/release/zypp-io/df_to_azure)](https://github.com/zypp-io/df_to_azure/releases)DF to Azure===&gt; Python module for fast upload of pandas DataFrame to Azure SQL Database using automatic created pipelines in Azure Data Factory.## IntroductionThe purpose of this project is to upload large datasets using Azure Data Factory combined with an Azure SQL Server.In steps the following process kicks off:&lt;p&gt;    1. The data will be uploaded as a .csv file to Azure Blob storage.&lt;br&gt;    2. A SQL table is prepared based on [pandas DataFrame types](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#basics-dtypes),which will be converted to the corresponding [SQLAlchemy types](https://docs.sqlalchemy.org/en/14/core/type_basics.html). &lt;br&gt;    3. A pipeline is created in datafactory for uploading the .csv from the Blob storage into the SQL table.&lt;br&gt;    4. The pipeline is triggered, so that the .csv file is bulk inserted into the SQL table.&lt;br&gt;## How it worksBased on the following attributes, it is possible to bulk insert your dataframe into the SQL Database:```pythonfrom df_to_azure import df_to_azuredf_to_azure(df=df, tablename=&quot;table_name&quot;, schema=&quot;schema&quot;, method=&quot;create&quot;)```1. `df`: dataframe you wish to export2. `tablename`: desired name of the table3. `schema`: desired sql schema4. `method`: option for &quot;create&quot; &quot;append&quot; or &quot;upsert&quot;5. `id_field`: id field of the table. Necessary if `method` is set to &quot;upsert&quot;**Important**: the csv's are uploaded to a container called `dftoazure`, so create this in your storage account before using this module.##### Upsert / create or appendIt is possible to upsert the SQL table with (new) records, if present in the dataframe you want to upload.Based on the id_field, the SQL table is being checked on overlapping values.If there are new records, the &quot;old&quot; records will be updated in the SQL table.The new records will be uploaded and appended to the current SQL table.# SettingsTo use this module, you need to add the `azure subscriptions settings` and `azure data factory settings` to your environment variables.We recommend to work with `.env` files (or even better, automatically load them with [Azure Keyvault](https://pypi.org/project/keyvault/)) and load them in during runtime. But this is optional and they can be set as system variables as well.Use the following template when using `.env`## ParquetSince version 0.6.0, functionality for uploading dataframe to parquet is supported. simply add argument `parquet=True` to upload the dataframe to the Azure storage container parquet.The arguments tablename and schema will be used to create a folder structure. if parquet is set to True, the dataset will not be uploaded to a SQL database.```text# --- ADF SETTINGS ---# data factory settingsrg_name : &quot;&quot;rg_location: &quot;westeurope&quot;df_name : &quot;&quot;# blob settingsls_blob_account_name : &quot;&quot;ls_blob_container_name : &quot;&quot;ls_blob_account_key : &quot;&quot;# SQL settingsSQL_SERVER: &quot;&quot;SQL_DB: &quot;&quot;SQL_USER: &quot;&quot;SQL_PW: &quot;&quot;# --- AZURE SETTINGS ---# azure credentials for connecting to azure subscription.client_id : &quot;&quot;secret : &quot;&quot;tenant : &quot;&quot;subscription_id : &quot;&quot;```## Maintained by [Zypp](https://github.com/zypp-io):- [Melvin Folkers](https://github.com/melvinfolkers)- [Erfan Nariman](https://github.com/erfannariman)## Support:For support on using this module, you can reach us at [hello@zypp.io](mailto:hello@zypp.io)---## TestingTo run the test suite, use:```commandlinepytest df_to_azure```To run pytest for a single test:```commandlinepytest df_to_azure/tests/test_df_to_azure.py::test_duplicate_keys_upsert```</longdescription>
</pkgmetadata>