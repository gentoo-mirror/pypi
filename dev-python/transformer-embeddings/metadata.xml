<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Transformer Embeddings[![PyPI](https://img.shields.io/pypi/v/transformer-embeddings.svg)][pypi_][![Status](https://img.shields.io/pypi/status/transformer-embeddings.svg)][status][![Python Version](https://img.shields.io/pypi/pyversions/transformer-embeddings)][python version][![License](https://img.shields.io/pypi/l/transformer-embeddings)][license][![Tests](https://github.com/HeadspaceMeditation/transformer-embeddings/workflows/Tests/badge.svg?branch=main)][tests][![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white)][pre-commit][![Black](https://img.shields.io/badge/code%20style-black-000000.svg)][black][pypi_]: https://pypi.org/project/transformer-embeddings/[status]: https://pypi.org/project/transformer-embeddings/[python version]: https://pypi.org/project/transformer-embeddings[read the docs]: https://transformer-embeddings.readthedocs.io/[tests]: https://github.com/HeadspaceMeditation/transformer-embeddings/actions?workflow=Tests[codecov]: https://app.codecov.io/gh/HeadspaceMeditation/transformer-embeddings[pre-commit]: https://github.com/pre-commit/pre-commit[black]: https://github.com/psf/blackThis library simplifies and streamlines the usage of encoder transformer models supported by [HuggingFace's `transformers` library](https://github.com/huggingface/transformers/) ([model hub](https://huggingface.co/models) or local) to generate embeddings for string inputs, similar to the way `sentence-transformers` does.Please note that starting with v4, we have dropped support for Python 3.7. If you need to use this library with Python 3.7, the latest compatible release is [`version 3.1.0`](https://pypi.org/project/transformer-embeddings/3.1.0/).## Why use this over HuggingFace's `transformers` or `sentence-transformers`?Under the hood, we take care of:1. Can be used with any model on the HF model hub, with sensible defaults for inference.2. Setting the PyTorch model to `eval` mode.3. Using `no_grad()` when doing the forward pass.4. Batching, and returning back output in the format produced by HF transformers.5. Padding / truncating to model defaults.6. Moving to and from GPUs if available.## InstallationYou can install _Transformer Embeddings_ via [pip] from [PyPI]:```console$ pip install transformer-embeddings```## Usage```pythonfrom transformer_embeddings import TransformerEmbeddingstransformer = TransformerEmbeddings(&quot;model_name&quot;)```If you have a previously instantiated `model` and / or `tokenizer`, you can pass that in.```pythontransformer = TransformerEmbeddings(model=model, tokenizer=tokenizer)``````pythontransformer = TransformerEmbeddings(model_name=&quot;model_name&quot;, model=model)```or```pythontransformer = TransformerEmbeddings(model_name=&quot;model_name&quot;, tokenizer=tokenizer)```**Note:** The `model_name` should be included if only 1 of model or tokenizer are passed in.### EmbeddingsTo get output embeddings:```pythonembeddings = transformer.encode([&quot;Lorem ipsum dolor sit amet&quot;,                                 &quot;consectetur adipiscing elit&quot;,                                 &quot;sed do eiusmod tempor incididunt&quot;,                                 &quot;ut labore et dolore magna aliqua.&quot;])embeddings.output```### Pooled OutputTo get pooled outputs:```pythonfrom transformer_embeddings import TransformerEmbeddings, mean_poolingtransformer = TransformerEmbeddings(&quot;model_name&quot;, return_output=False, pooling_fn=mean_pooling)embeddings = transformer.encode([&quot;Lorem ipsum dolor sit amet&quot;,                                &quot;consectetur adipiscing elit&quot;,                                &quot;sed do eiusmod tempor incididunt&quot;,                                &quot;ut labore et dolore magna aliqua.&quot;])embeddings.pooled```### Exporting the ModelOnce you are done testing and training the model, it can be exported into a single tarball:```pythonfrom transformer_embeddings import TransformerEmbeddingstransformer = TransformerEmbeddings(&quot;model_name&quot;)transformer.export(additional_files=[&quot;/path/to/other/files/to/include/in/tarball.pickle&quot;])```This tarball can also be uploaded to S3, but requires installing the S3 extras (`pip install transformer-embeddings[s3]`). And then using:```pythonfrom transformer_embeddings import TransformerEmbeddingstransformer = TransformerEmbeddings(&quot;model_name&quot;)transformer.export(    additional_files=[&quot;/path/to/other/files/to/include/in/tarball.pickle&quot;],    s3_path=&quot;s3://bucket/models/model-name/date-version/&quot;,)```## ContributingContributions are very welcome. To learn more, see the [Contributor Guide].## LicenseDistributed under the terms of the [Apache 2.0 license][license], _Transformer Embeddings_ is free and open source software.## IssuesIf you encounter any problems, please [file an issue] along with a detailed description.## CreditsThis project was partly generated from [@cjolowicz]'s [Hypermodern Python Cookiecutter] template.[@cjolowicz]: https://github.com/cjolowicz[pypi]: https://pypi.org/[hypermodern python cookiecutter]: https://github.com/cjolowicz/cookiecutter-hypermodern-python[file an issue]: https://github.com/HeadspaceMeditation/transformer-embeddings/issues[pip]: https://pip.pypa.io/&lt;!-- github-only --&gt;[license]: https://github.com/HeadspaceMeditation/transformer-embeddings/blob/main/LICENSE[contributor guide]: https://github.com/HeadspaceMeditation/transformer-embeddings/blob/main/CONTRIBUTING.md[command-line reference]: https://transformer-embeddings.readthedocs.io/en/latest/usage.html</longdescription>
</pkgmetadata>