<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>### Tetun TokenizerTetun tokenizer is a Python package used to tokenize an input text into tokens. There are several tokenization techniques we built alongside this package as follows:1. `TetunStandardTokenizer`: tokenizes the input text into individual tokens based on word boundaries, punctuations, and special characters.2. `TetunWhiteSpaceTokenizer`: tokenizer: breaks the input text into tokens using whitespace as the delimiter.3. `TetunSentenceTokenizer`: splits sentences by  its ending delimiters such as period (.), question mark (?), and exclamation mark (!). The period used to represent titles, such as Dr., P.hD., etc., are preserved.4. `TetunBlankLineTokenizer`: segments the input text based on the presence of blank lines.5. `TetunSimpleTokenizer`: extracts only strings and numbers from the input text while discarding punctuations and special characters.6. `TetunWordTokenizer`: extracts only word units from the input text and excludes numbers, punctuation, and special characters.### InstallationWith pip:```pip install tetun-tokenizer```### UsageTo use the Tetun tokenizer, from the tokenizer module on the Tetun tokenizer package, import a tokenizer feature/class.The examples of its usage are as follows:1. Using  `TetunStandardTokenizer` to tokenize the input text.```pythonfrom tetuntokenizer.tokenizer import TetunStandardTokenizertetun_tokenizer = TetunStandardTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```This will be the output:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak&quot;, '.', &quot;Ha'u&quot;, 'iha', '$', '0.25', 'atu', 'fó', 'ba', 'ita', '.']```2. Using `TetunWhiteSpaceTokenizer` to tokenize the input text.```pythonfrom tetuntokenizer.tokenizer import TetunWhiteSpaceTokenizertetun_tokenizer = TetunWhiteSpaceTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```This will be the output:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak.&quot;, &quot;Ha'u&quot;, 'iha', '$0.25', 'atu', 'fó', 'ba', 'ita.']```3. Using `TetunSentenceTokenizer` to tokenize the input text.```pythonfrom tetuntokenizer.tokenizer import TetunSentenceTokenizertetun_tokenizer = TetunSentenceTokenizer()text = &quot;Ha'u ema-ida ne'ebé baibain de'it. Tebes ga? Ita-nia maluk Dr. ka Ph.D sira hosi U.S.A mós dehan!&quot;output = tetun_tokenizer.tokenize(text)print(output)```This will be the output:```[&quot;Ha'u ema-ida ne'ebé baibain de'it.&quot;, 'Tebes ga?', 'Ita-nia maluk Dr. ka Ph.D sira hosi U.S.A mós dehan!']```4. Using `TetunBlankLineTokenizer` to tokenize the input text.```pythonfrom tetuntokenizer.tokenizer import TetunBlankLineTokenizertetun_tokenizer = TetunBlankLineTokenizer()text = &quot;&quot;&quot;        Ha'u mak ita-nia maluk di'ak.        Ha'u iha $0.25 atu fó ba ita.        &quot;&quot;&quot;output = tetun_tokenizer.tokenize(text)print(output)```This will be the output:```[&quot;\n            Ha'u mak ita-nia maluk di'ak.\n            Ha'u iha $0.25 atu fó ba ita.\n            &quot;]```5. Using `TetunSimpleTokenizer` to tokenize a given text.```pythonfrom tetuntokenizer.tokenizer import TetunSimpleTokenizertetun_tokenizer = TetunSimpleTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```This will be the output:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak&quot;, &quot;Ha'u&quot;, 'iha', '0.25', 'atu', 'fó', 'ba', 'ita']```6. Using `TetunWordTokenizer` to tokenize the input text.```pythonfrom tetuntokenizer.tokenizer import TetunWordTokenizertetun_tokenizer = TetunWordTokenizer()text = &quot;Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.&quot;output = tetun_tokenizer.tokenize(text)print(output)```This will be the output:```[&quot;Ha'u&quot;, 'mak', 'ita-nia', 'maluk', &quot;di'ak&quot;, &quot;Ha'u&quot;, 'iha', 'atu', 'fó', 'ba', 'ita']```To print the resulting output in the console, with each element on a new line, you can use simply use `join()` as follows:```print('\n'.join(output))```The output will be:```Ha'umakita-niamalukdi'akHa'uihaatufóbaita```You can also use the tokenizer to tokenize a text from a file. Here is an example:```python# Assume that we use Path instead of a string for the file pathfrom pathlib import Pathfrom tetuntokenizer.tokenizer import TetunSimpleTokenizerfile_path = Path(&quot;myfile/example.txt&quot;)try:    with file_path.open('r', encoding='utf-8') as f:    contents = [line.strip() for line in f]except FileNotFoundError:    print(f&quot;File not found at: {file_path}&quot;)# You can also lowercase the contents before tokenizing them.lowercase_contents = contents.lower()tetun_tokenizer = TetunSimpleTokenizer()output = '\n'.join(tetun_tokenizer.tokenize(str(lowercase_contents)))print(output)```This is the example of the output:```ha'uorgulludezenvolveha'u-nialiantetun ...```For the source code, visit the [GitHub repository](https://github.com/borulilitimornews/tetun-tokenizer) for this project.</longdescription>
</pkgmetadata>