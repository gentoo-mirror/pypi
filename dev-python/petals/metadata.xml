<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://i.imgur.com/7eR7Pan.png&quot; width=&quot;400&quot;&gt;&lt;br&gt;    Run large language models at home, BitTorrent-style.&lt;br&gt;    Fine-tuning and inference &lt;a href=&quot;https://github.com/bigscience-workshop/petals#benchmarks&quot;&gt;up to 10x faster&lt;/a&gt; than offloading    &lt;br&gt;&lt;br&gt;    &lt;a href=&quot;https://pypi.org/project/petals/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/petals.svg?color=green&quot;&gt;&lt;/a&gt;    &lt;a href=&quot;https://discord.gg/tfHfe8B34k&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/865254854262652969?label=discord&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;    &lt;br&gt;&lt;/p&gt;Generate text with distributed **Llama 2** (70B), **Falcon** (40B+), **BLOOM** (176B) (or their derivatives), and fine‚Äëtune them for your own tasks &amp;mdash; right from your desktop computer or Google Colab:```pythonfrom transformers import AutoTokenizerfrom petals import AutoDistributedModelForCausalLM# Choose any model available at https://health.petals.devmodel_name = &quot;petals-team/StableBeluga2&quot;  # This one is fine-tuned Llama 2 (70B)# Connect to a distributed network hosting model layerstokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoDistributedModelForCausalLM.from_pretrained(model_name)# Run the model as if it were on your computerinputs = tokenizer(&quot;A cat sat&quot;, return_tensors=&quot;pt&quot;)[&quot;input_ids&quot;]outputs = model.generate(inputs, max_new_tokens=5)print(tokenizer.decode(outputs[0]))  # A cat sat on a mat...```&lt;p align=&quot;center&quot;&gt;    üöÄ &amp;nbsp;&lt;b&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing&quot;&gt;Try now in Colab&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;üîè **Privacy.** Your data will be processed with the help of other people in the public swarm. Learn more about privacy [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety). For sensitive data, you can set up a [private swarm](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm) among people you trust.ü¶ô **Want to run Llama 2?** Request access to its weights at the ‚ôæÔ∏è [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and ü§ó [Model Hub](https://huggingface.co/meta-llama/Llama-2-70b-hf), then run `huggingface-cli login` in the terminal before loading the model. Or just try it in our [chatbot app](https://chat.petals.dev).üí¨ **Any questions?** Ping us in [our Discord](https://discord.gg/KdThf2bWVU)!## Connect your GPU and increase Petals capacityPetals is a community-run system &amp;mdash; we rely on people sharing their GPUs. You can check out [available models](https://health.petals.dev) and help serving one of them! As an example, here is how to host a part of [Stable Beluga 2](https://huggingface.co/stabilityai/StableBeluga2) on your GPU:üêß **Linux + Anaconda.** Run these commands for NVIDIA GPUs (or follow [this](https://github.com/bigscience-workshop/petals/wiki/Running-on-AMD-GPU) for AMD):```bashconda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidiapip install git+https://github.com/bigscience-workshop/petalspython -m petals.cli.run_server petals-team/StableBeluga2```ü™ü **Windows + WSL.** Follow [this guide](https://github.com/bigscience-workshop/petals/wiki/Run-Petals-server-on-Windows) on our Wiki.üêã **Docker.** Run our [Docker](https://www.docker.com) image for NVIDIA GPUs (or follow [this](https://github.com/bigscience-workshop/petals/wiki/Running-on-AMD-GPU) for AMD):```bashsudo docker run -p 31330:31330 --ipc host --gpus all --volume petals-cache:/cache --rm \    learningathome/petals:main \    python -m petals.cli.run_server --port 31330 petals-team/StableBeluga2```üçè **macOS + Apple M1/M2 GPU.** Install [Homebrew](https://brew.sh/), then run these commands:```bashbrew install pythonpython3 -m pip install git+https://github.com/bigscience-workshop/petalspython3 -m petals.cli.run_server petals-team/StableBeluga2```&lt;p align=&quot;center&quot;&gt;    üìö &amp;nbsp;&lt;b&gt;&lt;a href=&quot;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-server&quot;&gt;Learn more&lt;/a&gt;&lt;/b&gt; (how to use multiple GPUs, start the server on boot, etc.)&lt;/p&gt;üí¨ **Any questions?** Ping us in [our Discord](https://discord.gg/X7DgtxgMhc)!ü¶ô **Want to host Llama 2?** Request access to its weights at the ‚ôæÔ∏è [Meta AI website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and ü§ó [Model Hub](https://huggingface.co/meta-llama/Llama-2-70b-hf), generate an üîë [access token](https://huggingface.co/settings/tokens), then add `--token YOUR_TOKEN_HERE` to the `python -m petals.cli.run_server` command.üîí **Security.** Hosting a server does not allow others to run custom code on your computer. Learn more [here](https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety).üèÜ **Thank you!** Once you load and host 10+ blocks, we can show your name or link on the [swarm monitor](https://health.petals.dev) as a way to say thanks. You can specify them with `--public_name YOUR_NAME`.## How does it work?- You load a small part of the model, then join a [network](https://health.petals.dev) of people serving the other parts. Single‚Äëbatch inference runs at up to **6 tokens/sec** for **Llama 2** (70B) and up to **4 tokens/sec** for **Falcon** (180B) ‚Äî enough for [chatbots](https://chat.petals.dev) and interactive apps.- You can employ any fine-tuning and sampling methods, execute custom paths through the model, or see its hidden states. You get the comforts of an API with the flexibility of **PyTorch** and **ü§ó Transformers**.&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://i.imgur.com/RTYF3yW.png&quot; width=&quot;800&quot;&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    üìú &amp;nbsp;&lt;b&gt;&lt;a href=&quot;https://arxiv.org/pdf/2209.01188.pdf&quot;&gt;Read paper&lt;/a&gt;&lt;/b&gt;    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;    üìö &amp;nbsp;&lt;b&gt;&lt;a href=&quot;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions&quot;&gt;See FAQ&lt;/a&gt;&lt;/b&gt;&lt;/p&gt;## üìö Tutorials, examples, and moreBasic tutorials:- Getting started: [tutorial](https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing)- Prompt-tune Llama-65B for text semantic classification: [tutorial](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb)- Prompt-tune BLOOM to create a personified chatbot: [tutorial](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb)Useful tools:- [Chatbot web app](https://chat.petals.dev) (connects to Petals via an HTTP/WebSocket endpoint): [source code](https://github.com/petals-infra/chat.petals.dev)- [Monitor](https://health.petals.dev) for the public swarm: [source code](https://github.com/petals-infra/health.petals.dev)Advanced guides:- Launch a private swarm: [guide](https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm)- Run a custom model: [guide](https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals)### BenchmarksPlease see **Section 3.3** of our [paper](https://arxiv.org/pdf/2209.01188.pdf).### üõ†Ô∏è ContributingPlease see our [FAQ](https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#contributing) on contributing.### üìú CitationAlexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel.[Petals: Collaborative Inference and Fine-tuning of Large Models.](https://arxiv.org/abs/2209.01188)_arXiv preprint arXiv:2209.01188,_ 2022.```bibtex@article{borzunov2022petals,  title = {Petals: Collaborative Inference and Fine-tuning of Large Models},  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Ryabinin, Max and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},  journal = {arXiv preprint arXiv:2209.01188},  year = {2022},  url = {https://arxiv.org/abs/2209.01188}}```--------------------------------------------------------------------------------&lt;p align=&quot;center&quot;&gt;    This project is a part of the &lt;a href=&quot;https://bigscience.huggingface.co/&quot;&gt;BigScience&lt;/a&gt; research workshop.&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;https://petals.dev/bigscience.png&quot; width=&quot;150&quot;&gt;&lt;/p&gt;</longdescription>
</pkgmetadata>