<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># CSVS ConvertConverts CSV files into XLSX/SQLITE/POSTGRESQL/PARQUET fast.  ## Install```bashpip install csvs_convert```## Docs[Full Documentaion](http://datapackage_convert.opendata.coop)## Aims* Thorough type guessing of CSV columns, so there is no need to configure types of each field. Scans whole file first to make sure all types in a column are consistent. Can detect over 30 date/time formats as well as JSON data.* Quick conversions/type guessing (uses rust underneath). Uses fast methods specific for each output format:    * `copy` for postgres    * Prepared statements for sqlite using c API.    * Arrow reader for parquet    * Write only mode for libxlsxwriter* Tries to limit errors when inserting data into database by resorting to &quot;text&quot; if type guessing can't determine a more specific type.* When inserting into existing databases automatically migrate schema of target to allow for new data (`evolve` option).* Memory efficient. All csvs and outputs are streamed so all conversions should take up very little memory.* Gather stats and information about CSV files into datapacakge.json file which can use it for customizing conversion.## Drawbacks* CSV files currently need header rows.* Whole file needs to be on disk as whole CSV is analyzed therefore files are read twice.## Conversion DocsThis is the python library, providing bindings to the [rust library](https://github.com/kindly/csvs_convert).[**Contribute on github**](https://github.com/kindly/csvs_convert_py)## Usage From CSV files.```import csvs_convert#sqlitecsvs_convert.csvs_to_sqlite(&quot;sqlite.db&quot;, [&quot;file.csv&quot;])#postgrescsvs_convert.csvs_to_postgres(&quot;postgresql://user:postgres@localhost/db&quot;, [&quot;file.csv&quot;])#parquetcsvs_convert.csvs_to_parquet(&quot;output&quot;, [&quot;file.csv&quot;])#xlsxcsvs_convert.csvs_to_xlsx(&quot;output.xlsx&quot;, [&quot;sqlite.db&quot;])```## Usage from datapackageA datapackage is a file that contains metadata about the tables its specification is described [here](https://datahub.io/docs/data-packages/tabular).To generate `datapackage.json` file you can use:```csvs_convert.csvs_to_datapackage('path/to/datapackage.json', [&quot;fixtures/large/csv/data.csv&quot;])```Other tools can also generate these files.You can use this file and alter it as needed. Mostly it is useful if you want to use the same schema across multiple files, as it will save time not having to do the type guessing for every file.When referring to a datapackage you can either reference:* A `datapackage.json` file.* A datapackage directory containing a `datapackage.json` file. e.g.  `/a/datapackage/dir`* A zip file containing a `datapackage.json` file. e.g. `my_datapackage.zip`### Examples:```import csvs_convert#sqlitecsvs_convert.datapackage_to_sqlite(&quot;sqlite.db&quot;, &quot;path/to/datapackage.json&quot;)#postgrescsvs_convert.datapackage_to_postgres(&quot;postgresql://user:postgres@localhost/db&quot;, &quot;path/to/datapackage.json&quot;)#parquetcsvs_convert.datapackage_to_parquet(&quot;path/to/directory&quot;, [&quot;sqlite.db&quot;])#xlsxcsvs_convert.datapackage_to_xlsx(&quot;output.xlsx&quot;, &quot;path/to/datapackage.json&quot;)```</longdescription>
</pkgmetadata>