<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![pytest](https://github.com/wimpomp/parfor/actions/workflows/pytest.yml/badge.svg)](https://github.com/wimpomp/parfor/actions/workflows/pytest.yml)# ParforUsed to parallelize for-loops using parfor in Matlab? This package allows you to do the same in python.Take any normal serial but parallelizable for-loop and execute it in parallel using easy syntax.Don't worry about the technical details of using the multiprocessing module, race conditions, queues,parfor handles all that. Tested on linux, Windows and OSX with python 3.10.## Why is parfor better than just using multiprocessing?- Easy to use- Using dill instead of pickle: a lot more objects can be used when parallelizing- Progress bars are built-in## Installation`pip install parfor`## UsageParfor decorates a functions and returns the result of that function evaluated in parallel for each iteration ofan iterator.## Requirestqdm, dill## LimitationsObjects passed to the pool need to be dillable (dill needs to serialize them). Generators and SwigPyObjects are examplesof objects that cannot be used. They can be used however, for the iterator argument when using parfor, but itsiterations need to be dillable. You might be able to make objects dillable anyhow using `dill.register` or with`__reduce__`, `__getstate__`, etc.## Arguments### Required:    fun:      function taking arguments: iteration from  iterable, other arguments defined in args &amp; kwargs    iterable: iterable or iterator from which an item is given to fun as a first argument### Optional:    args:   tuple with other unnamed arguments to fun    kwargs: dict with other named arguments to fun    total:  give the length of the iterator in cases where len(iterator) results in an error    desc:   string with description of the progress bar    bar:    bool enable progress bar,                or a callback function taking the number of passed iterations as an argument    pbar:   bool enable buffer indicator bar, or a callback function taking the queue size as an argument    rP:     ratio workers to cpu cores, default: 1    nP:     number of workers, default, None, overrides rP if not None    serial: execute in series instead of parallel if True, None (default): let pmap decide    qsize:  maximum size of the task queue    length: deprecated alias for total    **bar_kwargs: keywords arguments for tqdm.tqdm### Return    list with results from applying the function 'fun' to each iteration of the iterable / iterator## Examples### Normal serial for loop    &lt;&lt;    from time import sleep    a = 3    fun = []    for i in range(10):        sleep(1)        fun.append(a*i**2)    print(fun)    &gt;&gt; [0, 3, 12, 27, 48, 75, 108, 147, 192, 243]    ### Using parfor to parallelize    &lt;&lt;    from time import sleep    from parfor import parfor    @parfor(range(10), (3,))    def fun(i, a):        sleep(1)        return a*i**2    print(fun)    &gt;&gt; [0, 3, 12, 27, 48, 75, 108, 147, 192, 243]    &lt;&lt;    @parfor(range(10), (3,), bar=False)    def fun(i, a):        sleep(1)        return a*i**2    print(fun)    &gt;&gt; [0, 3, 12, 27, 48, 75, 108, 147, 192, 243]### Using parfor in a script/module/.py-fileParfor should never be executed during the import phase of a .py-file. To prevent that from happeninguse the `if __name__ == '__main__':` structure:    &lt;&lt;    from time import sleep    from parfor import parfor        if __name__ == '__main__':        @parfor(range(10), (3,))        def fun(i, a):            sleep(1)            return a*i**2        print(fun)    &gt;&gt; [0, 3, 12, 27, 48, 75, 108, 147, 192, 243]    or:    &lt;&lt;    from time import sleep    from parfor import parfor        def my_fun(*args, **kwargs):        @parfor(range(10), (3,))        def fun(i, a):            sleep(1)            return a*i**2        return fun        if __name__ == '__main__':        print(my_fun())    &gt;&gt; [0, 3, 12, 27, 48, 75, 108, 147, 192, 243]### If you hate decorators not returning a functionpmap maps an iterator to a function like map does, but in parallel    &lt;&lt;    from parfor import pmap    from time import sleep    def fun(i, a):        sleep(1)        return a*i**2    print(pmap(fun, range(10), (3,)))    &gt;&gt; [0, 3, 12, 27, 48, 75, 108, 147, 192, 243]         ### Using generatorsIf iterators like lists and tuples are too big for the memory, use generators instead.Since generators don't have a predefined length, give parfor the length (total) as an argument (optional).         &lt;&lt;    import numpy as np    c = (im for im in imagereader)    @parfor(c, total=len(imagereader))    def fun(im):        return np.mean(im)            &gt;&gt; [list with means of the images]    # Extra's## `pmap`The function parfor decorates, use it like `map`.## `Chunks`Split a long iterator in bite-sized chunks to parallelize## `ParPool`More low-level accessibility to parallel execution. Submit tasks and request the result at any time,(although necessarily submit first, then request a specific task), use different functions and functionarguments for different tasks.</longdescription>
</pkgmetadata>