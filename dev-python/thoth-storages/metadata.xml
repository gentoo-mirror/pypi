<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Thoth Storages--------------.. image:: https://img.shields.io/github/v/tag/thoth-station/storages?style=plastic  :target: https://github.com/thoth-station/storages/releases  :alt: GitHub tag (latest by date).. image:: https://img.shields.io/pypi/v/thoth-storages?style=plastic  :target: https://pypi.org/project/thoth-storages  :alt: PyPI - Module Version.. image:: https://img.shields.io/pypi/l/thoth-storages?style=plastic  :target: https://pypi.org/project/thoth-storages  :alt: PyPI - License.. image:: https://img.shields.io/pypi/dm/thoth-storages?style=plastic  :target: https://pypi.org/project/thoth-storages  :alt: PyPI - DownloadsThis library provides a library called `thoth-storages&lt;https://pypi.org/project/thoth-storages&gt;`__ used in project `Thoth&lt;https://thoth-station.ninja&gt;`__.  The library exposes core queries and methodsfor `PostgreSQL database &lt;https://www.postgresql.org/&gt;`__ as well as adaptersfor manipulating with `Ceph &lt;https://ceph.io/&gt;`__ via its S3 compatible API.Quick Start===========Pre-requisites:* make sure you have ``podman`` and ``podman-compose`` installed. You can install those tools by running ``dnf install -y podman podman-compose``* make sure you are in an environment created with ``pipenv install --dev``To develop locally the first time:* Have a pg dump that you can `retrieve from aws s3  &lt;https://github.com/thoth-station/storages#automatic-backups-of-thoth-deployment&gt;`__* Get the latest PostgreSQL container image from: https://catalog.redhat.com/software/containers/rhel8/postgresql-13/5ffdbdef73a65398111b8362?container-tabs=gti&amp;gti-tabs=red-hat-login* Run ``podman-compose up`` to scale up pods for database and pgweb. For more detail, refer to the `Running PostgreSQL locally section  &lt;https://github.com/thoth-station/storages#running-postgresql-locally&gt;`__* Run this command to sync the pg dump into the local database:  .. code-block:: console    psql -h localhost -p 5432 --username=postgres &lt; pg_dump.sqlNow you are ready to test new queries or `create new migrations&lt;https://github.com/thoth-station/storages#generating-migrations-and-schema-adjustment-in-deployment&gt;`__If you already have a local database, make sure it is not outdated and rember to follow the `Generating migrations and schema adjustment in deployment&lt;https://github.com/thoth-station/storages#generating-migrations-and-schema-adjustment-in-deployment&gt;`__section before testing any changes.Installation and Usage======================The library can be installed via pip or Pipenv from `PyPI&lt;https://pypi.org/project/thoth-storages&gt;`__:.. code-block:: console   pipenv install thoth-storagesThe library provides a CLI that can assist you with exploring schema and datastoring:.. code-block:: console  thoth-storages --help  # In a cloned repo, run:  PYTHONPATH=. pipenv run python3 thoth-storages --helpYou can run prepared test-suite via the following command:.. code-block:: console  pipenv install --dev  pipenv run python3 setup.py testRunning PostgreSQL locally==========================You can use ``docker-compose.yaml`` present in this repository to run a localPostgreSQL instance, (make sure you installed `podman-compose&lt;https://github.com/containers/podman-compose&gt;`__):.. code-block:: console  $ dnf install -y podman podman-compose  $ # Also available from PyPI: pip install podman-compose  $ podman-compose upAfter running the commands above, you should be able to access a localPostgreSQL instance at `localhost:5432 &lt;http://localhost:5432&gt;`__. This is alsothe default configuration for PostgreSQL's adapter that connects to localhostunless ``KNOWLEDGE_GRAPH_HOST`` is supplied explicitly (see also otherenvironment variables in the adapter constructor for more info on configuringthe connection). The default configuration uses database named ``postgres``which can be accessed using ``postgres`` user and ``postgres`` password (SSL isdisabled).The provided ``docker-compose.yaml`` has also `PGweb&lt;https://sosedoff.github.io/pgweb/&gt;`__ enabled to enable data exploration usingUI. To access it visit `localhost:8081 &lt;http://localhost:8081&gt;`__.The provided ``docker-compose.yaml`` does not use any volume. After youcontainers restart, the content will not be available anymore.You can sync your local instance using ``pgsql``:.. code-block:: console  $ psql -h localhost -p 5432 --username=postgres &lt; pg_dump.sqlIf you would like to experiment with PostgreSQL programmatically, you can usethe following code snippet as a starting point:.. code-block:: python  from thoth.storages import GraphDatabase  graph = GraphDatabase()  graph.connect()  # To clear database:  # graph.drop_all()  # To initialize schema in the graph database:  # graph.initialize_schema()Generating migrations and schema adjustment in deployment=========================================================If you make any changes to data model of the main PostgreSQL database, you needto generate migrations. These migrations state how to adjust already existingdatabase with data in deployments. For this purpose, `Alembic migrations&lt;https://alembic.sqlalchemy.org&gt;`__ are used. Alembic can (`partially&lt;https://alembic.sqlalchemy.org/en/latest/autogenerate.html#what-does-autogenerate-detect-and-what-does-it-not-detect&gt;`__)automatically detect what has changed and how to adjust already existingdatabase in a deployment.Alembic uses incremental version control, where each migration is versioned andstates how to migrate from previous state of database to the desired next state- these versions are present in ``alembic/versions`` directory and areautomatically generated with procedure described bellow.If you make any changes, follow the following steps which will generate versionfor you:* Make sure your local PostgreSQL instance is running (follow `Running  PostgreSQL locally` instructions above):  .. code-block:: console    $ podman-compose up* Run Alembic CLI to generate versions for you:  .. code-block:: console    # Make sure you have your environment setup:    # pipenv install --dev    # Make sure you are running the most recent version of schema:    $ PYTHONPATH=. pipenv run alembic upgrade head    # Actually generate a new version:    $ PYTHONPATH=. pipenv run alembic revision --autogenerate -m &quot;Added row to calculate sum of sums which will be divided by 42&quot;* Review migrations generated by Alembic. Note `NOT all changes are  automatically detected by Alembic  &lt;https://alembic.sqlalchemy.org/en/latest/autogenerate.html#what-does-autogenerate-detect-and-what-does-it-not-detect&gt;`__.* Make sure generated migrations are part of your pull request so changes are  propagated to deployments:  .. code-block:: console    $ git add thoth/storages/data/alembic/versions/* In a deployment, use Management API and its ``/graph/initialize`` endpoint to  propagate database schema changes in deployment (Management API has to have  recent schema changes present which are populated with new ``thoth-storages``  releases).* If running locally and you would like to propagate changes, run the following  Alembic command to update migrations to the latest version:  .. code-block:: console    $ PYTHONPATH=. pipenv run alembic upgrade head  If you would like to update schema programmatically run the following Python  code:  .. code-block:: python    from thoth.storages import GraphDatabase    graph = GraphDatabase()    graph.connect()    graph.initilize_schema()When updating a deployment, make sure all the components use the same databaseschema. Metrics exposed from a deployment should state schema version of allthe components in a deployment.Generate schema images======================You can use shipped CLI ``thoth-storages`` to automatically generate schemaimages out of the current models:.. code-block:: console  # First, make sure you have dev packages installed:  $ pipenv install --dev  $ PYTHONPATH=. pipenv run python3 ./thoth-storages generate-schemaThe command above will produce an image named ``schema.png``. Check ``--help``to get more info on available options.If the command above fails with the following exception:.. code-block:: console  FileNotFoundError: [Errno 2] &quot;dot&quot; not found in path.make sure you have ``graphviz`` package installed:.. code-block:: console  dnf install -y graphvizCreating own performance indicators===================================Performance indicators report performance aspect of a library on `Amun&lt;https://github.com/thoth-station/amun-api&gt;`__ and results can be automaticallysynced if the following procedure is respected.To create own performanceindicator, create a script which tests desired functionality of a library. Anexample can be matrix multiplication script present in `thoth-station/performance&lt;https://github.com/thoth-station/performance/blob/master/tensorflow/matmul.py&gt;`__repository. This script can be supplied to `Dependency Monkey&lt;https://thoth-station.ninja/docs/developers/adviser/dependency_monkey.html&gt;`__to validate certain combination of libraries in desired runtime and buildtimeenvironment. Please follow instructions on how to create a performance scriptshown in the `README of performance repo&lt;https://github.com/thoth-station/performance&gt;`__.To create relevant models, adjust``thoth/storages/graph/models_performance.py`` file and add your model.Describe parameters (reported in ``@parameters`` section of performanceindicator result) and result (reported in ``@result``). The name of classshould match ``name`` which is reported by performance indicator run... code-block:: python  class PiMatmul(Base, BaseExtension, PerformanceIndicatorBase):      &quot;&quot;&quot;A class for representing a matrix multiplication micro-performance test.&quot;&quot;&quot;      # Device used during performance indicator run - CPU/GPU/TPU/...      device = Column(String(128), nullable=False)      matrix_size = Column(Integer, nullable=False)      dtype = Column(String(128), nullable=False)      reps = Column(Integer, nullable=False)      elapsed = Column(Float, nullable=False)      rate = Column(Float, nullable=False)All the models use `SQLAchemy &lt;https://www.sqlalchemy.org/&gt;`__.  See `docs&lt;https://docs.sqlalchemy.org/&gt;`__ for more info.Online debugging of queries===========================You can print to logger all the queries that are performed to a PostgreSQLinstance. To do so, set the following environment variable:.. code-block:: console  export THOTH_STORAGES_DEBUG_QUERIES=1Memory usage statisticts========================You can print information about PostgreSQL adapter together with statistics onthe adapter in-memory cache usage to logger (it has to have at least level``INFO`` set). To do so, set the following environment variable:.. code-block:: console  export THOTH_STORAGES_LOG_STATS=1These statistics will be printed once the database adapter is destructed.Automatic backups of Thoth deployment=====================================In each deployment, an automatic knowledge `graph backup cronjob&lt;https://github.com/thoth-station/graph-backup-job&gt;`__ is run, usually once aday. Results of automatic backups are stored on Ceph - you can find them in``s3://&lt;bucket-name&gt;/&lt;prefix&gt;/&lt;deployment-name&gt;/graph-backup/pg_dump-&lt;timestamp&gt;.sql``.Refer to deployment configuration for expansion of parameters in the path.To create a database instance out of this backup file, run a fresh localPostgreSQL instance and fill it from the backup file:.. code-block:: console  $ cd thoth-station/storages  $ aws s3 --endpoint &lt;ceph-s3-endpoint&gt; cp s3://&lt;bucket-name&gt;/&lt;prefix&gt;/&lt;deployment-name&gt;/graph-backup/pg_dump-&lt;timestamp&gt; pg_dump-&lt;timestamp&gt;.sql  $ podman-compose up  $ psql -h localhost -p 5432 --username=postgres &lt; pg_dump-&lt;timestamp&gt;.sql  password: &lt;type password &quot;postgres&quot; here&gt;  &lt;logs will show up&gt;Manual backups of Thoth deployment==================================You can use ``pg_dump`` and ``psql`` utilities to create dumps and restore thedatabase content from dumps. This tool is pre-installed in the container imagewhich is running PostgreSQL so the only thing you need to do is execute``pg_dump`` in Thoth's deployment in a PostgreSQL container to create a dump,use ``oc cp`` to retrieve dump (or directly use ``oc exec`` and create the dumpfrom the cluster) and subsequently ``psql`` to restore the database content.The prerequisite for this is to have access to the running container (editrights)... code-block:: console  # Execute the following commands from the root of this Git repo:  # List PostgreSQL pods running:  $ oc get pod -l name=postgresql  NAME                 READY     STATUS    RESTARTS   AGE  postgresql-1-glwnr   1/1       Running   0          3d  # Open remote shell to the running container in the PostgreSQL pod:  $ oc rsh -t postgresql-1-glwnr bash  # Perform dump of the database:  (cluster-postgres) $ pg_dump &gt; pg_dump-$(date +&quot;%s&quot;).sql  (cluster-postgres) $ ls pg_dump-*.sql   # Remember the current dump name  (cluster-postgres) pg_dump-1569491024.sql  (cluster-postgres) $ exit  # Copy the dump to the current dir:  $ oc cp thoth-test-core/postgresql-1-glwnr:/opt/app-root/src/pg_dump-1569491024.sql  .  # Start local PostgreSQL instance:  $ podman-compose up --detach  &lt;logs will show up&gt;  $ psql -h localhost -p 5432 --username=postgres &lt; pg_dump-1569491024.sql  password: &lt;type password &quot;postgres&quot; here&gt;  &lt;logs will show up&gt;You can ignore error messages related to an owner error like this:.. code-block:: console  STATEMENT:  ALTER TABLE public.python_software_stack OWNER TO thoth;  ERROR:  role &quot;thoth&quot; does not existThe PostgreSQL container uses user &quot;postgres&quot; by default which is differentfrom the one run in the cluster (&quot;thoth&quot;). The role assignment will simply notbe created but data will be available.Syncing results of a workflow run in the cluster================================================Each workflow task in the cluster reports a JSON which states necessaryinformation about the task run (metadata) and actual results. These results ofworkflow tasks are stored on object storage `Ceph &lt;https://ceph.io/&gt;`__ via S3compatible API and later on synced via graph syncs to the knowledge graph. Thecomponent responsible for graph syncs is `graph-sync-job&lt;https://github.com/thoth-station/graph-sync-job&gt;`__ which is written genericenough to sync any data and report metrics about synced data so you don't needto provide such logic on each new workload registered in the system. To syncyour own results of job results (workload) done in the cluster, implementrelated syncing logic in the `sync.py&lt;https://github.com/thoth-station/storages/blob/master/thoth/storages/sync.py&gt;`__and register handler in the ``HANDLERS_MAPPING`` in the same file. The mappingmaps prefix of the document id to the handler (function) which is responsiblefor syncing data into the knowledge base (please mind signatures of existingsyncing functions to automatically integrate with ``sync_documents`` functionwhich is called from ``graph-sync-job``).Query Naming conventions in Thoth===================================For query naming conventions, please read all the docs in `conventions forquery name&lt;https://github.com/thoth-station/storages/blob/master/docs/conventions/README.md&gt;`__.Accessing data on Ceph======================To access data on Ceph, you need to know ``aws_access_key_id`` and ``aws_secret_access_key`` credentialsof endpoint you are connecting to.Absolute file path of data you are accessing is constructed as: ``s3://&lt;bucket_name&gt;/&lt;prefix_name&gt;/&lt;file_path&gt;``There are two ways to initialize the data handler:1. Configure environment variables   .. list-table::      :widths: 25 25      :header-rows: 1      * - Variable name        - Content      * - ``S3_ENDPOINT_URL``        - Ceph Host name      * - ``CEPH_BUCKET``        - Ceph Bucket name      * - ``CEPH_BUCKET_PREFIX``        - Ceph Prefix      * - ``CEPH_KEY_ID``        - Ceph Key ID      * - ``CEPH_SECRET_KEY``        - Ceph Secret Key   .. code-block:: python       from thoth.storages.ceph import CephStore       ceph = CephStore()2. Initialize the object directly with parameters   .. code-block:: python       from thoth.storages.ceph import CephStore       ceph = CephStore(           key_id=&lt;aws_access_key_id&gt;,           secret_key=&lt;aws_secret_access_key&gt;,           prefix=&lt;prefix_name&gt;,           host=&lt;endpoint_url&gt;,           bucket=&lt;bucket_name&gt;)After initialization, you are ready to retrieve data.. code-block:: python    ceph.connect()    try:        # For dictionary stored as json        json_data = ceph.retrieve_document(&lt;file_path&gt;)        # For general blob        blob = ceph.retrieve_blob(&lt;file_path&gt;)    except NotFoundError:        # File does not existAccessing Thoth Data on the Operate-First Public Bucket=======================================================A public instance of Thoth's database is available on the `Operate-First Public Bucket&lt;https://github.com/operate-first/apps/blob/master/docs/content/odh/trino/access_public_bucket.md&gt;`__ for external contributors to start developing components of Thoth.Instructions for accessing the bucket are available in the `documentation&lt;https://github.com/thoth-station/datasets#accessing-thoth-data-on-the-operate-first-public-bucket&gt;`__ of the `thoth/datasets&lt;https://github.com/thoth-station/datasets&gt;`__ repository.Be careful not to store any confidential or valuable information in this bucket as its content can be wiped out at any time.</longdescription>
</pkgmetadata>