<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>![GitHub Repo stars](https://img.shields.io/github/stars/TorkamaniLab/zoish?style=social) ![GitHub forks](https://img.shields.io/github/forks/TorkamaniLab/zoish?style=social) ![GitHub language count](https://img.shields.io/github/languages/count/TorkamaniLab/zoish) ![GitHub repo size](https://img.shields.io/github/repo-size/TorkamaniLab/zoish) ![GitHub](https://img.shields.io/github/license/TorkamaniLab/zoish) ![PyPI - Downloads](https://img.shields.io/pypi/dd/zoish) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/zoish) # ZoishZoish is a Python package that simplifies the machine learning process by using SHAP values for feature importance. It integrates with a range of machine learning models, provides feature selection to enhance performance, and improves model interpretability. With Zoish, users can also visualize feature importance through SHAP summary and bar plots, creating an efficient and user-friendly environment for machine learning development.# IntroductionZoish is a powerful tool for streamlining your machine learning pipeline by leveraging SHAP (SHapley Additive exPlanations) values for feature selection. Designed to work seamlessly with binary and multi-class classification models as well as regression models from sklearn, Zoish is also compatible with gradient boosting frameworks such as CatBoost and LightGBM.## Features- **Model Flexibility:** Zoish exhibits outstanding flexibility as it can work with any tree-based estimator or a superior estimator emerging from a tree-based optimization process. This enables it to integrate seamlessly into binary or multi-class Sklearn classification models, all Sklearn regression models, as well as with advanced gradient boosting frameworks such as CatBoost and LightGBM.  - **Feature Selection:** By utilizing SHAP values, Zoish efficiently determines the most influential features for your predictive models. This improves the interpretability of your model and can potentially enhance model performance by reducing overfitting.- **Visualization:** Zoish includes capabilities for plotting important features using SHAP summary plots and SHAP bar plots, providing a clear and visual representation of feature importance.## DependenciesThe core dependency of Zoish is the `shap` package, which is used to compute the SHAP values for tree based machine learning model. SHAP values are a unified measure of feature importance and they offer an improved interpretation of machine learning models. They are based on the concept of cooperative game theory and provide a fair allocation of the contribution of each feature to the prediction of each instance.## InstallationTo install Zoish, use pip:## InstallationZoish package is available on PyPI and can be installed with pip:```shpip install zoish```For log configuration in development environment use ```shexport env=dev```For log configuration in production environment use ```shexport env=prod```# Examples ```# Built-in librariesimport pandas as pd# Scikit-learn libraries for model selection, metrics, pipeline, impute, preprocessing, compose, and ensemblefrom sklearn.compose import ColumnTransformerfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.feature_selection import SelectFromModelfrom sklearn.impute import SimpleImputerfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorerfrom sklearn.model_selection import GridSearchCV, KFold, train_test_splitfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScaler# Other librariesfrom category_encoders import TargetEncoderfrom xgboost import XGBClassifierfrom zoish.feature_selectors.shap_selectors import ShapFeatureSelector, ShapPlotFeaturesimport loggingfrom zoish import loggerlogger.setLevel(logging.ERROR)from feature_engine.imputation import (    CategoricalImputer,    MeanMedianImputer    )# Set logging levellogger.setLevel(logging.ERROR)```#### Example: Audiology (Standardized) Data Set###### https://archive.ics.uci.edu/ml/datasets/Audiology+%28Standardized%29#### Read data```urldata = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/lymphography/lymphography.data&quot;urlname = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/lung-cancer/lung-cancer.names&quot;# column namescol_names = [    &quot;class&quot;,    &quot;lymphatics&quot;,    &quot;block of affere&quot;,    &quot;bl. of lymph. c&quot;,    &quot;bl. of lymph. s&quot;,    &quot;by pass&quot;,    &quot;extravasates&quot;,    &quot;regeneration of&quot;,    &quot;early uptake in&quot;,    &quot;lym.nodes dimin&quot;,    &quot;lym.nodes enlar&quot;,    &quot;changes in lym.&quot;,    &quot;defect in node&quot;,    &quot;changes in node&quot;,    &quot;special forms&quot;,    &quot;dislocation of&quot;,    &quot;exclusion of no&quot;,    &quot;no. of nodes in&quot;,]data = pd.read_csv(urldata,names=col_names)data.head()```#### Define labels and train-test split```data.loc[(data[&quot;class&quot;] == 1) | (data[&quot;class&quot;] == 2), &quot;class&quot;] = 0data.loc[data[&quot;class&quot;] == 3, &quot;class&quot;] = 1data.loc[data[&quot;class&quot;] == 4, &quot;class&quot;] = 2data[&quot;class&quot;] = data[&quot;class&quot;].astype(int)```#### Train test split```X = data.loc[:, data.columns != &quot;class&quot;]y = data.loc[:, data.columns == &quot;class&quot;]X_train, X_test, y_train, y_test = train_test_split(    X, y, test_size=0.33,  random_state=42)```#### Defining the feature pipeline steps:Here, we use an untuned XGBClassifier model with the ShapFeatureSelector.In the next section, we will repeat the same process but with a tuned XGBClassifier. The aim is to demonstrate that a better estimator can yield improved results when used with the ShapFeatureSelector.```estimator_for_feature_selector= XGBClassifier()     estimator_for_feature_selector.fit(X_train, y_train)shap_feature_selector = ShapFeatureSelector(model=estimator_for_feature_selector, num_features=5, cv = 5, scoring='accuracy', direction='maximum', n_iter=10, algorithm='auto')        # Define pre-processing for numeric columns (float and integer types)numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columnsnumeric_transformer = Pipeline(steps=[    ('imputer', SimpleImputer(strategy='mean')),    ('scaler', StandardScaler())])# Define pre-processing for categorical featurescategorical_features = X_train.select_dtypes(include=['object']).columnscategorical_transformer = Pipeline(steps=[    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),    ('encoder', TargetEncoder(handle_missing='return_nan'))])# Combine preprocessing into one column transformerpreprocessor = ColumnTransformer(    transformers=[        ('num', numeric_transformer, numeric_features),        ('cat', categorical_transformer, categorical_features)])# Feature Selection using ShapSelector feature_selection = shap_feature_selector # Classifier modelclassifier = RandomForestClassifier(n_estimators=100)# Create a pipeline that combines the preprocessor with a feature selection and a classifierpipeline = Pipeline(steps=[('preprocessor', preprocessor),                           ('feature_selection', feature_selection),                           ('classifier', classifier)])# Fit the modelpipeline.fit(X_train, y_train)# Predict on test datay_test_pred = pipeline.predict(X_test)# Output first 10 predictionsprint(y_test_pred[:10])```#### Check performance of the Pipeline```print(&quot;F1 score : &quot;)print(f1_score(y_test, y_test_pred,average='micro'))print(&quot;Classification report : &quot;)print(classification_report(y_test, y_test_pred))print(&quot;Confusion matrix : &quot;)print(confusion_matrix(y_test, y_test_pred))```#### Use better estimator:In this iteration, we will utilize the optimally tuned estimator with the ShapFeatureSelector, which is expected to yield improved results.&quot;```int_cols =  X_train.select_dtypes(include=['int']).columns.tolist()# Define the XGBClassifierxgb_clf = XGBClassifier()# Define the parameter grid for XGBClassifierparam_grid = {    'learning_rate': [0.01, 0.1],    'max_depth': [ 4, 5],    'min_child_weight': [1, 2, 3],    'gamma': [0, 0.1, 0.2],}# Define the scoring functionscoring = make_scorer(f1_score, average='micro')  # Use 'micro' average in case of multiclass target# Set up GridSearchCVgrid_search = GridSearchCV(xgb_clf, param_grid, cv=5, scoring=scoring, verbose=1)grid_search.fit(X_train, y_train)# Fit the GridSearchCV objectestimator_for_feature_selector= grid_search.best_estimator_ shap_feature_selector = ShapFeatureSelector(model=estimator_for_feature_selector, num_features=5, scoring='accuracy', algorithm='auto',cv = 5, n_iter=10, direction='maximum') pipeline =Pipeline([            # int missing values imputers            ('floatimputer', MeanMedianImputer(                imputation_method='mean', variables=int_cols)),                       ('shap_feature_selector', shap_feature_selector),            ('classfier', RandomForestClassifier(n_estimators=100)) ])# Fit the modelpipeline.fit(X_train, y_train)# Predict on test datay_test_pred = pipeline.predict(X_test)# Output first 10 predictionsprint(y_test_pred[:10])            ```#### Performance has improved```print(&quot;F1 score : &quot;)print(f1_score(y_test, y_test_pred,average='micro'))print(&quot;Classification report : &quot;)print(classification_report(y_test, y_test_pred))print(&quot;Confusion matrix : &quot;)print(confusion_matrix(y_test, y_test_pred))#### Shap related plots```#### Plot the features importance```plot_factory = ShapPlotFeatures(shap_feature_selector) ```#### Summary Plot of the selected features```plot_factory.summary_plot()```![summary plot](https://i.imgur.com/jVfNMw8.png)#### Summary Plot of the all features```plot_factory.summary_plot_full()```![summary plot full](https://i.imgur.com/m56u7Me.png)#### Bar Plot of the selected features```plot_factory.bar_plot()```![bar plot](https://i.imgur.com/nRSKoKB.png)#### Bar Plot of the all features```plot_factory.bar_plot_full()```![bar plot full](https://i.imgur.com/UPygQjV.png)More examples are available in the [examples](https://github.com/drhosseinjavedani/zoish/tree/main/examples). ## LicenseLicensed under the [BSD 2-Clause](https://opensource.org/licenses/BSD-2-Clause) License.</longdescription>
</pkgmetadata>