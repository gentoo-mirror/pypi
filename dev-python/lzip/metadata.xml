<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>`lzip` is a Python wrapper for lzlib (https://www.nongnu.org/lzip/lzlib.html) to encode and decode Lzip archives (https://www.nongnu.org/lzip/).This package is compatible with arbitrary byte sequences but provides features to facilitate interoperability with Numpy's `frombuffer` and `tobytes` functions. Decoding and encoding can be performed in chunks, enabling the decompression, processing and compression of files that do not fit in RAM. URLs can be used as well to download, decompress and process the chunks of a remote Lzip archive in one go.```shpip3 install lzip```# Quickstart## CompressCompress an in-memory buffer and write it to a file:```pyimport lziplzip.compress_to_file(&quot;/path/to/output.lz&quot;, b&quot;data to compress&quot;)```Compress multiple chunks and write the result to a single file (useful to avoid large in-memory buffers):```pyimport lzipwith lzip.FileEncoder(&quot;/path/to/output.lz&quot;) as encoder:    encoder.compress(b&quot;data&quot;)    encoder.compress(b&quot; to&quot;)    encoder.compress(b&quot; compress&quot;)```Use `FileEncoder` without context management (`with`):```pyimport lzipencoder = lzip.FileEncoder(&quot;/path/to/output.lz&quot;)encoder.compress(b&quot;data&quot;)encoder.compress(b&quot; to&quot;)encoder.compress(b&quot; compress&quot;)encoder.close()```Compress a Numpy array and write the result to a file:```pyimport lzipimport numpyvalues = numpy.arange(100, dtype=&quot;&lt;u4&quot;)lzip.compress_to_file(&quot;/path/to/output.lz&quot;, values.tobytes())````lzip` can use different compression levels. See the [documentation](#documentation) below for details.## DecompressRead and decompress a file to an in-memory buffer:```pyimport lzipbuffer = lzip.decompress_file(&quot;/path/to/input.lz&quot;)```Read and decompress a file one chunk at a time (useful for large files):```pyimport lzipfor chunk in lzip.decompress_file_iter(&quot;/path/to/input.lz&quot;):    # chunk is a bytes object```Read and decompress a file one chunk at a time, and ensure that each chunk contains a number of bytes that is a multiple of `word_size` (useful to parse numpy arrays with a known dtype):```pyimport lzipimport numpyfor chunk in lzip.decompress_file_iter(&quot;/path/to/input.lz&quot;, word_size=4):    values = numpy.frombuffer(chunk, dtype=&quot;&lt;u4&quot;)```Download and decompress data from a URL:```pyimport lzip# option 1: store the whole decompressed file in a single bufferbuffer = lzip.decompress_url(&quot;http://download.savannah.gnu.org/releases/lzip/lzip-1.22.tar.lz&quot;)# option 2: iterate over the decompressed file in small chunksfor chunk in lzip.decompress_url_iter(&quot;http://download.savannah.gnu.org/releases/lzip/lzip-1.22.tar.lz&quot;):    # chunk is a bytes object````lzip` can also decompress data from an in-memory buffer. See the [documentation](#documentation) below for details.# DocumentationThe present package contains two libraries. `lzip` deals with high-level operations (open and close files, download remote data, change default arguments...) whereas `lzip_extension` focuses on efficiently compressing and decompressing in-memory byte buffers.`lzip` uses `lzip_extension` internally. The latter should only be used in advanced scenarios where fine buffer control is required.- [Quickstart](#quickstart)    - [Compress](#compress)    - [Decompress](#decompress)- [Documentation](#documentation)    - [lzip](#lzip)        - [FileEncoder](#fileencoder)        - [BufferEncoder](#bufferencoder)        - [RemainingBytesError](#remainingbyteserror)        - [compress\_to\_buffer](#compress_to_buffer)        - [compress\_to\_file](#compress_to_file)        - [decompress\_buffer](#decompress_buffer)        - [decompress\_buffer\_iter](#decompress_buffer_iter)        - [decompress\_file](#decompress_file)        - [decompress\_file\_iter](#decompress_file_iter)        - [decompress\_file\_like](#decompress_file_like)        - [decompress\_file\_like\_iter](#decompress_file_like_iter)        - [decompress\_url](#decompress_url)        - [decompress\_url\_iter](#decompress_url_iter)    - [lzip\_extension](#lzip_extension)        - [Decoder](#decoder)        - [Encoder](#encoder)    - [Compare options](#compare-options)    - [Word size and remaining bytes](#word-size-and-remaining-bytes)    - [Default parameters](#default-parameters)- [Publish](#publish)## lzip### FileEncoder```pyclass FileEncoder:    def __init__(self, path, level=6, member_size=(1 &lt;&lt; 51)):        &quot;&quot;&quot;        Encode sequential byte buffers and write the compressed bytes to a file        - path is the output file name, it must be a path-like object such as a string or a pathlib path        - level must be either an integer in [0, 9] or a tuple (directory_size, match_length)          0 is the fastest compression level, 9 is the slowest          see https://www.nongnu.org/lzip/manual/lzip_manual.html for the mapping between          integer levels, directory sizes and match lengths        - member_size can be used to change the compressed file's maximum member size          see the Lzip manual for details on the tradeoffs incurred by this value        &quot;&quot;&quot;    def compress(self, buffer):        &quot;&quot;&quot;        Encode a buffer and write the compressed bytes into the file        - buffer must be a byte-like object, such as bytes or a bytearray        &quot;&quot;&quot;    def close(self):        &quot;&quot;&quot;        Flush the encoder contents and close the file        compress must not be called after calling close        Failing to call close results in a corrupted encoded file        &quot;&quot;&quot;````FileEncoder` can be used as a context manager (`with FileEncoder(...) as encoder`). `close` is called automatically in this case.### BufferEncoder```pyclass BufferEncoder:    def __init__(self, level=6, member_size=(1 &lt;&lt; 51)):        &quot;&quot;&quot;        Encode sequential byte buffers and return the compressed bytes as in-memory buffers        - level: see FileEncoder        - member_size: see FileEncoder        &quot;&quot;&quot;    def compress(self, buffer):        &quot;&quot;&quot;        Encode a buffer and return the compressed bytes as an in-memory buffer        - buffer must be a byte-like object, such as bytes or a bytearray        This function returns a bytes object        The compression algorithm may decide to buffer part or all of the data,        hence the relationship between input (non-compressed) buffers and        output (conpressed) buffers is not one-to-one        In particular, the returned buffer can be empty (b&quot;&quot;) even if the input buffer is not        &quot;&quot;&quot;    def finish(self):        &quot;&quot;&quot;        Flush the encoder contents        This function returns a bytes object        compress must not be called after calling finish        Failing to call finish results in corrupted encoded buffers        &quot;&quot;&quot;```### RemainingBytesError```pyclass RemainingBytesError(Exception):    def __init__(self, word_size, buffer):        &quot;&quot;&quot;        Raised by decompress_* functions if the total number of bytes is not a multiple of word_size        The remaining bytes are stored in self.buffer        See &quot;Word size and remaining bytes&quot; for details        &quot;&quot;&quot;```### compress_to_buffer```pydef compress_to_buffer(buffer, level=6, member_size=(1 &lt;&lt; 51)):    &quot;&quot;&quot;    Encode a single buffer and return the compressed bytes as an in-memory buffer    - buffer must be a byte-like object, such as bytes or a bytearray    - level: see FileEncoder    - member_size: see FileEncoder    This function returns a bytes object    &quot;&quot;&quot;```### compress_to_file```pydef compress_to_file(path, buffer, level=6, member_size=(1 &lt;&lt; 51)):    &quot;&quot;&quot;    Encode a single buffer and write the compressed bytes into a file    - path is the output file name, it must be a path-like object such as a string or a pathlib path    - buffer must be a byte-like object, such as bytes or a bytearray    - level: see FileEncoder    - member_size: see FileEncoder    &quot;&quot;&quot;```### decompress_buffer```pydef decompress_buffer(buffer, word_size=1):    &quot;&quot;&quot;    Decode a single buffer and return the decompressed bytes as an in-memory buffer    - buffer must be a byte-like object, such as bytes or a bytearray    - word_size: see &quot;Word size and remaining bytes&quot;    This function returns a bytes object    &quot;&quot;&quot;```### decompress_buffer_iter```pydef decompress_buffer_iter(buffer, word_size=1):    &quot;&quot;&quot;    Decode a single buffer and return an in-memory buffer iterator    - buffer must be a byte-like object, such as bytes or a bytearray    - word_size: see &quot;Word size and remaining bytes&quot;    This function returns a bytes object iterator    &quot;&quot;&quot;```### decompress_file```pydef decompress_file(path, word_size=1, chunk_size=(1 &lt;&lt; 16)):    &quot;&quot;&quot;    Read and decode a file and return the decompressed bytes as an in-memory buffer    - path is the input file name, it must be a path-like object such as a string or a pathlib path    - word_size: see &quot;Word size and remaining bytes&quot;    - chunk_size: the number of bytes to read from the file at once      large values increase memory usage but very small values impede performance    This function returns a bytes object    &quot;&quot;&quot;```### decompress_file_iter```pydef decompress_file_iter(path, word_size=1, chunk_size=(1 &lt;&lt; 16)):    &quot;&quot;&quot;    Read and decode a file and return an in-memory buffer iterator    - path is the input file name, it must be a path-like object such as a string or a pathlib path    - word_size: see &quot;Word size and remaining bytes&quot;    - chunk_size: see decompress_file    This function returns a bytes object iterator    &quot;&quot;&quot;```### decompress_file_like```pydef decompress_file_like(file_like, word_size=1, chunk_size=(1 &lt;&lt; 16)):    &quot;&quot;&quot;    Read and decode a file-like object and return the decompressed bytes as an in-memory buffer    - file_like is a file-like object, such as a file or a HTTP response    - word_size: see &quot;Word size and remaining bytes&quot;    - chunk_size: see decompress_file    This function returns a bytes object    &quot;&quot;&quot;```### decompress_file_like_iter```pydef decompress_file_like_iter(file_like, word_size=1, chunk_size=(1 &lt;&lt; 16)):    &quot;&quot;&quot;    Read and decode a file-like object and return an in-memory buffer iterator    - file_like is a file-like object, such as a file or a HTTP response    - word_size: see &quot;Word size and remaining bytes&quot;    - chunk_size: see decompress_file    This function returns a bytes object iterator    &quot;&quot;&quot;```### decompress_url```pydef decompress_url(    url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, cafile=None, capath=None, context=None,    word_size=1,    chunk_size=(1 &lt;&lt; 16)):    &quot;&quot;&quot;    Download and decode data from a URL and return the decompressed bytes as an in-memory buffer    - url must be a string or a urllib.Request object    - data, timeout, cafile, capath and context are passed to urllib.request.urlopen      see https://docs.python.org/3/library/urllib.request.html for details    - word_size: see &quot;Word size and remaining bytes&quot;    - chunk_size: see decompress_file    This function returns a bytes object    &quot;&quot;&quot;```### decompress_url_iter```pydef decompress_url_iter(    url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, cafile=None, capath=None, context=None,    word_size=1,    chunk_size=(1 &lt;&lt; 16)):    &quot;&quot;&quot;    Download and decode data from a URL and return an in-memory buffer iterator    - url must be a string or a urllib.Request object    - data, timeout, cafile, capath and context are passed to urllib.request.urlopen      see https://docs.python.org/3/library/urllib.request.html for details    - word_size: see &quot;Word size and remaining bytes&quot;    - chunk_size: see decompress_file    This function returns a bytes object iterator    &quot;&quot;&quot;```## lzip_extensionEven though `lzip_extension` behaves like a conventional Python module, it is written in C++. To keep the implementation simple, only positional arguments are supported (keyword arguments do not work). The Python classes documented below are equivalent to the classes exported by this low-level implementation.You can use `lzip_extension` by importing it like any other module. _lzip.py_ uses it extensively.### Decoder```pyclass Decoder:    def __init__(self, word_size=1):        &quot;&quot;&quot;        Decode sequential byte buffers and return the decompressed bytes as in-memory buffers        - word_size is a non-zero positive integer          all the output buffers contain a number of bytes that is a multiple of word_size        &quot;&quot;&quot;    def decompress(self, buffer):        &quot;&quot;&quot;        Decode a buffer and return the decompressed bytes as an in-memory buffer        - buffer must be a byte-like object, such as bytes or a bytearray        This function returns a bytes object        The compression algorithm may decide to buffer part or all of the data,        hence the relationship between input (compressed) buffers and        output (decompressed) buffers is not one-to-one        In particular, the returned buffer can be empty (b&quot;&quot;) even if the input buffer is not        &quot;&quot;&quot;    def finish(self):        &quot;&quot;&quot;        Flush the encoder contents        This function returns a tuple (buffer, remaining_bytes)          Both buffer and remaining_bytes and bytes objects          buffer should be empty (b&quot;&quot;) unless the file was truncated          remaining_bytes is empty (b&quot;&quot;) unless the total number of bytes decoded          is not a multiple of word_size        decompress must not be called after calling finish        Failing to call finish delays garbage collection which can be an issue        when decoding many files in a row, and prevents the algorithm from detecting        remaining bytes (if the size is not a multiple of word_size)        &quot;&quot;&quot;```### Encoder```pyclass Encoder:    def __init__(self, dictionary_size=(1 &lt;&lt; 23), match_len_limit=36, member_size=(1 &lt;&lt; 51)):        &quot;&quot;&quot;        Encode sequential byte buffers and return the compressed bytes as in-memory buffers        - dictionary_size is an integer in the range [(1 &lt;&lt; 12), (1 &lt;&lt; 29)]        - match_len_limit is an integer in the range [5, 273]        - member_size is an integer in the range [(1 &lt;&lt; 12), (1 &lt;&lt; 51)]        &quot;&quot;&quot;    def compress(self, buffer):        &quot;&quot;&quot;        Encode a buffer and return the compressed bytes as an in-memory buffer        - buffer must be a byte-like object, such as bytes or a bytearray        This function returns a bytes object        The compression algorithm may decide to buffer part or all of the data,        hence the relationship between input (decompressed) buffers and        output (compressed) buffers is not one-to-one        In particular, the returned buffer can be empty (b&quot;&quot;) even if the input buffer is not        &quot;&quot;&quot;    def finish(self):        &quot;&quot;&quot;        Flush the encoder contents        This function returns a bytes object        compress must not be called after calling finish        Failing to call finish results in corrupted encoded buffers        &quot;&quot;&quot;```## Compare optionsThe script _compare_options.py_ uses the `lzip` library to compare the compression ratio of different pairs (dictionary_size, match_len_limit). It runs multiple compressions in parallel and does not store the compressed bytes. About 3 GB of RAM are required to run the script. Processing time depends on the file size and the number of processors on the machine.The script requires matplotlib (`pip3 install matplotlib`) to display the results.```shpython3 compare_options /path/to/uncompressed/file [--chunk-size=65536]```## Word size and remaining bytesDecoding functions take an optional parameter `word_size` that defaults to `1`. Decoded buffers are guaranteed to contain a number of bytes that is a multiple of `word_size` to facilitate fixed-sized words parsing (for example `numpy.frombytes`). If the total size of the uncompressed archive is not a multiple of `word_size`, `lzip.RemainingBytesError` is raised after iterating over the last chunk. The raised exception provides access to the remaining bytes.Non-iter decoding functions do not provide access to the decoded buffers if the total size is not a multiple of `word_size` (only the remaining bytes).The following example decodes a file and converts the decoded bytes to 4-bytes unsigned integers:```pyimport lzipimport numpytry:    for chunk in lzip.decompress_file_iter(&quot;/path/to/archive.lz&quot;, 4):        values = numpy.frombuffer(chunk, dtype=&quot;&lt;u4&quot;)except lzip.RemainingBytesError as error:    # this block is executed only if the number of bytes in &quot;/path/to/archive.lz&quot;    # is not a multiple of 4 (after decompression)    print(error) # prints &quot;The total number of bytes is not a multiple of 4 (k remaining)&quot;                 # where k is in [1, 3]    # error.buffer is a bytes object and contains the k remaining bytes```## Default parametersThe default parameters in `lzip` functions are not constants, despite what is presented in the documentation. The actual implementation looks like this:```pydef some_function(some_parameter=None):    if some_parameter is None:        some_paramter = some_paramter_default_value```This approach makes it possible to change default values at the module level at any time. For example:```pyimport lziplzip.compress_to_file(&quot;/path/to/output0.lz&quot;, b&quot;data to compress&quot;) # encoded at level 6 (default)lzip.default_level = 9lzip.compress_to_file(&quot;/path/to/output1.lz&quot;, b&quot;data to compress&quot;) # encoded at level 9lzip.compress_to_file(&quot;/path/to/output2.lz&quot;, b&quot;data to compress&quot;) # encoded at level 9lzip_default_level = 0lzip.compress_to_file(&quot;/path/to/output1.lz&quot;, b&quot;data to compress&quot;) # encoded at level 0````lzip` exports the following _default_ default values:```pydefault_level = 6default_word_size = 1default_chunk_size = 1 &lt;&lt; 16default_member_size = 1 &lt;&lt; 51```# Publish1. Bump the version number in _version.py_.2. Create a new release on GitHub.</longdescription>
</pkgmetadata>