<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;./docs/source/_static/logo.png&quot; alt=&quot;Outlines Logo&quot; width=300&gt;&lt;/img&gt;# Outlines „Ä∞Ô∏èFast and reliable neural text generation.[Install](#installation) ‚Ä¢[Guided generation](#guided-generation) ‚Ä¢[Prompting primitives](#prompting) ‚Ä¢[Examples](#examples) ‚Ä¢[Stay tuned](#stay-tuned-for)&lt;/div&gt;**Outlines** „Ä∞ is a library for neural text generation. You can think of it as amore flexible replacement for the `generate` method in the[transformers](https://github.com/huggingface/transformers) library.**Outlines** „Ä∞ helps developers *guide text generation* to build robustinterfaces with external systems. Provides generation methods thatguarantee that the output will match a regular expressions, or followa JSON schema.**Outlines** „Ä∞ provides *robust prompting primitives* that separate the promptingfrom the execution logic and lead to simple implementations of few-shotgenerations, ReAct, meta-prompting, agents, etc.**Outlines** „Ä∞ is designed as a *library* that is meant to be compatible thebroader ecosystem, not to replace it. We use as few abstractions as possible,and generation can be interleaved with control flow, conditionals, custom Pythonfunctions and calls to other libraries.**Outlines** „Ä∞ is *compatible with all models*. It only interfaces with modelsvia the next-token logits. It can be used with API-based models as well.## Features- [x] üñçÔ∏èSimple and powerful prompting primitives based on the [Jinja templating engine](https://jinja.palletsprojects.com/)- [x] üöÑ Guided generation, including multiple choice, type constraints and dynamic stopping- [x] ‚ö° Fast [regex-guided generation](#efficient-regex-guided-generation)- [x] üî• Fast [JSON generation](#efficient-json-generation-following-a-pydantic-model) following a JSON schema or a Pydantic model- [x] üêç Interleave completions with loops, conditionals, and custom Python functions- [x] üíæ Caching of generations- [x] ü§ó Integration with HuggingFace's `transformers` modelsOutlines „Ä∞ has new releases and features coming every week! Make sure to ‚≠ê star and üëÄ watch this repository to stay up to date.## Stay tuned for- Context-Free Grammar guided generation ([#178](https://github.com/normal-computing/outlines/pull/178));- Prompt-token alignment so you don't have to think about tokenization details ([#201](https://github.com/normal-computing/outlines/pull/201))- An infilling DSL ([#182](https://github.com/normal-computing/outlines/issues/182))You can follow [@NormalComputing](https://twitter.com/NormalComputing), [@remilouf](https://twitter.com/remilouf) or [@BrandonTWillard](https://twitter.com/BrandonTWillard) for regular updates!## Installation**Outlines** is available on PyPi:``` bashpip install outlines```## Guided generationThe first step towards reliability of systems that include large language modelsis to ensure that there is a well-defined interface between their output anduser-defined code. **Outlines** provides ways to control the generation oflanguage models to make their output more predictable.### Early stoppingYou can stop the generation after a given sequence has been found:``` pythonimport outlines.text.generate as generateimport outlines.models as modelsmodel = models.transformers(&quot;gpt2&quot;)answer = generate.continuation(model, stop=[&quot;.&quot;])(&quot;Tell me a one-sentence joke.&quot;)```### Multiple choicesYou can reduce the completion to a choice between multiple possibilities:``` pythonimport outlines.text.generate as generateimport outlines.models as modelsmodel = models.transformers(&quot;gpt2&quot;)prompt = labelling(&quot;Just awesome&quot;, examples)answer = generate.choice(model, [&quot;Positive&quot;, &quot;Negative&quot;])(prompt)```### Type constraintYou can instruct the model to only return integers or floats:``` pythonimport outlines.text.generate as generateimport outlines.models as modelsmodel = models.transformers(&quot;gpt2&quot;)prompt = &quot;1+1=&quot;answer = generate.integer(model)(prompt)prompt = &quot;sqrt(2)=&quot;answer = generate.float(model)(prompt)```### Efficient regex-guided generationOutlines also comes with fast regex-guided generation. In fact, the `choice`,`integer` and `float` functions above all use regex-guided generation under thehood:``` pythonimport outlines.models as modelsimport outlines.text.generate as generatemodel = models.transformers(&quot;gpt2-medium&quot;)prompt = &quot;Is 1+1=2? &quot;unguided = generate.continuation(model, max_tokens=30)(prompt)guided = generate.regex(model, r&quot;\s*([Yy]es|[Nn]o|[Nn]ever|[Aa]lways)&quot;, max_tokens=30)(    prompt)print(unguided)# Is 1+1=2?## This is probably the most perplexing question.# As I said in one of my articles describing how# I call 2 and 1, there isn'tprint(guided)# Is 1+1=2? Always`````` pythonimport outlines.models as modelsimport outlines.text.generate as generatemodel = models.transformers(&quot;gpt2-medium&quot;)prompt = &quot;What is the IP address of the Google DNS servers? &quot;unguided = generate.continuation(model, max_tokens=30)(prompt)guided = generate.regex(    model,    r&quot;((25[0-5]|2[0-4]\d|[01]?\d\d?)\.){3}(25[0-5]|2[0-4]\d|[01]?\d\d?)&quot;,    max_tokens=30,)(prompt)print(unguided)# What is the IP address of the Google DNS servers?## Passive DNS servers are at DNS servers that are private.# In other words, both IP servers are private. The database# does not contain Chelsea Manningprint(guided)# What is the IP address of the Google DNS servers?# 2.2.6.1```Unlike other libraries, regex-guided generation in Outlines is almost as fastas non-guided generation.### Efficient JSON generation following a Pydantic modelOutlines „Ä∞ allows to guide the generation process so the output is *guaranteed* to follow a [JSON schema](https://json-schema.org/) or [Pydantic model](https://docs.pydantic.dev/latest/):```pythonfrom typing import Listfrom enum import Enumfrom pydantic import BaseModel, constrimport outlines.models as modelsimport outlines.text.generate as generateclass Weapon(str, Enum):    sword = &quot;sword&quot;    axe = &quot;axe&quot;    mace = &quot;mace&quot;    spear = &quot;spear&quot;    bow = &quot;bow&quot;    crossbow = &quot;crossbow&quot;class Armor(str, Enum):    leather = &quot;leather&quot;    chainmail = &quot;chainmail&quot;    plate = &quot;plate&quot;class Character(BaseModel):    name: constr(max_length=10)    age: int    armor: Armor    weapon: Weapon    strength: intmodel = models.transformers(&quot;gpt2&quot;)sequence = generate.json(model, Character)(&quot;Give me a character description&quot;)print(sequence)# {#   &quot;name&quot;: &quot;ranbelt&quot;,#   &quot;age&quot;: 26,#   &quot;armor&quot;: &quot;chainmail&quot;,#   &quot;weapon&quot;: &quot;bow&quot;,#   &quot;strength&quot;: 5# }parsed = Character.model_validate_json(sequence)print(parsed)# name='ranbelt' age=26 armor=&lt;Armor.chainmail: 'chainmail'&gt; weapon=&lt;Weapon.bow: 'bow'&gt; strength=5```The method works with union types, optional types, arrays, nested schemas, etc. Some field constraints are [not supported yet](https://github.com/normal-computing/outlines/issues/215), but everything else should work.## PromptingWriting prompts by concatenating strings in pure Python quickly becomescumbersome: the prompt building logic gets entangled with the rest of theprogram, and the structure of the rendered prompt is obfuscated.**Outlines**makes it easier to write and manage prompts by encapsulating templates inside&quot;template functions&quot;.These functions make it possible to neatly separate the prompt logic from thegeneral program logic; they can be imported from other modules and libraries.Template functions require no superfluous abstraction, they use the Jinja2templating engine to help build complex prompts in a concise manner:``` pythonimport outlines.text as textimport outlines.models as modelsexamples = [    (&quot;The food was digusting&quot;, &quot;Negative&quot;),    (&quot;We had a fantastic night&quot;, &quot;Positive&quot;),    (&quot;Recommended&quot;, &quot;Positive&quot;),    (&quot;The waiter was rude&quot;, &quot;Negative&quot;)]@text.promptdef labelling(to_label, examples):    &quot;&quot;&quot;You are a sentiment-labelling assistant.    {% for example in examples %}    {{ example[0] }} // {{ example[1] }}    {% endfor %}    {{ to_label }} //    &quot;&quot;&quot;model = models.transformers(&quot;gpt2&quot;)prompt = labelling(&quot;Just awesome&quot;, examples)answer = text.generate.continuation(model, max_tokens=100)(prompt)```### ToolsWe can teach language models to call external functions to get additionalinformations or perform tasks, by encoding the functions' description in theprompt. To avoid duplicating information between the function definition and thedescription passed to the prompt, we define custom Jinja filters that canextract the function's name, description, signature and source:``` pythonfrom typing import Callable, Listimport outlines.text as textdef google_search(query: str):    &quot;&quot;&quot;Google Search&quot;&quot;&quot;    passdef wikipedia_search(query: str):    &quot;&quot;&quot;Wikipedia Search&quot;&quot;&quot;    pass@text.promptdef agent(tools: List[Callable]):    &quot;&quot;&quot;AVAILABLE COMMANDS:    {% for tool in tools %}    TOOL    {{ tool | name }}, {{ tool | description }}, args: {{ tool | signature }}    {{ tool | source }}    {% endfor %}    &quot;&quot;&quot;prompt = my_commands([google_search, wikipedia_search])```### Response modelsWe can instruct models to return their output in a pre-defined format, oftenJSON. To avoid duplicating information between the function definition and thedescription passed to the prompt we define a custom Jinja filter that canextract the expected response's schema:``` pythonfrom pydantic import BaseModelimport outlines.text as textclass Joke(BaseModel):    joke: str    explanation: str@text.promptdef joke_ppt(response_model):    &quot;&quot;&quot;Tell a joke and explain why the joke is funny.    RESPONSE FORMAT:    {{ response_model | schema }}    &quot;&quot;&quot;joke_ppt(Joke)# Tell a joke and explain why the joke is funny.## RESPONSE FORMAT:# {#    &quot;joke&quot;: &quot;The joke&quot;#    &quot;explanation&quot;: &quot;The explanation of why the joke is funny&quot;#  }```With these prompting primitives **Outlines** makes building agents like[AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT),[BabyAGI](https://github.com/yoheinakajima/babyagi),[ViperGPT](https://viper.cs.columbia.edu/) or [TransformersAgent](https://huggingface.co/docs/transformers/transformers_agents) easier byremoving boilerplate prompting code.## Contributing### What contributions?We curently only accept bug fixes and documentation contributions. If you have afeature request, please start a new[discussion](https://github.com/normal-computing/outlines/discussions). Theissue tracker is only intended for actionable items.### How to contribute?Run `pip install -e .[test]` or `conda env create -f environment.yml`. To build the documentation you will also need to run `pip install -r requirements-doc.txt`.Before pushing your code to repository please run `pre-commit run --all-files` and `pytest` to make sure that the code is formatted correctly and that the tests pass.Do not hesitate to open a draft PR before your contribution is ready, especially if you have questions and/or need feedback.## Examples- [Pick the odd one out](https://github.com/normal-computing/outlines/blob/main/examples/pick_odd_one_out.py)- [Meta prompting](https://github.com/normal-computing/outlines/blob/main/examples/meta_prompting.py)- [ReAct](https://github.com/normal-computing/outlines/blob/main/examples/meta_prompting.py)- [Generate code to solve math problems](https://github.com/normal-computing/outlines/blob/main/examples/dust/math-generate-code.py)- [BabyAGI](https://github.com/normal-computing/outlines/blob/main/examples/babyagi.py)- [Uncertainty](https://github.com/normal-computing/outlines/blob/main/examples/sampling.ipynb)- [Simulation-based inference](https://github.com/normal-computing/outlines/blob/main/examples/simulation_based_inference.ipynb)## Cite Outlines```@article{willard2023efficient,  title={Efficient Guided Generation for LLMs},  author={Willard, Brandon T and Louf, R{\'e}mi},  journal={arXiv preprint arXiv:2307.09702},  year={2023}}```## LicenseOutlines is open-source and licensed under the [Apache License 2.0](LICENSE).</longdescription>
</pkgmetadata>