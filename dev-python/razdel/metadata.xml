<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;img src=&quot;https://github.com/natasha/natasha-logos/blob/master/razdel.svg&quot;&gt;![CI](https://github.com/natasha/razdel/workflows/CI/badge.svg) [![codecov](https://codecov.io/gh/natasha/razdel/branch/master/graph/badge.svg)](https://codecov.io/gh/natasha/razdel)`razdel` — rule-based system for Russian sentence and word tokenization..## Usage```python&gt;&gt;&gt; from razdel import tokenize&gt;&gt;&gt; tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))&gt;&gt;&gt; tokens[Substring(0, 13, 'Кружка-термос'), Substring(14, 16, 'на'), Substring(17, 20, '0.5'), Substring(20, 21, 'л'), Substring(22, 23, '(') ...]&gt;&gt;&gt; [_.text for _ in tokens]['Кружка-термос', 'на', '0.5', 'л', '(', '50/64', 'см³', ',', '516', ';', '...', ')']``````python&gt;&gt;&gt; from razdel import sentenize&gt;&gt;&gt; text = '''... - &quot;Так в чем же дело?&quot; - &quot;Не ра-ду-ют&quot;.... И т. д. и т. п. В общем, вся газета... '''&gt;&gt;&gt; list(sentenize(text))[Substring(1, 23, '- &quot;Так в чем же дело?&quot;'), Substring(24, 40, '- &quot;Не ра-ду-ют&quot;.'), Substring(41, 56, 'И т. д. и т. п.'), Substring(57, 76, 'В общем, вся газета')]```## Installation`razdel` supports Python 3.5+ and PyPy 3.```bash$ pip install razdel```## Quality, performance&lt;a name=&quot;evalualtion&quot;&gt;&lt;/a&gt;Unfortunately, there is no single correct way to split text into sentences and tokens. For example, one may split `«Как же так?! Захар...» — воскликнут Пронин.` into three sentences `[&quot;«Как же так?!&quot;,  &quot;Захар...»&quot;, &quot;— воскликнут Пронин.&quot;]` while `razdel` splits it into two `[&quot;«Как же так?!&quot;, &quot;Захар...» — воскликнут Пронин.&quot;]`. What would be the correct way to tokenizer `т.е.`? One may split in into `т.|е.`, `razdel` splits into `т|.|е|.`.`razdel` tries to mimic segmentation of these 4 datasets : &lt;a href=&quot;https://github.com/natasha/corus#load_ud_syntag&quot;&gt;SynTagRus&lt;/a&gt;, &lt;a href=&quot;https://github.com/natasha/corus#load_morphoru_corpora&quot;&gt;OpenCorpora&lt;/a&gt;, &lt;a href=&quot;https://github.com/natasha/corus#load_morphoru_gicrya&quot;&gt;GICRYA&lt;/a&gt; and &lt;a href=&quot;https://github.com/natasha/corus#load_morphoru_rnc&quot;&gt;RNC&lt;/a&gt;. These datasets mainly consist of news and fiction. `razdel` rules are optimized for these kinds of texts. Library may perform worse on other domains like social media, scientific articles, legal documents.We measure absolute number of errors. There are a lot of trivial cases in the tokenization task. For example, text `чуть-чуть?!` is not non-trivial, one may split it into `чуть|-|чуть|?|!` while the correct tokenization is `чуть-чуть|?!`, such examples are rare. Vast majority of cases are trivial, for example text `в 5 часов ...` is correctly tokenized even via Python native `str.split` into `в| |5| |часов| |...`. Due to the large number of trivial case overall quality of all segmenators is high, it is hard to compare differentiate between for examlpe 99.33%, 99.95% and 99.88%, so we report the absolute number of errors.`errors` — number of errors. For example, consider etalon segmentation is `что-то|?`, prediction is `что|-|то?`, then the number of errors is 3: 1 for missing split `то?` + 2 for extra splits `что|-|то`.`time` — total seconds taken.`spacy_tokenize`, `aatimofeev` and others a defined in &lt;a href=&quot;https://github.com/natasha/naeval/blob/master/naeval/segment/models.py&quot;&gt;naeval/segment/models.py&lt;/a&gt;. Tables are computed in &lt;a href=&quot;https://github.com/natasha/naeval/blob/master/scripts/segment/main.ipynb&quot;&gt;segment/main.ipynb&lt;/a&gt;.### Tokens&lt;!--- token ---&gt;&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;corpora&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;syntag&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;gicrya&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;rnc&lt;/th&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;re.findall(\w+|\d+|\p+)&lt;/th&gt;      &lt;td&gt;4161&lt;/td&gt;      &lt;td&gt;0.5&lt;/td&gt;      &lt;td&gt;2660&lt;/td&gt;      &lt;td&gt;0.5&lt;/td&gt;      &lt;td&gt;2277&lt;/td&gt;      &lt;td&gt;0.4&lt;/td&gt;      &lt;td&gt;7606&lt;/td&gt;      &lt;td&gt;0.4&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;spacy&lt;/th&gt;      &lt;td&gt;4388&lt;/td&gt;      &lt;td&gt;6.2&lt;/td&gt;      &lt;td&gt;2103&lt;/td&gt;      &lt;td&gt;5.8&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1740&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;4.1&lt;/td&gt;      &lt;td&gt;4057&lt;/td&gt;      &lt;td&gt;3.9&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;nltk.word_tokenize&lt;/th&gt;      &lt;td&gt;14245&lt;/td&gt;      &lt;td&gt;3.4&lt;/td&gt;      &lt;td&gt;60893&lt;/td&gt;      &lt;td&gt;3.3&lt;/td&gt;      &lt;td&gt;13496&lt;/td&gt;      &lt;td&gt;2.7&lt;/td&gt;      &lt;td&gt;41485&lt;/td&gt;      &lt;td&gt;2.9&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;mystem&lt;/th&gt;      &lt;td&gt;4514&lt;/td&gt;      &lt;td&gt;5.0&lt;/td&gt;      &lt;td&gt;3153&lt;/td&gt;      &lt;td&gt;4.7&lt;/td&gt;      &lt;td&gt;2497&lt;/td&gt;      &lt;td&gt;3.7&lt;/td&gt;      &lt;td&gt;&lt;b&gt;2028&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;3.9&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;mosestokenizer&lt;/th&gt;      &lt;td&gt;&lt;b&gt;1886&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;2.1&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1330&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.9&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;1796&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.6&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;2123&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.7&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;segtok.word_tokenize&lt;/th&gt;      &lt;td&gt;2772&lt;/td&gt;      &lt;td&gt;&lt;b&gt;2.3&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1288&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;2.3&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;1759&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.8&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1229&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.8&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;aatimofeev/spacy_russian_tokenizer&lt;/th&gt;      &lt;td&gt;2930&lt;/td&gt;      &lt;td&gt;48.7&lt;/td&gt;      &lt;td&gt;&lt;b&gt;719&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;51.1&lt;/td&gt;      &lt;td&gt;&lt;b&gt;678&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;39.5&lt;/td&gt;      &lt;td&gt;2681&lt;/td&gt;      &lt;td&gt;52.2&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;koziev/rutokenizer&lt;/th&gt;      &lt;td&gt;&lt;b&gt;2627&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.1&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;1386&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.0&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;2893&lt;/td&gt;      &lt;td&gt;&lt;b&gt;0.8&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;9411&lt;/td&gt;      &lt;td&gt;&lt;b&gt;0.9&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;razdel.tokenize&lt;/th&gt;      &lt;td&gt;&lt;b&gt;1510&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;2.9&lt;/td&gt;      &lt;td&gt;1483&lt;/td&gt;      &lt;td&gt;2.8&lt;/td&gt;      &lt;td&gt;&lt;b&gt;322&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;2.0&lt;/td&gt;      &lt;td&gt;2124&lt;/td&gt;      &lt;td&gt;2.2&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;!--- token ---&gt;### Sentencies&lt;!--- sent ---&gt;&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;corpora&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;syntag&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;gicrya&lt;/th&gt;      &lt;th colspan=&quot;2&quot; halign=&quot;left&quot;&gt;rnc&lt;/th&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;      &lt;th&gt;errors&lt;/th&gt;      &lt;th&gt;time&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;re.split([.?!…])&lt;/th&gt;      &lt;td&gt;20456&lt;/td&gt;      &lt;td&gt;0.9&lt;/td&gt;      &lt;td&gt;6576&lt;/td&gt;      &lt;td&gt;0.6&lt;/td&gt;      &lt;td&gt;10084&lt;/td&gt;      &lt;td&gt;0.7&lt;/td&gt;      &lt;td&gt;23356&lt;/td&gt;      &lt;td&gt;1.0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;segtok.split_single&lt;/th&gt;      &lt;td&gt;19008&lt;/td&gt;      &lt;td&gt;17.8&lt;/td&gt;      &lt;td&gt;4422&lt;/td&gt;      &lt;td&gt;13.4&lt;/td&gt;      &lt;td&gt;159738&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1.1&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;164218&lt;/td&gt;      &lt;td&gt;&lt;b&gt;2.8&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;mosestokenizer&lt;/th&gt;      &lt;td&gt;41666&lt;/td&gt;      &lt;td&gt;&lt;b&gt;8.9&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;22082&lt;/td&gt;      &lt;td&gt;&lt;b&gt;5.7&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;12663&lt;/td&gt;      &lt;td&gt;6.4&lt;/td&gt;      &lt;td&gt;50560&lt;/td&gt;      &lt;td&gt;&lt;b&gt;7.4&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;nltk.sent_tokenize&lt;/th&gt;      &lt;td&gt;&lt;b&gt;16420&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;10.1&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;4350&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;5.3&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;7074&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;5.6&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;32534&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;8.9&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;deeppavlov/rusenttokenize&lt;/th&gt;      &lt;td&gt;&lt;b&gt;10192&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;10.9&lt;/td&gt;      &lt;td&gt;&lt;b&gt;1210&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;7.9&lt;/td&gt;      &lt;td&gt;&lt;b&gt;8910&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;6.8&lt;/td&gt;      &lt;td&gt;&lt;b&gt;21410&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;7.0&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;razdel.sentenize&lt;/th&gt;      &lt;td&gt;&lt;b&gt;9274&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;6.1&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;824&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;3.9&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;11414&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;4.5&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;10594&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;7.5&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;!--- sent ---&gt;## Support- Chat — https://telegram.me/natural_language_processing- Issues — https://github.com/natasha/razdel/issues## DevelopmentTest:```bashpip install -e .pip install -r requirements/ci.txtmake testmake int  # 2000 integration tests```Package:```bashmake versiongit pushgit push --tagsmake clean wheel upload````mystem` errors on `syntag`:```bash# see naeval/datacat syntag_tokens.txt | razdel-ctl sample 1000 | razdel-ctl gen | razdel-ctl diff --show moses_tokenize | less```Non-trivial token tests:```bashpv data/*_tokens.txt | razdel-ctl gen --recall | razdel-ctl diff space_tokenize &gt; tests.txtpv data/*_tokens.txt | razdel-ctl gen --precision | razdel-ctl diff re_tokenize &gt;&gt; tests.txt```Update integration tests:```bashcd razdel/tests/data/pv sents.txt | razdel-ctl up sentenize &gt; t; mv t sents.txt````razdel` and `moses` diff:```bashcat data/*_tokens.txt | razdel-ctl sample 1000 | razdel-ctl gen | razdel-ctl up tokenize | razdel-ctl diff moses_tokenize | less````razdel` performance:```bashcat data/*_tokens.txt | razdel-ctl sample 10000 | pv -l | razdel-ctl gen | razdel-ctl diff tokenize | wc -l```</longdescription>
</pkgmetadata>