<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>## Commons Este repositorio de código se crea para implementar la microservicio `Commons` de la infraestructura de Klopp.A continuación se proporciona una descripción de la estructura de los archivos y directorios más importantes:## Template- `setup.py`- [`Notebook`]- `test`- `requirements.txt`    - Blibliotecas necesarias para reproducir el entorno## Estructura del proyecto```├── LICENSE├── Makefile           &lt;- Makefile with commands like `make data` or `make train`├── README.md          &lt;- The top-level README for developers using this project.├── docs               &lt;- A default Sphinx project; see sphinx-doc.org for details├── models             &lt;- Trained and serialized models, model predictions, or model summaries├── experiments │   ├── notebooks      &lt;- Jupyter notebooks. Naming convention is a number (for ordering),│   │    └── mlflow    &lt;- Metretrics and model management │   ├── references     &lt;- Data dictionaries, manuals, and all other explanatory materials.│   ├── processed      &lt;- The final, canonical data sets for modeling. │   └── data  │     ├── external       &lt;- Data from third party sources.│     ├── interim        &lt;- Intermediate data that has been transformed.│     ├── processed      &lt;- The final, canonical data sets for modeling.│     └── raw            &lt;- The original, immutable data dump.├── requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.│                         generated with `pip freeze &gt; requirements.txt`├── setup.py           &lt;- Run this project ├── pipeline           &lt;- Source pipeline for load, preprocessing, training and test │   ├── __init__.py    &lt;- Makes src a Python module│   ├── data           &lt;- Scripts to download or generate data│   │   └── make_dataset.py│   ├── features       &lt;- Scripts to turn raw data into features for modeling│   │   └── build_features.py│   ├── models         &lt;- Scripts to train models and then use trained models to make│   │   │                 predictions│   │   ├── predict_model.py│   │   └── train_model.py│   └── visualization  &lt;- Scripts to create exploratory and results oriented visualizations│       └── visualize.py├── categorization     &lt;- Source code for use in this project.│   ├── __init__.py    &lt;- Makes src a Python module│   ├── categorization.py &lt;- class and method run() for app running │   ├── classifier.py   &lt;- Class for model ML│   ├── consumer.py  &lt;- class for Kafka consumer │   ├── controller_dynamo_db.py &lt;- class for management CRUD │   ├── controller_ml_fow.py   &lt;- Class for management models│   ├── controller_posgrest_db.py  &lt;- class for managemen CRUD  │   ├── producer.py &lt;- class for Kafka producer│   ├── nicknames.py   &lt;- Class │   ├── merchantnames.py  &lt;- class │   └── logs       &lt;- folder for logs files └── tox.ini            &lt;- tox file with settings for running tox;(automate and standardize testing)```## Reproducir proyectos ## Software necesarioEl proyecto se desarrollo con los siguientes requisitos a primer nivel :Python 3.10.4Se recomienda a nivel de desarrollo utilizar un entorno virtual administrado por conda.`conda create -n categorization python=3.10.4` Use sólo pip como gestor de paquetería después de crear en entorno virtual con conda.Los requisitos de las bibliotecas necesarias se pueden pasar a pip a través del archivo `requiremets.txt`pip install -r requirements.txtVer pagína de [python](https://requirements-txt.readthedocs.io/en/latest/#:~:text=txt%20installing%20them%20using%20pip.&amp;text=The%20installation%20process%20include%20only,That's%20it.&amp;text=Customize%20it%20the%20way%20you,allow%20or%20disallow%20automated%20requirements)Otra opcíon es utilizar un docker oficial de python con la versión cómo  3.10 como mínima. Esta es sólo si utilizas Linux o Windows como sistema operativo, existe problemas de compatibilidad para MacBooks M1[Docker Hub de Python](https://hub.docker.com/_/python)- Para el entorno local se utiliza [Jupyer Notebook] como entorno de experimentación- Para administrar los modelos de ML se utiliza [MLFlow]() con Posgrestdb- Como gestor de bases de datos relacional se utiliza PosgrestDB- Para almacenar información no estructurada se utiliza DynamoDB- Para versionamiento de los dataset se utiliza [DVC]- Para autoformatting se utilizan los paquetes [`Back`](), [Flake8]()  y [autopep8] () - Para pruebas unitarias se utiliza el paquete estándar de python `unittest` </longdescription>
</pkgmetadata>