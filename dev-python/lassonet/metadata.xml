<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>[![PyPI version](https://badge.fury.io/py/lassonet.svg)](https://badge.fury.io/py/lassonet)[![Downloads](https://static.pepy.tech/badge/lassonet)](https://pepy.tech/project/lassonet)# LassoNetLassoNet is a new family of models to incorporate feature selection and neural networks.LassoNet works by adding a linear skip connection from the input features to the output. A L1 penalty (LASSO-inspired) is added to that skip connection along with a constraint on the network so that whenever a feature is ignored by the skip connection, it is ignored by the whole network.&lt;a href=&quot;https://www.youtube.com/watch?v=bbqpUfxA_OA&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/lasso-net/lassonet/master/docs/images/video_screenshot.png&quot; width=&quot;450&quot; alt=&quot;Promo Video&quot;/&gt;&lt;/a&gt;## Installation```pip install lassonet```## UsageWe have designed the code to follow scikit-learn's standards to the extent possible (e.g. [linear_model.Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)).```from lassonet import LassoNetClassifierCV model = LassoNetClassifierCV() # LassoNetRegressorCVpath = model.fit(X_train, y_train)print(&quot;Best model scored&quot;, model.score(X_test, y_test))print(&quot;Lambda =&quot;, model.best_lambda_)```You should always try to give normalized data to LassoNet as it uses neural networks under the hood.You can read the full [documentation](https://lassonet.ml/lassonet/api/) or read the [examples](https://github.com/lasso-net/lassonet/tree/master/examples) that cover all features.## Features- regression, classification and [Cox regression](https://en.wikipedia.org/wiki/Proportional_hazards_model) with `LassoNetRegressor`, `LassoNetClassifier` and `LassoNetCoxRegressor`.- cross-validation with `LassoNetRegressorCV`, `LassoNetClassifierCV` and `LassoNetCoxRegressorCV`- group feature selection with the `groups` argument- `lambda_start=&quot;auto&quot;` heuristicNote that cross-validation, group feature selection and automatic `lambda_start` selection have not been published in papers, you can read the code or [post as issue](https://github.com/lasso-net/lassonet/issues/new) to request more details.We are currently working (among others) on adding support for convolution layers, auto-encoders and online logging of experiments.## Cross-validationThe original paper describes how to train LassoNet along a regularization path. This requires the user to manually select a model from the path and made the `.fit()` method useless since the resulting model is always empty. This feature is still available with the `.path()` method for any model or the `lassonet_path` function and returns a list of checkpoints that can be loaded with `.load()`.Since then, we integrated support for cross-validation (5-fold by default) in the estimators whose name ends with `CV`. For each fold, a path is trained. The best regularization value is then chosen to maximize the average performance over all folds. The model is then retrained on the whole training dataset to reach that regularization.## WebsiteLassoNet's website is [https://lassonet.ml](https://lassonet.ml). It contains many useful references including the paper, live talks and additional documentation.## References- Lemhadri, Ismael, Feng Ruan, Louis Abraham, and Robert Tibshirani. &quot;LassoNet: A Neural Network with Feature Sparsity.&quot; Journal of Machine Learning Research 22, no. 127 (2021). [pdf](https://arxiv.org/pdf/1907.12207.pdf) [bibtex](https://github.com/lasso-net/lassonet/blob/master/citation.bib)- Yang, Xuelin, Louis Abraham, Sejin Kim, Petr Smirnov, Feng Ruan, Benjamin Haibe-Kains, and Robert Tibshirani. &quot;FastCPH: Efficient Survival Analysis for Neural Networks.&quot; arXiv preprint arXiv:2208.09793 (2022). [pdf](https://arxiv.org/pdf/2208.09793.pdf)</longdescription>
</pkgmetadata>