<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Steamship Python Client Library For LangChain (ü¶úÔ∏èüîó)[![Steamship](https://raw.githubusercontent.com/steamship-core/python-client/main/badge.svg)](https://www.steamship.com/build/langchain-apps?utm_source=github&amp;utm_medium=badge&amp;utm_campaign=github_repo&amp;utm_id=github_langchain_repo) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/getsteamship.svg?style=social&amp;label=Follow%20%40GetSteamship)](https://twitter.com/GetSteamship) [![](https://dcbadge.vercel.app/api/server/5Vry5ANVwT?compact=true&amp;style=flat)](https://discord.gg/5Vry5ANVwT)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Run Tests](https://github.com/steamship-core/steamship-langchain/actions/workflows/test-main.yml/badge.svg?branch=main)](https://github.com/steamship-core/steamship-langchain/actions/workflows/test-main.yml)[Steamship](https://steamship.com/) is the fastest way to build, ship, and use full-lifecycle language AI.This repository contains [LangChain](https://langchain.readthedocs.io/en/latest/) adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship to automatically get:- Production-ready API endpoint(s)- Horizontal scaling across dependencies / backends- Persistent storage of app state (including caches)- Built-in support for Authn/z - Multi-tenancy support- Seamless integration with other Steamship skills (ex: audio transcription) - Usage Metrics and Logging- And more...Read more about Steamship and LangChain on our [website](https://www.steamship.com/build/langchain-apps?utm_source=github&amp;utm_medium=explainer&amp;utm_campaign=github_repo&amp;utm_id=github_langchain_repo). ## InstallingInstall via pip:```commandlinepip install steamship-langchain```## AdaptersInitial support is offered for the following (with more to follow soon):- LLMs  - An adapter is provided for Steamship's OpenAI integration (`steamship_langchain.llms.OpenAI`)  - An adapter is provided for *caching* LLM calls, via Steamship's Key-Value store (`SteamshipCache`) - Callbacks  - A callback that uses Python's `logging` module to record events is provided (`steamship_langchain.callbacks.LoggingCallbackHandler`). This can be used with `ship logs` to access verbose logs when deployed.- Document Loaders  - An adapter for exporting Steamship Files as LangChain Documents is provided (`steamship_langchain.document_loaders.SteamshipLoader`)- Tools  - Search:    - An adapter is provided for Steamship's SERPAPI integration (`SteamshipSERP`)- Memory  - Chat History (`steamship_langchain.memory.ChatMessageHistory`)- VectorStores  - An adapter is provided for a persistent VectorStore (`steamship_langchain.vectorstores.SteamshipVectorStore`)- Text Splitters  - A splitter for Python code, based on the AST, is provided (`steamship_langchain.python_splitter.PythonCodeSplitter`). This provides additional context for code snippets (parent classes) while breaking the code into segments around function definitions.- Miscellaneous Utilities  - Importing data into Steamship    - In order to take advantage of Steamship's persistent storage, an initial set of loader utilities are provided for a variety of sources, including:      - Text files: `steamship_langchain.file_loaders.TextFileLoader`      - Directories: `steamship_langchain.file_loaders.DirectoryLoader`      - GitHub repositories: `steamship_langchain.file_loaders.GitHubRepositoryLoader`      - Sphinx documentation sites: `steamship_langchain.file_loaders.SphinxSiteLoader` (and others)      - YouTube videos: `steamship_langchain.file_loaders.YouTubeFileLoader`      - Various text and image formats: `steamship_langchain.file_loaders.UnstructuredFileLoader`## üìñ DocumentationPlease see our [here](https://steamship-langchain.readthedocs.org/) for full documentation on:- Getting started (installation, setting up the environment, simple examples)- How-To examples (demos, integrations, helper functions)## Example Use CasesHere are a few examples of using LangChain on Steamship:- [Basic Prompting](#basic-prompting)- [Self Ask With Search](#self-ask-with-search)- [ChatBot](#chatbot)- [Summarize Audio](#summarize-audio--async-chaining-)- [Question Answering With Sources](#question-answering-with-sources--embeddings-)The examples use temporary workspaces to provide full cleanup during experimentation.[Workspaces](https://docs.steamship.com/workspaces/index.html) provide a unit of tenant isolation within Steamship.For production uses, persistent workspaces can be created and retrieved via `Steamship(workspace_handle=&quot;my_workspace&quot;)` .&gt; **NOTE**&gt; These examples omit `import` blocks. Please consult the `examples/` directory for complete source code. &gt; **NOTE** &gt; Client examples assume that the user has a Steamship API key and that it is exposed to the environment (see: [API Keys](#api-keys))### Basic PromptingExample of a basic prompt using a Steamship LLM integration (full source: [examples/greeting](./examples/greeting))[![Run on Repl.it](https://replit.com/badge/github/@SteamshipDoug/Simple-LangChain-Prompting-on-Steamship)](https://replit.com/@SteamshipDoug/Simple-LangChain-Prompting-on-Steamship)#### Server Snippet```pythonfrom steamship_langchain.llms import OpenAI@post(&quot;greet&quot;)def greet(self, user: str) -&gt; str:    prompt = PromptTemplate(      input_variables=[&quot;user&quot;],      template=      &quot;Create a welcome message for user {user}. Thank them for running their LangChain app on Steamship. &quot;      &quot;Encourage them to deploy their app via `ship it` when ready.&quot;,    )    llm = OpenAI(client=self.client, temperature=0.8)    return llm(prompt.format(user=user))```#### Client Snippet```pythonwith Steamship.temporary_workspace() as client:    api = client.use(&quot;my-langchain-app&quot;)    while True:        name = input(&quot;Name: &quot;)        print(f'{api.invoke(&quot;/greet&quot;, user=name).strip()}\n')```### Self Ask With SearchExecutes the LangChain `self-ask-with-search` agent using the Steamship GPT and SERP Tool plugins (full source: [examples/self-ask-with-search](./examples/self-ask-with-search))[![Run on Repl.it](https://replit.com/badge/github/@SteamshipDoug/Self-Ask-With-Search-with-LangChain-and-Steamship)](https://replit.com/@SteamshipDoug/Self-Ask-With-Search-with-LangChain-and-Steamship)#### Server Snippet```pythonfrom steamship_langchain.llms import OpenAI@post(&quot;/self_ask_with_search&quot;)def self_ask_with_search(self, query: str) -&gt; str:    llm = OpenAI(client=self.client, temperature=0.0, cache=True)    serp_tool = SteamshipSERP(client=self.client, cache=True)    tools = [Tool(name=&quot;Intermediate Answer&quot;, func=serp_tool.search)]    self_ask_with_search = initialize_agent(tools, llm, agent=&quot;self-ask-with-search&quot;, verbose=False)    return self_ask_with_search.run(query)```#### Client Snippet```pythonwith Steamship.temporary_workspace() as client:    api = client.use(&quot;my-langchain-app&quot;)    query = &quot;Who was president the last time the Twins won the World Series?&quot;    print(f&quot;Query: {query}&quot;)    print(f&quot;Answer: {api.invoke('/self_ask_with_search', query=query)}&quot;)```### ChatBotImplements a basic Chatbot (similar to ChatGPT) in Steamship with LangChain (full source: [examples/chatbot](./examples/chatbot)).[![Run on Repl.it](https://replit.com/badge/github/@SteamshipDoug/Persistent-ChatBot-with-LangChain-and-Steamship)](https://replit.com/@SteamshipDoug/Persistent-ChatBot-with-LangChain-and-Steamship)&gt; **NOTE**&gt; The full ChatBot transcript will persist for the lifetime of the Steamship Workspace. #### Server Snippet```pythonfrom langchain.memory import ConversationBufferWindowMemoryfrom steamship_langchain.llms import OpenAIfrom steamship_langchain.memory import ChatMessageHistory@post(&quot;/send_message&quot;)def send_message(self, message: str, chat_history_handle: str) -&gt; str:  chat_memory = ChatMessageHistory(client=self.client, key=chat_history_handle)  mem = ConversationBufferWindowMemory(chat_memory=chat_memory, k=2)  chatgpt = LLMChain(    llm=OpenAI(client=self.client, temperature=0),    prompt=CHATBOT_PROMPT,    memory=mem,  )  return chatgpt.predict(human_input=message)```#### Client Snippet```pythonwith Steamship.temporary_workspace() as client:    api = client.use(&quot;my-langchain-app&quot;)    session_handle = &quot;foo-user-session-1234&quot;    while True:        msg = input(&quot;You: &quot;)        print(f&quot;AI: {api.invoke('/send_message', message=msg, chat_history_handle=session_handle)}&quot;)```### Summarize Audio (Async Chaining)&gt; [![stability-experimental](https://img.shields.io/badge/stability-experimental-orange.svg)](https://github.com/mkenney/software-guides/blob/master/STABILITY-BADGES.md#experimental)&gt;&gt; Audio transcription support not yet considered fully-production ready on Steamship. We are working hard on&gt; productionizing support for audio transcription at scale, but there may be some existing issues that you encounter&gt; as you try this out.This provides an example of using LangChain to process audio transcriptionsobtained via Steamship's speech-to-text plugins (full source: [examples/summarize-audio](./examples/summarize-audio))A brief introduction to the Task system (and Task dependencies, for chaining) isprovided in this example. Here, we use `task.wait()` style polling, but time-based`task.refresh()` style polling, etc., is also available.[![Run on Repl.it](https://replit.com/badge/github/@SteamshipDoug/Summarize-Audio-with-LangChain-and-Steamship)](https://replit.com/@SteamshipDoug/Summarize-Audio-with-LangChain-and-Steamship)#### Server Snippet```pythonfrom steamship_langchain.llms import OpenAI@post(&quot;summarize_file&quot;)def summarize_file(self, file_handle: str) -&gt; str:    file = File.get(self.client, handle=file_handle)    text_splitter = CharacterTextSplitter()    texts = []    for block in file.blocks:        texts.extend(text_splitter.split_text(block.text))    docs = [Document(page_content=t) for t in texts]    llm = OpenAI(client=self.client, cache=True)    chain = load_summarize_chain(llm, chain_type=&quot;map_reduce&quot;)    return chain.run(docs)@post(&quot;summarize_audio_file&quot;)def summarize_audio_file(self, audio_file_handle: str) -&gt; Task[str]:    transcriber = self.client.use_plugin(&quot;whisper-s2t-blockifier&quot;)    audio_file = File.get(self.client, handle=audio_file_handle)    transcribe_task = audio_file.blockify(plugin_instance=transcriber.handle)    return self.invoke_later(&quot;summarize_file&quot;, wait_on_tasks=[transcribe_task], arguments={&quot;file_handle&quot;: audio_file.handle})```#### Client Snippet```pythonchurchill_yt_url = &quot;https://www.youtube.com/watch?v=MkTw3_PmKtc&quot;with Steamship.temporary_workspace() as client:    api = client.use(&quot;my-langchain-app&quot;)    yt_importer = client.use_plugin(&quot;youtube-file-importer&quot;)    import_task = File.create_with_plugin(client=client,                                         plugin_instance=yt_importer.handle,                                          url=churchill_yt_url)    import_task.wait()    audio_file = import_task.output        summarize_task_response = api.invoke(&quot;/summarize_audio_file&quot;, audio_file_handle=audio_file.handle)    summarize_task = Task(client=client, **summarize_task_response)    summarize_task.wait()        if summarize_task.state == TaskState.succeeded:      summary = base64.b64decode(summarize_task.output).decode(&quot;utf-8&quot;)      print(f&quot;Summary: {summary.strip()}&quot;)```### Question Answering with Sources (Embeddings)Provides a basic example of using Steamship to manage embeddings and power a LangChain agentfor question answering with sources (full source: [examples/qa_with_sources](./examples/qa_with_sources))&gt; **NOTE** &gt; The embeddings will persist for the lifetime of the Workspace.[![Run on Repl.it](https://replit.com/badge/github/@SteamshipDoug/Question-Answering-with-Sources-using-LangChain-on-Steamship)](https://replit.com/@SteamshipDoug/Question-Answering-with-Sources-using-LangChain-on-Steamship)#### Server Snippet```pythonfrom steamship_langchain.llms import OpenAIdef __init__(self, **kwargs):    super().__init__(**kwargs)    langchain.llm_cache = SteamshipCache(self.client)    self.llm = OpenAI(client=self.client, temperature=0, cache=True, max_words=250)    # create a persistent embedding store      self.index = SteamshipVectorStore(        client=self.client, index_name=&quot;qa-demo&quot;, embedding=&quot;text-embedding-ada-002&quot;    )@post(&quot;index_file&quot;)def index_file(self, file_handle: str) -&gt; bool:    text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=0)    file = File.get(self.client, handle=file_handle)    texts = [text for block in file.blocks for text in text_splitter.split_text(block.text)]    metadatas = [{&quot;source&quot;: f&quot;{file.handle}-offset-{i * 250}&quot;} for i, text in enumerate(texts)]    self.index.add_texts(texts=texts, metadatas=metadatas)    return True@post(&quot;search_embeddings&quot;)def search_embeddings(self, query: str, k: int) -&gt; List[SearchResult]:    &quot;&quot;&quot;Return the `k` closest items in the embedding index.&quot;&quot;&quot;    search_results = self.index.search(query, k=k)    search_results.wait()    items = search_results.output.items    return items@post(&quot;/qa_with_sources&quot;)def qa_with_sources(self, query: str) -&gt; Dict[str, Any]:    chain = VectorDBQAWithSourcesChain.from_chain_type(        OpenAI(client=self.client, temperature=0),        chain_type=&quot;map_reduce&quot;,        vectorstore=self.index,    )    return chain({&quot;question&quot;: query}, return_only_outputs=False)```#### Client Snippet```pythonwith Steamship.temporary_workspace() as client:    api = client.use(&quot;my-langchain-app&quot;)        # Upload the State of the Union address    with open(&quot;state_of_the_union.txt&quot;) as f:        sotu_file = File.create(client, blocks=[Block(text=f.read())])    # Embed    api.invoke(&quot;/index_file&quot;, file_handle=sotu_file.handle)    # Issue Query    query = &quot;What did the president say about Justice Breyer?&quot;    response = api.invoke(&quot;/qa_with_sources&quot;, query=query)    print(f&quot;Answer: {response['result'].strip()}&quot;)```## API KeysSteamship API Keys provide access to our SDK for AI models, including OpenAI, GPT, Cohere, Whisper, and more.Get your free API key here: https://steamship.com/account/api.Once you have an API Key, you can :* Set the env var `STEAMSHIP_API_KEY` for your client* Pass it directly via `Steamship(api_key=)` or `Steamship.tempory_workspace(api_key=)`.Alternatively, you can run `ship login`, which will guide you through setting up your environment.## Deploying on SteamshipDeploying LangChain apps on Steamship is simple: `ship it`.From your package directory (where your `api.py` lives), you can issue the `ship it` command to generate a manifest file and push your package to Steamship. You may then use the Steamship SDK to create instances of your package in Workspaces as best fits your needs.More on deployment and Workspaces can be found in [our docs](https://docs.steamship.com/).## Feedback and SupportHave any feedback on this package? Or on [Steamship](https://steamship.com) in general?We'd love to hear from you. Please reach out to: hello@steamship.com, or join us on our [Discord](https://discord.gg/5Vry5ANVwT).</longdescription>
</pkgmetadata>