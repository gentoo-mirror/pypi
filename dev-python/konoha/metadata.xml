<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># 🌿 Konoha: Simple wrapper of Japanese Tokenizers[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/himkt/konoha/blob/main/example/Konoha_Example.ipynb)&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/5164000/120913279-e7d62380-c6d0-11eb-8d17-6571277cdf27.gif&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;[![GitHub stars](https://img.shields.io/github/stars/himkt/konoha?style=social)](https://github.com/himkt/konoha/stargazers)[![Downloads](https://pepy.tech/badge/konoha)](https://pepy.tech/project/konoha)[![Downloads](https://pepy.tech/badge/konoha/month)](https://pepy.tech/project/konoha/month)[![Downloads](https://pepy.tech/badge/konoha/week)](https://pepy.tech/project/konoha/week)[![Build Status](https://github.com/himkt/konoha/workflows/Python%20package/badge.svg?style=flat-square)](https://github.com/himkt/konoha/actions)[![Documentation Status](https://readthedocs.org/projects/konoha/badge/?version=latest)](https://konoha.readthedocs.io/en/latest/?badge=latest)![Python](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8-blue?logo=python)[![PyPI](https://img.shields.io/pypi/v/konoha.svg)](https://pypi.python.org/pypi/konoha)[![GitHub Issues](https://img.shields.io/github/issues/himkt/konoha.svg?cacheSeconds=60&amp;color=yellow)](https://github.com/himkt/konoha/issues)[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/himkt/konoha.svg?cacheSeconds=60&amp;color=yellow)](https://github.com/himkt/konoha/issues)`Konoha` is a Python library for providing easy-to-use integrated interface of various Japanese tokenizers,which enables you to switch a tokenizer and boost your pre-processing.## Supported tokenizers&lt;a href=&quot;https://github.com/buruzaemon/natto-py&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/MeCab-natto--py-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/chezou/Mykytea-python&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/KyTea-Mykytea--python-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/mocobeta/janome&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Janome-janome-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/WorksApplications/SudachiPy&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Sudachi-sudachipy-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/google/sentencepiece&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Sentencepiece-sentencepiece-ff69b4&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/taishi-i/nagisa&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/nagisa-nagisa-ff69b4&quot;&gt;&lt;/a&gt;Also, `konoha` provides rule-based tokenizers (whitespace, character) and a rule-based sentence splitter.## Quick Start with DockerSimply run followings on your computer:```bashdocker run --rm -p 8000:8000 -t himkt/konoha  # from DockerHub```Or you can build image on your machine:```bashgit clone https://github.com/himkt/konoha  # download konohacd konoha &amp;&amp; docker-compose up --build  # build and launch container```Tokenization is done by posting a json object to `localhost:8000/api/v1/tokenize`.You can also batch tokenize by passing `texts: [&quot;１つ目の入力&quot;, &quot;２つ目の入力&quot;]` to `localhost:8000/api/v1/batch_tokenize`.(API documentation is available on `localhost:8000/redoc`, you can check it using your web browser)Send a request using `curl` on your terminal.Note that a path to an endpoint is changed in v4.6.4.Please check our release note (https://github.com/himkt/konoha/releases/tag/v4.6.4).```json$ curl localhost:8000/api/v1/tokenize -X POST -H &quot;Content-Type: application/json&quot; \    -d '{&quot;tokenizer&quot;: &quot;mecab&quot;, &quot;text&quot;: &quot;これはペンです&quot;}'{  &quot;tokens&quot;: [    [      {        &quot;surface&quot;: &quot;これ&quot;,        &quot;part_of_speech&quot;: &quot;名詞&quot;      },      {        &quot;surface&quot;: &quot;は&quot;,        &quot;part_of_speech&quot;: &quot;助詞&quot;      },      {        &quot;surface&quot;: &quot;ペン&quot;,        &quot;part_of_speech&quot;: &quot;名詞&quot;      },      {        &quot;surface&quot;: &quot;です&quot;,        &quot;part_of_speech&quot;: &quot;助動詞&quot;      }    ]  ]}```## InstallationI recommend you to install konoha by `pip install 'konoha[all]'`.- Install konoha with a specific tokenizer: `pip install 'konoha[(tokenizer_name)]`.- Install konoha with a specific tokenizer and remote file support: `pip install 'konoha[(tokenizer_name),remote]'`If you want to install konoha with a tokenizer, please install konoha with a specific tokenizer(e.g. `konoha[mecab]`, `konoha[sudachi]`, ...etc) or install tokenizers individually.## Example### Word level tokenization```pythonfrom konoha import WordTokenizersentence = '自然言語処理を勉強しています'tokenizer = WordTokenizer('MeCab')print(tokenizer.tokenize(sentence))# =&gt; [自然, 言語, 処理, を, 勉強, し, て, い, ます]tokenizer = WordTokenizer('Sentencepiece', model_path=&quot;data/model.spm&quot;)print(tokenizer.tokenize(sentence))# =&gt; [▁, 自然, 言語, 処理, を, 勉強, し, ています]```For more detail, please see the `example/` directory.### Remote filesKonoha supports dictionary and model on cloud storage (currently supports Amazon S3).It requires installing konoha with the `remote` option, see [Installation](#installation).```python# download user dictionary from S3word_tokenizer = WordTokenizer(&quot;mecab&quot;, user_dictionary_path=&quot;s3://abc/xxx.dic&quot;)print(word_tokenizer.tokenize(sentence))# download system dictionary from S3word_tokenizer = WordTokenizer(&quot;mecab&quot;, system_dictionary_path=&quot;s3://abc/yyy&quot;)print(word_tokenizer.tokenize(sentence))# download model file from S3word_tokenizer = WordTokenizer(&quot;sentencepiece&quot;, model_path=&quot;s3://abc/zzz.model&quot;)print(word_tokenizer.tokenize(sentence))```### Sentence level tokenization```pythonfrom konoha import SentenceTokenizersentence = &quot;私は猫だ。名前なんてものはない。だが，「かわいい。それで十分だろう」。&quot;tokenizer = SentenceTokenizer()print(tokenizer.tokenize(sentence))# =&gt; ['私は猫だ。', '名前なんてものはない。', 'だが，「かわいい。それで十分だろう」。']```You can change symbols for a sentence splitter and bracket expression.1. sentence splitter```pythonsentence = &quot;私は猫だ。名前なんてものはない．だが，「かわいい。それで十分だろう」。&quot;tokenizer = SentenceTokenizer(period=&quot;．&quot;)print(tokenizer.tokenize(sentence))# =&gt; ['私は猫だ。名前なんてものはない．', 'だが，「かわいい。それで十分だろう」。']```2. bracket expression```pythonsentence = &quot;私は猫だ。名前なんてものはない。だが，『かわいい。それで十分だろう』。&quot;tokenizer = SentenceTokenizer(    patterns=SentenceTokenizer.PATTERNS + [re.compile(r&quot;『.*?』&quot;)],)print(tokenizer.tokenize(sentence))# =&gt; ['私は猫だ。', '名前なんてものはない。', 'だが，『かわいい。それで十分だろう』。']```## Test```python -m pytest```## Article- [トークナイザをいい感じに切り替えるライブラリ konoha を作った](https://qiita.com/klis/items/bb9ffa4d9c886af0f531)- [日本語解析ツール Konoha に AllenNLP 連携機能を実装した](https://qiita.com/klis/items/f1d29cb431d1bf879898)## AcknowledgementSentencepiece model used in test is provided by @yoheikikuta. Thanks!</longdescription>
</pkgmetadata>