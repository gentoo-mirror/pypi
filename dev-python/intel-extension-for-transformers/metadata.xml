<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;div align=&quot;center&quot;&gt;  IntelÂ® Extension for Transformers===========================&lt;h3&gt; An innovative toolkit to accelerate Transformer-based models on Intel platforms&lt;/h3&gt;[Architecture](./docs/architecture.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[NeuralChat](./intel_extension_for_transformers/neural_chat)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Inference](./intel_extension_for_transformers/llm/runtime/graph)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Examples](./docs/examples.md)&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;[Documentations](https://intel.github.io/intel-extension-for-transformers/latest/docs/Welcome.html)&lt;/div&gt;## ðŸš€ Latest News&lt;b&gt; &lt;span style=&quot;color:orange&quot; &gt; NeuralChat, a customizable chatbot framework under IntelÂ® Extension for Transformers, is now available for you to create your own chatbot within minutes on multiple architectures.&lt;/span&gt;&lt;/b&gt;NeuralChat offers a rich set of plugins to allow your personalized chatbot smarter with knowledge retrieval, more interactive through speech, faster through query caching, and more secure with guardrails.* [Plugins] [Knowledge Retrieval](./intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md), [Speech Interaction](./intel_extension_for_transformers/neural_chat/pipeline/plugins/audio/README.md), [Query Caching](./intel_extension_for_transformers/neural_chat/pipeline/plugins/caching/README.md), [Security Guardrail](./intel_extension_for_transformers/neural_chat/pipeline/plugins/security/README.md)* [Architectures] IntelÂ® XeonÂ® Scalable Processors, Habana GaudiÂ® Accelerator, and othersCheck out the below sample code and have a try now!```python# follow the installation instructionsfrom intel_extension_for_transformers.neural_chat import build_chatbotchatbot = build_chatbot()response = chatbot.predict(&quot;Tell me about Intel Xeon Scalable Processors.&quot;)```---&lt;div align=&quot;left&quot;&gt;IntelÂ® Extension for Transformers is an innovative toolkit to accelerate Transformer-based models on Intel platforms, in particular effective on 4th Intel Xeon Scalable processorÂ Sapphire Rapids (codenamed [Sapphire Rapids](https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/4th-gen-xeon-scalable-processors.html)). The toolkit provides the below key features and examples:*  Seamless user experience of model compressions on Transformer-based models by extending [Hugging Face transformers](https://github.com/huggingface/transformers)Â APIs and leveraging [IntelÂ® Neural Compressor](https://github.com/intel/neural-compressor)*  Advanced software optimizations and unique compression-aware runtime (released with NeurIPS 2022's paper [Fast Distilbert on CPUs](https://arxiv.org/abs/2211.07715) and [QuaLA-MiniLM: a Quantized Length Adaptive MiniLM](https://arxiv.org/abs/2210.17114), and NeurIPS 2021's paper [Prune Once for All: Sparse Pre-Trained Language Models](https://arxiv.org/abs/2111.05754))*  Optimized Transformer-based model packages such as [Stable Diffusion](examples/huggingface/pytorch/text-to-image/deployment/stable_diffusion), [GPT-J-6B](examples/huggingface/pytorch/text-generation/deployment), [GPT-NEOX](examples/huggingface/pytorch/language-modeling/quantization#2-validated-model-list), [BLOOM-176B](examples/huggingface/pytorch/language-modeling/inference#BLOOM-176B), [T5](examples/huggingface/pytorch/summarization/quantization#2-validated-model-list), [Flan-T5](examples/huggingface/pytorch/summarization/quantization#2-validated-model-list) and end-to-end workflows such as [SetFit-based text classification](docs/tutorials/pytorch/text-classification/SetFit_model_compression_AGNews.ipynb) and [document level sentiment analysis (DLSA)](workflows/dlsa) *  [NeuralChat](workflows/chatbot), a custom Chatbot trained on Intel CPUs through parameter-efficient fine-tuning [PEFT](https://github.com/huggingface/peft) on domain knowledge*  [Inference](intel_extension_for_transformers/llm/runtime/graph) of Large Language Model (LLM) in pure C/C++ with weight-only quantization kernels. It already enabled [GPT-NEOX](intel_extension_for_transformers/llm/runtime/graph/application/ChatGPTNEOX), [LLAMA-7B](intel_extension_for_transformers/llm/runtime/graph/application/ChatLLAMA), [MPT-7B](intel_extension_for_transformers/llm/runtime/graph/application/ChatMPT) and [FALCON-7B](intel_extension_for_transformers/llm/runtime/graph/application/ChatFALCON)## Installation### Install from Pypi```bashpip install intel-extension-for-transformers```&gt; For more installation method, please refer to [Installation Page](docs/installation.md)## Getting Started### Sentiment Analysis with Quantization#### Prepare Dataset```pythonfrom datasets import load_dataset, load_metricfrom transformers import AutoConfig,AutoModelForSequenceClassification,AutoTokenizerraw_datasets = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;)raw_datasets = raw_datasets.map(lambda e: tokenizer(e['sentence'], truncation=True, padding='max_length', max_length=128), batched=True)```#### Quantization```pythonfrom intel_extension_for_transformers.transformers import QuantizationConfig, metrics, objectivesfrom intel_extension_for_transformers.transformers.trainer import NLPTrainerconfig = AutoConfig.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;,num_labels=2)model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;,config=config)model.config.label2id = {0: 0, 1: 1}model.config.id2label = {0: 'NEGATIVE', 1: 'POSITIVE'}# Replace transformers.Trainer with NLPTrainer# trainer = transformers.Trainer(...)trainer = NLPTrainer(model=model,     train_dataset=raw_datasets[&quot;train&quot;],     eval_dataset=raw_datasets[&quot;validation&quot;],    tokenizer=tokenizer)q_config = QuantizationConfig(metrics=[metrics.Metric(name=&quot;eval_loss&quot;, greater_is_better=False)])model = trainer.quantize(quant_config=q_config)input = tokenizer(&quot;I like Intel Extension for Transformers&quot;, return_tensors=&quot;pt&quot;)output = model(**input).logits.argmax().item()```&gt; For more quick samples, please refer to [Get Started Page](docs/get_started.md). For more validated examples, please refer to [Support Model Matrix](docs/examples.md)## Validated Performance| Model |  FP32 | BF16 | INT8 ||---------------------|:----------------------:|-----------------------|-----------------------------------|| [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B) | 4163.67 (ms) | 1879.61 (ms) | 1612.24 (ms) || [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) | 10.33 (s) | 3.02 (s) | N/A |&gt; Note*: GPT-J-6B software/hardware configuration please refer to [text-generation](./examples/huggingface/pytorch/text-generation/README.md). Stable-diffusion software/hardware configuration please refer to [text-to-image](./examples/huggingface/pytorch/text-to-image/deployment/stable_diffusion/README.md)## Documentation&lt;table&gt;&lt;thead&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;OVERVIEW&lt;/th&gt;  &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs&quot;&gt;Model Compression&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;workflows/chatbot&quot;&gt;NeuralChat&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs&quot;&gt;Neural Engine&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/README.md&quot;&gt;Kernel Libraries&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;MODEL COMPRESSION&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/quantization.md&quot;&gt;Quantization&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/pruning.md&quot;&gt;Pruning&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/distillation.md&quot;&gt;Distillation&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/text-classification/orchestrate_optimizations/README.md&quot;&gt;Orchestration&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/language-modeling/nas/README.md&quot;&gt;Neural Architecture Search&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/export.md&quot;&gt;Export&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/metrics.md&quot;&gt;Metrics&lt;/a&gt;/&lt;a href=&quot;docs/objectives.md&quot;&gt;Objectives&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;a href=&quot;docs/pipeline.md&quot;&gt;Pipeline&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;NEURAL ENGINE&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/onnx_compile.md&quot;&gt;Model Compilation&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/add_customized_pattern.md&quot;&gt;Custom Pattern&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/deploy_and_integration.md&quot;&gt;Deployment&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/engine_profiling.md&quot;&gt;Profiling&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;KERNEL LIBRARIES&lt;/th&gt;  &lt;/tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/kernel_desc&quot;&gt;Sparse GEMM Kernels&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/kernel_desc&quot;&gt;Custom INT8 Kernels&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/profiling.md&quot;&gt;Profiling&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/test/kernels/benchmark/benchmark.md&quot;&gt;Benchmark&lt;/a&gt;&lt;/td&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;ALGORITHMS&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td align=&quot;center&quot; colspan=&quot;4&quot;&gt;&lt;a href=&quot;examples/huggingface/pytorch/question-answering/dynamic/README.md&quot;&gt;Length Adaptive&lt;/a&gt;&lt;/td&gt;    &lt;td align=&quot;center&quot; colspan=&quot;4&quot;&gt;&lt;a href=&quot;docs/data_augmentation.md&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/td&gt;      &lt;/tr&gt;  &lt;tr&gt;    &lt;th colspan=&quot;8&quot; align=&quot;center&quot;&gt;TUTORIALS AND RESULTS&lt;/a&gt;&lt;/th&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/tutorials/pytorch&quot;&gt;Tutorials&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;docs/examples.md&quot;&gt;Supported Models&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.md&quot;&gt;Model Performance&lt;/a&gt;&lt;/td&gt;    &lt;td colspan=&quot;2&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;intel_extension_for_transformers/llm/runtime/deprecated/kernels/docs/validated_data.md&quot;&gt;Kernel Performance&lt;/a&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;## Selected Publications/Events* Blog published on Medium: [Faster Stable Diffusion Inference with Intel Extension for Transformers](https://medium.com/intel-analytics-software/faster-stable-diffusion-inference-with-intel-extension-for-transformers-on-intel-platforms-7e0f563186b0) (July 2023)* Blog of Intel Developer News: [The Moat Is Trust, Or Maybe Just Responsible AI](https://www.intel.com/content/www/us/en/developer/articles/technical/moat-is-trust-minimizing-risks-generative-ai.html) (July 2023)* Blog of Intel Developer News: [Create Your Own Custom Chatbot](https://www.intel.com/content/www/us/en/developer/articles/technical/train-large-language-models-create-custom-chatbot.html) (July 2023)* Blog of Intel Developer News: [Accelerate Llama 2 with Intel AI Hardware and Software Optimizations](https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html) (July 2023)* Arxiv: [An Efficient Sparse Inference Software Accelerator for Transformer-based Language Models on CPUs](https://arxiv.org/abs/2306.16601) (June 2023)* Blog published on Medium: [Simplify Your Custom Chatbot Deployment](https://medium.com/intel-analytics-software/simplify-your-custom-chatbot-deployment-on-intel-platforms-c8a911d906cf) (June 2023)* Blog published on Medium: [Create Your Own Custom Chatbot](https://medium.com/intel-analytics-software/create-your-own-chatbot-on-cpus-b8d186cfefb2) (April 2023)&gt; View [Full Publication List](./docs/publication.md).## Additional Content* [Release Information](./docs/release.md)* [Contribution Guidelines](./docs/contributions.md)* [Legal Information](./docs/legal.md)* [Security Policy](SECURITY.md)## CollaborationsWelcome to raise any interesting ideas on model compression techniques and LLM-based chatbot development! Feel free to reach [us](mailto:inc.maintainers@intel.com) and look forward to our collaborations on Intel Extension for Transformers!</longdescription>
</pkgmetadata>