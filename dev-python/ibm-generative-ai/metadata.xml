<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># IBM Generative AI Python SDKThis is the Python SDK for IBM Foundation Models Studio to bring IBM Generative AI into Python programs and to also extend it with useful operations and types.:books:**API Documentation: [Link](https://ibm.github.io/ibm-generative-ai/)***This is an early access library and requires invitation to use the technical preview of [watsonx.ai](https://watsonx.ai/). You can join the waitlist by visiting. https://www.ibm.com/products/watsonx-ai.**Looking for the JavaScript/TypeScript version? Check out https://github.com/IBM/ibm-generative-ai-node-sdk.*[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://github.com/IBM/ibm-generative-ai/blob/main/LICENSE)![PyPI](https://img.shields.io/pypi/v/ibm-generative-ai)[![Build &amp; Test](https://github.com/IBM/ibm-generative-ai/actions/workflows/main.yml/badge.svg?branch=main)](https://github.com/IBM/ibm-generative-ai/actions/workflows/main.yml)[![Integration Tests](https://github.com/IBM/ibm-generative-ai/actions/workflows/integration-test.yml/badge.svg)](https://github.com/IBM/ibm-generative-ai/actions/workflows/integration-test.yml)![PyPI - Downloads](https://img.shields.io/pypi/dm/ibm-generative-ai)[![Coverage Status](https://coveralls.io/repos/github/IBM/ibm-generative-ai/badge.svg?branch=main)](https://coveralls.io/github/IBM/ibm-generative-ai?branch=main)## &lt;a name='TableofContents'&gt;&lt;/a&gt;Table of Contents&lt;!-- vscode-markdown-toc --&gt;* [Table of Contents](#table-of-contents)* [Installation](#installation)* [Gen AI Endpoint](#gen-ai-endpoint)    * [Example](#example)* [Examples](#examples)    * [Async Example](#async-example)    * [Synchronous Example](#synchronous-example)* [Tips and Troubleshooting](#tips-and-troubleshooting)    * [Enabling Logs](#enabling-logs)    * [Experimenting with a Large Number of Prompts](#many-prompts)* [Extensions](#extensions)    * [LangChain Extension](#langchain-extension)* [Support](#support)* [API Documentation](#APIDocumentation)* [Important Information for Contributors](#important-information-for-contributors)* [Authors](#authors)&lt;!-- vscode-markdown-toc-confignumbering=falseautoSave=true/vscode-markdown-toc-config --&gt;&lt;!-- /vscode-markdown-toc --&gt;## &lt;a name='Installation'&gt;&lt;/a&gt;Installation```bashpip install ibm-generative-ai```#### &lt;a name='KnownIssueFixes:'&gt;&lt;/a&gt;Known Issue Fixes:- **[SSL Issue]** If you run into &quot;SSL_CERTIFICATE_VERIFY_FAILED&quot; please run the following code snippet here: [support](SUPPORT.md).### &lt;a name='Prerequisites'&gt;&lt;/a&gt;PrerequisitesPython version &gt;= 3.9Pip version &gt;= 22.0.1Check your pip version with `pip --version` and if needed run the following command to upgrade pip.```bashpip install --upgrade &quot;pip&gt;=22.0.1&quot;```## &lt;a name='GenAIEndpoint'&gt;&lt;/a&gt;Gen AI EndpointBy default, IBM Generative AI will automatically use the following API endpoint: `https://workbench-api.res.ibm.com/v1/`. However, if you wish to target a different Gen AI API, you can do so by defining it with the `api_endpoint` argument when you instansiate the `Credentials` object.### &lt;a name='Example'&gt;&lt;/a&gt;ExampleYour `.env` file:```iniGENAI_KEY=YOUR_GENAI_API_KEYGENAI_API=https://workbench-api.res.ibm.com/v1/``````pythonimport osfrom dotenv import load_dotenvfrom genai.model import Credentials# make sure you have a .env file under genai root with# GENAI_KEY=&lt;your-genai-key&gt;# GENAI_API=&lt;genai-api-endpoint&gt;load_dotenv()my_api_key = os.getenv(&quot;GENAI_KEY&quot;, None)my_api_endpoint = os.getenv(&quot;GENAI_API&quot;, None)# creds objectcreds = Credentials(api_key=my_api_key, api_endpoint=my_api_endpoint)# Now start using GenAI!```## &lt;a name='Examples'&gt;&lt;/a&gt;ExamplesThere are a number of examples you can try in the [`examples/user`](examples/user) directory.Login to [workbench.res.ibm.com](https://workbench.res.ibm.com/) and get your GenAI API key. Then, create a `.env` file and assign the `GENAI_KEY` value as below example. [More information](#gen-ai-endpoint)```iniGENAI_KEY=YOUR_GENAI_API_KEY# GENAI_API=GENAI_API_ENDPOINT &lt;&lt; for a different endpoint```### &lt;a name='AsyncExample'&gt;&lt;/a&gt;Async Example```pythonimport osfrom dotenv import load_dotenvfrom genai.model import Credentials, Modelfrom genai.schemas import GenerateParams# make sure you have a .env file under genai root with# GENAI_KEY=&lt;your-genai-key&gt;# GENAI_API=&lt;genai-api-endpoint&gt;load_dotenv()api_key = os.getenv(&quot;GENAI_KEY&quot;, None)api_endpoint = os.getenv(&quot;GENAI_API&quot;, None)# Using Python &quot;with&quot; contextprint(&quot;\n------------- Example (Greetings)-------------\n&quot;)# Instantiate the GENAI Proxy Objectparams = GenerateParams(    decoding_method=&quot;sample&quot;,    max_new_tokens=10,    min_new_tokens=1,    stream=False,    temperature=0.7,    top_k=50,    top_p=1,)# creds objectcreds = Credentials(api_key, api_endpoint)# model objectmodel = Model(&quot;google/flan-ul2&quot;, params=params, credentials=creds)greeting = &quot;Hello! How are you?&quot;lots_of_greetings = [greeting] * 1000num_of_greetings = len(lots_of_greetings)num_said_greetings = 0greeting1 = &quot;Hello! How are you?&quot;# yields batch of results that are produced asynchronously and in parallelfor result in model.generate_async(lots_of_greetings):    if result is not None:        num_said_greetings += 1        print(f&quot;[Progress {str(float(num_said_greetings/num_of_greetings)*100)}%]&quot;)        print(f&quot;\t {result.input_text} --&gt; {result.generated_text}&quot;)```If you are planning on sending a large number of prompts _and_ using logging, you might want to re-direct genai logs to a file instead of stdout.Check the section [Tips and TroubleShooting](#tips-and-troubleshooting) for further help.### &lt;a name='SynchronousExample'&gt;&lt;/a&gt;Synchronous Example```pythonimport osfrom dotenv import load_dotenvfrom genai.model import Credentials, Modelfrom genai.schemas import GenerateParams# make sure you have a .env file under genai root with# GENAI_KEY=&lt;your-genai-key&gt;# GENAI_API=&lt;genai-api-endpoint&gt;load_dotenv()api_key = os.getenv(&quot;GENAI_KEY&quot;, None)api_endpoint = os.getenv(&quot;GENAI_API&quot;, None)# Using Python &quot;with&quot; contextprint(&quot;\n------------- Example (Greetings)-------------\n&quot;)# Instantiate the GENAI Proxy Objectparams = GenerateParams(    decoding_method=&quot;sample&quot;,    max_new_tokens=10,    min_new_tokens=1,    stream=False,    temperature=0.7,    top_k=50,    top_p=1,)# creds objectcreds = Credentials(api_key, api_endpoint)# model objectmodel = Model(&quot;google/flan-ul2&quot;, params=params, credentials=creds)greeting1 = &quot;Hello! How are you?&quot;greeting2 = &quot;I am fine and you?&quot;# Call generate functionresponses = model.generate_as_completed([greeting1, greeting2] * 4)for response in responses:    print(f&quot;Generated text: {response.generated_text}&quot;)```## &lt;a name='TipsAndTroubleshooting'&gt;&lt;/a&gt;Tips and Troubleshooting### &lt;a name='EnablingLogs'&gt;&lt;/a&gt;Enabling LogsIf you're building an application or example and would like to see the GENAI logs, you can enable them in the following way:```pythonimport loggingimport os# Most GENAI logs are at Debug level.logging.basicConfig(level=os.environ.get(&quot;LOGLEVEL&quot;, &quot;DEBUG&quot;))```If you only want genai logs, or those logs at a specific level, you can set this using the following syntax:```pythonlogging.getLogger(&quot;genai&quot;).setLevel(logging.DEBUG)```Example log message from GENAI:```logDEBUG:genai.model:Model Created:  Model: google/flan-t5-xxl, endpoint: https://workbench-api.res.ibm.com/v1/```Example of directing genai logs to a file:```python# create file handler which logs even debug messagesfh = logging.FileHandler('genai.log')fh.setLevel(logging.DEBUG)logging.getLogger(&quot;genai&quot;).addHandler(fh)```To learn more about logging in python, you can follow the tutorial [here](https://docs.python.org/3/howto/logging.html).### &lt;a name='ManyPrompts'&gt;&lt;/a&gt;Experimenting with a Large Number of PromptsSince generating responses for a large number of prompts can be time-consuming and there could be unforeseen circumstances such as internet connectivity issues, here are some strategiesto work with:- Start with a small number of prompts to prototype the code. You can enable logging as described above for debugging during prototyping.- Include exception handling in sensitive sections such as callbacks.- Checkpoint/save prompts and received responses periodically.- Check examples in `examples/user` directory and modify them for your needs.```pythondef my_callback(result):    try:        ...    except:        ...outputs = []count = 0for result in model.generate_async(prompts, callback=my_callback):    if result is not None:        print(result.input_text, &quot; --&gt; &quot;, result.generated_text)        # check if prompts[count] and result.input_text are the same        outputs.append((result.input_text, result.generated_text))        # periodically save outputs to disk or some location        ...    else:        # ... save failed prompts for retrying    count += 1```## &lt;a name='Extensions'&gt;&lt;/a&gt;ExtensionsGenAI currently supports a langchain extension and more extensions are in the pipeline. Please reach out tous if you want support for some framework as an extension or want to design an extension yourself.### &lt;a name='LangChainExtension'&gt;&lt;/a&gt;LangChain ExtensionInstall the langchain extension as follows:```bashpip install &quot;ibm-generative-ai[langchain]&quot;```Currently the langchain extension allows IBM Generative AI models to be wrapped as Langchain LLMs and translation between genai PromptPatterns and LangChain PromptTemplates. Below are sample snippets```pythonimport osfrom dotenv import load_dotenvimport genai.extensions.langchainfrom genai.extensions.langchain import LangChainInterfacefrom genai.schemas import GenerateParamsfrom genai import Credentials, Model, PromptPatternload_dotenv()api_key = os.getenv(&quot;GENAI_KEY&quot;, None)api_endpoint = os.getenv(&quot;GENAI_API&quot;, None)creds = Credentials(api_key, api_endpoint)params = GenerateParams(decoding_method=&quot;greedy&quot;)# As LangChain Modellangchain_model = LangChainInterface(model=&quot;google/flan-ul2&quot;, params=params, credentials=creds)print(langchain_model(&quot;Answer this question: What is life?&quot;))# As GenAI Modelgenai_model = Model(model=&quot;google/flan-ul2&quot;, params=params, credentials=creds)print(genai_model.generate([&quot;Answer this question: What is life?&quot;])[0].generated_text)# GenAI prompt pattern to langchain PromptTemplate and vice versaseed_pattern = PromptPattern.from_str(&quot;Answer this question: {{question}}&quot;)template = seed_pattern.langchain.as_template()pattern = PromptPattern.langchain.from_template(template)print(langchain_model(template.format(question=&quot;What is life?&quot;)))print(genai_model.generate([pattern.sub(&quot;question&quot;, &quot;What is life?&quot;)])[0].generated_text)```## &lt;a name='Model Types'&gt;&lt;/a&gt;Model TypesModel types can be imported from the [ModelType class](src/genai/schemas/models.py). If you want to use a model that is not included in this class, you can pass it as a string as exemplified [here](src/genai/schemas/models.py).## &lt;a name='Support'&gt;&lt;/a&gt;SupportNeed help? Check out how to get [support](SUPPORT.md)## &lt;a name='APIDocumentation'&gt;&lt;/a&gt;API DocumentationRead our Python API documentation [here](https://ibm.github.io/ibm-generative-ai/).## &lt;a name='ContributionInfo'&gt;&lt;/a&gt;Important Information for ContributorsIBM Generative AI is an open-source project that welcomes the community to contribute with documentation, tests, bug corrections, and new fuctionality in the form of [extensions](/EXTENSIONS.md). Please read our [code of counduct](/CODE_OF_CONDUCT.md) to learn the expected behavior from participants that contribute to the project, and our [contribution guide](DEVELOPMENT.md) to learn the gitflow and steps to submit pull requests.## &lt;a name='Authors'&gt;&lt;/a&gt;Authors  - Onkar Bhardwaj, onkarbhardwaj@ibm.com  - Veronique Demers, vdemers@ibm.com  - James Sutton, james.sutton@uk.ibm.com  - Mirian Silva, mirianfrsilva@ibm.com  - Mairead O'Neill, moneill@ibm.com  - Ja Young Lee, younglee@ibm.com  - Ana Fucs, ana.fucs@ibm.com  - Lee Martie, lee.martie@ibm.com</longdescription>
</pkgmetadata>