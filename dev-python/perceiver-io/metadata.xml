<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Perceiver, Perceiver IO and Perceiver ARThis repository is a PyTorch implementation of Perceiver, Perceiver IO and Perceiver AR, with PyTorch Lightninginterfaces for model training and Hugging Face ðŸ¤— interfaces for inference.&lt;table&gt;  &lt;tr&gt;    &lt;td&gt;       &lt;b&gt;Perceiver&lt;/b&gt;: General Perception with Iterative Attention       (&lt;a href=&quot;https://arxiv.org/abs/2103.03206&quot;&gt;paper&lt;/a&gt;,        &lt;a href=&quot;https://www.youtube.com/watch?v=P_xeshTnPZg&quot;&gt;video&lt;/a&gt;)    &lt;/td&gt;    &lt;td&gt;&lt;img src=&quot;docs/images/small-perceiver.png&quot; alt=&quot;Perceiver&quot;/&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;      &lt;b&gt;Perceiver IO&lt;/b&gt;: A General Architecture for Structured Inputs &amp; Outputs      (&lt;a href=&quot;https://arxiv.org/abs/2107.14795&quot;&gt;paper&lt;/a&gt;,       &lt;a href=&quot;https://www.deepmind.com/blog/building-architectures-that-can-handle-the-worlds-data&quot;&gt;blog post&lt;/a&gt;)    &lt;/td&gt;    &lt;td&gt;&lt;img src=&quot;docs/images/small-perceiver-io.png&quot; alt=&quot;Perceiver IO&quot;/&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;      General-purpose, long-context autoregressive modeling with &lt;b&gt;Perceiver AR&lt;/b&gt;      (&lt;a href=&quot;https://arxiv.org/abs/2202.07765&quot;&gt;paper&lt;/a&gt;,       &lt;a href=&quot;https://www.deepmind.com/blog/perceiver-ar-general-purpose-long-context-autoregressive-generation&quot;&gt;blog post&lt;/a&gt;)    &lt;/td&gt;    &lt;td&gt;&lt;img src=&quot;docs/images/small-perceiver-ar.png&quot; alt=&quot;Perceiver AR&quot;/&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/table&gt;## OverviewCore of the `perceiver-io` library are *backend models*, lightweight PyTorch implementations of Perceiver,Perceiver IO and Perceiver AR. They can be wrapped into [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/)modules for training (*Lightning interface*) and ðŸ¤— modules for inference (*Hugging Face interface*). See[library design](docs/library-design.md) for details.&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;docs/images/library-design-small.jpg&quot; alt=&quot;library-design&quot;/&gt;&lt;/p&gt;The command line interface for training is implemented with [Lightning CLI](https://pytorch-lightning.readthedocs.io/en/stable/cli/lightning_cli.html).Training datasets are ðŸ¤— [datasets](https://huggingface.co/docs/datasets) wrapped into PyTorch Lightning data modules.For NLP tasks, `perceiver-io` supports all ðŸ¤— [fast tokenizers](https://huggingface.co/docs/transformers/fast_tokenizers)and the ðŸ¤— Perceiver UTF-8 bytes tokenizer.## Documentation- [Installation](#installation)- [Getting started](#getting-started)- [Library design](docs/library-design.md)- [Pretrained models](docs/pretrained-models.md)- [Training examples](docs/training-examples.md)- [Inference examples](examples/inference.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/krasserm/perceiver-io/blob/main/examples/inference.ipynb)- [Model construction](docs/model-construction.md)- [Building blocks](docs/building-blocks.md)## Installation### Via pip```shellpip install perceiver-io[text,vision,audio]```### From sourcesInstallation from sources requires a [Miniconda](https://docs.conda.io/en/latest/miniconda.html) and a[Poetry](https://python-poetry.org/docs/#installation) (1.2.0 or higher) installation.Create and activate the `perceiver-io` conda environment:```shellconda env create -f environment.ymlconda activate perceiver-io```Install main and test dependencies, including all extras:```shell# Without dependencies required for examplespoetry install --all-extras```If you want to run the [examples](examples) locally, additionally use `--with examples`:```shellpoetry install --all-extras --with examples```### Docker image```shelldocker pull ghcr.io/krasserm/perceiver-io:latest```See [Docker image](docs/docker-image.md) for details.## Getting started### Inference#### Optical flowCompute the optical flow between consecutive frames of an input video and write the rendered results to an outputvideo:```pythonfrom urllib.request import urlretrievefrom transformers import pipelinefrom perceiver.data.vision import video_utilsfrom perceiver.model.vision import optical_flow  # register auto-classes and pipelineurlretrieve(    url=&quot;https://martin-krasser.com/perceiver/flow/sintel_clip_cave_dragon_fight.mp4&quot;,    filename=&quot;sintel_clip_cave_dragon_fight.mp4&quot;,)# Create optical flow pipelineoptical_flow_pipeline = pipeline(&quot;optical-flow&quot;, model=&quot;krasserm/perceiver-io-optical-flow&quot;, device=&quot;cuda:0&quot;)# load consecutive video frame pairsframe_pairs = video_utils.read_video_frame_pairs(&quot;sintel_clip_cave_dragon_fight.mp4&quot;)# create and render optical flow for all frame pairsoptical_flows = optical_flow_pipeline(frame_pairs, render=True, device=&quot;cuda:0&quot;)# create video with rendered optical flowsvideo_utils.write_video(&quot;sintel_clip_cave_dragon_fight_output.mp4&quot;, optical_flows, fps=24)```Here is a side-by-side comparison of the input and output video:&lt;p align=&quot;center&quot;&gt;    &lt;img src=&quot;docs/images/optical-flow.gif&quot; alt=&quot;optical-flow-sbs&quot;&gt;&lt;/p&gt;#### Symbolic audio generationCreate audio sequences by generating symbolic ([MIDI](https://en.wikipedia.org/wiki/MIDI)) audio data and converting thegenerated audio symbols into WAV output using [fluidsynth](https://www.fluidsynth.org/) (_Note:_ fluidsynth must be installedin order for the following example to work):  ```pythonfrom transformers import pipelinefrom pretty_midi import PrettyMIDIfrom perceiver.model.audio import symbolic  # auto-class registrationrepo_id = &quot;krasserm/perceiver-ar-sam-giant-midi&quot;prompt = PrettyMIDI(&quot;prompt.mid&quot;)audio_generator = pipeline(&quot;symbolic-audio-generation&quot;, model=repo_id)output = audio_generator(prompt, max_new_tokens=64, num_latents=1, do_sample=True, top_p=0.95, temperature=1.0, render=True)with open(&quot;generated_audio.wav&quot;, &quot;wb&quot;) as f:    f.write(output[&quot;generated_audio_wav&quot;])```Examples of generated audio sequences are available on the ðŸ¤— [hub](https://huggingface.co/krasserm/perceiver-ar-sam-giant-midi#audio-samples).See [inference examples](https://colab.research.google.com/github/krasserm/perceiver-io/blob/main/examples/inference.ipynb)for more examples.### TrainingTrain a small Perceiver IO image classifier (907K parameters) on MNIST from the command line. The classifiercross-attends to individual pixels of input images with [repeated cross-attention](docs/building-blocks.md).See [image classification](docs/training-examples.md#image-classification) training example for more details.```shellpython -m perceiver.scripts.vision.image_classifier fit \  --model.num_latents=32 \  --model.num_latent_channels=128 \  --model.encoder.num_frequency_bands=32 \  --model.encoder.num_cross_attention_layers=2 \  --model.encoder.num_self_attention_blocks=3 \  --model.encoder.num_self_attention_layers_per_block=3 \  --model.encoder.first_self_attention_block_shared=false \  --model.encoder.dropout=0.1 \  --model.encoder.init_scale=0.1 \  --model.decoder.num_output_query_channels=128 \  --model.decoder.dropout=0.1 \  --model.decoder.init_scale=0.1 \  --data=MNISTDataModule \  --data.batch_size=64 \  --optimizer=AdamW \  --optimizer.lr=1e-3 \  --lr_scheduler.warmup_steps=500 \  --trainer.accelerator=gpu \  --trainer.devices=1 \  --trainer.max_epochs=30 \  --trainer.logger=TensorBoardLogger \  --trainer.logger.save_dir=logs \  --trainer.logger.name=logs```[Model construction](docs/model-construction.md) describes how to implement model-specific command line interfaceswith the Lightning CLI. Training checkpoints are written to the `logs/img_clf/version_0/checkpoints` directory. Assuminga checkpoint with filename `epoch=025-val_loss=0.065.ckpt` exists, it can be converted to a `perceiver-io` ðŸ¤— model with```pythonfrom perceiver.model.vision.image_classifier import convert_mnist_classifier_checkpointconvert_mnist_classifier_checkpoint(    save_dir=&quot;example/mnist-classifier&quot;,    ckpt_url=&quot;logs/img_clf/version_0/checkpoints/epoch=025-val_loss=0.065.ckpt&quot;,)```so that it can be used in a ðŸ¤— image classification pipeline```pythonfrom datasets import load_datasetfrom transformers import pipelinemnist_dataset = load_dataset(&quot;mnist&quot;, split=&quot;test&quot;)[:9]images = mnist_dataset[&quot;image&quot;]labels = mnist_dataset[&quot;label&quot;]classifier = pipeline(&quot;image-classification&quot;, model=&quot;example/mnist-classifier&quot;)predictions = [pred[0][&quot;label&quot;] for pred in classifier(images)]print(f&quot;Labels:      {labels}&quot;)print(f&quot;Predictions: {predictions}&quot;)``````Labels:      [7, 2, 1, 0, 4, 1, 4, 9, 5]Predictions: [7, 2, 1, 0, 4, 1, 4, 9, 5]```or loaded directly:```pythonimport torchfrom transformers import AutoModelForImageClassification, AutoImageProcessormodel = AutoModelForImageClassification.from_pretrained(&quot;example/mnist-classifier&quot;)processor = AutoImageProcessor.from_pretrained(&quot;example/mnist-classifier&quot;)inputs = processor(images, return_tensors=&quot;pt&quot;)with torch.no_grad():    # use perceiver-io Hugging Face model    output_1 = model(**inputs).logitswith torch.no_grad():    # or use perceiver-io backend model directly      output_2 = model.backend_model(inputs.pixel_values)print(f&quot;Predictions: {output_1.argmax(dim=-1).numpy().tolist()}&quot;)print(f&quot;Predictions: {output_2.argmax(dim=-1).numpy().tolist()}&quot;)``````Predictions: [7, 2, 1, 0, 4, 1, 4, 9, 5]Predictions: [7, 2, 1, 0, 4, 1, 4, 9, 5]```See [training examples](docs/training-examples.md) for more examples.## ArticlesArticles referencing this repository:- [Training compute-optimal Perceiver AR language models](https://krasserm.github.io/2023/01/23/scaling-perceiver-ar/)- [A gentle introduction to Rotary Position Embedding](https://krasserm.github.io/2022/12/13/rotary-position-embedding/)## Other implementations- [Perceiver](https://paperswithcode.com/paper/perceiver-general-perception-with-iterative#code)- [Perceiver IO](https://paperswithcode.com/paper/perceiver-io-a-general-architecture-for#code)- [Perceiver AR](https://paperswithcode.com/paper/general-purpose-long-context-autoregressive#code)</longdescription>
</pkgmetadata>