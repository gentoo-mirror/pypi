<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;!-- &lt;h1 align='center'&gt;GPJax&lt;/h1&gt;&lt;h2 align='center'&gt;Gaussian processes in Jax.&lt;/h2&gt; --&gt;&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;700&quot; height=&quot;300&quot; src=&quot;https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/_static/gpjax_logo.svg&quot; alt=&quot;GPJax's logo&quot;&gt;&lt;/p&gt;[![codecov](https://codecov.io/gh/JaxGaussianProcesses/GPJax/branch/master/graph/badge.svg?token=DM1DRDASU2)](https://codecov.io/gh/JaxGaussianProcesses/GPJax)[![CodeFactor](https://www.codefactor.io/repository/github/jaxgaussianprocesses/gpjax/badge)](https://www.codefactor.io/repository/github/jaxgaussianprocesses/gpjax)[![Documentation Status](https://readthedocs.org/projects/gpjax/badge/?version=latest)](https://gpjax.readthedocs.io/en/latest/?badge=latest)[![PyPI version](https://badge.fury.io/py/GPJax.svg)](https://badge.fury.io/py/GPJax)[![DOI](https://joss.theoj.org/papers/10.21105/joss.04455/status.svg)](https://doi.org/10.21105/joss.04455)[![Downloads](https://pepy.tech/badge/gpjax)](https://pepy.tech/project/gpjax)[![Slack Invite](https://img.shields.io/badge/Slack_Invite--blue?style=social&amp;logo=slack)](https://join.slack.com/t/gpjax/shared_invite/zt-1da57pmjn-rdBCVg9kApirEEn2E5Q2Zw)[**Quickstart**](#simple-example)| [**Install guide**](#installation)| [**Documentation**](https://gpjax.readthedocs.io/en/latest/)| [**Slack Community**](https://join.slack.com/t/gpjax/shared_invite/zt-1da57pmjn-rdBCVg9kApirEEn2E5Q2Zw)GPJax aims to provide a low-level interface to Gaussian process (GP) models in[Jax](https://github.com/google/jax), structured to give researchers maximumflexibility in extending the code to suit their own needs. The idea is that thecode should be as close as possible to the maths we write on paper when workingwith GP models.# Package supportGPJax was founded by [Thomas Pinder](https://github.com/thomaspinder). Today,the maintenance of GPJax is undertaken by [ThomasPinder](https://github.com/thomaspinder) and [DanielDodd](https://github.com/Daniel-Dodd).We would be delighted to receive contributions from interested individuals andgroups. To learn how you can get involved, please read our [guide forcontributing](https://github.com/JaxGaussianProcesses/GPJax/blob/master/CONTRIBUTING.md).If you have any questions, we encourage you to [open anissue](https://github.com/JaxGaussianProcesses/GPJax/issues/new/choose). Forbroader conversations, such as best GP fitting practices or questions about themathematics of GPs, we invite you to [open adiscussion](https://github.com/JaxGaussianProcesses/GPJax/discussions).Feel free to join our [SlackChannel](https://join.slack.com/t/gpjax/shared_invite/zt-1da57pmjn-rdBCVg9kApirEEn2E5Q2Zw),where we can discuss the development of GPJax and broader support for Gaussianprocess modelling.# Supported methods and interfaces## Notebook examples&gt; - [**Conjugate Inference**](https://gpjax.readthedocs.io/en/latest/examples/regression.html)&gt; - [**Classification with MCMC**](https://gpjax.readthedocs.io/en/latest/examples/classification.html)&gt; - [**Sparse Variational Inference**](https://gpjax.readthedocs.io/en/latest/examples/uncollapsed_vi.html)&gt; - [**BlackJax Integration**](https://gpjax.readthedocs.io/en/latest/examples/classification.html)&gt; - [**Laplace Approximation**](https://gpjax.readthedocs.io/en/latest/examples/classification.html#Laplace-approximation)&gt; - [**Inference on Non-Euclidean Spaces**](https://gpjax.readthedocs.io/en/latest/examples/kernels.html#Custom-Kernel)&gt; - [**Inference on Graphs**](https://gpjax.readthedocs.io/en/latest/examples/graph_kernels.html)&gt; - [**Learning Gaussian Process Barycentres**](https://gpjax.readthedocs.io/en/latest/examples/barycentres.html)&gt; - [**Deep Kernel Regression**](https://gpjax.readthedocs.io/en/latest/examples/haiku.html)## Guides for customisation&gt;&gt; - [**Custom kernels**](https://gpjax.readthedocs.io/en/latest/examples/kernels.html#Custom-Kernel)&gt; - [**UCI regression**](https://gpjax.readthedocs.io/en/latest/examples/yacht.html)## Conversion between `.ipynb` and `.py`Above examples are stored in [examples](examples) directory in the doublepercent (`py:percent`) format. Checkout [jupytextusing-cli](https://jupytext.readthedocs.io/en/latest/using-cli.html) for moreinfo.* To convert `example.py` to `example.ipynb`, run:```bashjupytext --to notebook example.py```* To convert `example.ipynb` to `example.py`, run:```bashjupytext --to py:percent example.ipynb```# Simple exampleLet us import some dependencies and simulate a toy dataset $\mathcal{D}$.```pythonimport gpjax as gpxfrom jax import grad, jitimport jax.numpy as jnpimport jax.random as jrimport optax as oxkey = jr.PRNGKey(123)f = lambda x: 10 * jnp.sin(x)n = 50x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,1)).sort()y = f(x) + jr.normal(key, shape=(n,1))D = gpx.Dataset(X=x, y=y)# Construct the priormeanf = gpx.mean_functions.Zero()kernel = gpx.kernels.RBF()prior = gpx.Prior(mean_function=meanf, kernel = kernel)# Define a likelihoodlikelihood = gpx.Gaussian(num_datapoints = n)# Construct the posteriorposterior = prior * likelihood# Define an optimiseroptimiser = ox.adam(learning_rate=1e-2)# Define the marginal log-likelihoodnegative_mll = jit(gpx.objectives.ConjugateMLL(negative=True))# Obtain Type 2 MLEs of the hyperparametersopt_posterior, history = gpx.fit(    model=posterior,    objective=negative_mll,    train_data=D,    optim=optimiser,    num_iters=500,    safe=True,    key=key,)# Infer the predictive posterior distributionxtest = jnp.linspace(-3., 3., 100).reshape(-1, 1)latent_dist = opt_posterior(xtest, D)predictive_dist = opt_posterior.likelihood(latent_dist)# Obtain the predictive mean and standard deviationpred_mean = predictive_dist.mean()pred_std = predictive_dist.stddev()```# Installation## Stable versionThe latest stable version of GPJax can be installed viapip:```bashpip install gpjax```&gt; **Note**&gt;&gt; We recommend you check your installation version:&gt; ```python&gt; python -c 'import gpjax; print(gpjax.__version__)'&gt; ```## Development version&gt; **Warning**&gt;&gt; This version is possibly unstable and may contain bugs.Clone a copy of the repository to your local machine and run the setupconfiguration in development mode.```bashgit clone https://github.com/JaxGaussianProcesses/GPJax.gitcd GPJaxpoetry install```&gt; **Note**&gt;&gt; We advise you create virtual environment before installing:&gt; ```&gt; conda create -n gpjax_experimental python=3.10.0&gt; conda activate gpjax_experimental&gt;  ```&gt;&gt; and recommend you check your installation passes the supplied unit tests:&gt;&gt; ```python&gt; poetry run pytest&gt; ```# Citing GPJaxIf you use GPJax in your research, please cite our [JOSS paper](https://joss.theoj.org/papers/10.21105/joss.04455#).```@article{Pinder2022,  doi = {10.21105/joss.04455},  url = {https://doi.org/10.21105/joss.04455},  year = {2022},  publisher = {The Open Journal},  volume = {7},  number = {75},  pages = {4455},  author = {Thomas Pinder and Daniel Dodd},  title = {GPJax: A Gaussian Process Framework in JAX},  journal = {Journal of Open Source Software}}```</longdescription>
</pkgmetadata>