<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># MIM: MIM Installs OpenMMLab PackagesMIM provides a unified interface for launching and installing OpenMMLab projects and their extensions, and managing the OpenMMLab model zoo.## Major Features- **Package Management**  You can use MIM to manage OpenMMLab codebases, install or uninstall them conveniently.- **Model Management**  You can use MIM to manage OpenMMLab model zoo, e.g., download checkpoints by name, search checkpoints that meet specific criteria.- **Unified Entrypoint for Scripts**  You can execute any script provided by all OpenMMLab codebases with unified commands. Train, test and inference become easier than ever. Besides, you can use `gridsearch` command for vanilla hyper-parameter search.## LicenseThis project is released under the [Apache 2.0 license](LICENSE).## Changelogv0.1.1 was released in 13/6/2021.## CustomizationYou can use `.mimrc` for customization. Now we support customize default values of each sub-command. Please refer to [customization.md](docs/en/customization.md) for details.## Build custom projects with MIMWe provide some examples of how to build custom projects based on OpenMMLAB codebases and MIM in [MIM-Example](https://github.com/open-mmlab/mim-example).Without worrying about copying codes and scripts from existing codebases, users can focus on developing new components and MIM helps integrate and run the new project.## InstallationPlease refer to [installation.md](docs/en/installation.md) for installation.## Command&lt;details&gt;&lt;summary&gt;1. install&lt;/summary&gt;- command  ```bash  # install latest version of mmcv-full  &gt; mim install mmcv-full  # wheel  # install 1.5.0  &gt; mim install mmcv-full==1.5.0  # install latest version of mmcls  &gt; mim install mmcls  # install master branch  &gt; mim install git+https://github.com/open-mmlab/mmclassification.git  # install local repo  &gt; git clone https://github.com/open-mmlab/mmclassification.git  &gt; cd mmclassification  &gt; mim install .  # install extension based on OpenMMLab  mim install git+https://github.com/xxx/mmcls-project.git  ```- api  ```python  from mim import install  # install mmcv  install('mmcv-full')  # install mmcls will automatically install mmcv if it is not installed  install('mmcls')  # install extension based on OpenMMLab  install('git+https://github.com/xxx/mmcls-project.git')  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;2. uninstall&lt;/summary&gt;- command  ```bash  # uninstall mmcv  &gt; mim uninstall mmcv-full  # uninstall mmcls  &gt; mim uninstall mmcls  ```- api  ```python  from mim import uninstall  # uninstall mmcv  uninstall('mmcv-full')  # uninstall mmcls  uninstall('mmcls')  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;3. list&lt;/summary&gt;- command  ```bash  &gt; mim list  &gt; mim list --all  ```- api  ```python  from mim import list_package  list_package()  list_package(True)  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;4. search&lt;/summary&gt;- command  ```bash  &gt; mim search mmcls  &gt; mim search mmcls==0.23.0 --remote  &gt; mim search mmcls --config resnet18_8xb16_cifar10  &gt; mim search mmcls --model resnet  &gt; mim search mmcls --dataset cifar-10  &gt; mim search mmcls --valid-field  &gt; mim search mmcls --condition 'batch_size&gt;45,epochs&gt;100'  &gt; mim search mmcls --condition 'batch_size&gt;45 epochs&gt;100'  &gt; mim search mmcls --condition '128&lt;batch_size&lt;=256'  &gt; mim search mmcls --sort batch_size epochs  &gt; mim search mmcls --field epochs batch_size weight  &gt; mim search mmcls --exclude-field weight paper  ```- api  ```python  from mim import get_model_info  get_model_info('mmcls')  get_model_info('mmcls==0.23.0', local=False)  get_model_info('mmcls', models=['resnet'])  get_model_info('mmcls', training_datasets=['cifar-10'])  get_model_info('mmcls', filter_conditions='batch_size&gt;45,epochs&gt;100')  get_model_info('mmcls', filter_conditions='batch_size&gt;45 epochs&gt;100')  get_model_info('mmcls', filter_conditions='128&lt;batch_size&lt;=256')  get_model_info('mmcls', sorted_fields=['batch_size', 'epochs'])  get_model_info('mmcls', shown_fields=['epochs', 'batch_size', 'weight'])  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;5. download&lt;/summary&gt;- command  ```bash  &gt; mim download mmcls --config resnet18_8xb16_cifar10  &gt; mim download mmcls --config resnet18_8xb16_cifar10 --dest .  ```- api  ```python  from mim import download  download('mmcls', ['resnet18_8xb16_cifar10'])  download('mmcls', ['resnet18_8xb16_cifar10'], dest_root='.')  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;6. train&lt;/summary&gt;- command  ```bash  # Train models on a single server with CPU by setting `gpus` to 0 and  # 'launcher' to 'none' (if applicable). The training script of the  # corresponding codebase will fail if it doesn't support CPU training.  &gt; mim train mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 0  # Train models on a single server with one GPU  &gt; mim train mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 1  # Train models on a single server with 4 GPUs and pytorch distributed  &gt; mim train mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 4 \      --launcher pytorch  # Train models on a slurm HPC with one 8-GPU node  &gt; mim train mmcls resnet101_b16x8_cifar10.py --launcher slurm --gpus 8 \      --gpus-per-node 8 --partition partition_name --work-dir tmp  # Print help messages of sub-command train  &gt; mim train -h  # Print help messages of sub-command train and the training script of mmcls  &gt; mim train mmcls -h  ```- api  ```python  from mim import train  train(repo='mmcls', config='resnet18_8xb16_cifar10.py', gpus=0,        other_args='--work-dir tmp')  train(repo='mmcls', config='resnet18_8xb16_cifar10.py', gpus=1,        other_args='--work-dir tmp')  train(repo='mmcls', config='resnet18_8xb16_cifar10.py', gpus=4,        launcher='pytorch', other_args='--work-dir tmp')  train(repo='mmcls', config='resnet18_8xb16_cifar10.py', gpus=8,        launcher='slurm', gpus_per_node=8, partition='partition_name',        other_args='--work-dir tmp')  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;7. test&lt;/summary&gt;- command  ```bash  # Test models on a single server with 1 GPU, report accuracy  &gt; mim test mmcls resnet101_b16x8_cifar10.py --checkpoint \      tmp/epoch_3.pth --gpus 1 --metrics accuracy  # Test models on a single server with 1 GPU, save predictions  &gt; mim test mmcls resnet101_b16x8_cifar10.py --checkpoint \      tmp/epoch_3.pth --gpus 1 --out tmp.pkl  # Test models on a single server with 4 GPUs, pytorch distributed,  # report accuracy  &gt; mim test mmcls resnet101_b16x8_cifar10.py --checkpoint \      tmp/epoch_3.pth --gpus 4 --launcher pytorch --metrics accuracy  # Test models on a slurm HPC with one 8-GPU node, report accuracy  &gt; mim test mmcls resnet101_b16x8_cifar10.py --checkpoint \      tmp/epoch_3.pth --gpus 8 --metrics accuracy --partition \      partition_name --gpus-per-node 8 --launcher slurm  # Print help messages of sub-command test  &gt; mim test -h  # Print help messages of sub-command test and the testing script of mmcls  &gt; mim test mmcls -h  ```- api  ```python  from mim import test  test(repo='mmcls', config='resnet101_b16x8_cifar10.py',       checkpoint='tmp/epoch_3.pth', gpus=1, other_args='--metrics accuracy')  test(repo='mmcls', config='resnet101_b16x8_cifar10.py',       checkpoint='tmp/epoch_3.pth', gpus=1, other_args='--out tmp.pkl')  test(repo='mmcls', config='resnet101_b16x8_cifar10.py',       checkpoint='tmp/epoch_3.pth', gpus=4, launcher='pytorch',       other_args='--metrics accuracy')  test(repo='mmcls', config='resnet101_b16x8_cifar10.py',       checkpoint='tmp/epoch_3.pth', gpus=8, partition='partition_name',       launcher='slurm', gpus_per_node=8, other_args='--metrics accuracy')  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;8. run&lt;/summary&gt;- command  ```bash  # Get the Flops of a model  &gt; mim run mmcls get_flops resnet101_b16x8_cifar10.py  # Publish a model  &gt; mim run mmcls publish_model input.pth output.pth  # Train models on a slurm HPC with one GPU  &gt; srun -p partition --gres=gpu:1 mim run mmcls train \      resnet101_b16x8_cifar10.py --work-dir tmp  # Test models on a slurm HPC with one GPU, report accuracy  &gt; srun -p partition --gres=gpu:1 mim run mmcls test \      resnet101_b16x8_cifar10.py tmp/epoch_3.pth --metrics accuracy  # Print help messages of sub-command run  &gt; mim run -h  # Print help messages of sub-command run, list all available scripts in  # codebase mmcls  &gt; mim run mmcls -h  # Print help messages of sub-command run, print the help message of  # training script in mmcls  &gt; mim run mmcls train -h  ```- api  ```python  from mim import run  run(repo='mmcls', command='get_flops',      other_args='resnet101_b16x8_cifar10.py')  run(repo='mmcls', command='publish_model',      other_args='input.pth output.pth')  run(repo='mmcls', command='train',      other_args='resnet101_b16x8_cifar10.py --work-dir tmp')  run(repo='mmcls', command='test',      other_args='resnet101_b16x8_cifar10.py tmp/epoch_3.pth --metrics accuracy')  ```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;9. gridsearch&lt;/summary&gt;- command  ```bash  # Parameter search on a single server with CPU by setting `gpus` to 0 and  # 'launcher' to 'none' (if applicable). The training script of the  # corresponding codebase will fail if it doesn't support CPU training.  &gt; mim gridsearch mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 0 \      --search-args '--optimizer.lr 1e-2 1e-3'  # Parameter search with on a single server with one GPU, search learning  # rate  &gt; mim gridsearch mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 1 \      --search-args '--optimizer.lr 1e-2 1e-3'  # Parameter search with on a single server with one GPU, search  # weight_decay  &gt; mim gridsearch mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 1 \      --search-args '--optimizer.weight_decay 1e-3 1e-4'  # Parameter search with on a single server with one GPU, search learning  # rate and weight_decay  &gt; mim gridsearch mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 1 \      --search-args '--optimizer.lr 1e-2 1e-3 --optimizer.weight_decay 1e-3 \      1e-4'  # Parameter search on a slurm HPC with one 8-GPU node, search learning  # rate and weight_decay  &gt; mim gridsearch mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 8 \      --partition partition_name --gpus-per-node 8 --launcher slurm \      --search-args '--optimizer.lr 1e-2 1e-3 --optimizer.weight_decay 1e-3 \      1e-4'  # Parameter search on a slurm HPC with one 8-GPU node, search learning  # rate and weight_decay, max parallel jobs is 2  &gt; mim gridsearch mmcls resnet101_b16x8_cifar10.py --work-dir tmp --gpus 8 \      --partition partition_name --gpus-per-node 8 --launcher slurm \      --max-jobs 2 --search-args '--optimizer.lr 1e-2 1e-3 \      --optimizer.weight_decay 1e-3 1e-4'  # Print the help message of sub-command search  &gt; mim gridsearch -h  # Print the help message of sub-command search and the help message of the  # training script of codebase mmcls  &gt; mim gridsearch mmcls -h  ```- api  ```python  from mim import gridsearch  gridsearch(repo='mmcls', config='resnet101_b16x8_cifar10.py', gpus=0,             search_args='--optimizer.lr 1e-2 1e-3',             other_args='--work-dir tmp')  gridsearch(repo='mmcls', config='resnet101_b16x8_cifar10.py', gpus=1,             search_args='--optimizer.lr 1e-2 1e-3',             other_args='--work-dir tmp')  gridsearch(repo='mmcls', config='resnet101_b16x8_cifar10.py', gpus=1,             search_args='--optimizer.weight_decay 1e-3 1e-4',             other_args='--work-dir tmp')  gridsearch(repo='mmcls', config='resnet101_b16x8_cifar10.py', gpus=1,             search_args='--optimizer.lr 1e-2 1e-3 --optimizer.weight_decay'                         '1e-3 1e-4',             other_args='--work-dir tmp')  gridsearch(repo='mmcls', config='resnet101_b16x8_cifar10.py', gpus=8,             partition='partition_name', gpus_per_node=8, launcher='slurm',             search_args='--optimizer.lr 1e-2 1e-3 --optimizer.weight_decay'                         ' 1e-3 1e-4',             other_args='--work-dir tmp')  gridsearch(repo='mmcls', config='resnet101_b16x8_cifar10.py', gpus=8,             partition='partition_name', gpus_per_node=8, launcher='slurm',             max_workers=2,             search_args='--optimizer.lr 1e-2 1e-3 --optimizer.weight_decay'                         ' 1e-3 1e-4',             other_args='--work-dir tmp')  ```&lt;/details&gt;## ContributingWe appreciate all contributions to improve mim. Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/master/CONTRIBUTING.md) for the contributing guideline.## LicenseThis project is released under the [Apache 2.0 license](LICENSE).## Projects in OpenMMLab- [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.- [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.- [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.- [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.- [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.- [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.- [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.- [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.- [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.- [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.- [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.- [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.- [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.- [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.- [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.- [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.- [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.- [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.</longdescription>
</pkgmetadata>