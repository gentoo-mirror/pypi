<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>==================Welcome to RETURNN==================`GitHub repository &lt;https://github.com/rwth-i6/returnn&gt;`__.`RETURNN paper 2016 &lt;https://arxiv.org/abs/1608.00895&gt;`_,`RETURNN paper 2018 &lt;https://arxiv.org/abs/1805.05225&gt;`_.RETURNN - RWTH extensible training framework for universal recurrent neural networks,is a Theano/TensorFlow-based implementation of modern recurrent neural network architectures.It is optimized for fast and reliable training of recurrent neural networks in a multi-GPU environment.The high-level features and goals of RETURNN are:* **Simplicity**  * Writing config / code is simple &amp; straight-forward (setting up experiment, defining model)  * Debugging in case of problems is simple  * Reading config / code is simple (defined model, training, decoding all becomes clear)* **Flexibility**  * Allow for many different kinds of experiments / models* **Efficiency**  * Training speed  * Decoding speedAll items are important for research, decoding speed is esp. important for production.See our `Interspeech 2020 tutorial &quot;Efficient and Flexible Implementation of Machine Learning for ASR and MT&quot; video &lt;https://www.youtube.com/watch?v=wPKdYqSOlAY&gt;`__(`slides &lt;https://www-i6.informatik.rwth-aachen.de/publications/download/1154/Zeyer--2020.pdf&gt;`__)with an introduction of the core concepts.More specific features include:- Mini-batch training of feed-forward neural networks- Sequence-chunking based batch training for recurrent neural networks- Long short-term memory recurrent neural networks  including our own fast CUDA kernel- Multidimensional LSTM (GPU only, there is no CPU version)- Memory management for large data sets- Work distribution across multiple devices- Flexible and fast architecture which allows all kinds of encoder-attention-decoder modelsSee `documentation &lt;https://returnn.readthedocs.io/&gt;`__.See `basic usage &lt;https://returnn.readthedocs.io/en/latest/basic_usage.html&gt;`__and `technological overview &lt;https://returnn.readthedocs.io/en/latest/tech_overview.html&gt;`__.`Here is the video recording of a RETURNN overview talk &lt;https://www-i6.informatik.rwth-aachen.de/web/Software/returnn/downloads/workshop-2019-01-29/01.recording.cut.mp4&gt;`_(`slides &lt;https://www-i6.informatik.rwth-aachen.de/web/Software/returnn/downloads/workshop-2019-01-29/01.returnn-overview.session1.handout.v1.pdf&gt;`__,`exercise sheet &lt;https://www-i6.informatik.rwth-aachen.de/web/Software/returnn/downloads/workshop-2019-01-29/01.exercise_sheet.pdf&gt;`__;hosted by eBay).There are `many example demos &lt;https://github.com/rwth-i6/returnn/blob/master/demos/&gt;`_which work on artificially generated data,i.e. they should work as-is.There are `some real-world examples &lt;https://github.com/rwth-i6/returnn-experiments&gt;`_such as setups for speech recognition on the Switchboard or LibriSpeech corpus.Some benchmark setups against other frameworkscan be found `here &lt;https://github.com/rwth-i6/returnn-benchmarks&gt;`_.The results are in the `RETURNN paper 2016 &lt;https://arxiv.org/abs/1608.00895&gt;`_.Performance benchmarks of our LSTM kernel vs CuDNN and other TensorFlow kernelsare in `TensorFlow LSTM benchmark &lt;https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html&gt;`__.There is also `a wiki &lt;https://github.com/rwth-i6/returnn/wiki&gt;`_.Questions can also be asked on`StackOverflow using the RETURNN tag &lt;https://stackoverflow.com/questions/tagged/returnn&gt;`_... image:: https://github.com/rwth-i6/returnn/workflows/CI/badge.svg    :target: https://github.com/rwth-i6/returnn/actions</longdescription>
</pkgmetadata>