<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>## gptwc: wc for GPT tokensThe `wc` utility counts words or characters. The `gptwc` utility functions similarly but counts tokens.Tokens are smaller than words but larger than characters, and are a more compact representation of text used by large language models.Use `gptwc` to check the number of tokens in a string, in order to remain under the token limit (eg. 4097) for your large language model API. Uses `tiktoken`.## Installation```$ pip install gptwc$ echo &quot;Simple is better than complex.&quot; | gptwc7```## Example Usage```$ cat LICENSE  | gptwc257$ cat LICENSE | wc -c1059$ cat LICENSE | wc -w165$ curl -s 'https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt' | wc -w26470curl -s 'https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt' | gptwc40085$ cat LICENSE | gptwc --model text-davinci-003257$ cat LICENSE | gptwc --model gpt-3.5-turbo201$ cat README.md | pbcopy$ gptwc -c517```## Options```usage: gptwc [-h] [--files0-from F] [--model MODEL] [-c] [--version] [FILE ...]Count tokens in text files using OpenAI's tiktoken library.positional arguments:  FILE             Text files to count tokens inoptions:  -h, --help       show this help message and exit  --files0-from F  Read input from the files specified by NUL-terminated names in file F  --model MODEL    Model name to use for tokenization (default: text-davinci-003)  -c, --clipboard  Read input from the system clipboard  --version        show program's version number and exit```## Which Tokenizer Does Each Model Use?From [tiktoken/model.py](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py)```&quot;gpt-4&quot;: &quot;cl100k_base&quot;,&quot;gpt-3.5-turbo&quot;: &quot;cl100k_base&quot;,&quot;text-embedding-ada-002&quot;: &quot;cl100k_base&quot;,&quot;text-davinci-003&quot;: &quot;p50k_base&quot;,&quot;text-davinci-002&quot;: &quot;p50k_base&quot;,&quot;code-davinci-002&quot;: &quot;p50k_base&quot;,&quot;code-davinci-001&quot;: &quot;p50k_base&quot;,&quot;code-cushman-002&quot;: &quot;p50k_base&quot;,&quot;code-cushman-001&quot;: &quot;p50k_base&quot;,&quot;davinci-codex&quot;: &quot;p50k_base&quot;,&quot;cushman-codex&quot;: &quot;p50k_base&quot;,&quot;text-davinci-001&quot;: &quot;r50k_base&quot;,&quot;text-curie-001&quot;: &quot;r50k_base&quot;,&quot;text-babbage-001&quot;: &quot;r50k_base&quot;,&quot;text-ada-001&quot;: &quot;r50k_base&quot;,&quot;davinci&quot;: &quot;r50k_base&quot;,&quot;curie&quot;: &quot;r50k_base&quot;,&quot;babbage&quot;: &quot;r50k_base&quot;,&quot;ada&quot;: &quot;r50k_base&quot;,&quot;text-similarity-davinci-001&quot;: &quot;r50k_base&quot;,&quot;text-similarity-curie-001&quot;: &quot;r50k_base&quot;,&quot;text-similarity-babbage-001&quot;: &quot;r50k_base&quot;,&quot;text-similarity-ada-001&quot;: &quot;r50k_base&quot;,&quot;text-search-davinci-doc-001&quot;: &quot;r50k_base&quot;,&quot;text-search-curie-doc-001&quot;: &quot;r50k_base&quot;,&quot;text-search-babbage-doc-001&quot;: &quot;r50k_base&quot;,&quot;text-search-ada-doc-001&quot;: &quot;r50k_base&quot;,&quot;code-search-babbage-code-001&quot;: &quot;r50k_base&quot;,&quot;code-search-ada-code-001&quot;: &quot;r50k_base&quot;,&quot;text-davinci-edit-001&quot;: &quot;p50k_edit&quot;,&quot;code-davinci-edit-001&quot;: &quot;p50k_edit&quot;,&quot;gpt2&quot;: &quot;gpt2&quot;,```</longdescription>
</pkgmetadata>