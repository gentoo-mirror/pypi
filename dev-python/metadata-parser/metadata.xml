<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>MetadataParser==============.. |build_status| image:: https://github.com/jvanasco/metadata_parser/workflows/Python%20package/badge.svgBuild Status: |build_status|MetadataParser is a Python module for pulling metadata out of web documents.It requires BeautifulSoup , and was largely based on Erik River's`opengraph module &lt;https://github.com/erikriver/opengraph&gt;`_. Something moreaggressive than Erik's module was needed, so this project was started.Installation=============pip install metadata_parserInstallation Recommendation===========================The ``requests`` library version 2.4.3 or newer is strongly recommended.This is not required, but it is better.  On earlier versions it is possible tohave an uncaught DecodeError exception when there is an underlying redirect/404.Recent fixes to ``requests`` improve redirect handling, urllib3 and urllib3errors.Features========* ``metadata_parser`` pulls as much metadata out of a document as possible* Developers can set a 'strategy' for finding metadata (i.e. only accept  opengraph or page attributes)* Lightweight but functional(!) url validation* Verbose loggingLogging=======This file has extensive logging to help developers pinpoint problems.* ``log.debug``  This log level is mostly used to handle library maintenance and  troubleshooting, aka &quot;Library Debugging&quot;.  Library Debugging is verbose, but  is nested under ``if __debug__:`` statements, so it is compiled away when  PYTHONOPTIMIZE is set.  Several sections of logic useful to developers will also emit logging  statements at the ``debug`` level, regardless of PYTHONOPTIMIZE.* ``log.info``  Currently unused* ``log.warning``  Currently unused* ``log.error``  This log level is mostly used to alert developers of errors that were  encountered during url fetching and document parsing, and often emits a log  statement just before an Exception is raised. The log statements will contain  at least the exception type, and may contain the active URL and additional  debugging information, if any of that information is available.* ``log.critical``  Currently unusedIt is STRONGLY recommended to keep Python's logging at ``debug``.Optional Integrations=====================* ``tldextract``  This package will attempt to use the package ``tldextract`` for advanced domain  and hostname analysis. If ``tldextract`` is not found, a fallback is used.Environment Variables=====================* ``METADATA_PARSER__DISABLE_TLDEXTRACT``  Default: &quot;0&quot;.  If set to &quot;1&quot;, the package will not attempt to load ``tldextract``.* ``METADATA_PARSER__ENCODING_FALLBACK``  Default: &quot;ISO-8859-1&quot;  Used as the fallback when trying to decode a response.*  ``METADATA_PARSER__DUMMY_URL``   Used as the fallback URL when calculating url data.Notes=====1. This package requires BeautifulSoup 4.2. For speed, it will instantiate a BeautifulSoup parser with lxml, and   fallback to 'none' (the internal pure Python) if it can't load lxml.3. URL Validation is not RFC compliant, but tries to be &quot;Real World&quot; compliant.It is HIGHLY recommended that you install lxml for usage.lxml is considerably faster.Considerably faster.Developers should also use a very recent version of lxml.segfaults have been reported on lxml versions &lt; 2.3.x;Using at least the most recent 3.x versions is strongly recommendedThe default 'strategy' is to look in this order::    og,dc,meta,pageWhich stands for the following::    og = OpenGraph    dc = DublinCore    meta = metadata    page = page elementsDevelopers can specify a strategy as a comma-separated list of the above.The only 2 page elements currently supported are::    &lt;title&gt;VALUE&lt;/title&gt; -&gt; metadata['page']['title']    &lt;link rel=&quot;canonical&quot; href=&quot;VALUE&quot;&gt; -&gt; metadata['page']['link']'metadata' elements are supported by ``name`` and ``property``.The MetadataParser object also wraps some convenience functions, which can beused otherwise , that are designed to turn alleged urls into well formed urls.For example, you may pull a page::    http://www.example.com/path/to/file.htmland that file indicates a canonical url which is simple &quot;/file.html&quot;.This package will try to 'remount' the canonical url to the absolute url of&quot;http://www.example.com/file.html&quot;.Tt will return None if the end result is not a valid url.This all happens under-the-hood, and is honestly really useful when dealingwith indexers and spiders.URL Validation==============&quot;Real World&quot; URL validation is enabled by default.  This is not RFC compliant.There are a few gaps in the RFCs that allow for &quot;odd behavior&quot;.Just about any use-case for this package will desire/expect rules that parseURLs &quot;in the wild&quot;, not theoretical.The differences:* If an entirely numeric ip address is encountered, it is assumed to be a  dot-notation IPV4 and it is checked to have the right amount of valid octets.    The default behavior is to invalidate these hosts::        http://256.256.256.256        http://999.999.999.999.999  According to RFCs those are valid hostnames that would fail as &quot;IP Addresses&quot;  but pass as &quot;Domain Names&quot;.  However in the real world, one would never  encounter domain names like those.* The only non-domain hostname that is allowed, is &quot;localhost&quot;  The default behavior is to invalidate these hosts ::        http://example        http://examplecom  Those are considered to be valid hosts, and might exist on a local network or  custom hosts file.  However, they are not part of the public internet.Although this behavior breaks RFCs, it greatly reduces the number of&quot;False Positives&quot; generated when analyzing internet pages. If you want toinclude bad data, you can submit a kwarg to ``MetadataParser.__init__``Handling Bad URLs and Encoded URIs==================================This library tries to safeguard against a few common situations.Encoded URIs and relative urls------------------------------Most website publishers will define an image as a URL::    &lt;meta property=&quot;og:image&quot; content=&quot;http://example.com/image.jpg&quot; /&gt;Some will define an image as an encoded URI::    &lt;meta property=&quot;og:image&quot; content=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNM+Q8AAc0BZX6f84gAAAAASUVORK5CYII=&quot; /&gt;By default, the ``get_metadata_link()`` method can be used to ensure a valid linkis extracted from the metadata payload::    &gt;&gt;&gt; import metadata_parser    &gt;&gt;&gt; page = metadata_parser.MetadataParser(url=&quot;http://www.example.com&quot;)    &gt;&gt;&gt; print page.get_metadata_link('image')This method accepts a kwarg ``allow_encoded_uri`` (default False) which willreturn the image without further processing::    &gt;&gt;&gt; print page.get_metadata_link('image', allow_encoded_uri=True)Similarly, if a url is local::    &lt;meta property=&quot;og:image&quot; content=&quot;/image.jpg&quot; /&gt;The ``get_metadata_link`` method will automatically upgrade it onto the domain::    &gt;&gt;&gt; print page.get_metadata_link('image')    http://example.com/image.jpgPoorly Constructed Canonical URLs---------------------------------Many website publishers implement canonical URLs incorrectly.  This packagetries to fix that.By default ``MetadataParser`` is constructed with ``require_public_netloc=True``and ``allow_localhosts=True``.This will require somewhat valid 'public' network locations in the url.For example, these will all be valid URLs::    http://example.com    http://1.2.3.4    http://localhost    http://127.0.0.1    http://0.0.0.0If these known 'localhost' urls are not wanted, they can be filtered out with``allow_localhosts=False``::    http://localhost    http://127.0.0.1    http://0.0.0.0There are two convenience methods that can be used to get a canonical url orcalculate the effective url::* MetadataParser.get_discrete_url* MetadataParser.get_metadata_linkThese both accept an argument ``require_public_global``, which defaults to ``True``.Assuming we have the following content on the url ``http://example.com/path/to/foo``::    &lt;link rel=&quot;canonical&quot; href=&quot;http://localhost:8000/alt-path/to/foo&quot;&gt;By default, versions 0.9.0 and later will detect 'localhost:8000' as animproper canonical url, and remount the local part &quot;/alt-path/to/foo&quot; onto thedomain that served the file.  The vast majority of times this 'behavior'has been encountered, this is the intended canonical::    print page.get_discrete_url()    &gt;&gt;&gt; http://example.com/alt-path/to/fooIn contrast, versions 0.8.3 and earlier will not catch this situation::    print page.get_discrete_url()    &gt;&gt;&gt; http://localhost:8000/alt-path/to/fooIn order to preserve the earlier behavior, just submit ``require_public_global=False``::    print page.get_discrete_url(require_public_global=False)    &gt;&gt;&gt; http://localhost:8000/alt-path/to/fooHandling Bad Data=================Many CMS systems (and developers) create malformed content or incorrectdocument identifiers.  When this happens, the BeautifulSoup parser will losedata or move it into an unexpected place.There are two arguments that can help you analyze this data:* force_doctype::    ``MetadataParser(..., force_doctype=True, ...)````force_doctype=True`` will try to replace the identified doctype with &quot;html&quot;via regex.  This will often make the input data usable by BS4.* search_head_only::    ``MetadataParser(..., search_head_only=False, ...)````search_head_only=False`` will not limit the search path to the &quot;&lt;head&gt;&quot; element.This will have a slight performance hit and will incorporate data from CMS/Usercontent, not just templates/Site-Operators.WARNING=============1.0 will be a complete API overhaul.  pin your releases to avoid sadness.Version 0.9.19 Breaking Changes===============================Issue #12 exposed some flaws in the existing package1. ``MetadataParser.get_metadatas`` replaces ``MetadataParser.get_metadata``----------------------------------------------------------------------------Until version 0.9.19, the recommended way to get metadata was to use``get_metadata`` which will either return a string (or None).Starting with version 0.9.19, the recommended way to get metadata is to use``get_metadatas`` which will always return a list (or None).This change was made because the library incorrectly stored a single metadatakey value when there were duplicates.2. The ``ParsedResult`` payload stores mixed content and tracks it's version==--------------------------------------------------------------------------Many users (including the maintainer) archive the parsed metadata. Aftertesting a variety of payloads with an all-list format and a mixed format(string or list), a mixed format had a much smaller payload size with anegligible performance hit. A new ``_v`` attribute tracks the payload version.In the future, payloads without a ``_v`` attribute will be interpreted as thepre-versioning format.3. ``DublinCore`` payloads might be a dict------------------------------------------Tests were added to handle dublincore data. An extra attribute may be needed toproperly represent the payload, so always returning a dict with at least aname+content (and possibly ``lang`` or ``scheme`` is the best approach.Usage=====Until version ``0.9.19``, the recommended way to get metadata was to use``get_metadata`` which will return a string (or None):**From an URL**::    &gt;&gt;&gt; import metadata_parser    &gt;&gt;&gt; page = metadata_parser.MetadataParser(url=&quot;http://www.example.com&quot;)    &gt;&gt;&gt; print page.metadata    &gt;&gt;&gt; print page.get_metadatas('title')    &gt;&gt;&gt; print page.get_metadatas('title', strategy=['og',])    &gt;&gt;&gt; print page.get_metadatas('title', strategy=['page', 'og', 'dc',])**From HTML**::    &gt;&gt;&gt; HTML = &quot;&quot;&quot;&lt;here&gt;&quot;&quot;&quot;    &gt;&gt;&gt; page = metadata_parser.MetadataParser(html=HTML)    &gt;&gt;&gt; print page.metadata    &gt;&gt;&gt; print page.get_metadatas('title')    &gt;&gt;&gt; print page.get_metadatas('title', strategy=['og',])    &gt;&gt;&gt; print page.get_metadatas('title', strategy=['page', 'og', 'dc',])Malformed Data==============It is very common to find malformed data. As of version ``0.9.20`` the followingmethods should be used to allow malformed presentation::    &gt;&gt;&gt; page = metadata_parser.MetadataParser(html=HTML, support_malformed=True)or::    &gt;&gt;&gt; parsed = page.parse(html=html, support_malformed=True)    &gt;&gt;&gt; parsed = page.parse(html=html, support_malformed=False)The above options will support parsing common malformed options.  Currentlythis only looks at alternate (improper) ways of producing twitter tags, but maybe expanded.Notes=====when building on Python3, a ``static`` toplevel directory may be needed</longdescription>
</pkgmetadata>