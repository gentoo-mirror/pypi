<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># TorchSnapshot (Beta Release)&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/pytorch/torchsnapshot/actions?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/workflow/status/pytorch/torchsnapshot/Unit%20tests/main&quot; alt=&quot;build status&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://pypi.org/project/torchsnapshot&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/torchsnapshot&quot; alt=&quot;pypi version&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://pypi.org/project/torchsnapshot-nightly&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/torchsnapshot-nightly?label=nightly&quot; alt=&quot;pypi nightly version&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://codecov.io/gh/pytorch/torchsnapshot&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/pytorch/torchsnapshot/branch/main/graph/badge.svg?token=DR67Q6T7YF&quot; alt=&quot;codecov&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/pytorch/torchsnapshot/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/torchsnapshot&quot; alt=&quot;bsd license&quot;&gt;&lt;/a&gt;&lt;/div&gt;A performant, memory-efficient checkpointing library for PyTorch applications, designed with large, complex distributed workloads in mind.## InstallRequires Python &gt;= 3.7 and PyTorch &gt;= 1.12From pip:```bash# Stablepip install torchsnapshot# Nightlypip install --pre torchsnapshot-nightly```From source:```bashgit clone https://github.com/pytorch/torchsnapshotcd torchsnapshotpip install -r requirements.txtpython setup.py install```## Why TorchSnapshot**Performance**- TorchSnapshot provides a fast checkpointing implementation employing various optimizations, including zero-copy serialization for most tensor types, overlapped device-to-host copy and storage I/O, parallelized storage I/O.- TorchSnapshot greatly speeds up checkpointing for DistributedDataParallel workloads by distributing the write load across all ranks ([benchmark](https://github.com/pytorch/torchsnapshot/tree/main/benchmarks/ddp)).- When host memory is abundant, TorchSnapshot allows training to resume before all storage I/O completes, reducing the time blocked by checkpoint saving.**Memory Usage**- TorchSnapshot's memory usage adapts to the host's available resources, greatly reducing the chance of out-of-memory issues when saving and loading checkpoints.- TorchSnapshot supports efficient random access to individual objects within a snapshot, even when the snapshot is stored in a cloud object storage.**Usability**- Simple APIs that are consistent between distributed and non-distributed workloads.- Out of the box integration with commonly used cloud object storage systems.- Automatic resharding (elasticity) on world size change for supported workloads ([more details](https://pytorch.org/torchsnapshot/getting_started.html#elasticity-experimental)).**Security**- Secure tensor serialization without pickle dependency [WIP].## Getting Started```pythonfrom torchsnapshot import Snapshot# Taking a snapshotapp_state = {&quot;model&quot;: model, &quot;optimizer&quot;: optimizer}snapshot = Snapshot.take(path=&quot;/path/to/snapshot&quot;, app_state=app_state)# Restoring from a snapshotsnapshot.restore(app_state=app_state)```See the [documentation](https://pytorch.org/torchsnapshot/main/getting_started.html) for more details.## Licensetorchsnapshot is BSD licensed, as found in the [LICENSE](LICENSE) file.</longdescription>
</pkgmetadata>