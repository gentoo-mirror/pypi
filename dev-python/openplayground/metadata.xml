<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># openplaygroundAn LLM playground you can run on your laptop.https://user-images.githubusercontent.com/111631/227399583-39b23f48-9823-4571-a906-985dbe282b20.mp4#### Features- Use any model from [OpenAI](), [Anthropic](), [Cohere](), [Forefront](), [HuggingFace](), [Aleph Alpha](), and [llama.cpp]().- Full playground UI, including history, parameter tuning, keyboard shortcuts, and logprops.- Compare models side-by-side with the same prompt, individually tune model parameters, and retry with different parameters.- Automatically detects local models in your HuggingFace cache, and lets you install new ones.- Works OK on your phone.- Probably won't kill everyone.## Try on nat.devTry the hosted version: [nat.dev](https://nat.dev).## How to install and run```sh$ pip install openplayground$ openplayground run```Alternatively, run it as a docker container:```sh$ docker run --name openplayground -p 5432:5432 -d --volume openplayground:/web/config natorg/openplayground```This runs a Flask process, so you can add the typical flags such as setting a different port `openplayground run -p 1235` and others.## How to run for development```sh$ git clone https://github.com/nat/openplayground$ cd app &amp;&amp; npm install &amp;&amp; npx parcel watch src/index.html --no-cache$ cd server &amp;&amp; pip3 install -r requirements.txt &amp;&amp; cd .. &amp;&amp; python3 -m server.app```## Docker```sh$ docker build . --tag &quot;openplayground&quot;$ docker run --name openplayground -p 5432:5432 -d --volume openplayground:/web/config openplayground```First volume is optional. It's used to store API keys, models settings.## Ideas for contributions- Add a token counter to the playground- Add a cost counter to the playground and the compare page- Measure and display time to first token- Setup automatic builds with GitHub Actions- The default parameters for each model are configured in the `server/models.json` file. If you find better default parameters for a model, please submit a pull request!- Someone can help us make a homebrew package, and a dockerfile- Easier way to install open source models directly from openplayground, with `openplayground install &lt;model&gt;` or in the UI.- Find and fix bugs- ChatGPT UI, with turn-by-turn, markdown rendering, chatgpt plugin support, etc.- We will probably need multimodal inputs and outputs at some point in 2023### llama.cpp## Adding models to openplaygroundModels and providers have three types in openplayground:- Searchable- Local inference- APIYou can add models in `server/models.json` with the following schema:#### Local inferenceFor models running locally on your device you can add them to openplayground like the following (a minimal example):```json&quot;llama&quot;: {    &quot;api_key&quot; : false,    &quot;models&quot; : {        &quot;llama-70b&quot;: {            &quot;parameters&quot;: {                &quot;temperature&quot;: {                    &quot;value&quot;: 0.5,                    &quot;range&quot;: [                        0.1,                        1.0                    ]                },            }        }    }}```Keep in mind you will need to add a generation method for your model in `server/app.py`. Take a look at `local_text_generation()` as an example.#### API Provider InferenceThis is for model providers like OpenAI, cohere, forefront, and more. You can connect them easily into openplayground (a minimal example):```json&quot;cohere&quot;: {    &quot;api_key&quot; : true,    &quot;models&quot; : {        &quot;xlarge&quot;: {            &quot;parameters&quot;: {                &quot;temperature&quot;: {                    &quot;value&quot;: 0.5,                    &quot;range&quot;: [                        0.1,                        1.0                    ]                },            }        }    }}```Keep in mind you will need to add a generation method for your model in `server/app.py`. Take a look at `openai_text_generation()` or `cohere_text_generation()` as an example.#### Searchable modelsWe use this for Huggingface Remote Inference models, the search endpoint is useful for scaling to N models in the settings page.```json&quot;provider_name&quot;: {    &quot;api_key&quot;: true,    &quot;search&quot;: {        &quot;endpoint&quot;: &quot;ENDPOINT_URL&quot;    },    &quot;parameters&quot;: {        &quot;parameter&quot;: {            &quot;value&quot;: 1.0,            &quot;range&quot;: [                0.1,                1.0            ]        },    }}```#### CreditsInstigated by Nat Friedman. Initial implementation by [Zain Huda](https://github.com/zainhuda) as a repl.it bounty. Many features and extensive refactoring by Alex Lourenco.</longdescription>
</pkgmetadata>