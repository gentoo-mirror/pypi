<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;!-- start inference-banner --&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://cloud.jina.ai/user/inference&quot;&gt;  &lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/banner.svg?raw=true&quot; width=&quot;100%&quot;&gt;&lt;/a&gt;&lt;!-- end inference-banner --&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://clip-as-service.jina.ai&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/docs/_static/logo-light.svg?raw=true&quot; alt=&quot;CLIP-as-service logo: The data structure for unstructured data&quot; width=&quot;200px&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;p align=center&gt;&lt;a href=&quot;https://pypi.org/project/clip_server/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/clip_server?label=Release&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://slack.jina.ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-3.1k-blueviolet?logo=slack&amp;amp;logoColor=white&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://codecov.io/gh/jina-ai/clip-as-service&quot;&gt;&lt;img alt=&quot;Codecov branch&quot; src=&quot;https://img.shields.io/codecov/c/github/jina-ai/clip-as-service/main?logo=Codecov&amp;logoColor=white&amp;style=flat-square&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://colab.research.google.com/github/jina-ai/clip-as-service/blob/main/docs/hosting/cas-on-colab.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Host-on%20Google%20Colab%20(GPU/TPU)-brightgreen?style=flat-square&amp;logo=googlecolab&amp;&amp;logoColor=white&quot; alt=&quot;Host on Google Colab with GPU/TPU support&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;!-- start elevator-pitch --&gt;CLIP-as-service is a low-latency high-scalability service for embedding images and text. It can be easily integrated as a microservice into neural search solutions.‚ö° **Fast**: Serve CLIP models with TensorRT, ONNX runtime and PyTorch w/o JIT with 800QPS&lt;sup&gt;[*]&lt;/sup&gt;. Non-blocking duplex streaming on requests and responses, designed for large data and long-running tasks. ü´ê **Elastic**: Horizontally scale up and down multiple CLIP models on single GPU, with automatic load balancing.üê• **Easy-to-use**: No learning curve, minimalist design on client and server. Intuitive and consistent API for image and sentence embedding. üëí **Modern**: Async client support. Easily switch between gRPC, HTTP, WebSocket protocols with TLS and compression.üç± **Integration**: Smooth integration with neural search ecosystem including [Jina](https://github.com/jina-ai/jina) and [DocArray](https://github.com/jina-ai/docarray). Build cross-modal and multi-modal solutions in no time. &lt;sup&gt;[*] with default config (single replica, PyTorch no JIT) on GeForce RTX 3090. &lt;/sup&gt;&lt;!-- end elevator-pitch --&gt;## Try it!An always-online server `api.clip.jina.ai` loaded with `ViT-L-14-336::openai` is there for you to play &amp; test.Before you start, make sure you have obtained a personal access token from the [Jina AI Cloud](https://cloud.jina.ai/settings/tokens), or via CLI as described in [this guide](https://docs.jina.ai/jina-ai-cloud/login/#create-a-new-pat):```bash jina auth token create &lt;name of PAT&gt; -e &lt;expiration days&gt;```Then, you need to configure the access token in the parameter `credential` of the client in python or set it in the HTTP request header `Authorization` as `&lt;your access token&gt;`.‚ö†Ô∏è Our demo server `demo-cas.jina.ai` is sunset and no longer available after **15th of Sept 2022**. ### Text &amp; image embedding&lt;table&gt;&lt;tr&gt;&lt;td&gt; via HTTPS üîê &lt;/td&gt;&lt;td&gt; via gRPC üîê‚ö°‚ö° &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```bashcurl \-X POST https://api.clip.jina.ai:8443/post \-H 'Content-Type: application/json' \-H 'Authorization: &lt;your access token&gt;' \-d '{&quot;data&quot;:[{&quot;text&quot;: &quot;First do it&quot;},     {&quot;text&quot;: &quot;then do it right&quot;},     {&quot;text&quot;: &quot;then do it better&quot;},     {&quot;uri&quot;: &quot;https://picsum.photos/200&quot;}],     &quot;execEndpoint&quot;:&quot;/&quot;}'```&lt;/td&gt;&lt;td&gt;```python# pip install clip-clientfrom clip_client import Clientc = Client(    'grpcs://api.clip.jina.ai:2096', credential={'Authorization': '&lt;your access token&gt;'})r = c.encode(    [        'First do it',        'then do it right',        'then do it better',        'https://picsum.photos/200',    ])print(r)```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;### Visual reasoningThere are four basic visual reasoning skills: object recognition, object counting, color recognition, and spatial relation understanding. Let's try some:&gt; You need to install [`jq` (a JSON processor)](https://stedolan.github.io/jq/) to prettify the results.&lt;table&gt;&lt;tr&gt;&lt;td&gt; Image &lt;/td&gt;&lt;td&gt; via HTTPS üîê &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;https://picsum.photos/id/1/300/300&quot;&gt;&lt;/td&gt;&lt;td&gt;```bashcurl \-X POST https://api.clip.jina.ai:8443/post \-H 'Content-Type: application/json' \-H 'Authorization: &lt;your access token&gt;' \-d '{&quot;data&quot;:[{&quot;uri&quot;: &quot;https://picsum.photos/id/1/300/300&quot;,&quot;matches&quot;: [{&quot;text&quot;: &quot;there is a woman in the photo&quot;},            {&quot;text&quot;: &quot;there is a man in the photo&quot;}]}],            &quot;execEndpoint&quot;:&quot;/rank&quot;}' \| jq &quot;.data[].matches[] | (.text, .scores.clip_score.value)&quot;```gives:```&quot;there is a woman in the photo&quot;0.626907229423523&quot;there is a man in the photo&quot;0.37309277057647705```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;https://picsum.photos/id/133/300/300&quot;&gt;&lt;/td&gt;&lt;td&gt;```bashcurl \-X POST https://api.clip.jina.ai:8443/post \-H 'Content-Type: application/json' \-H 'Authorization: &lt;your access token&gt;' \-d '{&quot;data&quot;:[{&quot;uri&quot;: &quot;https://picsum.photos/id/133/300/300&quot;,&quot;matches&quot;: [{&quot;text&quot;: &quot;the blue car is on the left, the red car is on the right&quot;},{&quot;text&quot;: &quot;the blue car is on the right, the red car is on the left&quot;},{&quot;text&quot;: &quot;the blue car is on top of the red car&quot;},{&quot;text&quot;: &quot;the blue car is below the red car&quot;}]}],&quot;execEndpoint&quot;:&quot;/rank&quot;}' \| jq &quot;.data[].matches[] | (.text, .scores.clip_score.value)&quot;```gives:```&quot;the blue car is on the left, the red car is on the right&quot;0.5232442617416382&quot;the blue car is on the right, the red car is on the left&quot;0.32878655195236206&quot;the blue car is below the red car&quot;0.11064132302999496&quot;the blue car is on top of the red car&quot;0.03732786327600479```&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;https://picsum.photos/id/102/300/300&quot;&gt;&lt;/td&gt;&lt;td&gt;```bashcurl \-X POST https://api.clip.jina.ai:8443/post \-H 'Content-Type: application/json' \-H 'Authorization: &lt;your access token&gt;' \-d '{&quot;data&quot;:[{&quot;uri&quot;: &quot;https://picsum.photos/id/102/300/300&quot;,&quot;matches&quot;: [{&quot;text&quot;: &quot;this is a photo of one berry&quot;},            {&quot;text&quot;: &quot;this is a photo of two berries&quot;},            {&quot;text&quot;: &quot;this is a photo of three berries&quot;},            {&quot;text&quot;: &quot;this is a photo of four berries&quot;},            {&quot;text&quot;: &quot;this is a photo of five berries&quot;},            {&quot;text&quot;: &quot;this is a photo of six berries&quot;}]}],            &quot;execEndpoint&quot;:&quot;/rank&quot;}' \| jq &quot;.data[].matches[] | (.text, .scores.clip_score.value)&quot;```gives:```&quot;this is a photo of three berries&quot;0.48507222533226013&quot;this is a photo of four berries&quot;0.2377079576253891&quot;this is a photo of one berry&quot;0.11304923892021179&quot;this is a photo of five berries&quot;0.0731358453631401&quot;this is a photo of two berries&quot;0.05045759305357933&quot;this is a photo of six berries&quot;0.04057715833187103```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;## [Documentation](https://clip-as-service.jina.ai)## InstallCLIP-as-service consists of two Python packages `clip-server` and `clip-client` that can be installed _independently_. Both require Python 3.7+. ### Install server&lt;table&gt;&lt;tr&gt;&lt;td&gt; Pytorch Runtime ‚ö° &lt;/td&gt;&lt;td&gt; ONNX Runtime ‚ö°‚ö°&lt;/td&gt;&lt;td&gt; TensorRT Runtime ‚ö°‚ö°‚ö° &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;```bashpip install clip-server```&lt;/td&gt;&lt;td&gt;```bashpip install &quot;clip-server[onnx]&quot;```&lt;/td&gt;&lt;td&gt;```bashpip install nvidia-pyindex pip install &quot;clip-server[tensorrt]&quot;```&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;You can also [host the server on Google Colab](https://clip-as-service.jina.ai/hosting/colab/), leveraging its free GPU/TPU.### Install client```bashpip install clip-client```### Quick checkYou can run a simple connectivity check after install.&lt;table&gt;&lt;tr&gt;&lt;th&gt; C/S &lt;/th&gt; &lt;th&gt; Command &lt;/th&gt; &lt;th&gt; Expect output &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Server&lt;/td&gt;&lt;td&gt; ```bashpython -m clip_server```     &lt;/td&gt;&lt;td&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/server-output.svg?raw=true&quot; alt=&quot;Expected server output&quot; width=&quot;300px&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Client&lt;/td&gt;&lt;td&gt; ```pythonfrom clip_client import Clientc = Client('grpc://0.0.0.0:23456')c.profile()```     &lt;/td&gt;&lt;td&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/pyclient-output.svg?raw=true&quot; alt=&quot;Expected clip-client output&quot; width=&quot;300px&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;You can change `0.0.0.0` to the intranet or public IP address to test the connectivity over private and public network. ## Get Started### Basic usage1. Start the server: `python -m clip_server`. Remember its address and port.2. Create a client:   ```python    from clip_client import Client       c = Client('grpc://0.0.0.0:51000')    ```3. To get sentence embedding:    ```python        r = c.encode(['First do it', 'then do it right', 'then do it better'])        print(r.shape)  # [3, 512]     ```4. To get image embedding:    ```python        r = c.encode(['apple.png',  # local image                   'https://clip-as-service.jina.ai/_static/favicon.png',  # remote image                  'data:image/gif;base64,R0lGODlhEAAQAMQAAORHHOVSKudfOulrSOp3WOyDZu6QdvCchPGolfO0o/XBs/fNwfjZ0frl3/zy7////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH5BAkAABAALAAAAAAQABAAAAVVICSOZGlCQAosJ6mu7fiyZeKqNKToQGDsM8hBADgUXoGAiqhSvp5QAnQKGIgUhwFUYLCVDFCrKUE1lBavAViFIDlTImbKC5Gm2hB0SlBCBMQiB0UjIQA7'])  # in image URI        print(r.shape)  # [3, 512]    ```More comprehensive server and client user guides can be found in the [docs](https://clip-as-service.jina.ai/).### Text-to-image cross-modal search in 10 linesLet's build a text-to-image search using CLIP-as-service. Namely, a user can input a sentence and the program returns matching images. We'll use the [Totally Looks Like](https://sites.google.com/view/totally-looks-like-dataset) dataset and [DocArray](https://github.com/jina-ai/docarray) package. Note that DocArray is included within `clip-client` as an upstream dependency, so you don't need to install it separately.#### Load imagesFirst we load images. You can simply pull them from Jina Cloud:```pythonfrom docarray import DocumentArrayda = DocumentArray.pull('ttl-original', show_progress=True, local_cache=True)```&lt;details&gt;&lt;summary&gt;or download TTL dataset, unzip, load manually&lt;/summary&gt;Alternatively, you can go to [Totally Looks Like](https://sites.google.com/view/totally-looks-like-dataset) official website, unzip and load images:```pythonfrom docarray import DocumentArrayda = DocumentArray.from_files(['left/*.jpg', 'right/*.jpg'])```&lt;/details&gt;The dataset contains 12,032 images, so it may take a while to pull. Once done, you can visualize it and get the first taste of those images:```pythonda.plot_image_sprites()```&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/ttl-image-sprites.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; width=&quot;50%&quot;&gt;&lt;/p&gt;#### Encode imagesStart the server with `python -m clip_server`. Let's say it's at `0.0.0.0:51000` with `GRPC` protocol (you will get this information after running the server).Create a Python client script:```pythonfrom clip_client import Clientc = Client(server='grpc://0.0.0.0:51000')da = c.encode(da, show_progress=True)```Depending on your GPU and client-server network, it may take a while to embed 12K images. In my case, it took about two minutes.&lt;details&gt;&lt;summary&gt;Download the pre-encoded dataset&lt;/summary&gt;If you're impatient or don't have a GPU, waiting can be Hell. In this case, you can simply pull our pre-encoded image dataset:```pythonfrom docarray import DocumentArrayda = DocumentArray.pull('ttl-embedding', show_progress=True, local_cache=True)```&lt;/details&gt;#### Search via sentence Let's build a simple prompt to allow a user to type sentence:```pythonwhile True:    vec = c.encode([input('sentence&gt; ')])    r = da.find(query=vec, limit=9)    r[0].plot_image_sprites()```#### ShowcaseNow you can input arbitrary English sentences and view the top-9 matching images. Search is fast and instinctive. Let's have some fun:&lt;table&gt;&lt;tr&gt;&lt;th&gt; &quot;a happy potato&quot; &lt;/th&gt; &lt;th&gt; &quot;a super evil AI&quot; &lt;/th&gt; &lt;th&gt; &quot;a guy enjoying his burger&quot; &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/a-happy-potato.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/a-super-evil-AI.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/a-guy-enjoying-his-burger.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;table&gt;&lt;tr&gt;&lt;th&gt; &quot;professor cat is very serious&quot; &lt;/th&gt; &lt;th&gt; &quot;an ego engineer lives with parent&quot; &lt;/th&gt; &lt;th&gt; &quot;there will be no tomorrow so lets eat unhealthy&quot; &lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/professor-cat-is-very-serious.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/an-ego-engineer-lives-with-parent.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/there-will-be-no-tomorrow-so-lets-eat-unhealthy.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; width=&quot;100%&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;Let's save the embedding result for our next example: ```pythonda.save_binary('ttl-image')```### Image-to-text cross-modal search in 10 LinesWe can also switch the input and output of the last program to achieve image-to-text search. Precisely, given a query image find the sentence that best describes the image.Let's use all sentences from the book &quot;Pride and Prejudice&quot;. ```pythonfrom docarray import Document, DocumentArrayd = Document(uri='https://www.gutenberg.org/files/1342/1342-0.txt').load_uri_to_text()da = DocumentArray(    Document(text=s.strip()) for s in d.text.replace('\r\n', '').split('.') if s.strip())```Let's look at what we got:```pythonda.summary()``````text            Documents Summary                                                       Length                 6403              Homogenous Documents   True              Common Attributes      ('id', 'text')                                                                Attributes Summary                                                                                   Attribute   Data type   #Unique values   Has empty value   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   id          ('str',)    6403             False              text        ('str',)    6030             False            ```#### Encode sentencesNow encode these 6,403 sentences, it may take 10 seconds or less depending on your GPU and network: ```pythonfrom clip_client import Clientc = Client('grpc://0.0.0.0:51000')r = c.encode(da, show_progress=True)```&lt;details&gt;&lt;summary&gt;Download the pre-encoded dataset&lt;/summary&gt;Again, for people who are impatient or don't have a GPU, we have prepared a pre-encoded text dataset:```pythonfrom docarray import DocumentArrayda = DocumentArray.pull('ttl-textual', show_progress=True, local_cache=True)```&lt;/details&gt;#### Search via imageLet's load our previously stored image embedding, randomly sample 10 image Documents, then find top-1 nearest neighbour of each.```pythonfrom docarray import DocumentArrayimg_da = DocumentArray.load_binary('ttl-image')for d in img_da.sample(10):    print(da.find(d.embedding, limit=1)[0].text)```#### ShowcaseFun time! Note, unlike the previous example, here the input is an image and the sentence is the output. All sentences come from the book &quot;Pride and Prejudice&quot;. &lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/Besides,-there-was-truth-in-his-looks.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/Gardiner-smiled.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/what‚Äôs-his-name.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/By-tea-time,-however,-the-dose-had-been-enough,-and-Mr.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/You-do-not-look-well.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Besides, there was truth in his looks&lt;/td&gt;&lt;td&gt;Gardiner smiled&lt;/td&gt;&lt;td&gt;what‚Äôs his name&lt;/td&gt;&lt;td&gt;By tea time, however, the dose had been enough, and Mr&lt;/td&gt;&lt;td&gt;You do not look well&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/‚ÄúA-gamester!‚Äù-she-cried.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/If-you-mention-my-name-at-the-Bell,-you-will-be-attended-to.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/Never-mind-Miss-Lizzy‚Äôs-hair.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/Elizabeth-will-soon-be-the-wife-of-Mr.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/I-saw-them-the-night-before-last.png?raw=true&quot; alt=&quot;Visualization of the image sprite of Totally looks like dataset&quot; height=&quot;100px&quot;&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;‚ÄúA gamester!‚Äù she cried&lt;/td&gt;&lt;td&gt;If you mention my name at the Bell, you will be attended to&lt;/td&gt;&lt;td&gt;Never mind Miss Lizzy‚Äôs hair&lt;/td&gt;&lt;td&gt;Elizabeth will soon be the wife of Mr&lt;/td&gt;&lt;td&gt;I saw them the night before last&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;### Rank image-text matches via CLIP modelFrom `0.3.0` CLIP-as-service adds a new `/rank` endpoint that re-ranks cross-modal matches according to their joint likelihood in CLIP model. For example, given an image Document with some predefined sentence matches as below:```pythonfrom clip_client import Clientfrom docarray import Documentc = Client(server='grpc://0.0.0.0:51000')r = c.rank(    [        Document(            uri='.github/README-img/rerank.png',            matches=[                Document(text=f'a photo of a {p}')                for p in (                    'control room',                    'lecture room',                    'conference room',                    'podium indoor',                    'television studio',                )            ],        )    ])print(r['@m', ['text', 'scores__clip_score__value']])``````text[['a photo of a television studio', 'a photo of a conference room', 'a photo of a lecture room', 'a photo of a control room', 'a photo of a podium indoor'], [0.9920725226402283, 0.006038925610482693, 0.0009973491542041302, 0.00078492151806131, 0.00010626466246321797]]```One can see now `a photo of a television studio` is ranked to the top with `clip_score` score at `0.992`. In practice, one can use this endpoint to re-rank the matching result from another search system, for improving the cross-modal search quality.&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/rerank.png?raw=true&quot; alt=&quot;Rerank endpoint image input&quot; height=&quot;150px&quot;&gt;&lt;/td&gt;&lt;td&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/rerank-chart.svg?raw=true&quot; alt=&quot;Rerank endpoint output&quot;&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;### Rank text-image matches via CLIP modelIn the [DALL¬∑E Flow](https://github.com/jina-ai/dalle-flow) project, CLIP is called for ranking the generated results from DALL¬∑E. [It has an Executor wrapped on top of `clip-client`](https://github.com/jina-ai/dalle-flow/blob/main/executors/rerank/executor.py), which calls `.arank()` - the async version of `.rank()`:```pythonfrom clip_client import Clientfrom jina import Executor, requests, DocumentArrayclass ReRank(Executor):    def __init__(self, clip_server: str, **kwargs):        super().__init__(**kwargs)        self._client = Client(server=clip_server)    @requests(on='/')    async def rerank(self, docs: DocumentArray, **kwargs):        return await self._client.arank(docs)```&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/jina-ai/clip-as-service/blob/main/.github/README-img/client-dalle.png?raw=true&quot; alt=&quot;CLIP-as-service used in DALLE Flow&quot; width=&quot;300px&quot;&gt;&lt;/p&gt;Intrigued? That's only scratching the surface of what CLIP-as-service is capable of. [Read our docs to learn more](https://clip-as-service.jina.ai).&lt;!-- start support-pitch --&gt;## Support- Join our [Slack community](https://slack.jina.ai) and chat with other community members about ideas.- Watch our [Engineering All Hands](https://youtube.com/playlist?list=PL3UBBWOUVhFYRUa_gpYYKBqEAkO4sxmne) to learn Jina's new features and stay up-to-date with the latest AI techniques.- Subscribe to the latest video tutorials on our [YouTube channel](https://youtube.com/c/jina-ai)## Join UsCLIP-as-service is backed by [Jina AI](https://jina.ai) and licensed under [Apache-2.0](./LICENSE). [We are actively hiring](https://jobs.jina.ai) AI engineers, solution engineers to build the next neural search ecosystem in open-source.&lt;!-- end support-pitch --&gt;</longdescription>
</pkgmetadata>