<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>mrjob: the Python MapReduce library===================================.. image:: https://github.com/Yelp/mrjob/raw/master/docs/logos/logo_medium.pngmrjob is a Python 2.7/3.4+ package that helps you write and run HadoopStreaming jobs.`Stable version (v0.7.4) documentation &lt;http://mrjob.readthedocs.org/en/stable/&gt;`_`Development version documentation &lt;http://mrjob.readthedocs.org/en/latest/&gt;`_.. image:: https://travis-ci.org/Yelp/mrjob.png   :target: https://travis-ci.org/Yelp/mrjobmrjob fully supports Amazon's Elastic MapReduce (EMR) service, which allows youto buy time on a Hadoop cluster on an hourly basis. mrjob has basic support for Google Cloud Dataproc (Dataproc)which allows you to buy time on a Hadoop cluster on a minute-by-minute basis.  It also works with your ownHadoop cluster.Some important features:* Run jobs on EMR, Google Cloud Dataproc, your own Hadoop cluster, or locally (for testing).* Write multi-step jobs (one map-reduce step feeds into the next)* Easily launch Spark jobs on EMR or your own Hadoop cluster* Duplicate your production environment inside Hadoop  * Upload your source tree and put it in your job's ``$PYTHONPATH``  * Run make and other setup scripts  * Set environment variables (e.g. ``$TZ``)  * Easily install python packages from tarballs (EMR only)  * Setup handled transparently by ``mrjob.conf`` config file* Automatically interpret error logs* SSH tunnel to hadoop job tracker (EMR only)* Minimal setup  * To run on EMR, set ``$AWS_ACCESS_KEY_ID`` and ``$AWS_SECRET_ACCESS_KEY``  * To run on Dataproc, set ``$GOOGLE_APPLICATION_CREDENTIALS``  * No setup needed to use mrjob on your own Hadoop clusterInstallation------------``pip install mrjob``As of v0.7.0, Amazon Web Services and Google Cloud Services are optionaldepedencies. To use these, install with the ``aws`` and ``google`` targets,respectively. For example:``pip install mrjob[aws]``A Simple Map Reduce Job-----------------------Code for this example and more live in ``mrjob/examples``... code-block:: python   &quot;&quot;&quot;The classic MapReduce job: count the frequency of words.   &quot;&quot;&quot;   from mrjob.job import MRJob   import re   WORD_RE = re.compile(r&quot;[\w']+&quot;)   class MRWordFreqCount(MRJob):       def mapper(self, _, line):           for word in WORD_RE.findall(line):               yield (word.lower(), 1)       def combiner(self, word, counts):           yield (word, sum(counts))       def reducer(self, word, counts):           yield (word, sum(counts))   if __name__ == '__main__':        MRWordFreqCount.run()Try It Out!-----------::    # locally    python mrjob/examples/mr_word_freq_count.py README.rst &gt; counts    # on EMR    python mrjob/examples/mr_word_freq_count.py README.rst -r emr &gt; counts    # on Dataproc    python mrjob/examples/mr_word_freq_count.py README.rst -r dataproc &gt; counts    # on your Hadoop cluster    python mrjob/examples/mr_word_freq_count.py README.rst -r hadoop &gt; countsSetting up EMR on Amazon------------------------* create an `Amazon Web Services account &lt;http://aws.amazon.com/&gt;`_* Get your access and secret keys (click &quot;Security Credentials&quot; on  `your account page &lt;http://aws.amazon.com/account/&gt;`_)* Set the environment variables ``$AWS_ACCESS_KEY_ID`` and  ``$AWS_SECRET_ACCESS_KEY`` accordinglySetting up Dataproc on Google-----------------------------* `Create a Google Cloud Platform account &lt;http://cloud.google.com/&gt;`_, see top-right* `Learn about Google Cloud Platform &quot;projects&quot; &lt;https://cloud.google.com/docs/overview/#projects&gt;`_* `Select or create a Cloud Platform Console project &lt;https://console.cloud.google.com/project&gt;`_* `Enable billing for your project &lt;https://console.cloud.google.com/billing&gt;`_* Go to the `API Manager &lt;https://console.cloud.google.com/apis&gt;`_ and search for / enable the following APIs...  * Google Cloud Storage  * Google Cloud Storage JSON API  * Google Cloud Dataproc API* Under Credentials, **Create Credentials** and select **Service account key**.  Then, select **New service account**, enter a Name and select **Key type** JSON.* Install the `Google Cloud SDK &lt;https://cloud.google.com/sdk/&gt;`_Advanced Configuration----------------------To run in other AWS regions, upload your source tree, run ``make``, and useother advanced mrjob features, you'll need to set up ``mrjob.conf``. mrjob looksfor its conf file in:* The contents of ``$MRJOB_CONF``* ``~/.mrjob.conf``* ``/etc/mrjob.conf``See `the mrjob.conf documentation&lt;https://mrjob.readthedocs.io/en/latest/guides/configs-basics.html&gt;`_ for moreinformation.Project Links-------------* `Source code &lt;http://github.com/Yelp/mrjob&gt;`__* `Documentation &lt;https://mrjob.readthedocs.io/en/latest/&gt;`_* `Discussion group &lt;http://groups.google.com/group/mrjob&gt;`_Reference---------* `Hadoop Streaming &lt;http://hadoop.apache.org/docs/stable1/streaming.html&gt;`_* `Elastic MapReduce &lt;http://aws.amazon.com/documentation/elasticmapreduce/&gt;`_* `Google Cloud Dataproc &lt;https://cloud.google.com/dataproc/overview&gt;`_More Information----------------* `PyCon 2011 mrjob overview &lt;http://blip.tv/pycon-us-videos-2009-2010-2011/pycon-2011-mrjob-distributed-computing-for-everyone-4898987/&gt;`_* `Introduction to Recommendations and MapReduce with mrjob &lt;http://aimotion.blogspot.com/2012/08/introduction-to-recommendations-with.html&gt;`_  (`source code &lt;https://github.com/marcelcaraciolo/recsys-mapreduce-mrjob&gt;`__)* `Social Graph Analysis Using Elastic MapReduce and PyPy &lt;http://postneo.com/2011/05/04/social-graph-analysis-using-elastic-mapreduce-and-pypy&gt;`_Thanks to `Greg Killion &lt;mailto:greg@blind-works.net&gt;`_(`ROMEO ECHO_DELTA &lt;http://www.romeoechodelta.net/&gt;`_) for the logo.</longdescription>
</pkgmetadata>