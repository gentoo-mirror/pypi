<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Imaginary Dev OpenAI wrapper[![image](https://img.shields.io/pypi/v/im_openai.svg)](https://pypi.python.org/pypi/im_openai)[![image](https://img.shields.io/travis/alecf/im_openai.svg)](https://travis-ci.com/alecf/im_openai)[![Documentation Status](https://readthedocs.org/projects/im-openai/badge/?version=latest)](https://im-openai.readthedocs.io/en/latest/?version=latest)Wrapper library for openai to send events to the Imaginary Programmingmonitor-   Free software: MIT license-   Documentation: &lt;https://im-openai.readthedocs.io&gt;.## Features-   Patches the openai library to allow user to set an ip_api_key and ip_api_name    for each request-   Works out of the box with langchain## Get StartedTo send events to Imaginary Programming, you'll need to create a project. From the project you'll need two things:1. **API key**: This is generated for the project and is used to identify the project and environment (dev, staging, prod) that the event is coming from.2. **API Name**: This uniquely identifies a particular prompt that you are using. This allows projects to have multiple prompts. You do not need to generate this in advance: if the API name does not exist, then it will be created automatically. This can be in any format but we recommend using a dash-separated format, e.g. `my-prompt-name`.### OpenAIYou can use the `patched_openai` context manager to patch your code.To allow our tools to separate the &quot;prompt&quot; from the &quot;prompt parameters&quot;, use `TemplateChat` and `TemplateText` to create templates.Use `TemplateChat` For the ChatCompletion APIs:```pythonfrom im_openai import patched_openai, TemplateChatwith patched_openai(api_key=&quot;...&quot;, api_name=&quot;sport-emoji&quot;):    import openai    completion = openai.ChatCompletion.create(        # Standard OpenAI parameters        model=&quot;gpt-3.5-turbo&quot;,        messages=TemplateChat(            [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Show me an emoji that matches the sport: {sport}&quot;}],            {&quot;sport&quot;: &quot;soccer&quot;},        ),    )```Use `TemplateText` for the Completion API:```pythonfrom im_openai import patched_openai, TemplateTextwith patched_openai(api_key=&quot;...&quot;, api_name=&quot;sport-emoji&quot;):    import openai    completion = openai.Completion.create(        # Standard OpenAI parameters        model=&quot;text-davinci-003&quot;,        prompt=TemplateText(&quot;Show me an emoji that matches the sport: {sport}&quot;, {&quot;sport&quot;: &quot;soccer&quot;}),    )```#### Advanced usage##### Patching at startupRather than using a context manager, you can patch the library once at startup:```pythonfrom im_openai import patch_openaipatch_openai(api_key=&quot;...&quot;)```Then, you can use the patched library as normal:```pythonimport openaicompletion = openai.ChatCompletion.create(    # Standard OpenAI parameters    ...)```##### Manually passing parametersWhile the use of `TemplateText` and `TemplateChat` are preferred, Most of the parameters passed during patch can also be passed directly to the `create()`, with an `ip_` prefix.```pythoncompletion = openai.ChatCompletion.create(    model=&quot;gpt-3.5-turbo&quot;,    # Note we are passing the raw chat object here    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Show me an emoji that matches the sport: soccer&quot;}],    # call configuration    ip_api_key=&quot;...&quot;,    ip_api_name=&quot;sport-emoji&quot;,    # Here the prompt and parameters is passed seperately    ip_template_params={&quot;sport&quot;: &quot;soccer&quot;},    ip_template_chat=[        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Show me an emoji that matches the sport: {sport}&quot;}    ],)```### LangchainFor langchain, you can directly patch, or use a context manager before setting up a chain:Using a context manager: (recommended)```pythonfrom im_openai.langchain import prompt_watch_tracingwith prompt_watch_tracing(&quot;4b2a6608-86cd-4819-aba6-479f9edd8bfb&quot;, &quot;sport-emoji&quot;):    chain = LLMChain(llm=...)    chain.run(&quot;Hello world&quot;, inputs={&quot;name&quot;: &quot;world&quot;})```The `api_key` parameter is visible from your project's settings page.the api_name parameter can also be passed directly to a template when you create it, so that it can be tracked separately from other templates:```pythonfrom langchain import OpenAI, PromptTemplate, LLMChainwith prompt_watch_tracing(&quot;4b2a6608-86cd-4819-aba6-479f9edd8bfb&quot;, &quot;default-questions&quot;):    template = PromptTemplate(&quot;&quot;&quot;Please answer the following question: {question}.&quot;&quot;&quot;,        input_variables=[&quot;question&quot;])    llm = LLMChain(prompt=prompt, llm=OpenAI())    llm.run(question=&quot;What is the meaning of life?&quot;)    # Track user greetings separately under the `user-greeting` api name    greeting_prompt = PromptTemplate(&quot;&quot;&quot;Please greet our newest forum member, {user}. Be nice and enthusiastic but not overwhelming.&quot;&quot;&quot;,        input_variables=[&quot;user&quot;],        additional_kwargs={&quot;ip_api_name&quot;: &quot;user-greeting&quot;})    llm = LLMChain(prompt=prompt, llm=OpenAI(openai_api_key=...))    llm.run(user=&quot;Bob&quot;)```#### Advanced usageYou can patch directly:```pythonfrom im_openai.langchain import prompt_watch_tracingold_tracer = enable_prompt_watch_tracing(&quot;emojification&quot;, &quot;sport-emoji&quot;)template_chat=ChatPromptTemplate.from_messages([{    &quot;role&quot;: &quot;user&quot;,    &quot;content&quot;: &quot;Show me an emoji that matches the sport: {sport}&quot;}])chain = LLMChain(llm=ChatOpenAI(), prompt=template_chat)chain.run(sport=&quot;Soccer&quot;)# optional, if you need to disable tracing laterdisable_prompt_watch_tracing(old_tracer)```### Additional ParametersEach of the above APIs accept the same additional parameters. The OpenAI API requires a `ip_` prefix for each parameter.-   `template_chat` / `ip_template_chat`: The chat template to use for the    request. This is a list of dictionaries with the following keys:    -   `role`: The role of the speaker. Either `&quot;system&quot;`, `&quot;user&quot;` or `&quot;ai&quot;`.    -   `content`: The content of the message. This can be a string or a template string with `{}` placeholders.    For example:    ```python    [      {&quot;role&quot;: &quot;ai&quot;, &quot;content&quot;: &quot;Hello, I'm {system_name}!&quot;},      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi {system_name}, I'm {user_name}!&quot;}    ]    ```    To represent an array of chat messages, use the artificial role `&quot;chat_history&quot;` with `content` set to the variable name in substitution format: `[{&quot;role&quot;: &quot;chat_history&quot;, &quot;content&quot;: &quot;{prev_messages}&quot;}}]`-   `template_text` / `ip_template_text`: The text template to use for    completion-style requests. This is a string or a template string with `{}`    placeholders, e.g. `&quot;Hello, {user_name}!&quot;`.-   `chat_id` / `ip_chat_id`: The id of a &quot;chat session&quot; - if the chat API is    being used in a conversational context, then the same chat id can be    provided so that the events are grouped together, in order. If not provided,    this will be left blank.These parameters are only available in the patched OpenAI client:-   `ip_template_params`: The parameters to use for template    strings. This is a dictionary of key-value pairs. **Note**: This value is inferred in the Langchain wrapper.-   `ip_event_id`: A unique UUID for a specific call. If not provided,    one will be generated. **Note**: In the langchain wrapper, this value is inferred from the `run_id`.-   `ip_parent_event_id`: The UUID of the parent event. If not provided,    one will be generated. **Note**: In the langchain wrapper, this value is inferred from the `parent_run_id`.## CreditsThis package was created with Cookiecutter* and the `audreyr/cookiecutter-pypackage`* project template... _Cookiecutter: https://github.com/audreyr/cookiecutter.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage======= History =======## 0.1.0 (2023-06-20)-   First release on PyPI.## 0.1.1 (2023-06-23)-   add TemplateString helper and support for data / params## 0.1.2 (2023-06-23)-   add support for original template too## 0.2.0 (2023-06-26)-   add explicit support for passing the &quot;prompt template text&quot;## 0.3.0 (2023-06-28)-   add support for chat templates (as objects instead of arrays)## 0.4.0 (2023-06-29)-   switch event reporting to be async / non-blocking## 0.4.1 (2023-06-29)-   add utility for formatting langchain messages## 0.4.2 (2023-06-29)-   remove stray breakpoint## 0.4.3 (2023-06-30)-   pass along chat_id-   attempt to auto-convert langchain prompt templates## 0.4.4 (2023-06-30)-   remove stray prints## 0.5.0 (2023-07-06)-   Add langchain callbacks handlers## 0.6.0 (2023-07-10)-   Handle duplicate callbacks, agents, etc## 0.6.1 (2023-07-12)-   Fix prompt retrieval in deep chains## 0.6.2 (2023-07-13)-   Handle cases where input values are not strings## 0.6.3 (2023-07-18)-   Better support for server-generated event ids    (pre-llm sends event, post-llm re-uses the same id)-   more tests for different kinds of templates## 0.6.4-   include temporary patched version of loads()## 0.7.0-   breaking change: move im_openai.langchain_util to im_openai.langchain-   add support for injecting callbacks into all langchain calls using tracing hooks## 0.7.1-   Pass along model params to the server## 0.7.3-   add explicit support for api_key## 0.8.0-   switch to api_key, pretend project_key isn't even a thing## 0.8.1-   Used root parent_run_id in langchain calls-   Unified langchain run id accounting## 0.8.2-   added ability to pass `ip_api_name` into langchain template's `additional_kwargs`, like:    ```python    template = TemplateString(        &quot;Hello, {{name}}!&quot;,        additional_kwargs={&quot;ip_api_name&quot;: &quot;my-api&quot;},    )    ```## 0.8.3-   Added context manager for basic openai calls-   Better docs## 0.8.4-   Switched to load() now that it is in langchain proper-   Resolved `None` to `{varname}` in templates rather than leaving it out## 0.9.0-   Complete rewrite of prompt resolution for chats: better support for agents## 0.9.1-   Thread through chat_id## 0.9.2-   Fixed typos in docs, clarified using `ip_` parameters-   Flushed out working `TemplateText` / `TemplateChat` templates</longdescription>
</pkgmetadata>