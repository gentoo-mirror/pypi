<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>*&lt;p align=&quot;center&quot;&gt;  &lt;img src=&quot;https://git.science.uu.nl/m.j.robeer/text_explainability/-/raw/main/img/TextLogo-Logo%20large.png&quot; alt=&quot;T_xt Explainability logo&quot; width=&quot;70%&quot;&gt;*&lt;/p&gt;**&lt;h3 align=&quot;center&quot;&gt;A generic explainability architecture for explaining text machine learning models**&lt;/h3&gt;[![PyPI](https://img.shields.io/pypi/v/text_explainability)](https://pypi.org/project/text-explainability/)[![Downloads](https://pepy.tech/badge/text-explainability)](https://pepy.tech/project/text-explainability)[![Python_version](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10-blue)](https://pypi.org/project/text-explainability/)[![Build_passing](https://img.shields.io/badge/build-passing-brightgreen)](https://git.science.uu.nl/m.j.robeer/text_explainability/-/pipelines)[![License](https://img.shields.io/pypi/l/text_explainability)](https://www.gnu.org/licenses/lgpl-3.0.en.html)[![Documentation Status](https://readthedocs.org/projects/text-explainability/badge/?version=latest)](https://text-explainability.readthedocs.io/en/latest/?badge=latest)[![Code style: black](https://img.shields.io/badge/code%20style-flake8-aa0000)](https://github.com/PyCQA/flake8)---`text_explainability` provides a **generic architecture** from which well-known state-of-the-art explainability approaches for text can be composed. This modular architecture allows components to be swapped out and combined, to **quickly develop new types of explainability approaches** for (natural language) text, or to **improve a plethora of approaches by improving a single module**.Several example methods are included, which provide **local explanations** (_explaining the prediction of a single instance_, e.g. `LIME` and `SHAP`) or **global explanations** (_explaining the dataset, or model behavior on the dataset_, e.g. `TokenFrequency` and `MMDCritic`). By replacing the default modules (e.g. local data generation, global data sampling or improved embedding methods), these methods can be improved upon or new methods can be introduced.&amp;copy; Marcel Robeer, 2021## Quick tour**Local explanation**: explain a models' prediction on a given sample, self-provided or from a dataset.```pythonfrom text_explainability import LIME, LocalTree# Define sample to explainsample = 'Explain why this is positive and not negative!'# LIME explanation (local feature importance)LIME().explain(sample, model).scores# List of local rules, extracted from treeLocalTree().explain(sample, model).rules``` **Global explanation**: explain the whole dataset (e.g. train set, test set), and what they look like for the ground-truth or predicted labels.```pythonfrom text_explainability import import_data, TokenFrequency, MMDCritic# Import datasetenv = import_data('./datasets/test.csv', data_cols=['fulltext'], label_cols=['label'])# Top-k most frequent tokens per labelTokenFrequency(env.dataset).explain(labelprovider=env.labels, explain_model=False, k=3)# 2 prototypes and 1 criticisms for the datasetMMDCritic(env.dataset)(n_prototypes=2, n_criticisms=1)```## InstallationSee the [installation](docs/INSTALLATION.md) instructions for an extended installation guide.| Method | Instructions ||--------|--------------|| `pip` | Install from [PyPI](https://pypi.org/project/text-explainability/) via `pip3 install text_explainability`. To speed up the explanation generation process use `pip3 install text_explainability[fast]`. || Local | Clone this repository and install via `pip3 install -e .` or locally run `python3 setup.py install`.## DocumentationFull documentation of the latest version is provided at [https://text-explainability.readthedocs.io/](https://text-explainability.readthedocs.io/).## Example usageSee [example usage](example_usage.md) to see an example of how the package can be used, or run the lines in `example_usage.py` to do explore it interactively.## Explanation methods included`text_explainability` includes methods for model-agnostic _local explanation_ and _global explanation_. Each of these methods can be fully customized to fit the explainees' needs.| Type | Explanation method | Description | Paper/link ||------|--------------------|-------------|-------|| *Local explanation* | `LIME` | Calculate feature attribution with _Local Intepretable Model-Agnostic Explanations_ (LIME). | [[Ribeiro2016](https://paperswithcode.com/method/lime)], [interpretable-ml/lime](https://christophm.github.io/interpretable-ml-book/lime.html) || |  `KernelSHAP` | Calculate feature attribution with _Shapley Additive Explanations_ (SHAP). | [[Lundberg2017](https://paperswithcode.com/paper/a-unified-approach-to-interpreting-model)], [interpretable-ml/shap](https://christophm.github.io/interpretable-ml-book/shap.html) || |  `LocalTree` | Fit a local decision tree around a single decision. | [[Guidotti2018](https://paperswithcode.com/paper/local-rule-based-explanations-of-black-box)] || | `LocalRules` | Fit a local sparse set of label-specific rules using `SkopeRules`. | [github/skope-rules](https://github.com/scikit-learn-contrib/skope-rules) || |  `FoilTree` | Fit a local contrastive/counterfactual decision tree around a single decision. | [[Robeer2018](https://github.com/MarcelRobeer/ContrastiveExplanation)] || | `BayLIME` | Bayesian extension of LIME for include prior knowledge and more consistent explanations. | [[Zhao201](https://paperswithcode.com/paper/baylime-bayesian-local-interpretable-model)] || *Global explanation* | `TokenFrequency` | Show the top-_k_ number of tokens for each ground-truth or predicted label. || |  `TokenInformation` | Show the top-_k_ token mutual information for a dataset or model. | [wikipedia/mutual_information](https://en.wikipedia.org/wiki/Mutual_information) || | `KMedoids` | Embed instances and find top-_n_ prototypes (can also be performed for each label using `LabelwiseKMedoids`). | [interpretable-ml/prototypes](https://christophm.github.io/interpretable-ml-book/proto.html) || | `MMDCritic` | Embed instances and find top-_n_ prototypes and top-_n_ criticisms (can also be performed for each label using `LabelwiseMMDCritic`). | [[Kim2016](https://papers.nips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html)], [interpretable-ml/prototypes](https://christophm.github.io/interpretable-ml-book/proto.html) |## Releases`text_explainability` is officially released through [PyPI](https://pypi.org/project/text-explainability/).See [CHANGELOG.md](CHANGELOG.md) for a full overview of the changes for each version.## Extensions&lt;a href=&quot;https://marcelrobeer.github.io/text_sensitivity/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://git.science.uu.nl/m.j.robeer/text_sensitivity/-/raw/main/img/TextLogo-Logo_large_sensitivity.png&quot; alt=&quot;T_xt sensitivity logo&quot; width=&quot;200px&quot;&gt;&lt;/a&gt;&lt;p&gt;`text_explainability` can be extended to also perform _sensitivity testing_, checking for machine learning model robustness and fairness. The `text_sensitivity` package is available through [PyPI](https://pypi.org/project/text-sensitivity/) and fully documented at [https://text-sensitivity.rtfd.io/](https://text-sensitivity.rtfd.io/).&lt;/p&gt;## Citation```bibtex@misc{text_explainability,  title = {Python package text\_explainability},  author = {Marcel Robeer},  howpublished = {\url{https://git.science.uu.nl/m.j.robeer/text_explainability}},  year = {2021}}```## Maintenance### Contributors- [Marcel Robeer](https://www.uu.nl/staff/MJRobeer) (`@m.j.robeer`)- [Michiel Bron](https://www.uu.nl/staff/MPBron) (`@mpbron-phd`)### TodoTasks yet to be done:* Implement local post-hoc explanations:    - Implement Anchors* Implement global post-hoc explanations:    - Representative subset* Add support for regression models* More complex data augmentation    - Top-k replacement (e.g. according to LM / WordNet)    - Tokens to exclude from being changed    - Bag-of-words style replacements* Add rule-based return type* Write more tests## Credits- Florian Gardin, Ronan Gautier, Nicolas Goix, Bibi Ndiaye and Jean-Matthieu Schertzer. _[Skope-rules](https://github.com/scikit-learn-contrib/skope-rules)_. 2020.- Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini and Fosca Gianotti. _[Local Rule-Based Explanations of Black Box Decision Systems](https://paperswithcode.com/paper/local-rule-based-explanations-of-black-box)_. 2018.- Been Kim, Rajiv Khanna and Oluwasanmi O. Koyejo. [Examples are not Enough, Learn to Criticize! Criticism for Interpretability](https://papers.nips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html). _Advances in Neural Information Processing Systems (NIPS 2016)_. 2016.- Scott Lundberg and Su-In Lee. [A Unified Approach to Interpreting Model Predictions](https://paperswithcode.com/paper/a-unified-approach-to-interpreting-model). _31st Conference on Neural Information Processing Systems (NIPS 2017)_. 2017.- Christoph Molnar. _[Interpretable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)_. 2021.- Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. [&quot;Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier](https://paperswithcode.com/method/lime). _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)_. 2016.- Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin. [Anchors: High-Precision Model-Agnostic Explanations](https://github.com/marcotcr/anchor). _AAAI Conference on Artificial Intelligence (AAAI)_. 2018.- Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu Brinkhuis and Mark Neerincx. [&quot;Contrastive Explanations with Local Foil Trees&quot;](https://github.com/MarcelRobeer/ContrastiveExplanation). _2018 Workshop on Human Interpretability in Machine Learning (WHI 2018)_. 2018.</longdescription>
</pkgmetadata>