<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>## This Repository is deprecated since version 0.1.14 and will not be further developed. Bugfixes might be released if necessary. ## DescriptionThis repository contains code to make datasets stored on the corpora network drive of the chair.You can use this project to easily create or reuse a data loader that is universally compatible with either plain python code or tensorflow / pytorch. Also you this code can be used to dynamically create a dataloader for a Nova database to directly work with Nova Datasets in Python. Compatible with the [tensorflow dataset api](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).Pytorch Dataset [is also supported](https://pytorch.org/vision/stable/datasets.html). ## Installation InformationFor efficient data loading we rely on the [decord](https://github.com/dmlc/decord) library. Decord ist not available as prebuild binary for non x86 architectures. If you want to install the project on other architecture you will need to compile it yourself. ## Currently available Datasets| Dataset       | Status        | Url  || :------------- |:-------------:| :-----|| ckplus        | ✅             | http://www.iainm.com/publications/Lucey2010-The-Extended/paper.pdf || affectnet     | ✅             | http://mohammadmahoor.com/affectnet/ || faces         | ✅             |    https://faces.mpdl.mpg.de/imeji/ || nova_dynamic  | ✅             |    https://github.com/hcmlab/nova || audioset      | ❌             | https://research.google.com/audioset/ || is2021_ess    | ❌             |    -|| librispeech   | ❌             |    https://www.openslr.org/12 |## Architecture![uml diagram](image/architecture.png)Dataset implementations are split into two parts.\Data access is handled by a generic python iterable, implemented by the DatasetIterable interface.\The access class is then extended by an API class, which implements tfds.core.GeneratorBasedBuilder.This results in the dataset being available by the Tensorflow Datasets API, and enables features such as local caching.The iterables themselves can also be used as-is, either in PyTorch native DataGenerators by wrapping them inthe utility class BridgePyTorch, or as tensorflow-native Datasets by passing them to BridgeTensorflow.The benefits of this setup are that a pytorch application can be served without installing or loading tensorflow, and vice versa, since the stack up to the adapters does not involve tf or pytorch. Also, when using tf, caching can be used or discarded by using tfds or the plain tensorflow Datasetprovided by the bridge.### Dynamic Dataset usage with Nova Example To use the hcai_datasets repository with Nova you can use the `HcaiNovaDynamicIterable`class from the `hcai_datasets.hcai_nova_dynamic.hcai_nova_dynamic_iterable` module to create an iterator for a specific data configuration. This readme assumes that you are already familiar with the terminology and the general concept of the NOVA annotation tool / database.The constructor of the class takes the following arguments as input: `db_config_path`: `string` path to a configfile with the nova database config. the config file looks like this:```[DB]ip = 127.0.0.1port = 37317user = my_userpassword = my_password````db_config_dict`: `string` dictionary with the nova database config. can be used instead of db_config_path. if both are specified db_config_dict is used.`dataset`: `string` the name of the dataset. Same as the entry in the Nova db.`nova_data_dir`: `string` the directory to look for data. same as the directory specified in the nova gui. `sessions`: `list` list of sessions that should be loaded. must match the session names in nova.`annotator`: `string` the name of the annotator that labeld the session. must match annotator names in nova.`schemes`: `list` list of the annotation schemes to fetch.`roles`: `list` list of roles for which the annotation should be loaded.`data_streams`: `list` list datastreams for which the annotation should be loaded. must match stream names in nova.`start`: `string | int | float` start time_ms. use if only a specific chunk of a session should be retrieved. can be passed as String (e.g. '1s' or '1ms'), Int (interpreted as milliseconds) or Float (interpreted as seconds).`end`: `string | int | float` optional end time_ms. use if only a specific chunk of a session should be retrieved. can be passed as String (e.g. '1s' or '1ms'), Int (interpreted as milliseconds) or Float (interpreted as seconds).`left_context`: `string | int | float` additional data to pass to the classifier on the left side of the frame. can be passed as String (e.g. '1s' or '1ms'), Int (interpreted as milliseconds) or Float (interpreted as seconds).`right_context`: `string | int | float` additional data to pass to the classifier on the left side of the frame. can be passed as String (e.g. '1s' or '1ms'), Int (interpreted as milliseconds) or Float (interpreted as seconds).`frame_size`: `string | int | float` the framesize to look at. the matching annotation will be calculated as majority vote from all annotations that are overlapping with the timeframe. can be passed as String (e.g. '1s' or '1ms'), Int (interpreted as milliseconds) or Float (interpreted as seconds).`stride`: `string | int | float`  how much a frame is moved to calculate the next sample. equals framesize by default. can be passed as String (e.g. '1s' or '1ms'), Int (interpreted as milliseconds) or Float (interpreted as seconds).`flatten_samples`: `bool` if set to `True` samples with the same annotation scheme but from different roles will be treated as separate samples. only &lt;scheme&gt; is used for the keys.  `add_rest_class`: `bool` when set to True an additional rest class will be added to the end the label list```pythonfrom pathlib import Pathfrom hcai_dataset_utils.bridge_tf import BridgeTensorflowimport tensorflow as tffrom hcai_datasets.hcai_nova_dynamic.hcai_nova_dynamic_iterable import HcaiNovaDynamicIterableds_iter = HcaiNovaDynamicIterable(    db_config_path=&quot;./nova_db.cfg&quot;,    db_config_dict=None,    dataset=&quot;affect-net&quot;,    nova_data_dir=Path(&quot;./nova/data&quot;),    sessions=[f&quot;{i}_man_eval&quot; for i in range(8)],    roles=[&quot;session&quot;],    schemes=[&quot;emotion_categorical&quot;],    annotator=&quot;gold&quot;,    data_streams=[&quot;video&quot;],    frame_size=0.04,    left_context=0,    right_context=0,    start = &quot;0s&quot;,    end = &quot;3000ms&quot;,    flatten_samples=False,)for sample in ds_iter:    print(sample)```## Pytorch ExampleThe BridePyTorch module can be used to create a Pytorch DataLoader directly from the Dataset iterable. ```pythonfrom torch.utils.data import DataLoaderfrom hcai_dataset_utils.bridge_pytorch import BridgePyTorchfrom hcai_datasets.hcai_affectnet.hcai_affectnet_iterable import HcaiAffectnetIterableds_iter = HcaiAffectnetIterable(    dataset_dir=&quot;path/to/data_sets/AffectNet&quot;,    split=&quot;test&quot;)dataloader = DataLoader(BridgePyTorch(ds_iter))for sample in dataloader:    print(sample)```## Tensorflow ExampleThe BridgeTensorflow module can be used to create a Pytorch DataLoader directly from the Dataset iterable. ```pythonfrom hcai_dataset_utils.bridge_tf import BridgeTensorflowfrom hcai_datasets.hcai_affectnet.hcai_affectnet_iterable import HcaiAffectnetIterableds_iter = HcaiAffectnetIterable(    dataset_dir=&quot;path/to/data_sets/AffectNet&quot;,    split=&quot;test&quot;)tf_dataset = BridgeTensorflow.make(ds_iter)for sample in tf_dataset:    print(sample)```## Tensorflow Dataset API (DEPRECATED)### Example Usage```pythonimport osimport tensorflow as tfimport tensorflow_datasets as tfdsimport hcai_datasetsfrom matplotlib import pyplot as plt# Preprocessing functiondef preprocess(x, y):  img = x.numpy()  return img, y# Creating a datasetds, ds_info = tfds.load(  'hcai_example_dataset',  split='train',  with_info=True,  as_supervised=True,  builder_kwargs={'dataset_dir': os.path.join('path', 'to', 'directory')})# Input output mappingds = ds.map(lambda x, y: (tf.py_function(func=preprocess, inp=[x, y], Tout=[tf.float32, tf.int64])))# Manually iterate over datasetimg, label = next(ds.as_numpy_iterator())# Visualizeplt.imshow(img / 255.)plt.show()```</longdescription>
</pkgmetadata>