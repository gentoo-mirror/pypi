<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>Glide: Easy ETL===============[![Generic badge](https://img.shields.io/badge/Status-Alpha-yellow.svg)](https://shields.io/)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![Documentation Status](https://readthedocs.org/projects/glide-etl/badge/?version=latest)](https://glide-etl.readthedocs.io/en/latest/?badge=latest)[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)![Python 3.6+](https://img.shields.io/badge/python-3.6+-blue.svg)[![Downloads](https://pepy.tech/badge/glide)](https://pepy.tech/project/glide)Update 2022-08-12------------Given my lack of bandwidth for this project and the presence of a number of great alternatives popping up in recent years, I currently recommend against using Glide in long term / production scenarios. Check out [pypeln](https://github.com/cgarciae/pypeln/) for an alternative that is morefocused on flow &amp; parallelization features that are useful in Glide.Introduction------------Glide is an easy-to-use data pipelining tool inspired by [Consecution](https://github.com/robdmc/consecution) and[Apache Storm Topologies](http://storm.apache.org/releases/current/Tutorial.html).Like those libraries, **Glide is:**- A simple, reusable approach to building robust ETL pipelines- A system for wiring together processing nodes to form a directed acyclic graph (DAG)**Glide also has:**- An expanding suite of built-in nodes and pipelines that extract, transform, and load data from/to any combination of:  - SQL databases (SQLite, DBAPI, and SQLAlchemy support)  - Local or remote files (CSVs, Excel, and raw file support)  - URLs (JSON endpoints, file downloads, APIs, etc.)  - HTML Tables  - Emails- Extensions for [Pandas](https://pandas.pydata.org/), [Dask](https://dask.org/), [Celery](http://www.celeryproject.org/), [Redis Queue](http://python-rq.org/) and more- A variety of node and DAG parallel/concurrent/distributed processing strategies- A simple decorator to generate a command line interface from a pipeline in ~one line of code- Flexible pipeline templating**Glide is not** a task orchestration and/or dependency management tool likeAirflow. Use Glide to define your easily developed/contained/reusable/testabledata processing pipelines and then rely on a tool like Airflow to do what it'sgood at, namely scheduling and complex task dependency management.Table of Contents-----------------* [Installation](#installation)* [Primer](#primer)* [Basic Examples](#basic-examples)  * [CSV Extract, Transform, and Load](#example-csv-extract-transform-and-load)  * [SQL Extract and Load](#example-sql-extract-and-load)  * [SQL Transactions](#example-sql-transactions)  * [URL Extraction](#example-url-extraction)* [Flow Control Examples](#flow-examples)  * [Filters](#example-filters)  * [IterPush](#example-iterpush)  * [SplitPush](#example-splitpush)  * [SplitByNode](#example-splitbynode)  * [Reduce](#example-reduce)  * [Join](#example-join)  * [Routers](#example-routers)  * [Window Processing](#example-window-processing)  * [Date Windows](#example-date-windows)  * [Return Values](#example-return-values)* [Parallelization &amp; Concurrency](#parallel-examples)  * [Parallel Transformation](#example-parallel-transformation)  * [Parallel Pipelines via ParaGlider](#example-parallel-pipelines-via-paraglider)  * [Parallel Branching](#example-parallel-branching)  * [Thread Reducers](#example-thread-reducers)  * [Asyncio](#example-asyncio)* [Utility Examples](#utility-examples)  * [Templated Nodes and Pipelines](#example-templated-nodes-and-pipelines)  * [Data Integrity Checks](#example-data-integrity-checks)  * [Debugging](#example-debugging)  * [Profiling Pipelines](#example-profiling-pipelines)  * [Complex Pipelines](#example-complex-pipelines)  * [Plotting Pipeline DAGs](#example-plotting-pipeline-dags)* [CLI Generation](#cli-generation)* [Extensions](#extensions)  * [Pandas](#pandas)  * [Dask](#dask-experimental)  * [Celery](#celery-experimental)  * [Redis Queue](#redis-queue-experimental)  * [Swifter](#swifter-experimental)* [Docs](#documentation)* [How to Contribute](#how-to-contribute)&lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;Installation------------&gt; ⚠️ **Warning**: This project is in an alpha state and is maintained on an as-needed basis. Please test carefully for production usage and report any issues.```shell$ pip install glide```&lt;a name=&quot;primer&quot;&gt;&lt;/a&gt;Primer------You are encouraged to take a deeper look at the[docs](https://glide-etl.readthedocs.io/en/latest/), but the short of it isthe following:1\. A `Node` is a part of a pipeline which has a `run` method that typicallyaccepts data from upstream nodes, and pushes data to downstream nodes. For example:```pythonclass MyNode(Node):    def run(self, data):        # Some node-specific code here        self.push(data)```2\. A `Glider` is a pipeline of `Node` objects wired together in a DAG. Itaccepts input data in its `consume` method. For example:```pythonglider = Glider(    MyExtractNode(&quot;extract&quot;)    | MyTransformNode(&quot;transform&quot;)    | MyLoadNode(&quot;load&quot;))glider.consume(data)```If a node's `run` method has additional parameters, they are populated fromthe node's `context`. More info on creating nodes and populating runtime contextcan be found [here](https://glide-etl.readthedocs.io/en/latest/nodes.html).&lt;a name=&quot;basic-examples&quot;&gt;&lt;/a&gt;Basic Examples--------------The following examples serve to quickly illustrate some core features andbuilt-in nodes. There is much more `Glide` can do that is not shownhere. Everything below assumes you have used the following shortcut to importall necessary node and pipeline classes:```pythonfrom glide import *```&lt;a name=&quot;example-csv-extract-transform-and-load&quot;&gt;&lt;/a&gt;### Example: CSV Extract, Transform, and LoadApply a transformation to data from a CSV, use a function to lowercase allstrings, and load into an output CSV:```pythondef lower_rows(data):    for row in data:        for k, v in row.items():            row[k] = v.lower() if type(v) == str else v    return dataglider = Glider(    CSVExtract(&quot;extract&quot;)    | Func(&quot;transform&quot;, func=lower_rows)    | CSVLoad(&quot;load&quot;))glider.consume(    [&quot;/path/to/infile.csv&quot;],    extract=dict(chunksize=100),    load=dict(outfile=&quot;/path/to/outfile.csv&quot;),)```&lt;a name=&quot;example-sql-extract-and-load&quot;&gt;&lt;/a&gt;### Example: SQL Extract and LoadRead from one table, write to another:```pythonconn = get_my_sqlalchemy_conn()sql = &quot;select * from in_table limit 10&quot;glider = Glider(    SQLExtract(&quot;extract&quot;)    | SQLLoad(&quot;load&quot;),    global_state=dict(conn=conn) # conn is automagically passed to any nodes that accept a &quot;conn&quot; argument)glider.consume(    [sql],    load=dict(table=&quot;out_table&quot;))```&lt;a name=&quot;example-sql-transactions&quot;&gt;&lt;/a&gt;### Example: SQL TransactionsStart a transaction before writing to a database, rollback on failure:```pythonglider = Glider(    SQLExtract(&quot;extract&quot;)    | SQLTransaction(&quot;tx&quot;)    | SQLLoad(&quot;load&quot;, rollback=True),    global_state=dict(conn=conn))glider.consume(...)```&lt;a name=&quot;example-url-extraction&quot;&gt;&lt;/a&gt;### Example: URL ExtractionExtract data from each URL in the list of requests and load to a URL endpoint:```pythonglider = Glider(URLExtract(&quot;extract&quot;) | URLLoad(&quot;load&quot;))reqs = [    &quot;https://jsonplaceholder.typicode.com/todos/1&quot;,    &quot;https://jsonplaceholder.typicode.com/todos/2&quot;,]glider.consume(    reqs,    extract=dict(data_type=&quot;json&quot;),    load=dict(        url=&quot;https://jsonplaceholder.typicode.com/todos&quot;,        data_param=&quot;json&quot;,        headers={&quot;Content-type&quot;: &quot;application/json; charset=UTF-8&quot;},    ),)```&lt;a name=&quot;flow-examples&quot;&gt;&lt;/a&gt;Flow Control Examples---------------------&lt;a name=&quot;example-filters&quot;&gt;&lt;/a&gt;### Example: FiltersFilter the propagation of data based on the result of a function:```pythondef data_check(node, data):    # do some check on data, return True/False to control filtering    return Trueglider = Glider(    MyExtract(&quot;extract&quot;)    | Filter(&quot;filter&quot;, func=data_check)    | MyLoad(&quot;load&quot;))```&lt;a name=&quot;example-iterpush&quot;&gt;&lt;/a&gt;### Example: IterPushPush each row of an input iterable individually:```pythonglider = Glider(    CSVExtract(&quot;extract&quot;, nrows=20)    | IterPush(&quot;iter&quot;)    | Print(&quot;load&quot;))```&lt;a name=&quot;example-splitpush&quot;&gt;&lt;/a&gt;### Example: SplitPushSplit an iterable before pushing:```pythonglider = Glider(SplitPush(&quot;push&quot;, split_count=2) | Print(&quot;print&quot;))glider.consume([range(4)])```&lt;a name=&quot;example-splitbynode&quot;&gt;&lt;/a&gt;### Example: SplitByNodeSplit an iterable evenly among downstream nodes:```pythonglider = Glider(SplitByNode(&quot;push&quot;) | [Print(&quot;print1&quot;), Print(&quot;print2&quot;)])glider.consume([range(4)])```&lt;a name=&quot;example-reduce&quot;&gt;&lt;/a&gt;### Example: ReduceCollect all upstream node data before pushing:```pythonglider = Glider(    CSVExtract(&quot;extract&quot;)    | Reduce(&quot;reduce&quot;)    | Print(&quot;load&quot;))glider.consume([&quot;/path/to/infile1.csv&quot;, &quot;/path/to/infile2.csv&quot;])```This will read both input CSVs and push them in a single iterable to thedownstream nodes. You can also use the `flatten` option of `Reduce` toflatten the depth of the iterable before pushing (effectively a concatoperation).&lt;a name=&quot;example-join&quot;&gt;&lt;/a&gt;### Example: JoinJoin data on one or more columns before pushing:```pythonglider = Glider(    Reduce(&quot;reduce&quot;)    | Join(&quot;join&quot;)    | Print(&quot;load&quot;))d1 = &lt;list of dicts or DataFrame&gt;d2 = &lt;list of dicts or DataFrame&gt;glider.consume([d1, d2], join=dict(on=&quot;common_key&quot;, how=&quot;inner&quot;))```&lt;a name=&quot;example-routers&quot;&gt;&lt;/a&gt;### Example: RoutersRoute data to a particular downstream node using a router function:```pythondef parity_router(row):    if int(row[&quot;mycolumn&quot;]) % 2 == 0:        return &quot;even&quot;    return &quot;odd&quot;glider = Glider(    CSVExtract(&quot;extract&quot;, nrows=20)    | IterPush(&quot;iter&quot;)    | [parity_zip_router, Print(&quot;even&quot;), Print(&quot;odd&quot;)])glider.consume(...)```This will push rows with even `mycolumn` values to the &quot;even&quot; `Print` node,and rows with odd `mycolumn` values to the &quot;odd&quot; `Print` node.&lt;a name=&quot;example-window-processing&quot;&gt;&lt;/a&gt;### Example: Window ProcessingPush a sliding window of the data:```pythonglider = Glider(    CSVExtract(&quot;extract&quot;, nrows=5)    | WindowPush(&quot;window&quot;, size=3)    | MyWindowCalcNode(&quot;calc&quot;))```&lt;a name=&quot;example-date-windows&quot;&gt;&lt;/a&gt;### Example: Date WindowsGenerate a set of datetime windows and push them downstream:```pythonimport datetimetoday = datetime.date.today()glider = Glider(DateTimeWindowPush(&quot;windows&quot;) | PrettyPrint(&quot;print&quot;))glider.consume(    windows=dict(        start_date=today - datetime.timedelta(days=3),        end_date=today,        num_windows=2    ))```Or use `DateWindowPush` for date objects. Note that the data arg to `consume`can be ignored because the top node (`DateTimeWindowPush`) is a subclass of`NoInputNode` which takes no input data and generates data to pushon its own.&lt;a name=&quot;example-return-values&quot;&gt;&lt;/a&gt;### Example: Return ValuesBy default `consume` does not return any values and assumes you will beoutputting your results to one or more endpoints in your terminatingnodes (files, databases, etc.). The `Return` node will collect the datafrom its parent node(s) and set it as a return value for `consume`.```pythonglider = Glider(    CSVExtract(&quot;extract&quot;)    | MyTransformer(&quot;transform&quot;)    | Return(&quot;return&quot;))data = glider.consume(...)```&lt;a name=&quot;parallel-examples&quot;&gt;&lt;/a&gt;Parallelization &amp; Concurrency-----------------------------&lt;a name=&quot;example-parallel-transformation&quot;&gt;&lt;/a&gt;### Example: Parallel TransformationCall a function in parallel processes on equal splits of data from a CSV:```pythonglider = Glider(    CSVExtract(&quot;extract&quot;)    | ProcessPoolSubmit(&quot;transform&quot;, push_type=PushTypes.Result)    | CSVLoad(&quot;load&quot;))glider.consume(    [&quot;infile.csv&quot;],    transform=dict(func=lower_rows),    load=dict(outfile=&quot;outfile.csv&quot;),)```We passed `push_type=PushTypes.Result` to force `ProcessPoolSubmit` to fetchand combine the asynchronous results before pushing to the downstreamnode. The default is to just pass the asynchronous task/futures objectsforward, so the following would be equivalent:```pythonglider = Glider(    CSVExtract(&quot;extract&quot;)    | ProcessPoolSubmit(&quot;transform&quot;)    | FuturesReduce(&quot;reduce&quot;)    | Flatten(&quot;flatten&quot;)    | CSVLoad(&quot;load&quot;))```The `FuturesReduce` node waits for the results from each futures object, andthen `Flatten` will combine each subresult back together into a single resultto be loaded in the final `CSVLoad` node.&lt;a name=&quot;example-parallel-pipelines-via-paraglider&quot;&gt;&lt;/a&gt;### Example: Parallel Pipelines via ParaGliderCompletely parallelize a pipeline using a `ParaGlider` (who said ETL isn'tfun?!?). Split processing of the inputs (two files in this case) over thepool, with each process running the entire pipeline on part of the consumeddata:```pythonglider = ProcessPoolParaGlider(    CSVExtract('extract')    | Print('load'))glider.consume(    [&quot;/path/to/infile1.csv&quot;, &quot;/path/to/infile2.csv&quot;],    extract=dict(nrows=50))```&lt;a name=&quot;example-parallel-branching&quot;&gt;&lt;/a&gt;### Example: Parallel BranchingBranch into parallel execution in the middle of the DAG utilizing a parallelpush node:```pythonglider = Glider(    CSVExtract(&quot;extract&quot;, nrows=60)    | ProcessPoolPush(&quot;push&quot;, split=True)    | [Print(&quot;load1&quot;), Print(&quot;load2&quot;), Print(&quot;load3&quot;)])glider.consume([&quot;/path/to/infile.csv&quot;])```The above example will extract 60 rows from a CSV and then push equal slicesto the logging nodes in parallel processes. Using `split=False` (default)would have passed the entire 60 rows to each logging node in parallelprocesses.Once you branch off into processes with a parallel push node there is no wayto reduce/join the pipeline back into the original process and resumesingle-process operation. The entire remainder of the pipeline is executed ineach subprocess. However, that is possible with threads as shown in the nextexample.&lt;a name=&quot;example-thread-reducers&quot;&gt;&lt;/a&gt;### Example: Thread Reducers```pythonglider = Glider(    CSVExtract(&quot;extract&quot;, nrows=60)    | ThreadPoolPush(&quot;push&quot;, split=True)    | [Print(&quot;load1&quot;), Print(&quot;load2&quot;), Print(&quot;load3&quot;)]    | ThreadReduce(&quot;reduce&quot;)    | Print(&quot;loadall&quot;))glider.consume([&quot;/path/to/infile.csv&quot;])```The above code will split the data and push to the first 3 logging nodes inmultiple threads. The `ThreadReduce` node won't push until all of the previousnodes have finished, and then the final logging node will print all of theresults.&lt;a name=&quot;example-asyncio&quot;&gt;&lt;/a&gt;### Example: AsyncioLimited, experimental support is also available for concurrency via `asyncio`in Python &gt;= 3.7:```pythonimport asyncioasync def async_sleep(data):    # Dummy example. Await some real async work in here.    await asyncio.sleep(0.5)    return dataglider = Glider(    CSVExtract(&quot;extract&quot;, nrows=5)    | AsyncIOSubmit(&quot;transform&quot;, push_type=PushTypes.Result)    | Print(&quot;load&quot;))glider.consume(    [&quot;/path/to/infile.csv&quot;],    transform=dict(func=async_sleep))```The above example will split the input data into items to be processed on an`asyncio` event loop and synchronously wait for the results before pushing.`AsyncIOSubmit` supports specifying a `split_count` as well as a `timeout`when waiting for results. Alternatively, one can push `asyncio` futures andlater reduce their results as follows:```pythonglider = Glider(      CSVExtract(&quot;extract&quot;, nrows=5)      | AsyncIOSubmit(&quot;transform&quot;, push_type=PushTypes.Async)      | AsyncIOFuturesReduce(&quot;reduce&quot;, flatten=True)      | Print(&quot;load&quot;))```Note that the `asyncio` nodes will create and start an event loop for you ifnecessary. It's also perfectly fine to manage the event loop on your own, inwhich case `glide` will run tasks on the current thread's event loop.&lt;a name=&quot;utility-examples&quot;&gt;&lt;/a&gt;Utility Examples----------------&lt;a name=&quot;example-templated-nodes-and-pipelines&quot;&gt;&lt;/a&gt;### Example: Templated Nodes and PipelinesDrop replacement nodes into an existing pipeline. Any node can be replaced byname:```pythonglider = Glider(    PlaceholderNode(&quot;extract&quot;)    | CSVLoad(&quot;load&quot;))glider[&quot;extract&quot;] = CSVExtract(&quot;extract&quot;)glider.consume(...)```Or reuse an existing structure of nodes with a `NodeTemplate`:```pythonnodes = NodeTemplate(    CSVExtract(&quot;extract&quot;)    | CSVLoad(&quot;load&quot;))glider = Glider(nodes()) # Copy of nodes created with each call```Or reuse an existing pipeline structure with `GliderTemplate`:```pythontemplate = GliderTemplate(    CSVExtract(&quot;extract&quot;)    | CSVLoad(&quot;load&quot;))glider = template() # Copy of pipeline created with each call```&lt;a name=&quot;example-data-integrity-checks&quot;&gt;&lt;/a&gt;### Example: Data Integrity ChecksYou can use the `AssertFunc` node to assert that some condition of the data ismet:```pythonglider = Glider(    CSVExtract(&quot;extract&quot;, chunksize=10, nrows=20)    | AssertFunc(&quot;length_check&quot;, func=lambda node, data: len(data) == 10)    | CSVLoad(&quot;load&quot;))```The `func` callable must accept two parameters, a reference to the node objectand the data passed into that node. Any truthy value returned will pass theassertion test.Similarly, you can do a sql-based check with `AssertSQL`, in this case simplyverifying the number of rows inserted:```pythonglider = Glider(    SQLExtract(&quot;extract&quot;)    | SQLLoad(&quot;load&quot;)    | AssertSQL(&quot;sql_check&quot;))sql = &quot;select * from in_table limit 10&quot;assert_sql = &quot;select (select count(*) as x from out_table) == 10 as assert&quot;glider.consume(    [sql],    extract=dict(conn=in_conn),    load=dict(conn=out_conn, table=&quot;out_table&quot;),    sql_check=dict(conn=out_conn, sql=assert_sql))```This looks for a truthy value in the `assert` column of the result to pass theassertion. You can also use the `data_check` option of `AssertSQL` to insteadhave it do a comparison to the result of some function of the data:```pythonglider = ...sql = &quot;select * from in_table limit 10&quot;assert_sql = &quot;select count(*) as assert from out_table&quot;glider.consume(    [sql],    extract=dict(conn=in_conn),    load=dict(conn=out_conn, table=&quot;out_table&quot;, push_data=True),    sql_check=dict(        conn=out_conn,        sql=assert_sql,        data_check=lambda node, data: len(data)    ))```Note that we also added `push_data=True` to the `SQLLoad` node to have it pushthe data instead of a table name.&lt;a name=&quot;example-debugging&quot;&gt;&lt;/a&gt;### Example: DebuggingTo enable debug logging for Glide change the log level of the &quot;glide&quot; logger:```pythonimport logginglogging.getLogger(&quot;glide&quot;).setLevel(logging.DEBUG)```Glide will then print debug information about data passed through yourpipeline.You can also pass `_log=True` to the init method of any node to enable loggingof processed data:```pythonglider = Glider(    SQLExtract(&quot;extract&quot;, _log=True)    ...)```Additionaly, you can pass `_debug=True` to the init method of any node tocause the node to drop into PDB right before calling `run`, assuming youaren't executing the pipeline in a subprocess:```pythonglider = Glider(    SQLExtract(&quot;extract&quot;, _debug=True)    ...)```Finally, there are a variety of print nodes you can place in your pipeline forgeneral logging or debugging, such as `Print`, `PrettyPrint`, `LenPrint`,`ReprPrint`, and `FormatPrint`. See the node documentation for more info.&lt;a name=&quot;example-profiling-pipelines&quot;&gt;&lt;/a&gt;### Example: Profiling PipelinesInsert a `Profile` node somewhere in your pipeline to get profiler informationfor all downstream nodes:```pythonglider = Glider(    Profile(&quot;profile&quot;)    ...)```&lt;a name=&quot;example-complex-pipelines&quot;&gt;&lt;/a&gt;### Example: Complex PipelinesIf the hierarchy of nodes you want to form is not achievable with the `|`operator, you can use the `add_downstream` `Node` method to form more complexgraphs. More info can be found[here](https://glide-etl.readthedocs.io/en/latest/pipelines.html#complex-pipelines).&lt;a name=&quot;example-plotting-pipeline-dags&quot;&gt;&lt;/a&gt;### Example: Plotting Pipeline DAGsIf you have the [Graphviz](http://www.graphviz.org/) package installed, youcan generate a plot of your pipelines by simply doing the following:```pythonglider = Glider(...)glider.plot(&quot;/path/to/filename.png&quot;)```&lt;a name=&quot;cli-generation&quot;&gt;&lt;/a&gt;CLI Generation--------------With Glide you can create parameterized command line scripts from any pipelinewith a simple decorator:```pythonglider = Glider(    SQLLoad(&quot;extract&quot;)    | SQLExtract(&quot;load&quot;))@glider.cli()def main(glide_data, node_contexts):    glider.consume(glide_data, **node_contexts)if __name__ == &quot;__main__&quot;:    main()```The script arguments, their types, and whether they are required or not is allinferred by inspecting the `run` arguments on the nodes of the pipeline andprefixing the node name. Please see the full documentation[here](https://glide-etl.readthedocs.io/en/latest/pipelines.html#cli-generation)for more details.&lt;a name=&quot;extensions&quot;&gt;&lt;/a&gt;Extensions----------To install all extensions and dev dependencies:```shell$ pip install glide[complete]```You can also just install Glide plus a specific extension:```shell$ pip install glide[dask]$ pip install glide[celery]$ pip install glide[rq]$ pip install glide[swifter]```To access installed extensions import from the `glide.extensions` submodulesas necessary. Review the documentation and tests for current extensions forhelp getting started.&lt;a name=&quot;pandas&quot;&gt;&lt;/a&gt;### PandasThe Pandas extension is actually supported by default with all `glide` installs.Below is a simple example that extracts from a CSV, lowercases all strings,and then loads to another CSV using Pandas under the hood:```pythondef lower(s):    return s.lower() if type(s) == str else sglider = Glider(    DataFrameCSVExtract(&quot;extract&quot;)    | DataFrameApplyMap(&quot;transform&quot;, func=lower)    | DataFrameCSVLoad(&quot;load&quot;, index=False, mode=&quot;a&quot;))glider.consume(...)```There are a variety of other helpful nodes built in, including `ToDataFrame`,`FromDataFrame`, nodes to read/write other datasources, and nodes to deal with`rolling` calculations. There is also a generic `DataFrameMethod` node thatpasses through to any DataFrame method.See the extension docs[here](https://glide-etl.readthedocs.io/en/latest/glide.extensions.pandas.html)for node/pipeline reference information. See the tests[here](https://github.com/kmatarese/glide/tree/master/tests/pandas_ext/test_pandas.py)for some additional examples.&lt;a name=&quot;dask-experimental&quot;&gt;&lt;/a&gt;### Dask - ExperimentalSee the extension docs[here](https://glide-etl.readthedocs.io/en/latest/glide.extensions.dask.html)for node/pipeline reference information. See the tests[here](https://github.com/kmatarese/glide/tree/master/tests/dask_ext/test_dask.py)for some additional examples.&lt;a name=&quot;celery-experimental&quot;&gt;&lt;/a&gt;### Celery - ExperimentalSee the extension docs[here](https://glide-etl.readthedocs.io/en/latest/glide.extensions.celery.html)for node/pipeline reference information. See the tests[here](https://github.com/kmatarese/glide/tree/master/tests/celery_ext/test_celery.py)for some additional examples.&lt;a name=&quot;redis-queue-experimental&quot;&gt;&lt;/a&gt;### Redis Queue - ExperimentalSee the extension docs[here](https://glide-etl.readthedocs.io/en/latest/glide.extensions.rq.html)for node/pipeline reference information. See the tests[here](https://github.com/kmatarese/glide/tree/master/tests/rq_ext/test_rq.py)for some additional examples.&lt;a name=&quot;swifter-experimental&quot;&gt;&lt;/a&gt;### Swifter - ExperimentalSee the extension docs[here](https://glide-etl.readthedocs.io/en/latest/glide.extensions.swifter.html)for node/pipeline reference information. See the tests[here](https://github.com/kmatarese/glide/tree/master/tests/swifter_ext/test_swifter.py)for some additional examples.&lt;a name=&quot;documentation&quot;&gt;&lt;/a&gt;Documentation-------------More thorough documentation can be found [here](https://glide-etl.readthedocs.io/en/latest/).You can supplement your knowledge by perusing the [tests](https://github.com/kmatarese/glide/tree/master/tests) directoryor the [module reference](https://glide-etl.readthedocs.io/en/latest/glide.html).&lt;a name=&quot;how-to-contribute&quot;&gt;&lt;/a&gt;How to Contribute-----------------See the [CONTRIBUTING](https://github.com/kmatarese/glide/blob/master/CONTRIBUTING.md) guide.</longdescription>
</pkgmetadata>