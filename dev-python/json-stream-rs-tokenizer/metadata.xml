<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&gt; **NOTE:** `json-stream-rs-tokenizer` is now automatically used by&gt; `json-stream`, so unless you find a bug, you can ignore this package's&gt; existence!# json-stream-rs-tokenizer[![CI build badge](https://github.com/smheidrich/py-json-stream-rs-tokenizer/actions/workflows/build.yml/badge.svg)](https://github.com/smheidrich/py-json-stream-rs-tokenizer/actions/workflows/build.yml)[![CI test badge](https://github.com/smheidrich/py-json-stream-rs-tokenizer/actions/workflows/test.yml/badge.svg)](https://github.com/smheidrich/py-json-stream-rs-tokenizer/actions/workflows/test.yml)[![PyPI package and version badge](https://img.shields.io/pypi/v/json-stream-rs-tokenizer)](https://pypi.org/project/json-stream-rs-tokenizer/)[![Supported Python versions badge](https://img.shields.io/pypi/pyversions/json-stream-rs-tokenizer)](https://pypi.org/project/json-stream-rs-tokenizer/)A faster tokenizer for the [json-stream](https://github.com/daggaz/json-stream)Python library.It's actually just `json-stream`'s own tokenizer (itself adapted from the[NAYA](https://github.com/danielyule/naya) project) ported to Rust almostverbatim and made available as a Python module using[PyO3](https://github.com/PyO3/pyo3).On my machine, it **speeds up parsing by a factor of 4â€“10**, depending on thenature of the data.## Installation### ImplicitStarting at its 2.0 release, **`json-stream` depends on and uses`json-stream-rs-tokenizer` by default**, so you don't need to install itexplicitly anymore.### ExplicitIf you use an older `json-stream` version (which you have no reason to do) orneed to install `json-stream-rs-tokenizer` explicitly for another reason, youcan do:```bashpip install json-stream-rs-tokenizer```The library will be installed as a prebuilt wheel if one is available for yourplatform. Otherwise, pip will try to build it from the source distribution,which requires a Rust toolchain to be installed and available to succeed.Note that if the build from source fails, the package installation will beconsidered successfully completed anyway, but `RustTokenizer` (see below) won'tbe available for import. This is so that packages (specifically, `json-stream`)can depend on the library but fall back to their own implementation if neithera prebuilt wheel is available nor the build succeeds.You can increase the installation command's verbosity with `-v` (repeated foreven more information, e.g. `-vv`) to see error messages when the build fromsource fails.**Note** that if the Rust library is compiled in debug mode, it will run*slower* than the pure-Python tokenizer. The setuptools configuration shouldmake sure this doesn't happen even when installing in development mode, butwhen in doubt, run installation commands with `-v` to see the Rust compilationcommands and verify that they used `--release`.## Usage### ImplicitAs described above, `json-stream-rs-tokenizer` is now used by `json-stream` bydefault, so you don't have to do anything special to use it. `json-stream` willfall back to its pure-Python tokenizer when `json-stream-rs-tokenizer` was notsuccessfully installed, however.### ExplicitFor older versions of `json-stream`, or if you want to *ensure* the Rusttokenizer is used no matter what, simply pass this package's `RustTokenizer` asthe `tokenizer` argument to `json-stream`'s `load` or `visit`:```pythonfrom io import StringIOfrom json_stream import loadfrom json_stream_rs_tokenizer import RustTokenizerjson_buf = StringIO('{ &quot;a&quot;: [1,2,3,4], &quot;b&quot;: [5,6,7] }')# uses the Rust tokenizer to load JSON:d = load(json_buf, tokenizer=RustTokenizer)for k, l in d.items():  print(f&quot;{k}: {' '.join(str(n) for n in l)}&quot;)```Note that the import of `RustTokenizer` will fail if the Rust extension is notavailable (i.e., when no prebuilt wheels were available and the installationfrom the source distribution failed).## Limitations- For PyPy, the speedup is only 1.0-1.5x (much lower than that for CPython).  This has yet to be  [investigated](https://github.com/smheidrich/py-json-stream-rs-tokenizer/issues/33).- In builds that don't support PyO3's  [`num-bigint` extension](https://pyo3.rs/main/doc/pyo3/num_bigint/)  (currently only PyPy builds and manual ones against Python's limited C API  (`Py_LIMITED_API`)), conversion of large integers is performed in Python  rather than in Rust, at a very small runtime cost.## BenchmarksThe package comes with a script for rudimentary benchmarks on randomlygenerated JSON data. To run it, you'll need to install the optional `benchmark`dependencies:```bashpip install 'json-stream-rs-tokenizer[benchmark]'```You can then run the benchmark as follows:```bashpython -m json_stream_rs_tokenizer.benchmark```Run it with `--help` to see more information.## TestsTo run the tests, you'll need to install the optional `test` dependencies:```bashpip install 'json-stream-rs-tokenizer[test]'```As the test dependencies depend on the benchmark dependencies but the featureenabling such[&quot;recursive optional dependencies&quot;](https://hynek.me/articles/python-recursive-optional-dependencies/)was only introduced in Pip 21.3, you'll need a version of Pip at least asrecent as that. For older versions, just install the test dependenciesmanually.## LicenseMIT license. Refer to the[LICENSE](https://github.com/smheidrich/py-json-stream-rs-tokenizer/blob/main/LICENSE)file for details.</longdescription>
</pkgmetadata>