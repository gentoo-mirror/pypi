<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Sequence Graph Transform (SGT) &amp;mdash; Sequence Embedding for Clustering, Classification, and Search#### Maintained by: Chitta Ranjan Email: &lt;cran2367@gmail.com&gt;| LinkedIn: [https://www.linkedin.com/in/chitta-ranjan-b0851911/](https://www.linkedin.com/in/chitta-ranjan-b0851911/)The following will cover,1. [SGT Class Definition](#sgt-class-def)2. [Installation](#install-sgt)3. [Test Examples](#installation-test-examples)4. [Sequence Clustering Example](#sequence-clustering)5. [Sequence Classification Example](#sequence-classification)6. [Sequence Search Example](#sequence-search)7. [SGT - Spark for Distributed Computing](#sgt-spark)## &lt;a name=&quot;sgt-class-def&quot;&gt;&lt;/a&gt; SGT Class DefinitionSequence Graph Transform (SGT) is a sequence embedding function. SGT extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. The long and short term patterns embedded in SGT can be tuned without any increase in the computation.&quot;```class SGT():    '''    Compute embedding of a single or a collection of discrete item     sequences. A discrete item sequence is a sequence made from a set    discrete elements, also known as alphabet set. For example,    suppose the alphabet set is the set of roman letters,     {A, B, ..., Z}. This set is made of discrete elements. Examples of    sequences from such a set are AABADDSA, UADSFJPFFFOIHOUGD, etc.    Such sequence datasets are commonly found in online industry,    for example, item purchase history, where the alphabet set is    the set of all product items. Sequence datasets are abundant in    bioinformatics as protein sequences.    Using the embeddings created here, classification and clustering    models can be built for sequence datasets.    Read more in https://arxiv.org/pdf/1608.03533.pdf    '''    Parameters    ----------    Input:    alphabets       Optional, except if mode is Spark.                     The set of alphabets that make up all                     the sequences in the dataset. If not passed, the                    alphabet set is automatically computed as the                     unique set of elements that make all the sequences.                    A list or 1d-array of the set of elements that make up the                          sequences. For example, np.array([&quot;A&quot;, &quot;B&quot;, &quot;C&quot;].                    If mode is 'spark', the alphabets are necessary.    kappa           Tuning parameter, kappa &gt; 0, to change the extraction of                     long-term dependency. Higher the value the lesser                    the long-term dependency captured in the embedding.                    Typical values for kappa are 1, 5, 10.    lengthsensitive Default false. This is set to true if the embedding of                    should have the information of the length of the sequence.                    If set to false then the embedding of two sequences with                    similar pattern but different lengths will be the same.                    lengthsensitive = false is similar to length-normalization.    flatten         Default True. If True the SGT embedding is flattened and returned as                    a vector. Otherwise, it is returned as a matrix with the row and col                    names same as the alphabets. The matrix form is used for                                interpretation purposes. Especially, to understand how the alphabets                    are &quot;related&quot;. Otherwise, for applying machine learning or deep                    learning algorithms, the embedding vectors are required.    mode            Choices in {'default', 'multiprocessing'}. Note: 'multiprocessing'                     mode requires pandas==1.0.3+ and pandarallel libraries.    processors      Used if mode is 'multiprocessing'. By default, the                     number of processors used in multiprocessing is                    number of available - 1.    '''    Attributes    ----------    def fit(sequence)    Extract Sequence Graph Transform features using Algorithm-2 in https://arxiv.org/abs/1608.03533.    Input:    sequence        An array of discrete elements. For example,                    np.array([&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;A&quot;].    Output:     sgt embedding   sgt matrix or vector (depending on Flatten==False or True) of the sequence    --    def fit_transform(corpus)    Extract SGT embeddings for all sequences in a corpus. It finds    the alphabets encompassing all the sequences in the corpus, if not inputted.     However, if the mode is 'spark', then the alphabets list has to be    explicitly given in Sgt object declaration.    Input:    corpus          A list of sequences. Each sequence is a list of alphabets.    Output:    sgt embedding of all sequences in the corpus.    --    def transform(corpus)    Find SGT embeddings of a new data sample belonging to the same population    of the corpus that was fitted initially.```## &lt;a name=&quot;install-sgt&quot;&gt;&lt;/a&gt; Install SGTInstall SGT in Python by running,```$ pip install sgt``````pythonimport sgtsgt.__version__from sgt import SGT```    '2.0.0'```python# -*- coding: utf-8 -*-# Authors: Chitta Ranjan &lt;cran2367@gmail.com&gt;## License: BSD 3 clause```## &lt;a name=&quot;installation-test-examples&quot;&gt;&lt;/a&gt; Installation Test ExamplesIn the following, there are a few test examples to verify the installation.```python# Learning a sgt embedding as a matrix with # rows and columns as the sequence alphabets. # This embedding shows the relationship between # the alphabets. The higher the value the # stronger the relationship.sgt = SGT(flatten=False)sequence = np.array([&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;A&quot;])sgt.fit(sequence)```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;A&lt;/th&gt;      &lt;th&gt;B&lt;/th&gt;      &lt;th&gt;C&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;A&lt;/th&gt;      &lt;td&gt;0.090616&lt;/td&gt;      &lt;td&gt;0.131002&lt;/td&gt;      &lt;td&gt;0.261849&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;B&lt;/th&gt;      &lt;td&gt;0.086569&lt;/td&gt;      &lt;td&gt;0.123042&lt;/td&gt;      &lt;td&gt;0.052544&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;C&lt;/th&gt;      &lt;td&gt;0.137142&lt;/td&gt;      &lt;td&gt;0.028263&lt;/td&gt;      &lt;td&gt;0.135335&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```python# SGT embedding to a vector. The vector# embedding is useful for directly applying# a machine learning algorithm.sgt = SGT(flatten=True)sequence = np.array([&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;A&quot;])sgt.fit(sequence)```    (A, A)    0.090616    (A, B)    0.131002    (A, C)    0.261849    (B, A)    0.086569    (B, B)    0.123042    (B, C)    0.052544    (C, A)    0.137142    (C, B)    0.028263    (C, C)    0.135335    dtype: float64```python'''SGT embedding on a corpus of sequences.Test the two processing modes within theSGT class: 'default', 'multiprocessing'.'''# A sample corpus of two sequences.corpus = pd.DataFrame([[1, [&quot;B&quot;,&quot;B&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;C&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;A&quot;]],                        [2, [&quot;C&quot;, &quot;Z&quot;, &quot;Z&quot;, &quot;Z&quot;, &quot;D&quot;]]],                       columns=['id', 'sequence'])corpus```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;sequence&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;1&lt;/td&gt;      &lt;td&gt;[B, B, A, C, A, C, A, A, B, A]&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;2&lt;/td&gt;      &lt;td&gt;[C, Z, Z, Z, D]&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```python# Learning the sgt embeddings as vector for# all sequences in a corpus.# mode: 'default'sgt = SGT(kappa=1,           flatten=True,           lengthsensitive=False,           mode='default')sgt.fit_transform(corpus)```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;(A, A)&lt;/th&gt;      &lt;th&gt;(A, B)&lt;/th&gt;      &lt;th&gt;(A, C)&lt;/th&gt;      &lt;th&gt;(A, D)&lt;/th&gt;      &lt;th&gt;(A, Z)&lt;/th&gt;      &lt;th&gt;(B, A)&lt;/th&gt;      &lt;th&gt;(B, B)&lt;/th&gt;      &lt;th&gt;(B, C)&lt;/th&gt;      &lt;th&gt;(B, D)&lt;/th&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;th&gt;(D, A)&lt;/th&gt;      &lt;th&gt;(D, B)&lt;/th&gt;      &lt;th&gt;(D, C)&lt;/th&gt;      &lt;th&gt;(D, D)&lt;/th&gt;      &lt;th&gt;(D, Z)&lt;/th&gt;      &lt;th&gt;(Z, A)&lt;/th&gt;      &lt;th&gt;(Z, B)&lt;/th&gt;      &lt;th&gt;(Z, C)&lt;/th&gt;      &lt;th&gt;(Z, D)&lt;/th&gt;      &lt;th&gt;(Z, Z)&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;1.0&lt;/td&gt;      &lt;td&gt;0.090616&lt;/td&gt;      &lt;td&gt;0.131002&lt;/td&gt;      &lt;td&gt;0.261849&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.086569&lt;/td&gt;      &lt;td&gt;0.123042&lt;/td&gt;      &lt;td&gt;0.052544&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;2.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.184334&lt;/td&gt;      &lt;td&gt;0.290365&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;2 rows × 26 columns&lt;/p&gt;&lt;/div&gt;```python# Learning the sgt embeddings as vector for# all sequences in a corpus.# mode: 'multiprocessing'import pandarallel  # required library for multiprocessingsgt = SGT(kappa=1,           flatten=True,           lengthsensitive=False,          mode='multiprocessing')sgt.fit_transform(corpus)```    INFO: Pandarallel will run on 7 workers.    INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;(A, A)&lt;/th&gt;      &lt;th&gt;(A, B)&lt;/th&gt;      &lt;th&gt;(A, C)&lt;/th&gt;      &lt;th&gt;(A, D)&lt;/th&gt;      &lt;th&gt;(A, Z)&lt;/th&gt;      &lt;th&gt;(B, A)&lt;/th&gt;      &lt;th&gt;(B, B)&lt;/th&gt;      &lt;th&gt;(B, C)&lt;/th&gt;      &lt;th&gt;(B, D)&lt;/th&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;th&gt;(D, A)&lt;/th&gt;      &lt;th&gt;(D, B)&lt;/th&gt;      &lt;th&gt;(D, C)&lt;/th&gt;      &lt;th&gt;(D, D)&lt;/th&gt;      &lt;th&gt;(D, Z)&lt;/th&gt;      &lt;th&gt;(Z, A)&lt;/th&gt;      &lt;th&gt;(Z, B)&lt;/th&gt;      &lt;th&gt;(Z, C)&lt;/th&gt;      &lt;th&gt;(Z, D)&lt;/th&gt;      &lt;th&gt;(Z, Z)&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;1.0&lt;/td&gt;      &lt;td&gt;0.090616&lt;/td&gt;      &lt;td&gt;0.131002&lt;/td&gt;      &lt;td&gt;0.261849&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.086569&lt;/td&gt;      &lt;td&gt;0.123042&lt;/td&gt;      &lt;td&gt;0.052544&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;2.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.184334&lt;/td&gt;      &lt;td&gt;0.290365&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;2 rows × 26 columns&lt;/p&gt;&lt;/div&gt;## Load Libraries for Illustrative Examples```pythonfrom sgt import SGTimport numpy as npimport pandas as pdfrom itertools import chainfrom itertools import product as iterproductimport warningsimport pickle########from sklearn.preprocessing import LabelEncoderimport tensorflow as tffrom keras.datasets import imdbfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Densefrom tensorflow.keras.layers import LSTMfrom tensorflow.keras.layers import Dropoutfrom tensorflow.keras.layers import Activationfrom tensorflow.keras.layers import Flattenfrom tensorflow.keras.layers import Embeddingfrom tensorflow.keras.preprocessing import sequencefrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import KFoldfrom sklearn.model_selection import StratifiedKFoldimport sklearn.metricsimport timefrom sklearn.decomposition import PCAfrom sklearn.cluster import KMeansimport matplotlib.pyplot as plt%matplotlib inlinenp.random.seed(7) # fix random seed for reproducibility# from sgt import Sgt```## &lt;a name=&quot;sequence-clustering&quot;&gt;&lt;/a&gt; Sequence ClusteringA form of unsupervised learning from sequences is clustering. For example, in - user weblogs sequences: clustering the weblogs segments users into groups with similar browsing behavior. This helps in targeted marketing, anomaly detection, and other web customizations.- protein sequences: clustering proteins with similar structures help researchers study the commonalities between species. It also helps in faster search in some search algorithms.In the following, clustering on a protein sequence dataset will be shown.### Protein Sequence ClusteringThe data used here is taken from www.uniprot.org. This is a public database for proteins. The data contains the protein sequences and their functions.```python# Loading datacorpus = pd.read_csv('data/protein_classification.csv')# Data preprocessingcorpus = corpus.loc[:,['Entry','Sequence']]corpus.columns = ['id', 'sequence']corpus['sequence'] = corpus['sequence'].map(list)corpus```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;sequence&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;M7MCX3&lt;/td&gt;      &lt;td&gt;[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;K6PL84&lt;/td&gt;      &lt;td&gt;[M, E, I, E, K, N, Y, R, M, N, S, L, F, E, F, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2&lt;/th&gt;      &lt;td&gt;R4W5V3&lt;/td&gt;      &lt;td&gt;[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;3&lt;/th&gt;      &lt;td&gt;T2A126&lt;/td&gt;      &lt;td&gt;[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;4&lt;/th&gt;      &lt;td&gt;L0SHD5&lt;/td&gt;      &lt;td&gt;[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2107&lt;/th&gt;      &lt;td&gt;A0A081R612&lt;/td&gt;      &lt;td&gt;[M, M, N, M, Q, N, M, M, R, Q, A, Q, K, L, Q, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2108&lt;/th&gt;      &lt;td&gt;A0A081QQM2&lt;/td&gt;      &lt;td&gt;[M, M, N, M, Q, N, M, M, R, Q, A, Q, K, L, Q, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2109&lt;/th&gt;      &lt;td&gt;J1A517&lt;/td&gt;      &lt;td&gt;[M, M, R, Q, A, Q, K, L, Q, K, Q, M, E, Q, S, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2110&lt;/th&gt;      &lt;td&gt;F5U1T6&lt;/td&gt;      &lt;td&gt;[M, M, N, M, Q, S, M, M, K, Q, A, Q, K, L, Q, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2111&lt;/th&gt;      &lt;td&gt;J3A2T7&lt;/td&gt;      &lt;td&gt;[M, M, N, M, Q, N, M, M, K, Q, A, Q, K, L, Q, ...&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;2112 rows × 2 columns&lt;/p&gt;&lt;/div&gt;```python%%time# Compute SGT embeddingssgt_ = SGT(kappa=1,            lengthsensitive=False,            mode='multiprocessing')sgtembedding_df = sgt_.fit_transform(corpus)```    INFO: Pandarallel will run on 7 workers.    INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.    CPU times: user 324 ms, sys: 68 ms, total: 392 ms    Wall time: 9.02 s```pythonsgtembedding_df```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;(A, A)&lt;/th&gt;      &lt;th&gt;(A, C)&lt;/th&gt;      &lt;th&gt;(A, D)&lt;/th&gt;      &lt;th&gt;(A, E)&lt;/th&gt;      &lt;th&gt;(A, F)&lt;/th&gt;      &lt;th&gt;(A, G)&lt;/th&gt;      &lt;th&gt;(A, H)&lt;/th&gt;      &lt;th&gt;(A, I)&lt;/th&gt;      &lt;th&gt;(A, K)&lt;/th&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;th&gt;(Y, M)&lt;/th&gt;      &lt;th&gt;(Y, N)&lt;/th&gt;      &lt;th&gt;(Y, P)&lt;/th&gt;      &lt;th&gt;(Y, Q)&lt;/th&gt;      &lt;th&gt;(Y, R)&lt;/th&gt;      &lt;th&gt;(Y, S)&lt;/th&gt;      &lt;th&gt;(Y, T)&lt;/th&gt;      &lt;th&gt;(Y, V)&lt;/th&gt;      &lt;th&gt;(Y, W)&lt;/th&gt;      &lt;th&gt;(Y, Y)&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;M7MCX3&lt;/td&gt;      &lt;td&gt;0.020180&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.009635&lt;/td&gt;      &lt;td&gt;0.013529&lt;/td&gt;      &lt;td&gt;0.009360&lt;/td&gt;      &lt;td&gt;0.003205&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.002226&lt;/td&gt;      &lt;td&gt;0.000379&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.009196&lt;/td&gt;      &lt;td&gt;0.007964&lt;/td&gt;      &lt;td&gt;0.036788&lt;/td&gt;      &lt;td&gt;0.000195&lt;/td&gt;      &lt;td&gt;0.001513&lt;/td&gt;      &lt;td&gt;0.020665&lt;/td&gt;      &lt;td&gt;0.000542&lt;/td&gt;      &lt;td&gt;0.007479&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.010419&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;K6PL84&lt;/td&gt;      &lt;td&gt;0.001604&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.012637&lt;/td&gt;      &lt;td&gt;0.006323&lt;/td&gt;      &lt;td&gt;0.006224&lt;/td&gt;      &lt;td&gt;0.004819&lt;/td&gt;      &lt;td&gt;3.560677e-03&lt;/td&gt;      &lt;td&gt;0.001124&lt;/td&gt;      &lt;td&gt;0.012136&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.135335&lt;/td&gt;      &lt;td&gt;0.006568&lt;/td&gt;      &lt;td&gt;0.038901&lt;/td&gt;      &lt;td&gt;0.011298&lt;/td&gt;      &lt;td&gt;0.012578&lt;/td&gt;      &lt;td&gt;0.009913&lt;/td&gt;      &lt;td&gt;0.001079&lt;/td&gt;      &lt;td&gt;0.000023&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.007728&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2&lt;/th&gt;      &lt;td&gt;R4W5V3&lt;/td&gt;      &lt;td&gt;0.012448&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008408&lt;/td&gt;      &lt;td&gt;0.016363&lt;/td&gt;      &lt;td&gt;0.027469&lt;/td&gt;      &lt;td&gt;0.003205&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.004249&lt;/td&gt;      &lt;td&gt;0.013013&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.008114&lt;/td&gt;      &lt;td&gt;0.007128&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000203&lt;/td&gt;      &lt;td&gt;0.001757&lt;/td&gt;      &lt;td&gt;0.022736&lt;/td&gt;      &lt;td&gt;0.000249&lt;/td&gt;      &lt;td&gt;0.012652&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008533&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;3&lt;/th&gt;      &lt;td&gt;T2A126&lt;/td&gt;      &lt;td&gt;0.010545&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.012560&lt;/td&gt;      &lt;td&gt;0.014212&lt;/td&gt;      &lt;td&gt;0.013728&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.007223&lt;/td&gt;      &lt;td&gt;0.000309&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000325&lt;/td&gt;      &lt;td&gt;0.009669&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.003182&lt;/td&gt;      &lt;td&gt;0.001904&lt;/td&gt;      &lt;td&gt;0.015607&lt;/td&gt;      &lt;td&gt;0.000577&lt;/td&gt;      &lt;td&gt;0.007479&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008648&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;4&lt;/th&gt;      &lt;td&gt;L0SHD5&lt;/td&gt;      &lt;td&gt;0.020180&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008628&lt;/td&gt;      &lt;td&gt;0.015033&lt;/td&gt;      &lt;td&gt;0.009360&lt;/td&gt;      &lt;td&gt;0.003205&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.002226&lt;/td&gt;      &lt;td&gt;0.000379&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.009196&lt;/td&gt;      &lt;td&gt;0.007964&lt;/td&gt;      &lt;td&gt;0.036788&lt;/td&gt;      &lt;td&gt;0.000195&lt;/td&gt;      &lt;td&gt;0.001513&lt;/td&gt;      &lt;td&gt;0.020665&lt;/td&gt;      &lt;td&gt;0.000542&lt;/td&gt;      &lt;td&gt;0.007479&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.010419&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2107&lt;/th&gt;      &lt;td&gt;A0A081R612&lt;/td&gt;      &lt;td&gt;0.014805&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.004159&lt;/td&gt;      &lt;td&gt;0.017541&lt;/td&gt;      &lt;td&gt;0.012701&lt;/td&gt;      &lt;td&gt;0.013099&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.017043&lt;/td&gt;      &lt;td&gt;0.004732&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2108&lt;/th&gt;      &lt;td&gt;A0A081QQM2&lt;/td&gt;      &lt;td&gt;0.010774&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.004283&lt;/td&gt;      &lt;td&gt;0.014732&lt;/td&gt;      &lt;td&gt;0.014340&lt;/td&gt;      &lt;td&gt;0.014846&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.016806&lt;/td&gt;      &lt;td&gt;0.005406&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2109&lt;/th&gt;      &lt;td&gt;J1A517&lt;/td&gt;      &lt;td&gt;0.010774&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.004283&lt;/td&gt;      &lt;td&gt;0.014732&lt;/td&gt;      &lt;td&gt;0.014340&lt;/td&gt;      &lt;td&gt;0.014846&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.014500&lt;/td&gt;      &lt;td&gt;0.005406&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2110&lt;/th&gt;      &lt;td&gt;F5U1T6&lt;/td&gt;      &lt;td&gt;0.015209&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.005175&lt;/td&gt;      &lt;td&gt;0.023888&lt;/td&gt;      &lt;td&gt;0.011410&lt;/td&gt;      &lt;td&gt;0.011510&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.021145&lt;/td&gt;      &lt;td&gt;0.009280&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2111&lt;/th&gt;      &lt;td&gt;J3A2T7&lt;/td&gt;      &lt;td&gt;0.005240&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.012301&lt;/td&gt;      &lt;td&gt;0.013178&lt;/td&gt;      &lt;td&gt;0.014744&lt;/td&gt;      &lt;td&gt;0.014705&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000981&lt;/td&gt;      &lt;td&gt;0.007957&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;2112 rows × 401 columns&lt;/p&gt;&lt;/div&gt;```python# Set the id column as the dataframe indexsgtembedding_df = sgtembedding_df.set_index('id')sgtembedding_df```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;(A, A)&lt;/th&gt;      &lt;th&gt;(A, C)&lt;/th&gt;      &lt;th&gt;(A, D)&lt;/th&gt;      &lt;th&gt;(A, E)&lt;/th&gt;      &lt;th&gt;(A, F)&lt;/th&gt;      &lt;th&gt;(A, G)&lt;/th&gt;      &lt;th&gt;(A, H)&lt;/th&gt;      &lt;th&gt;(A, I)&lt;/th&gt;      &lt;th&gt;(A, K)&lt;/th&gt;      &lt;th&gt;(A, L)&lt;/th&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;th&gt;(Y, M)&lt;/th&gt;      &lt;th&gt;(Y, N)&lt;/th&gt;      &lt;th&gt;(Y, P)&lt;/th&gt;      &lt;th&gt;(Y, Q)&lt;/th&gt;      &lt;th&gt;(Y, R)&lt;/th&gt;      &lt;th&gt;(Y, S)&lt;/th&gt;      &lt;th&gt;(Y, T)&lt;/th&gt;      &lt;th&gt;(Y, V)&lt;/th&gt;      &lt;th&gt;(Y, W)&lt;/th&gt;      &lt;th&gt;(Y, Y)&lt;/th&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;M7MCX3&lt;/th&gt;      &lt;td&gt;0.020180&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.009635&lt;/td&gt;      &lt;td&gt;0.013529&lt;/td&gt;      &lt;td&gt;0.009360&lt;/td&gt;      &lt;td&gt;0.003205&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.002226&lt;/td&gt;      &lt;td&gt;0.000379&lt;/td&gt;      &lt;td&gt;0.021703&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.009196&lt;/td&gt;      &lt;td&gt;0.007964&lt;/td&gt;      &lt;td&gt;0.036788&lt;/td&gt;      &lt;td&gt;0.000195&lt;/td&gt;      &lt;td&gt;0.001513&lt;/td&gt;      &lt;td&gt;0.020665&lt;/td&gt;      &lt;td&gt;0.000542&lt;/td&gt;      &lt;td&gt;0.007479&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.010419&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;K6PL84&lt;/th&gt;      &lt;td&gt;0.001604&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.012637&lt;/td&gt;      &lt;td&gt;0.006323&lt;/td&gt;      &lt;td&gt;0.006224&lt;/td&gt;      &lt;td&gt;0.004819&lt;/td&gt;      &lt;td&gt;3.560677e-03&lt;/td&gt;      &lt;td&gt;0.001124&lt;/td&gt;      &lt;td&gt;0.012136&lt;/td&gt;      &lt;td&gt;0.018427&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.135335&lt;/td&gt;      &lt;td&gt;0.006568&lt;/td&gt;      &lt;td&gt;0.038901&lt;/td&gt;      &lt;td&gt;0.011298&lt;/td&gt;      &lt;td&gt;0.012578&lt;/td&gt;      &lt;td&gt;0.009913&lt;/td&gt;      &lt;td&gt;0.001079&lt;/td&gt;      &lt;td&gt;0.000023&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.007728&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;R4W5V3&lt;/th&gt;      &lt;td&gt;0.012448&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008408&lt;/td&gt;      &lt;td&gt;0.016363&lt;/td&gt;      &lt;td&gt;0.027469&lt;/td&gt;      &lt;td&gt;0.003205&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.004249&lt;/td&gt;      &lt;td&gt;0.013013&lt;/td&gt;      &lt;td&gt;0.031118&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.008114&lt;/td&gt;      &lt;td&gt;0.007128&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000203&lt;/td&gt;      &lt;td&gt;0.001757&lt;/td&gt;      &lt;td&gt;0.022736&lt;/td&gt;      &lt;td&gt;0.000249&lt;/td&gt;      &lt;td&gt;0.012652&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008533&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;T2A126&lt;/th&gt;      &lt;td&gt;0.010545&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.012560&lt;/td&gt;      &lt;td&gt;0.014212&lt;/td&gt;      &lt;td&gt;0.013728&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.007223&lt;/td&gt;      &lt;td&gt;0.000309&lt;/td&gt;      &lt;td&gt;0.028531&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000325&lt;/td&gt;      &lt;td&gt;0.009669&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.003182&lt;/td&gt;      &lt;td&gt;0.001904&lt;/td&gt;      &lt;td&gt;0.015607&lt;/td&gt;      &lt;td&gt;0.000577&lt;/td&gt;      &lt;td&gt;0.007479&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008648&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;L0SHD5&lt;/th&gt;      &lt;td&gt;0.020180&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.008628&lt;/td&gt;      &lt;td&gt;0.015033&lt;/td&gt;      &lt;td&gt;0.009360&lt;/td&gt;      &lt;td&gt;0.003205&lt;/td&gt;      &lt;td&gt;2.944887e-10&lt;/td&gt;      &lt;td&gt;0.002226&lt;/td&gt;      &lt;td&gt;0.000379&lt;/td&gt;      &lt;td&gt;0.021703&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.009196&lt;/td&gt;      &lt;td&gt;0.007964&lt;/td&gt;      &lt;td&gt;0.036788&lt;/td&gt;      &lt;td&gt;0.000195&lt;/td&gt;      &lt;td&gt;0.001513&lt;/td&gt;      &lt;td&gt;0.020665&lt;/td&gt;      &lt;td&gt;0.000542&lt;/td&gt;      &lt;td&gt;0.007479&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.010419&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;A0A081R612&lt;/th&gt;      &lt;td&gt;0.014805&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.004159&lt;/td&gt;      &lt;td&gt;0.017541&lt;/td&gt;      &lt;td&gt;0.012701&lt;/td&gt;      &lt;td&gt;0.013099&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.017043&lt;/td&gt;      &lt;td&gt;0.004732&lt;/td&gt;      &lt;td&gt;0.014904&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;A0A081QQM2&lt;/th&gt;      &lt;td&gt;0.010774&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.004283&lt;/td&gt;      &lt;td&gt;0.014732&lt;/td&gt;      &lt;td&gt;0.014340&lt;/td&gt;      &lt;td&gt;0.014846&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.016806&lt;/td&gt;      &lt;td&gt;0.005406&lt;/td&gt;      &lt;td&gt;0.014083&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;J1A517&lt;/th&gt;      &lt;td&gt;0.010774&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.004283&lt;/td&gt;      &lt;td&gt;0.014732&lt;/td&gt;      &lt;td&gt;0.014340&lt;/td&gt;      &lt;td&gt;0.014846&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.014500&lt;/td&gt;      &lt;td&gt;0.005406&lt;/td&gt;      &lt;td&gt;0.014083&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;F5U1T6&lt;/th&gt;      &lt;td&gt;0.015209&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.005175&lt;/td&gt;      &lt;td&gt;0.023888&lt;/td&gt;      &lt;td&gt;0.011410&lt;/td&gt;      &lt;td&gt;0.011510&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.021145&lt;/td&gt;      &lt;td&gt;0.009280&lt;/td&gt;      &lt;td&gt;0.017466&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;J3A2T7&lt;/th&gt;      &lt;td&gt;0.005240&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.012301&lt;/td&gt;      &lt;td&gt;0.013178&lt;/td&gt;      &lt;td&gt;0.014744&lt;/td&gt;      &lt;td&gt;0.014705&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000981&lt;/td&gt;      &lt;td&gt;0.007957&lt;/td&gt;      &lt;td&gt;0.017112&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.0&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;2112 rows × 400 columns&lt;/p&gt;&lt;/div&gt;We perform PCA on the sequence embeddings and then do kmeans clustering.```pythonpca = PCA(n_components=2)pca.fit(sgtembedding_df)X=pca.transform(sgtembedding_df)print(np.sum(pca.explained_variance_ratio_))df = pd.DataFrame(data=X, columns=['x1', 'x2'])df.head()```    0.6432744907364981&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;x1&lt;/th&gt;      &lt;th&gt;x2&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;0.384913&lt;/td&gt;      &lt;td&gt;-0.269873&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;0.022764&lt;/td&gt;      &lt;td&gt;0.135995&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2&lt;/th&gt;      &lt;td&gt;0.177792&lt;/td&gt;      &lt;td&gt;-0.172454&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;3&lt;/th&gt;      &lt;td&gt;0.168074&lt;/td&gt;      &lt;td&gt;-0.147334&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;4&lt;/th&gt;      &lt;td&gt;0.383616&lt;/td&gt;      &lt;td&gt;-0.271163&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```pythonkmeans = KMeans(n_clusters=3, max_iter =300)kmeans.fit(df)labels = kmeans.predict(df)centroids = kmeans.cluster_centers_fig = plt.figure(figsize=(5, 5))colmap = {1: 'r', 2: 'g', 3: 'b'}colors = list(map(lambda x: colmap[x+1], labels))plt.scatter(df['x1'], df['x2'], color=colors, alpha=0.5, edgecolor=colors)```    &lt;matplotlib.collections.PathCollection at 0x14c5a77f0&gt;![png](output_23_1.png)## &lt;a name=&quot;sequence-classification&quot;&gt;&lt;/a&gt; Sequence Classification using Deep Learning in TensorFlowThe protein data set used above is also labeled. The labels represent the protein functions. Similarly, there are other labeled sequence data sets. For example, DARPA shared an intrusion weblog data set. It contains weblog sequences with positive labels if the log represents a network intrusion.In such problems supervised learning is employed. Classification is a supervised learning we will demonstrate here.### Protein Sequence ClassificationThe data set is taken from https://www.uniprot.org . The protein sequences in the data set have one of the two functions,- Binds to DNA and alters its conformation. May be involved in regulation of gene expression, nucleoid organization and DNA protection.- Might take part in the signal recognition particle (SRP) pathway. This is inferred from the conservation of its genetic proximity to ftsY/ffh. May be a regulatory protein.There are a total of 2113 samples. The sequence lengths vary between 100-700.```python# Loading datadata = pd.read_csv('data/protein_classification.csv')# Data preprocessingy = data['Function [CC]']encoder = LabelEncoder()encoder.fit(y)encoded_y = encoder.transform(y)corpus = data.loc[:,['Entry','Sequence']]corpus.columns = ['id', 'sequence']corpus['sequence'] = corpus['sequence'].map(list)```#### Sequence embeddings```python# Sequence embeddingsgt_ = SGT(kappa=1,            lengthsensitive=False,            mode='multiprocessing')sgtembedding_df = sgt_.fit_transform(corpus)X = sgtembedding_df.set_index('id')```    INFO: Pandarallel will run on 7 workers.    INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.We will perform a 10-fold cross-validation to measure the performance of the classification model.```pythonkfold = 10X = Xy = encoded_yrandom_state = 1test_F1 = np.zeros(kfold)skf = KFold(n_splits = kfold, shuffle = True, random_state = random_state)k = 0epochs = 50batch_size = 128for train_index, test_index in skf.split(X, y):    X_train, X_test = X.iloc[train_index], X.iloc[test_index]    y_train, y_test = y[train_index], y[test_index]    model = Sequential()    model.add(Dense(64, input_shape = (X_train.shape[1],)))     model.add(Activation('relu'))    model.add(Dropout(0.5))    model.add(Dense(32))    model.add(Activation('relu'))    model.add(Dropout(0.5))    model.add(Dense(1))    model.add(Activation('sigmoid'))    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=0)    y_pred = model.predict_proba(X_test).round().astype(int)    y_train_pred = model.predict_proba(X_train).round().astype(int)    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)    k+=1print ('Average f1 score', np.mean(test_F1))```    Average f1 score 1.0### Weblog Classification for Intrusion DetectionThis data sample is taken from https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset. This is a network intrusion data containing audit logs and any attack as a positive label. Since, network intrusion is a rare event, the data is unbalanced. Here we will,- build a sequence classification model to predict a network intrusion.Each sequence contains in the data is a series of activity, for example, {login, password}. The _alphabets_ in the input data sequences are already encoded into integers. The original sequences data file is also present in the `/data` directory.```python# Loading datadata = pd.read_csv('data/darpa_data.csv')data.columns```    Index(['timeduration', 'seqlen', 'seq', 'class'], dtype='object')```pythondata['id'] = data.index``````python# Data preprocessingy = data['class']encoder = LabelEncoder()encoder.fit(y)encoded_y = encoder.transform(y)corpus = data.loc[:,['id','seq']]corpus.columns = ['id', 'sequence']corpus['sequence'] = corpus['sequence'].map(list)```#### Sequence embeddingsIn this data, the sequence embeddings should be **length-sensitive**. The lengths are important here because sequences with similar patterns but different lengths can have different labels. Consider a simple example of two sessions: `{login, pswd, login, pswd,...}` and `{login, pswd,...(repeated several times)..., login, pswd}`. While the first session can be a regular user mistyping the password once, the other session is possibly an attack to guess the password. Thus, the sequence lengths are as important as the patterns.Therefore, `lengthsensitive=True` is used here.```python# Sequence embeddingsgt_ = SGT(kappa=5,            lengthsensitive=True,            mode='multiprocessing')sgtembedding_df = sgt_.fit_transform(corpus)sgtembedding_df = sgtembedding_df.set_index('id')sgtembedding_df```    INFO: Pandarallel will run on 7 workers.    INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;(0, 0)&lt;/th&gt;      &lt;th&gt;(0, 1)&lt;/th&gt;      &lt;th&gt;(0, 2)&lt;/th&gt;      &lt;th&gt;(0, 3)&lt;/th&gt;      &lt;th&gt;(0, 4)&lt;/th&gt;      &lt;th&gt;(0, 5)&lt;/th&gt;      &lt;th&gt;(0, 6)&lt;/th&gt;      &lt;th&gt;(0, 7)&lt;/th&gt;      &lt;th&gt;(0, 8)&lt;/th&gt;      &lt;th&gt;(0, 9)&lt;/th&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;th&gt;(~, 1)&lt;/th&gt;      &lt;th&gt;(~, 2)&lt;/th&gt;      &lt;th&gt;(~, 3)&lt;/th&gt;      &lt;th&gt;(~, 4)&lt;/th&gt;      &lt;th&gt;(~, 5)&lt;/th&gt;      &lt;th&gt;(~, 6)&lt;/th&gt;      &lt;th&gt;(~, 7)&lt;/th&gt;      &lt;th&gt;(~, 8)&lt;/th&gt;      &lt;th&gt;(~, 9)&lt;/th&gt;      &lt;th&gt;(~, ~)&lt;/th&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0.0&lt;/th&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.485034&lt;/td&gt;      &lt;td&gt;0.486999&lt;/td&gt;      &lt;td&gt;0.485802&lt;/td&gt;      &lt;td&gt;0.483097&lt;/td&gt;      &lt;td&gt;0.483956&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.178609&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1.0&lt;/th&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.025622&lt;/td&gt;      &lt;td&gt;0.228156&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;1.310714e-09&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.447620&lt;/td&gt;      &lt;td&gt;0.452097&lt;/td&gt;      &lt;td&gt;0.464568&lt;/td&gt;      &lt;td&gt;0.367296&lt;/td&gt;      &lt;td&gt;0.525141&lt;/td&gt;      &lt;td&gt;0.455018&lt;/td&gt;      &lt;td&gt;0.374364&lt;/td&gt;      &lt;td&gt;0.414081&lt;/td&gt;      &lt;td&gt;0.549981&lt;/td&gt;      &lt;td&gt;0.172479&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2.0&lt;/th&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.525605&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.193359&lt;/td&gt;      &lt;td&gt;0.071469&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;3.0&lt;/th&gt;      &lt;td&gt;0.077999&lt;/td&gt;      &lt;td&gt;0.208974&lt;/td&gt;      &lt;td&gt;0.230338&lt;/td&gt;      &lt;td&gt;1.830519e-01&lt;/td&gt;      &lt;td&gt;1.200926e-17&lt;/td&gt;      &lt;td&gt;1.696880e-01&lt;/td&gt;      &lt;td&gt;0.093646&lt;/td&gt;      &lt;td&gt;7.985870e-02&lt;/td&gt;      &lt;td&gt;2.896813e-05&lt;/td&gt;      &lt;td&gt;3.701710e-05&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.474072&lt;/td&gt;      &lt;td&gt;0.468353&lt;/td&gt;      &lt;td&gt;0.463594&lt;/td&gt;      &lt;td&gt;0.177507&lt;/td&gt;      &lt;td&gt;0.551270&lt;/td&gt;      &lt;td&gt;0.418652&lt;/td&gt;      &lt;td&gt;0.309652&lt;/td&gt;      &lt;td&gt;0.384657&lt;/td&gt;      &lt;td&gt;0.378225&lt;/td&gt;      &lt;td&gt;0.170362&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;4.0&lt;/th&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.023695&lt;/td&gt;      &lt;td&gt;0.217819&lt;/td&gt;      &lt;td&gt;2.188276e-33&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;6.075992e-11&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;5.681668e-39&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.464120&lt;/td&gt;      &lt;td&gt;0.468229&lt;/td&gt;      &lt;td&gt;0.452170&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.501242&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.300534&lt;/td&gt;      &lt;td&gt;0.161961&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.167082&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;...&lt;/th&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;106.0&lt;/th&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.024495&lt;/td&gt;      &lt;td&gt;0.219929&lt;/td&gt;      &lt;td&gt;2.035190e-17&lt;/td&gt;      &lt;td&gt;1.073271e-18&lt;/td&gt;      &lt;td&gt;5.656994e-11&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;5.047380e-29&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.502213&lt;/td&gt;      &lt;td&gt;0.544343&lt;/td&gt;      &lt;td&gt;0.477281&lt;/td&gt;      &lt;td&gt;0.175901&lt;/td&gt;      &lt;td&gt;0.461103&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.162796&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.167687&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;107.0&lt;/th&gt;      &lt;td&gt;0.110422&lt;/td&gt;      &lt;td&gt;0.227478&lt;/td&gt;      &lt;td&gt;0.217549&lt;/td&gt;      &lt;td&gt;1.723963e-01&lt;/td&gt;      &lt;td&gt;1.033292e-14&lt;/td&gt;      &lt;td&gt;3.896725e-07&lt;/td&gt;      &lt;td&gt;0.083685&lt;/td&gt;      &lt;td&gt;2.940589e-08&lt;/td&gt;      &lt;td&gt;8.864072e-02&lt;/td&gt;      &lt;td&gt;4.813990e-29&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.490398&lt;/td&gt;      &lt;td&gt;0.522016&lt;/td&gt;      &lt;td&gt;0.466808&lt;/td&gt;      &lt;td&gt;0.470603&lt;/td&gt;      &lt;td&gt;0.479795&lt;/td&gt;      &lt;td&gt;0.480057&lt;/td&gt;      &lt;td&gt;0.194888&lt;/td&gt;      &lt;td&gt;0.172397&lt;/td&gt;      &lt;td&gt;0.164873&lt;/td&gt;      &lt;td&gt;0.172271&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;108.0&lt;/th&gt;      &lt;td&gt;0.005646&lt;/td&gt;      &lt;td&gt;0.202424&lt;/td&gt;      &lt;td&gt;0.196786&lt;/td&gt;      &lt;td&gt;2.281242e-01&lt;/td&gt;      &lt;td&gt;1.133936e-01&lt;/td&gt;      &lt;td&gt;1.862098e-01&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;1.212869e-01&lt;/td&gt;      &lt;td&gt;9.180520e-08&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.432834&lt;/td&gt;      &lt;td&gt;0.434953&lt;/td&gt;      &lt;td&gt;0.439615&lt;/td&gt;      &lt;td&gt;0.390864&lt;/td&gt;      &lt;td&gt;0.481764&lt;/td&gt;      &lt;td&gt;0.600875&lt;/td&gt;      &lt;td&gt;0.166766&lt;/td&gt;      &lt;td&gt;0.165368&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.171729&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;109.0&lt;/th&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.025616&lt;/td&gt;      &lt;td&gt;0.238176&lt;/td&gt;      &lt;td&gt;3.889176e-55&lt;/td&gt;      &lt;td&gt;1.332427e-60&lt;/td&gt;      &lt;td&gt;1.408003e-09&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;9.845377e-60&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.421318&lt;/td&gt;      &lt;td&gt;0.439985&lt;/td&gt;      &lt;td&gt;0.467953&lt;/td&gt;      &lt;td&gt;0.440951&lt;/td&gt;      &lt;td&gt;0.527165&lt;/td&gt;      &lt;td&gt;0.864717&lt;/td&gt;      &lt;td&gt;0.407155&lt;/td&gt;      &lt;td&gt;0.399335&lt;/td&gt;      &lt;td&gt;0.251304&lt;/td&gt;      &lt;td&gt;0.171885&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;110.0&lt;/th&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.022868&lt;/td&gt;      &lt;td&gt;0.203513&lt;/td&gt;      &lt;td&gt;9.273472e-64&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;1.240870e-09&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;0.000000e+00&lt;/td&gt;      &lt;td&gt;...&lt;/td&gt;      &lt;td&gt;0.478090&lt;/td&gt;      &lt;td&gt;0.454871&lt;/td&gt;      &lt;td&gt;0.459109&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.490534&lt;/td&gt;      &lt;td&gt;0.370357&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.162997&lt;/td&gt;      &lt;td&gt;0.000000&lt;/td&gt;      &lt;td&gt;0.162089&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;111 rows × 121 columns&lt;/p&gt;&lt;/div&gt;#### Applying PCA on the embeddingsThe embeddings are sparse and high-dimensional. PCA is, therefore, applied for dimension reduction.```pythonfrom sklearn.decomposition import PCApca = PCA(n_components=35)pca.fit(sgtembedding_df)X = pca.transform(sgtembedding_df)print(np.sum(pca.explained_variance_ratio_))```    0.9962446146783123#### Building a Multi-Layer Perceptron ClassifierThe PCA transforms of the embeddings are used directly as inputs to an MLP classifier.```pythonkfold = 3random_state = 11X = Xy = encoded_ytest_F1 = np.zeros(kfold)time_k = np.zeros(kfold)skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)k = 0epochs = 300batch_size = 15# class_weight = {0 : 1., 1: 1.,}  # The weights can be changed and made inversely proportional to the class size to improve the accuracy.class_weight = {0 : 0.12, 1: 0.88,}for train_index, test_index in skf.split(X, y):    X_train, X_test = X[train_index], X[test_index]    y_train, y_test = y[train_index], y[test_index]    model = Sequential()    model.add(Dense(128, input_shape=(X_train.shape[1],)))     model.add(Activation('relu'))    model.add(Dropout(0.5))    model.add(Dense(1))    model.add(Activation('sigmoid'))    model.summary()    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])    start_time = time.time()    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weight)    end_time = time.time()    time_k[k] = end_time-start_time    y_pred = model.predict_proba(X_test).round().astype(int)    y_train_pred = model.predict_proba(X_train).round().astype(int)    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)    k += 1```    Model: &quot;sequential_10&quot;    _________________________________________________________________    Layer (type)                 Output Shape              Param #       =================================================================    dense_30 (Dense)             (None, 128)               4608          _________________________________________________________________    activation_30 (Activation)   (None, 128)               0             _________________________________________________________________    dropout_20 (Dropout)         (None, 128)               0             _________________________________________________________________    dense_31 (Dense)             (None, 1)                 129           _________________________________________________________________    activation_31 (Activation)   (None, 1)                 0             =================================================================    Total params: 4,737    Trainable params: 4,737    Non-trainable params: 0    _________________________________________________________________    WARNING:tensorflow:sample_weight modes were coerced from      ...        to        ['...']    Train on 74 samples    Epoch 1/300    74/74 [==============================] - 0s 7ms/sample - loss: 0.1487 - accuracy: 0.5270    Epoch 2/300    74/74 [==============================] - 0s 120us/sample - loss: 0.1421 - accuracy: 0.5000    ...    74/74 [==============================] - 0s 118us/sample - loss: 0.0299 - accuracy: 0.8784    Epoch 300/300    74/74 [==============================] - 0s 133us/sample - loss: 0.0296 - accuracy: 0.8649```pythonprint ('Average f1 score', np.mean(test_F1))print ('Average Run time', np.mean(time_k))```    Average f1 score 0.6341880341880342    Average Run time 3.880180994669596#### Building an LSTM Classifier on the sequences for comparisonWe built an LSTM Classifier on the sequences to compare the accuracy.```pythonX = data['seq']encoded_X = np.ndarray(shape=(len(X),), dtype=list)for i in range(0,len(X)):    encoded_X[i]=X.iloc[i].split(&quot;~&quot;)X```    0      1~2~3~3~3~3~3~3~1~4~5~1~2~3~3~3~3~3~3~1~4~5~1~...    1      6~5~5~6~5~6~5~2~5~5~5~5~5~5~5~5~5~5~5~5~5~5~5~...    2      19~19~19~19~19~19~19~19~19~19~19~19~19~19~19~1...    3      6~5~5~6~5~6~5~2~5~5~5~5~5~5~5~5~5~5~5~5~5~5~5~...    4      5~5~17~5~5~5~5~5~10~2~11~2~11~11~12~11~11~5~2~...                                 ...                            106    10~2~11~2~11~11~12~11~11~5~2~11~5~2~5~2~3~14~3...    107    5~5~2~5~17~6~5~6~5~5~2~6~17~3~2~2~3~5~2~3~5~6~...    108    6~5~6~5~5~6~5~5~6~6~6~6~6~6~6~6~6~6~6~6~6~6~6~...    109    6~5~5~6~5~6~5~2~38~2~3~5~22~39~5~5~5~5~5~5~5~5...    110    5~6~5~5~10~2~11~2~11~11~12~11~5~2~11~11~12~11~...    Name: seq, Length: 111, dtype: object```pythonmax_seq_length = np.max(data['seqlen'])encoded_X = tf.keras.preprocessing.sequence.pad_sequences(encoded_X, maxlen=max_seq_length)``````pythonkfold = 3random_state = 11test_F1 = np.zeros(kfold)time_k = np.zeros(kfold)epochs = 50batch_size = 15skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)k = 0for train_index, test_index in skf.split(encoded_X, y):    X_train, X_test = encoded_X[train_index], encoded_X[test_index]    y_train, y_test = y[train_index], y[test_index]    embedding_vecor_length = 32    top_words=50    model = Sequential()    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))    model.add(LSTM(32))    model.add(Dense(1))    model.add(Activation('sigmoid'))    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])    model.summary()    start_time = time.time()    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)    end_time=time.time()    time_k[k]=end_time-start_time    y_pred = model.predict_proba(X_test).round().astype(int)    y_train_pred=model.predict_proba(X_train).round().astype(int)    test_F1[k]=sklearn.metrics.f1_score(y_test, y_pred)    k+=1```    Model: &quot;sequential_13&quot;    _________________________________________________________________    Layer (type)                 Output Shape              Param #       =================================================================    embedding (Embedding)        (None, 1773, 32)          1600          _________________________________________________________________    lstm (LSTM)                  (None, 32)                8320          _________________________________________________________________    dense_36 (Dense)             (None, 1)                 33            _________________________________________________________________    activation_36 (Activation)   (None, 1)                 0             =================================================================    Total params: 9,953    Trainable params: 9,953    Non-trainable params: 0    _________________________________________________________________    Train on 74 samples    Epoch 1/50    74/74 [==============================] - 5s 72ms/sample - loss: 0.6894 - accuracy: 0.5676    Epoch 2/50    74/74 [==============================] - 4s 48ms/sample - loss: 0.6590 - accuracy: 0.8784    ...    Epoch 50/50    74/74 [==============================] - 4s 51ms/sample - loss: 0.1596 - accuracy: 0.9324```pythonprint ('Average f1 score', np.mean(test_F1))print ('Average Run time', np.mean(time_k))```    Average f1 score 0.36111111111111116    Average Run time 192.46954011917114We find that the LSTM classifier gives a significantly lower F1 score. This may be improved by changing the model. However, we find that the SGT embedding could work with a small and unbalanced data without the need of a complicated classifier model.LSTM models typically require more data for training and also has significantly more computation time. The LSTM model above took 425.6 secs while the MLP model took just 9.1 secs.## &lt;a name=&quot;sequence-search&quot;&gt;&lt;/a&gt; Sequence SearchSequence data sets are generally large. For example, sequences of listening history in music streaming services, such as Pandora, for more than 70M users are huge. In protein data bases there could be even larger size. For instance, the Uniprot data repository has more than 177M sequences.Searching for similar sequences in such large data bases is challenging. SGT embedding provides a simple solution. In the following it will be shown on a protein data set that SGT embedding can be used to compute similarity between a query sequence and the sequence corpus using a dot product. The sequences with the highest dot product are returned as the most similar sequence to the query.### Protein Sequence SearchIn the following, a sample of 10k protein sequences are used for illustration. The data is taken from https://www.uniprot.org .```python# Loading datadata = pd.read_csv('data/protein-uniprot-reviewed-Ano-10k.tab', sep='\t')# Data preprocessingcorpus = data.loc[:,['Entry','Sequence']]corpus.columns = ['id', 'sequence']corpus['sequence'] = corpus['sequence'].map(list)corpus.head(3)```&lt;div&gt;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;  &lt;thead&gt;    &lt;tr style=&quot;text-align: right;&quot;&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;sequence&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;I2WKR6&lt;/td&gt;      &lt;td&gt;[M, V, H, K, S, D, S, D, E, L, A, A, L, R, A, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;A0A2A6M8K9&lt;/td&gt;      &lt;td&gt;[M, Q, E, S, L, V, V, R, R, E, T, H, I, A, A, ...&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2&lt;/th&gt;      &lt;td&gt;A0A3G5KEC3&lt;/td&gt;      &lt;td&gt;[M, A, S, G, A, Y, S, K, Y, L, F, Q, I, I, G, ...&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;```python# Protein sequence alphabetsalphabets = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K',              'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V',              'W', 'X', 'Y', 'U', 'O']  # List of amino acids# Alphabets are known and inputted # as arguments for faster computationsgt_ = SGT(alphabets=alphabets,            lengthsensitive=True,            kappa=1,            flatten=True,            mode='multiprocessing')sgtembedding_df = sgt_.fit_transform(corpus)sgtembedding_df = sgtembedding_df.set_index('id')```    INFO: Pandarallel will run on 7 workers.    INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.```python'''Search proteins similar to a query protein.The approach is to find the SGT embedding of thequery protein and find its similarity with theembeddings of the protein database.'''query_protein = 'MSHVFPIVIDDNFLSPQDLVSAARSGCSLRLHTGVVDKIDRAHRFVLEIAGAEALHYGINTGFGSLCTTHIDPADLSTLQHNLLKSHACGVGPTVSEEVSRVVTLIKLLTFRTGNSGVSLSTVNRIIDLWNHGVVGAIAQKGTVGASGDLAPLAHLFLPLIGLGQVWHRGVLRPSREVMDELKLAPLTLQPKDGLCLTNGVQYLNAWGALSTVRAKRLVALADLCAAMSMMGFSAARSFIEAQIHQTSLHPERGHVALHLRTLTHGSNHADLPHCNPAMEDPYSFRCAPQVHGAARQVVGYLETVIGNECNSVSDNPLVFPDTRQILTCGNLHGQSTAFALDFAAIGITDLSNISERRTYQLLSGQNGLPGFLVAKPGLNSGFMVVQYTSAALLNENKVLSNPASVDTIPTCHLQEDHVSMGGTSAYKLQTILDNCETILAIELMTACQAIDMNPGLQLSERGRAIYEAVREEIPFVKEDHLMAGLISKSRDLCQHSTVIAQQLAEMQAQ'# Step 1. Compute sgt embedding for the query protein.query_protein_sgt_embedding = sgt_.fit(list(query_protein))# Step 2. Compute the dot product of query embedding # with the protein embedding database.similarity = sgtembedding_df.dot(query_protein_sgt_embedding)# Step 3. Return the top k protein names based on similarity.similarity.sort_values(ascending=False)```    id    K0ZGN5        2773.749663    A0A0Y1CPH7    1617.451379    A0A5R8LCJ1    1566.833152    A0A290WY40    1448.772820    A0A073K6N6    1392.267250                     ...         A0A1S7UBK4     160.074989    A0A2S7T1R9     156.580584    A0A0E0UQV6     155.834932    A0A1Y5Y0S0     148.862049    B0NRP3         117.656497    Length: 10063, dtype: float64## &lt;a name=&quot;sgt-spark&quot;&gt;&lt;/a&gt; SGT - Spark for Distributed ComputingAs mentioned in the previous section, sequence data sets can be large. SGT complexity is linear with the number of sequences in a data set. Still if the data size is large the computation becomes high. For example, for a set of 1M protein sequences the default SGT mode takes over 24 hours.Using distributed computing with Spark the runtime can be significantly reduced. For instance, SGT-Spark on the same 1M protein data set took less than 29 minutes.In the following, Spark implementation for SGT is shown. First, it is applied on a smaller 10k data set for comparison. Then it is applied on 1M data set without any syntactical change.```python'''Load the data and remove header.'''data = sc.textFile('data/protein-uniprot-reviewed-Ano-10k.tab')header = data.first() #extract headerdata = data.filter(lambda row: row != header)   #filter out headerdata.take(1)  # See one sample```&lt;div class=&quot;ansiout&quot;&gt;&lt;span class=&quot;ansired&quot;&gt;Out[&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;]: &lt;/span&gt;[&amp;apos;I2WKR6\tI2WKR6_ECOLX\tunreviewed\tType III restriction enzyme, res subunit (EC 3.1.21.5)\tEC90111_4246\tEscherichia coli 9.0111\t786\tMVHKSDSDELAALRAENVRLVSLLEAHGIEWRRKPQSPVPRVSVLSTNEKVALFRRLFRGRDDVWALRWESKTSGKSGYSPACANEWQLGICGKPRIKCGDCAHRQLIPVSDLVIYHHLAGTHTAGMYPLLEDDSCYFLAVDFDEAEWQKDASAFMRSCDELGVPAALEISRSRQGAHVWIFFASRVSAREARRLGTAIISYTCSRTRQLRLGSYDRLFPNQDTMPKGGFGNLIALPLQKRPRELGGSVFVDMNLQPYPDQWAFLVSVIPMNVQDIEPTILRATGSIHPLDVNFINEEDLGTPWEEKKSSGNRLNIAVTEPLIITLANQIYFEKAQLPQALVNRLIRLAAFPNPEFYKAQAMRMSVWNKPRVIGCAENYPQHIALPRGCLDSALSFLRYNNIAAELIDKRFAGTECNAVFTGNLRAEQEEAVSALLRYDTGVLCAPTAFGKTVTAAAVIARRKVNTLILVHRTELLKQWQERLAVFLQVGDSIGIIGGGKHKPCGNIDIAVVQSISRHGEVEPLVRNYGQIIVDECHHIGAVSFSAILKETNARYLLGLTATPIRRDGLHPIIFMYCGAIRHTAARPKESLHNLEVLTRSRFTSGHLPSDARIQDIFREIALDHDRTVAIAEEAMKAFGQGRKVLVLTERTDHLDDIASVMNTLKLSPFVLHSRLSKKKRTMLISGLNALPPDSPRILLSTGRLIGEGFDHPPLDTLILAMPVSWKGTLQQYAGRLHREHTGKSDVRIIDFVDTAYPVLLRMWDKRQRGYKAMGYRIVADGEGLSF&amp;apos;]&lt;/div&gt;```python# Repartition for increasing the parallel processesdata = data.repartition(500)``````pythondef preprocessing(line):    '''    Original data are lines where each line has \t    separated values. We are interested in preserving    the first value (entry id), tmp[0], and the last value    (the sequence), tmp[-1].    '''    tmp = line.split('\t')    id = tmp[0]    sequence = list(tmp[-1])    return (id, sequence)processeddata = data.map(lambda line: preprocessing(line))processeddata.take(1)  # See one sample```&lt;div class=&quot;ansiout&quot;&gt;&lt;span class=&quot;ansired&quot;&gt;Out[&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;ansired&quot;&gt;]: &lt;/span&gt;[(&amp;apos;A0A2E9WIJ1&amp;apos;,  [&amp;apos;M&amp;apos;,   &amp;apos;Y&amp;apos;,   &amp;apos;I&amp;apos;,   &amp;apos;F&amp;apos;,   &amp;apos;L&amp;apos;,   &amp;apos;T&amp;apos;,   &amp;apos;L&amp;apos;,...      &amp;apos;A&amp;apos;,   &amp;apos;K&amp;apos;,   &amp;apos;L&amp;apos;,   &amp;apos;D&amp;apos;,   &amp;apos;K&amp;apos;,   &amp;apos;N&amp;apos;,   &amp;apos;D&amp;apos;])]&lt;/div&gt;```python# Protein sequence alphabetsalphabets = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K',              'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V',              'W', 'X', 'Y', 'U', 'O']  # List of amino acids``````python'''Spark approach.In this approach the alphabets argument has tobe passed to the SGT class definition.The SGT.fit() is then called in parallel.'''sgt_ = sgt.SGT(alphabets=alphabets,                kappa=1,                lengthsensitive=True,                flatten=True)rdd = processeddata.map(lambda x: (x[0], list(sgt_.fit(x[1]))))sgtembeddings = rdd.collect()# Command took 29.66 seconds -- by cranjan@processminer.com at 4/22/2020, 12:31:23 PM on databricks```### Compare with the default SGT mode```python# Loading datadata = pd.read_csv('data/protein-uniprot-reviewed-Ano-10k.tab', sep='\t')# Data preprocessingcorpus = data.loc[:,['Entry','Sequence']]corpus.columns = ['id', 'sequence']corpus['sequence'] = corpus['sequence'].map(list)``````pythonsgt_ = sgt.SGT(alphabets=alphabets,                lengthsensitive=True,                kappa=1,                flatten=True,                mode='default')sgtembedding_df = sgt_.fit_transform(corpus)# Command took 13.08 minutes -- by cranjan@processminer.com at 4/22/2020, 1:48:02 PM on databricks```### 1M Protein DatabaseProtein 1M sequence data set is embedded here. The data set is available [here](https://mega.nz/file/1qAXhSAS#l7E60cLJzMGtFQzeHZL9PI8yX4tRQcAMFRN2xeHK81w).```python'''Load the data and remove header.'''data = sc.textFile('data/protein-uniprot-reviewed-Ano-1M.tab')header = data.first() #extract headerdata = data.filter(lambda row: row != header)   #filter out headerdata.take(1)  # See one sample``````python# Repartition for increasing the parallel processesdata = data.repartition(10000)``````pythonprocesseddata = data.map(lambda line: preprocessing(line))processeddata.take(1)  # See one sample# [('A0A2E9WIJ1',#   ['M','Y','I','F','L','T','L','A','L','F','S',...,'F','S','I','F','A','K','L','D','K','N','D'])]``````python# Protein sequence alphabetsalphabets = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K',              'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V',              'W', 'X', 'Y', 'U', 'O']  # List of amino acids``````python'''Spark approach.In this approach the alphabets argument has tobe passed to the SGT class definition.The SGT.fit() is then called in parallel.'''sgt_ = sgt.SGT(alphabets=alphabets,                kappa=1,                lengthsensitive=True,                flatten=True)rdd = processeddata.map(lambda x: (x[0], list(sgt_.fit(x[1]))))sgtembeddings = rdd.collect()# Command took 28.98 minutes -- by cranjan@processminer.com at 4/22/2020, 3:16:41 PM on databricks``````python'''OPTIONAL.Save the embeddings for future use or production deployment.'''# Save for deployment# pickle.dump(sgtembeddings, #             open(&quot;data/protein-sgt-embeddings-1M.pkl&quot;, &quot;wb&quot;))# The pickle dump is shared at https://mega.nz/file/hiAxAAoI#SStAIn_FZjAHvXSpXfdy8VpISG6rusHRf9HlUSqwcsw# sgtembeddings = pickle.load(open(&quot;data/protein-sgt-embeddings-1M.pkl&quot;, &quot;rb&quot;))```The pickle dump is shared [here](https://mega.nz/file/hiAxAAoI#SStAIn_FZjAHvXSpXfdy8VpISG6rusHRf9HlUSqwcsw).### Sequence Search using SGT - SparkSince `sgtembeddings` on the 1M data set is large it is recommended to use distributed computing to find similar proteins during a search.```pythonsgtembeddings_rdd = sc.parallelize(list(dict(sgtembeddings).items()))sgtembeddings_rdd = sgtembeddings_rdd.repartition(10000)``````python'''Search proteins similar to a query protein.The approach is to find the SGT embedding of thequery protein and find its similarity with theembeddings of the protein database.'''query_protein = 'MSHVFPIVIDDNFLSPQDLVSAARSGCSLRLHTGVVDKIDRAHRFVLEIAGAEALHYGINTGFGSLCTTHIDPADLSTLQHNLLKSHACGVGPTVSEEVSRVVTLIKLLTFRTGNSGVSLSTVNRIIDLWNHGVVGAIAQKGTVGASGDLAPLAHLFLPLIGLGQVWHRGVLRPSREVMDELKLAPLTLQPKDGLCLTNGVQYLNAWGALSTVRAKRLVALADLCAAMSMMGFSAARSFIEAQIHQTSLHPERGHVALHLRTLTHGSNHADLPHCNPAMEDPYSFRCAPQVHGAARQVVGYLETVIGNECNSVSDNPLVFPDTRQILTCGNLHGQSTAFALDFAAIGITDLSNISERRTYQLLSGQNGLPGFLVAKPGLNSGFMVVQYTSAALLNENKVLSNPASVDTIPTCHLQEDHVSMGGTSAYKLQTILDNCETILAIELMTACQAIDMNPGLQLSERGRAIYEAVREEIPFVKEDHLMAGLISKSRDLCQHSTVIAQQLAEMQAQ'# Step 1. Compute sgt embedding for the query protein.query_protein_sgt_embedding = sgt_.fit(list(query_protein))# Step 2. Broadcast the embedding to the cluster.query_protein_sgt_embedding_broadcasted = sc.broadcast(list(query_protein_sgt_embedding))# Step 3. Compute similarity between each sequence embedding and the query.similarity = sgtembeddings_rdd.map(lambda x: (x[0],                                               np.dot(query_protein_sgt_embedding_broadcasted.value,                                                      x[1]))).collect()# Step 4. Show the most similar sequences with the query.pd.DataFrame(similarity).sort_values(by=1, ascending=False)```</longdescription>
</pkgmetadata>