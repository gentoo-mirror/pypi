<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># mlfastThis Python machine learning package is built on top of scikit-learn and provides a simple API for regression and classification modeling. The main function in this package is called Regression() and Classification() which takes in the following arguments:`X`: The independent variables (features) of the data set`y`: The dependent variable (target) of the data set`model`: The name of the regression and classification algorithm to be used (e.g. `lr` for Linear Regression, or `rf` for Random Forest Classifier, etc.)`scaler`: The name of the data scaler to be used (e.g. &quot;standard&quot; for StandardScaler, &quot;robust&quot; for RobustScaler, etc.)`cat`: A boolean indicating [`True` or `False`] whether the data set has categorical variables that need to be one-hot encoded- PYPI link for this package - [mlfast](https://pypi.org/project/mlfast/)## Getting Started### Installations**note &quot;Installation steps&quot;****First let's do an easy pip installation of the library by running the following command -**```pythonpip install mlfast```## Usage### Regression Algorithms**!!! note &quot;For Regression Modeling&quot;**    **Import Regression Model -**```pythonfrom mlfast import Regression```**Linear Regression**  -&gt; 'lr' ```pythonRegression(X, y, model = 'lr')```**Ridge Regression**  -&gt; 'ridge ' ```pythonRegression(X, y, model = 'ridge', scaler =  'standard')```**Lasso Regression**  -&gt; 'lasso' ```pythonRegression(X, y, model = 'lasso', scaler =  'robust')```**ElasticNet**  -&gt; 'enet' ```pythonRegression(X, y, model = 'enet', cat=True)```**Random Forest Regressor**  -&gt; 'rf' ```pythonRegression(X, y, model = 'rf',scaler = 'standard', cat=True)```**Decision Tree Regressor**  -&gt; 'dt' ```pythonRegression(X, y, model = 'dt')```**Support Vector Machine Regression**  -&gt; 'svm ' ```pythonRegression(X, y, model = 'svm', scaler =  'standard')```**KNeighbors Regressor**  -&gt; 'knn' ```pythonRegression(X, y, model = 'knn', scaler =  'robust')```**Gradient Boosting Regressor**  -&gt; 'gb' ```pythonRegression(X, y, model = 'gb', cat=True)```**AdaBoost Regressor**  -&gt; 'ada' ```pythonRegression(X, y, model = 'ada',scaler = 'standard', cat=True)```**XGBoost Regressor**  -&gt; 'xgb' ```pythonRegression(X, y, model = 'xgb',scaler = 'standard', cat=True)```### Classification Algorithms**note &quot;For Classification Modeling&quot;****Import Classification Model -**```pythonfrom mlfast import Classification```**Logistic Regression**  -&gt; 'lr' ```pythonClassification(X, y, model = 'lr')```**Random Forest Classifier**  -&gt; 'rf' ```pythonClassification(X, y, model = 'rf',scaler = 'standard', cat=True)```**Decision Tree Classifier**  -&gt; 'dt' ```pythonClassification(X, y, model = 'dt')```**Support Vector Machine Classifier**  -&gt; 'svm ' ```pythonClassification(X, y, model = 'svm', scaler =  'standard')```**KNeighbors Classifier**  -&gt; 'knn' ```pythonClassification(X, y, model = 'knn', scaler =  'robust')```**Gradient Boosting Classifier**  -&gt; 'gb' ```pythonClassification(X, y, model = 'gb', cat=True)```**AdaBoost Classifier**  -&gt; 'ada' ```pythonClassification(X, y, model = 'ada',scaler = 'standard', cat=True)```**XGBoost Classifier**  -&gt; 'xgb'```pythonClassification(X, y, model = 'xgb',scaler = 'standard', cat=True)```### Regression hyperparameter**note &quot;For Regression Hyperparameter Modeling&quot;****Import Regression Hyperparameter Model -**```pythonfrom mlfast.Hyperparameter.Regression import Regression```**Ridge regression**```pythonhyperparams_ridge = {    &quot;alpha&quot;: [0.01, 0.1, 1.0, 10.0],    &quot;fit_intercept&quot;: [True, False],    &quot;solver&quot;: [&quot;auto&quot;, &quot;svd&quot;, &quot;cholesky&quot;, &quot;lsqr&quot;, &quot;sparse_cg&quot;, &quot;sag&quot;, &quot;saga&quot;],}Regression(X, y, model=&quot;ridge&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_ridge)```**Lasso Regression**```pythonhyperparams_lasso = {    &quot;alpha&quot;: [0.01, 0.1, 1.0, 10.0],}Regression(X, y, model=&quot;lasso&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_lasso, save_pkl=False)```**ElasticNet Regression**```pythonhyperparams_enet = {    &quot;alpha&quot;: [0.01, 0.1, 1.0, 10.0],    &quot;l1_ratio&quot;: [0.25, 0.5, 0.75],}Regression(X, y, model=&quot;enet&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_enet, save_pkl=False)```**Decision Tree Regression**```pythonhyperparams_dt = {    &quot;max_depth&quot;: [None, 5, 10, 20],    &quot;min_samples_split&quot;: [2, 5, 10],}Regression(X, y, model=&quot;dt&quot;, scaler=&quot;standard&quot;,  hyperparams=hyperparams_dt, save_pkl=False)```**Random Forest Regression**```pythonhyperparams_rf = {    &quot;n_estimators&quot;: [5, 10, 20],    &quot;max_depth&quot;: [None, 5, 10, 20],    &quot;min_samples_split&quot;: [2, 5, 10],}Regression(X, y, model=&quot;rf&quot;, scaler=&quot;standard&quot;,  hyperparams=hyperparams_rf, save_pkl=False)```**Support Vector Regression (SVR)**```pythonhyperparams_svm = {    &quot;C&quot;: [0.1, 1.0, 10.0],    &quot;kernel&quot;: [&quot;linear&quot;, &quot;rbf&quot;, &quot;poly&quot;],}Regression(X, y, model=&quot;svm&quot;, scaler=&quot;standard&quot;,  hyperparams=hyperparams_svm, save_pkl=False)```**K-Nearest Neighbors Regression (KNN)**```pythonhyperparams_knn = {    &quot;n_neighbors&quot;: [2, 3, 5],    &quot;weights&quot;: [&quot;uniform&quot;, &quot;distance&quot;],}Regression(X, y, model=&quot;knn&quot;, scaler=&quot;standard&quot;,  hyperparams=hyperparams_knn, save_pkl=False)```**Gradient Boosting Regression**```pythonhyperparams_gb = {    &quot;n_estimators&quot;: [5, 10, 20],    &quot;learning_rate&quot;: [0.01, 0.1, 0.2],}Regression(X, y, model=&quot;gb&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_gb, save_pkl=False)```**AdaBoost Regression**```pythonhyperparams_ada = {    &quot;n_estimators&quot;: [5, 10, 20],    &quot;learning_rate&quot;: [0.01, 0.1, 0.2],}Regression(X, y, model=&quot;ada&quot;, scaler=&quot;standard&quot;,  hyperparams=hyperparams_ada, save_pkl=False)```**XGBoost Regression**```pythonhyperparams_xgb = {    &quot;n_estimators&quot;: [5, 10, 20],    &quot;learning_rate&quot;: [0.01, 0.1, 0.2],}Regression(X, y, model=&quot;xgb&quot;, scaler=&quot;standard&quot;,  hyperparams=hyperparams_xgb, save_pkl=False)```### Classification Hyperparameter ```pythonfrom mlfast.Hyperparameter.Classification import Classification```**Logistic Regression**```pythonhyperparams_lr = {    &quot;C&quot;: [0.1, 1.0],}Classification(X, y, model=&quot;lr&quot;, scaler=&quot;robust&quot;, hyperparams=hyperparams_lr, save_pkl=True)```**Decision Tree Classifier**```pythonhyperparams_dt = {    &quot;max_depth&quot;: [None, 5, 10, 20],    &quot;min_samples_split&quot;: [2, 5, 10],}Classification(X, y, model=&quot;dt&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_dt)```**Random Forest Classifier**```pythonrf_hyperparams = {    &quot;n_estimators&quot;: [100, 200, 300],    &quot;max_depth&quot;: [None, 10, 20, 30],    &quot;min_samples_split&quot;: [2, 5, 10],    &quot;min_samples_leaf&quot;: [1, 2, 4],    &quot;bootstrap&quot;: [True, False],}Classification(X, y, model=&quot;rf&quot;, scaler=&quot;standard&quot;, cat=True, hyperparams=rf_hyperparams, save_pkl=False)```**Support Vector Classifier (SVC)**```pythonhyperparams_svm = {    &quot;C&quot;: [0.1, 1.0, 10.0],    &quot;kernel&quot;: [&quot;linear&quot;, &quot;rbf&quot;, &quot;poly&quot;],}Classification(X, y, model=&quot;svm&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_svm)```**K-Nearest Neighbors Classifier (KNN)**```pythonhyperparams_knn = {    &quot;n_neighbors&quot;: [3, 5, 10],    &quot;weights&quot;: [&quot;uniform&quot;, &quot;distance&quot;],}Classification(X, y, model=&quot;knn&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_knn)```**Gradient Boosting Classifier**```pythonhyperparams_gb = {    &quot;n_estimators&quot;: [5, 10, 20],    &quot;learning_rate&quot;: [0.01, 0.1, 0.2],}Classification(X, y, model=&quot;gb&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_gb)```**AdaBoost Classifier**```pythonhyperparams_ada = {    &quot;n_estimators&quot;: [5, 10, 20],    &quot;learning_rate&quot;: [0.01, 0.1, 0.2],}Classification(X, y, model=&quot;ada&quot;, scaler=&quot;standard&quot;, cat=True, hyperparams=hyperparams_ada)```**XGBoost Classifier**```pythonhyperparams_xgb = {    &quot;n_estimators&quot;: [5, 10, 20],    &quot;learning_rate&quot;: [0.01, 0.1, 0.2],}Classification(X, y, model=&quot;xgb&quot;, scaler=&quot;standard&quot;, hyperparams=hyperparams_xgb)```### Text PreprocessingThe provided code snippet applies Text **Preprocessing** to a series or column of text data. It allows for flexible control over different preprocessing steps through boolean flags. Here is a summary of the options:- `stem`: Determines whether stemming should be performed. Set to `True` to enable stemming, or `False` to disable it.- `lemmatize`: Controls lemmatization. Set to `True` to enable lemmatization, or `False` to disable it.- `remove_html`: Specifies whether HTML tags should be removed. Use `True` to remove HTML tags, or `False` to keep them.- `remove_emoji`: Determines whether emojis should be removed from the text. Set to `True` to remove emojis, or `False` to retain them.- `remove_special_chars`: Controls the removal of special characters. Use `True` to remove special characters, or `False` to keep them.- `remove_extra_spaces`: Specifies whether extra spaces should be removed. Set to `True` to remove extra spaces, or `False` to keep them.By setting these flags to either `True` or `False`, you can customize the preprocessing steps according to your requirements. The code applies the specified preprocessing steps to each text element in the series or column and returns the processed text.**Text preprocessing sample code**```pythonfrom mlfast import Text_preprocessingdf['review'].apply(Text_preprocessing,                  stem=False,                  lemmatize=True,                  remove_html=True,                  remove_emoji=True,                  remove_special_chars=True,                  remove_extra_spaces=True)```### ChatbotImport the `Chatbot` class: In your Python script, import the `Chatbot` class from `mlfast`. You can do this by adding the following line at the beginning of your code:```pythonfrom mlfast import Chatbot```- Obtain an OpenAI API key: To use the `mlfast` library, you'll need an API key from OpenAI. If you don't have one, sign up on the OpenAI website and obtain an API key.- Create a `Chatbot` instance: Initialize the Chatbot class with your OpenAI API key and specify the desired role for your chatbot. Here's an example of creating a `Chatbot` instance:- Replace `&quot;YOUR-OPENAI-API-KEY&quot;` with your actual OpenAI API key, and `&quot;ENTER-YOUR-CHATBOT-ROLE&quot;` with the desired role for your chatbot.- Deploy the chatbot: Set the `deploy` parameter to `True` when creating the `Chatbot` instance. This will deploy the chatbot and make it available for use.- To deploy or terminate the deployment of a chatbot created using the `mlfast` library, you can set the `deploy` parameter to `True` or `False` when creating the `Chatbot` instance, respectively.```pythonChatbot(api_key=&quot;YOUR-OPENAI-API-KEY&quot;,        role=&quot;ENTER-YOUR-CHATBOT-ROLE&quot;,        deploy=True)```# **Chatbot feature is depreciated will be added soon on different library**## Announcement- Unsupervised Machine Learning Algorithms- Bag of words, TFIDF and Word2Vec- Image Preprocessing- And many more**ADDED SOON**</longdescription>
</pkgmetadata>