<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Jittor: a Just-in-time(JIT) deep learning framework![Jittor Logo](https://cg.cs.tsinghua.edu.cn/jittor/favicon_package_v0/JittorLogo_Final1220.svg)[Quickstart](#quickstart) | [Install](#install) | [Tutorial](#tutorial) | [简体中文](./README.cn.md)Jittor is a high-performance deep learning framework based on JIT compiling and meta-operators. The whole framework and meta-operators are compiled just-in-time. A powerful op compiler and tuner are integrated into Jittor. It allowed us to generate high-performance code with specialized for your model. Jittor also contains a wealth of high-performance model libraries, including: image recognition, detection, segmentation, generation, differentiable rendering, geometric learning, reinforcement learning, etc. .The front-end language is Python. Module Design and Dynamic Graph Execution is used in the front-end, which is the most popular design for deeplearning framework interface. The back-end is implemented by high performance language, such as CUDA,C++.Related Links:*  [Jittor Website](https://cg.cs.tsinghua.edu.cn/jittor/)*  [Jittor Tutorials](https://cg.cs.tsinghua.edu.cn/jittor/tutorial/)*  [Jittor Models](https://cg.cs.tsinghua.edu.cn/jittor/resources/)*  [Jittor Documents](https://cg.cs.tsinghua.edu.cn/jittor/assets/docs/index.html)*  [Github](https://github.com/jittor/jittor), [GitLink](https://www.gitlink.org.cn/jittor/jittor), [Gitee](https://gitee.com/jittor/jittor)*  [Jittor Forum](https://discuss.jittor.org/)*  [Awesome Jittor List](https://github.com/Jittor/jittor/blob/master/AWESOME-JITTOR-LIST.md)*  IM: QQ Group(761222083)The following example shows how to model a two-layer neural network step by step and train from scratch In a few lines of Python code.```pythonimport jittor as jtfrom jittor import Modulefrom jittor import nnimport numpy as npclass Model(Module):    def __init__(self):        self.layer1 = nn.Linear(1, 10)        self.relu = nn.Relu()         self.layer2 = nn.Linear(10, 1)    def execute (self,x) :        x = self.layer1(x)        x = self.relu(x)        x = self.layer2(x)        return xdef get_data(n): # generate random data for training test.    for i in range(n):        x = np.random.rand(batch_size, 1)        y = x*x        yield jt.float32(x), jt.float32(y)learning_rate = 0.1batch_size = 50n = 1000model = Model()optim = nn.SGD(model.parameters(), learning_rate)for i,(x,y) in enumerate(get_data(n)):    pred_y = model(x)    dy = pred_y - y    loss = dy * dy    loss_mean = loss.mean()    optim.step(loss_mean)    print(f&quot;step {i}, loss = {loss_mean.data.sum()}&quot;)```## Contents* [Quickstart](#quickstart)* [Install](#install)* [Tutorial](#tutorial)* [Contributing](#contributing)* [The Team](#theteam)* [License](#license)## QuickstartWe provide some jupyter notebooks to help you quick start with Jittor.- [Example: Model definition and training][1]- [Basics: Op, Var][2]- [Meta-operator: Implement your own convolution with Meta-operator][3]## InstallJittor environment requirements:| OS                                                     | CPU                                 | Python | Compiler     | (Optional) GPU platform                                ||--------------------------------------------------------|-------------------------------------|--------|--------------|---------------------------------------------|| Linux&lt;br&gt;(Ubuntu, CentOS, Arch, &lt;br&gt;UOS, KylinOS, ...) | x86 &lt;br&gt;x86_64 &lt;br&gt;ARM &lt;br&gt;loongson | &gt;= 3.7 | g++ &gt;=5.4    | Nvidia CUDA &gt;= 10.0, [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-tar) &lt;br&gt; or [AMD ROCm](https://docs.amd.com/) &gt;= 4.0 &lt;br&gt; or [Hygon DCU DTK](https://tycloud.hpccube.com/doc/1.0.6/11277/general-handbook/software-tutorial/jittor.html) &gt;= 22.04 || macOS &lt;br&gt;(&gt;= 10.14 Mojave)                            | intel&lt;br&gt;Apple Silicon              | &gt;= 3.7 | clang &gt;= 8.0 | -                                           || Windows 10 &amp; 11                                        | x86_64                              | [&gt;= 3.8](https://www.python.org/downloads/windows/) | -            | Nvidia CUDA &gt;= 10.2 [cuDNN](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-windows)                               |Jittor offers three ways to install: pip, docker, or manual.## Pip install```bashsudo apt install python3.7-dev libomp-devpython3.7 -m pip install jittor# or install from github(latest version)# python3.7 -m pip install git+https://github.com/Jittor/jittor.gitpython3.7 -m jittor.test.test_example```### macOS installPlease first install additional dependencies with [homebrew](https://brew.sh).```bashbrew install libomp```Then you can install jittor through pip and run the example.```bashpython3.7 -m pip install jittorpython3.7 -m jittor.test.test_example```Currently jittor only supports CPU on macOS.### Windows install```bash# check your python version(&gt;=3.8)python --versionpython -m pip install jittor# if conda is usedconda install pywin32```In Windows, jittor will automatically detect and install CUDA, please make sure your NVIDIA driver support CUDA 10.2  or above, or you can manually let jittor install CUDA for you:```bashpython -m jittor_utils.install_cuda```## Docker InstallWe provide a Docker installation method to save you from configuring the environment. The Docker installation method is as follows:```# CPU only(Linux)docker run -it --network host jittor/jittor# CPU and CUDA(Linux)docker run -it --network host --gpus all jittor/jittor-cuda# CPU only(Mac and Windows)docker run -it -p 8888:8888 jittor/jittor```## manual installWe will show how to install Jittor in Ubuntu 16.04 step by step, Other Linux distributions may have similar commands.### Step 1: Choose your back-end compiler```bash# g++sudo apt install g++ build-essential libomp-dev# OR clang++-8wget -O - https://raw.githubusercontent.com/Jittor/jittor/master/script/install_llvm.sh &gt; /tmp/llvm.shbash /tmp/llvm.sh 8```### Step 2: Install Python and python-devJittor need python version &gt;= 3.7.```bashsudo apt install python3.7 python3.7-dev```### Step 3: Run JittorThe whole framework is compiled Just-in-time. Let's install jittor via pip```bashgit clone https://github.com/Jittor/jittor.gitsudo pip3.7 install ./jittorexport cc_path=&quot;clang++-8&quot;# if other compiler is used, change cc_path# export cc_path=&quot;g++&quot;# export cc_path=&quot;icc&quot;# run a simple testpython3.7 -m jittor.test.test_example```if the test is passed, your Jittor is ready.### Optional Step 4: Enable CUDAUsing CUDA in Jittor is very simple, Just setup environment value `nvcc_path````bash# replace this var with your nvcc location export nvcc_path=&quot;/usr/local/cuda/bin/nvcc&quot; # run a simple cuda testpython3.7 -m jittor.test.test_cuda ```if the test is passed, your can use Jittor with CUDA by setting `use_cuda` flag.```pythonimport jittor as jtjt.flags.use_cuda = 1```### Optional Step 5: Test Resnet18 trainingTo check the integrity of Jittor, you can run Resnet18 training test. Note: 6G GPU RAM is requires in this test.```bashpython3.7 -m jittor.test.test_resnet```if those tests are failed, please report bugs for us, and feel free to contribute ^_^## TutorialIn the tutorial section, we will briefly explain the basic concept of Jittor.To train your model with Jittor, there are only three main concepts you need to know:* Var: basic data type of jittor* Operations: Jittor'op is simular with numpy### VarFirst, let's get started with Var. Var is the basic data type of jittor. Computation process in Jittor is asynchronous for optimization. If you want to access the data, `Var.data` can be used for synchronous data accessing.```pythonimport jittor as jta = jt.float32([1,2,3])print (a)print (a.data)# Output: float32[3,]# Output: [ 1. 2. 3.]```And we can give the variable a name.```pythona.name('a')print(a.name())# Output: a```### OperationsJittor'op is simular with numpy. Let's try some operations. We create Var `a` and `b` via operation `jt.float32`, and add them. Printing those variables shows they have the same shape and dtype.```pythonimport jittor as jta = jt.float32([1,2,3])b = jt.float32([4,5,6])c = a*bprint(a,b,c)print(type(a), type(b), type(c))# Output: float32[3,] float32[3,] float32[3,]# Output: &lt;class 'jittor_core.Var'&gt; &lt;class 'jittor_core.Var'&gt; &lt;class 'jittor_core.Var'&gt;```Beside that, All the operators we used `jt.xxx(Var, ...)` have alias `Var.xxx(...)`. For example:```pythonc.max() # alias of jt.max(c)c.add(a) # alias of jt.add(c, a)c.min(keepdims=True) # alias of jt.min(c, keepdims=True)```if you want to know all the operation which Jittor supports. try `help(jt.ops)`. All the operation you found in `jt.ops.xxx`, can be used via alias `jt.xxx`.```pythonhelp(jt.ops)# Output:#   abs(x: core.Var) -&gt; core.Var#   add(x: core.Var, y: core.Var) -&gt; core.Var#   array(data: array) -&gt; core.Var#   binary(x: core.Var, y: core.Var, op: str) -&gt; core.Var#   ......```### MoreIf you want to know more about Jittor, please check out the notebooks below:* Quickstart    - [Example: Model definition and training][1]    - [Basics: Op, Var][2]    - [Meta-operator: Implement your own convolution with Meta-operator][3]* Advanced    - [Custom Op: write your operator with C++ and CUDA and JIT compile it][4]    - [Profiler: Profiling your model][5]    - Jtune: Tool for performance tuning[1]: python/jittor/notebook/example.src.md&quot;example&quot;[2]: python/jittor/notebook/basics.src.md&quot;basics&quot;[3]: python/jittor/notebook/meta_op.src.md&quot;meta_op&quot;[4]: python/jittor/notebook/custom_op.src.md&quot;custom_op&quot;[5]: python/jittor/notebook/profiler.src.md&quot;profiler&quot;Those notebooks can be started in your own computer by `python3.7 -m jittor.notebook`## ContributingJittor is still young. It may contain bugs and issues. Please report them in our bug track system. Contributions are welcome. Besides, if you have any ideas about Jittor, please let us know.You can help Jittor in the following ways:* Citing Jittor in your paper* recommend Jittor to your friends* Contributing code* Contributed tutorials and documentation* File an issue* Answer jittor related questions* Light up the stars* Keep an eye on jittor* ......## Contact UsWebsite: http://cg.cs.tsinghua.edu.cn/jittor/Email: jittor@qq.comFile an issue: https://github.com/Jittor/jittor/issuesQQ Group: 761222083&lt;img src=&quot;https://cg.cs.tsinghua.edu.cn/jittor/images/news/2020-12-8-21-19-1_2_2/fig4.png&quot; width=&quot;200&quot;/&gt;## The TeamJittor is currently maintained by the [Tsinghua CSCG Group](https://cg.cs.tsinghua.edu.cn/). If you are also interested in Jittor and want to improve it, Please join us!## Citation```@article{hu2020jittor,  title={Jittor: a novel deep learning framework with meta-operators and unified graph execution},  author={Hu, Shi-Min and Liang, Dun and Yang, Guo-Ye and Yang, Guo-Wei and Zhou, Wen-Yang},  journal={Science China Information Sciences},  volume={63},  number={222103},  pages={1--21},  year={2020}}```## LicenseJittor is Apache 2.0 licensed, as found in the LICENSE.txt file.</longdescription>
</pkgmetadata>