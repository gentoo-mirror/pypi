<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># img2table`img2table` is a simple, easy to use, table identification and extraction Python Library based on [OpenCV](https://opencv.org/) image processing that supports most common image file formats as well as PDF files.Thanks to its design, it provides a practical and lighter alternative to Neural Networks based solutions, especially for usage on CPU.## Table of contents* [Installation](#installation)* [Features](#features)* [Supported file formats](#supported-file-formats)   * [Images](#images-formats)   * [PDF](#pdf-formats)* [Usage](#usage)   * [Documents](#documents)      * [Images](#images-doc)      * [PDF](#pdf-doc)   * [OCR](#ocr)      * [Tesseract](#tesseract)      * [PaddleOCR](#paddle)      * [Google Vision](#vision)      * [AWS Textract](#textract)      * [Azure Cognitive Services](#azure)   * [Table extraction](#table-extract)   * [Excel export](#xlsx)* [Examples](#examples)* [Caveats / FYI](#fyi)## Installation &lt;a name=&quot;installation&quot;&gt;&lt;/a&gt;The library can be installed via pip.```python# Standard installation, supporting Tesseractpip install img2table# For usage with Paddle OCR (Python &lt;= 3.10 only)pip install img2table[paddle]# For usage with Paddle OCR - GPU (CUDA 9 / CUDA 10) (Python &lt;= 3.10 only)pip install img2table[paddle-gpu]# For usage with Google Vision OCRpip install img2table[gcp]# For usage with AWS Textract OCRpip install img2table[aws]# For usage with Azure Cognitive Services OCRpip install img2table[azure]```## Features &lt;a name=&quot;features&quot;&gt;&lt;/a&gt;* Table identification for images and PDF files, including bounding boxes at the table cell level* Handling of complex table structures such as merged cells* Handling of implicit rows - see [example](/examples/Implicit_rows.ipynb)* Table content extraction by providing support for OCR services / tools* Extracted tables are returned as a simple object, including a Pandas DataFrame representation* Export extracted tables to an Excel file, preserving their original structure## Supported file formats &lt;a name=&quot;supported-file-formats&quot;&gt;&lt;/a&gt;### Images &lt;a name=&quot;images-formats&quot;&gt;&lt;/a&gt;Images are loaded using the `opencv-python` library, supported formats are listed below.&lt;blockquote&gt;&lt;ul&gt;&lt;li&gt;Windows bitmaps - &lt;em&gt;.bmp, &lt;/em&gt;.dib&lt;/li&gt;&lt;li&gt;JPEG files - &lt;em&gt;.jpeg, &lt;/em&gt;.jpg, *.jpe&lt;/li&gt;&lt;li&gt;JPEG 2000 files - *.jp2&lt;/li&gt;&lt;li&gt;Portable Network Graphics - *.png&lt;/li&gt;&lt;li&gt;WebP - *.webp&lt;/li&gt;&lt;li&gt;Portable image format - &lt;em&gt;.pbm, &lt;/em&gt;.pgm, &lt;em&gt;.ppm &lt;/em&gt;.pxm, *.pnm&lt;/li&gt;&lt;li&gt;PFM files - *.pfm&lt;/li&gt;&lt;li&gt;Sun rasters - &lt;em&gt;.sr, &lt;/em&gt;.ras&lt;/li&gt;&lt;li&gt;TIFF files - &lt;em&gt;.tiff, &lt;/em&gt;.tif&lt;/li&gt;&lt;li&gt;OpenEXR Image files - *.exr&lt;/li&gt;&lt;li&gt;Radiance HDR - &lt;em&gt;.hdr, &lt;/em&gt;.pic&lt;/li&gt;&lt;li&gt;Raster and Vector geospatial data supported by GDAL&lt;br&gt;&lt;cite&gt;&lt;a href=&quot;https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56&quot;&gt;OpenCV: Image file reading and writing&lt;/a&gt;&lt;/cite&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;Multi-page images are not supported.---### PDF &lt;a name=&quot;pdf-formats&quot;&gt;&lt;/a&gt;Both native and scanned PDF files are supported.## Usage &lt;a name=&quot;usage&quot;&gt;&lt;/a&gt;### Documents &lt;a name=&quot;documents&quot;&gt;&lt;/a&gt;#### Images &lt;a name=&quot;images-doc&quot;&gt;&lt;/a&gt;Images are instantiated as follows :```pythonfrom img2table.document import Imageimage = Image(src,               dpi=200,              detect_rotation=False)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;src : str, &lt;code&gt;pathlib.Path&lt;/code&gt;, bytes or &lt;code&gt;io.BytesIO&lt;/code&gt;, required&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Image source&lt;/dd&gt;&gt;    &lt;dt&gt;dpi : int, optional, default &lt;code&gt;200&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Estimated image dpi, used to adapt OpenCV algorithm parameters&lt;/dd&gt;&gt;    &lt;dt&gt;detect_rotation : bool, optional, default &lt;code&gt;False&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Detect and correct skew/rotation of the image&lt;/dd&gt;&gt;&lt;/dl&gt;&lt;br&gt;The implemented method to handle skewed/rotated images supports skew angles up to 45Â° and isbased on the publication by &lt;a href=&quot;https://www.mdpi.com/2079-9292/9/1/55&quot;&gt;Huang, 2020&lt;/a&gt;.&lt;br&gt;Setting the `detect_rotation` parameter to `True`, image coordinates and bounding boxes returned by other methods might not correspond to the original image.#### PDF &lt;a name=&quot;pdf-doc&quot;&gt;&lt;/a&gt;PDF files are instantiated as follows :```pythonfrom img2table.document import PDFpdf = PDF(src, dpi=200, pages=[0, 2])```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;src : str, &lt;code&gt;pathlib.Path&lt;/code&gt;, bytes or &lt;code&gt;io.BytesIO&lt;/code&gt;, required&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;PDF source&lt;/dd&gt;&gt;    &lt;dt&gt;dpi : int, optional, default &lt;code&gt;200&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Dpi used for conversion of PDF pages to images&lt;/dd&gt;&gt;    &lt;dt&gt;pages : list, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;List of PDF page indexes to be processed. If None, all pages are processed&lt;/dd&gt;&gt;&lt;/dl&gt;---### OCR &lt;a name=&quot;ocr&quot;&gt;&lt;/a&gt;`img2table` provides an interface for several OCR services and tools in order to parse table content.&lt;br&gt;If possible (i.e for searchable PDF), PDF text will be extracted directly from the file and the OCR service/tool will not be called.#### Tesseract &lt;a name=&quot;tesseract&quot;&gt;&lt;/a&gt;```pythonfrom img2table.ocr import TesseractOCRocr = TesseractOCR(n_threads=1,                    lang=&quot;eng&quot;,                    psm=11,                   tessdata_dir=&quot;...&quot;)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;n_threads : int, optional, default &lt;code&gt;1&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Number of concurrent threads used to call Tesseract&lt;/dd&gt;&gt;    &lt;dt&gt;lang : str, optional, default &lt;code&gt;&quot;eng&quot;&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Lang parameter used in Tesseract for text extraction&lt;/dd&gt;&gt;    &lt;dt&gt;psm : int, optional, default &lt;code&gt;11&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;PSM parameter used in Tesseract, run &lt;code&gt;tesseract --help-psm&lt;/code&gt; for details&lt;/dd&gt;&gt;    &lt;dt&gt;tessdata_dir : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Directory containing Tesseract traineddata files. If None, the &lt;code&gt;TESSDATA_PREFIX&lt;/code&gt; env variable is used.&lt;/dd&gt;&gt;&lt;/dl&gt;*Usage of [Tesseract-OCR](https://github.com/tesseract-ocr/tesseract) requires prior installation. Check [documentation](https://tesseract-ocr.github.io/tessdoc/) for instructions.*#### &lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR&quot;&gt;PaddleOCR&lt;/a&gt; &lt;a name=&quot;paddle&quot;&gt;&lt;/a&gt;*Available for Python versions &lt;= 3.10*PaddleOCR is an open-source OCR based on Deep Learning models.&lt;br&gt;At first use, relevant languages models will be downloaded.```pythonfrom img2table.ocr import PaddleOCRocr = PaddleOCR(lang=&quot;en&quot;)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;lang : str, optional, default &lt;code&gt;&quot;en&quot;&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Lang parameter used in Paddle for text extraction, check &lt;a href=&quot;https://github.com/Mushroomcat9998/PaddleOCR/blob/main/doc/doc_en/multi_languages_en.md#5-support-languages-and-abbreviations&quot;&gt;documentation for available languages&lt;/a&gt;&lt;/dd&gt;&gt;&lt;/dl&gt;*Released in version 0.0.13*#### Google Vision &lt;a name=&quot;vision&quot;&gt;&lt;/a&gt;Authentication to GCP can be done by setting the standard `GOOGLE_APPLICATION_CREDENTIALS` environment variable.&lt;br&gt;If this variable is missing, an API key should be provided via the `api_key` parameter.```pythonfrom img2table.ocr import VisionOCRocr = VisionOCR(api_key=&quot;api_key&quot;, timeout=15)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;api_key : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Google Vision API key&lt;/dd&gt;&gt;    &lt;dt&gt;timeout : int, optional, default &lt;code&gt;15&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;API requests timeout, in seconds&lt;/dd&gt;&gt;&lt;/dl&gt;#### AWS Textract &lt;a name=&quot;textract&quot;&gt;&lt;/a&gt;When using AWS Textract, the DetectDocumentText API is exclusively called.Authentication to AWS can be done by passing credentials to the `TextractOCR` class.&lt;br&gt;If credentials are not provided, authentication is done using environment variables or configuration files. Check `boto3` [documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for more details.```pythonfrom img2table.ocr import TextractOCRocr = TextractOCR(aws_access_key_id=&quot;***&quot;,                  aws_secret_access_key=&quot;***&quot;,                  aws_session_token=&quot;***&quot;,                  region=&quot;eu-west-1&quot;)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;aws_access_key_id : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;AWS access key id&lt;/dd&gt;&gt;    &lt;dt&gt;aws_secret_access_key : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;AWS secret access key&lt;/dd&gt;&gt;    &lt;dt&gt;aws_session_token : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;AWS temporary session token&lt;/dd&gt;&gt;    &lt;dt&gt;region : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;AWS server region&lt;/dd&gt;&gt;&lt;/dl&gt;#### Azure Cognitive Services &lt;a name=&quot;azure&quot;&gt;&lt;/a&gt;```pythonfrom img2table.ocr import AzureOCRocr = AzureOCR(endpoint=&quot;abc.azure.com&quot;,               subscription_key=&quot;***&quot;)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;endpoint : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Azure Cognitive Services endpoint. If None, inferred from the &lt;code&gt;COMPUTER_VISION_ENDPOINT&lt;/code&gt; environment variable.&lt;/dd&gt;&gt;    &lt;dt&gt;subscription_key : str, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Azure Cognitive Services subscription key. If None, inferred from the &lt;code&gt;COMPUTER_VISION_SUBSCRIPTION_KEY&lt;/code&gt; environment variable.&lt;/dd&gt;&gt;&lt;/dl&gt;---### Table extraction &lt;a name=&quot;table-extract&quot;&gt;&lt;/a&gt;Multiple tables can be extracted at once from a PDF page/ an image using the `extract_tables` method of a document.```pythonfrom img2table.ocr import TesseractOCRfrom img2table.document import Image# Instantiation of OCRocr = TesseractOCR(n_threads=1, lang=&quot;eng&quot;)# Instantiation of document, either an image or a PDFdoc = Image(src, dpi=200)# Table extractionextracted_tables = doc.extract_tables(ocr=ocr,                                      implicit_rows=True,                                      borderless_tables=False,                                      min_confidence=50)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;ocr : OCRInstance, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;OCR instance used to parse document text. If None, cells content will not be extracted&lt;/dd&gt;&gt;    &lt;dt&gt;implicit_rows : bool, optional, default &lt;code&gt;True&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Boolean indicating if implicit rows should be identified - check related &lt;a href=&quot;/examples/Implicit_rows.ipynb&quot; target=&quot;_self&quot;&gt;example&lt;/a&gt;&lt;/dd&gt;&gt;    &lt;dt&gt;borderless_tables : bool, optional, default &lt;code&gt;False&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Boolean indicating if &lt;a href=&quot;/examples/borderless.ipynb&quot; target=&quot;_self&quot;&gt;borderless tables&lt;/a&gt; are extracted. It requires to provide an OCR to the method in order to be performed - &lt;b&gt;feature in alpha version&lt;/b&gt;&lt;/dd&gt;&gt;    &lt;dt&gt;min_confidence : int, optional, default &lt;code&gt;50&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Minimum confidence level from OCR in order to process text, from 0 (worst) to 99 (best)&lt;/dd&gt;&gt;&lt;/dl&gt;*Borderless table extraction released in version 0.0.14*#### Method returnThe [`ExtractedTable`](/src/img2table/tables/objects/extraction.py#L35) class is used to model extracted tables from documents.&gt; &lt;h4&gt;Attributes&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;bbox : &lt;code&gt;&lt;a href=&quot;/src/img2table/tables/objects/extraction.py#L12&quot; target=&quot;_self&quot;&gt;BBox&lt;/a&gt;&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Table bounding box&lt;/dd&gt;&gt;    &lt;dt&gt;title : str&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Extracted title of the table&lt;/dd&gt;&gt;    &lt;dt&gt;content : &lt;code&gt;OrderedDict&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Dict with with row indexes as keys and list of &lt;code&gt;&lt;a href=&quot;/src/img2table/tables/objects/extraction.py#L20&quot; target=&quot;_self&quot;&gt;TableCell&lt;/a&gt;&lt;/code&gt; objects as values&lt;/dd&gt;&gt;    &lt;dt&gt;df : &lt;code&gt;pd.DataFrame&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Pandas DataFrame representation of the table&lt;/dd&gt;&gt;&lt;/dl&gt;&lt;h5 style=&quot;color:grey&quot;&gt;Images&lt;/h5&gt;`extract_tables` method from the `Image` class returns a list of `ExtractedTable` objects. ```Pythonoutput = [ExtractedTable(...), ExtractedTable(...), ...]```&lt;h5 style=&quot;color:grey&quot;&gt;PDF&lt;/h5&gt;`extract_tables` method from the `PDF` class returns an `OrderedDict` object with page indexes as keys and lists of `ExtractedTable` objects. ```Pythonoutput = {    0: [ExtractedTable(...), ...],    1: [],    ...    last_page: [ExtractedTable(...), ...]}```### Excel export &lt;a name=&quot;xlsx&quot;&gt;&lt;/a&gt;Tables extracted from a document can be exported to a xlsx file. The resulting file is composed of one worksheet per extracted table.&lt;br&gt;Method arguments are mostly common with the `extract_tables` method.```pythonfrom img2table.ocr import TesseractOCRfrom img2table.document import Image# Instantiation of OCRocr = TesseractOCR(n_threads=1, lang=&quot;eng&quot;)# Instantiation of document, either an image or a PDFdoc = Image(src, dpi=200)# Extraction of tables and creation of an xlsx file containing tablesdoc.to_xlsx(dest=dest,            ocr=ocr,            implicit_rows=True,            borderless_tables=False,            min_confidence=50)```&gt; &lt;h4&gt;Parameters&lt;/h4&gt;&gt;&lt;dl&gt;&gt;    &lt;dt&gt;dest : str, &lt;code&gt;pathlib.Path&lt;/code&gt; or &lt;code&gt;io.BytesIO&lt;/code&gt;, required&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Destination for xlsx file&lt;/dd&gt;&gt;    &lt;dt&gt;ocr : OCRInstance, optional, default &lt;code&gt;None&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;OCR instance used to parse document text. If None, cells content will not be extracted&lt;/dd&gt;&gt;    &lt;dt&gt;implicit_rows : bool, optional, default &lt;code&gt;True&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Boolean indicating if implicit rows should be identified - check related &lt;a href=&quot;/examples/Implicit_rows.ipynb&quot; target=&quot;_self&quot;&gt;example&lt;/a&gt;&lt;/dd&gt;&gt;    &lt;dt&gt;borderless_tables : bool, optional, default &lt;code&gt;False&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Boolean indicating if &lt;a href=&quot;/examples/borderless.ipynb&quot; target=&quot;_self&quot;&gt;borderless tables&lt;/a&gt; are extracted. It requires to provide an OCR to the method in order to be performed - &lt;b&gt;feature in alpha version&lt;/b&gt;&lt;/dd&gt;&gt;    &lt;dt&gt;min_confidence : int, optional, default &lt;code&gt;50&lt;/code&gt;&lt;/dt&gt;&gt;    &lt;dd style=&quot;font-style: italic;&quot;&gt;Minimum confidence level from OCR in order to process text, from 0 (worst) to 99 (best)&lt;/dd&gt;&gt;&lt;/dl&gt;&gt; &lt;h4&gt;Returns&lt;/h4&gt;&gt; If a &lt;code&gt;io.BytesIO&lt;/code&gt; buffer is passed as dest arg, it is returned containing xlsx data## Examples &lt;a name=&quot;examples&quot;&gt;&lt;/a&gt;Several Jupyter notebooks with examples are available :&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;/examples/Basic_usage.ipynb&quot; target=&quot;_self&quot;&gt;Basic usage&lt;/a&gt;: generic library usage, including examples with images, PDF and OCRs&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;/examples/borderless.ipynb&quot; target=&quot;_self&quot;&gt;Borderless tables&lt;/a&gt;: specific examples dedicated to the extraction of borderless tables&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;/examples/Implicit_rows.ipynb&quot; target=&quot;_self&quot;&gt;Implicit rows&lt;/a&gt;: illustrated effect of the parameter &lt;code&gt;implicit_rows&lt;/code&gt; of the &lt;code&gt;extract_tables&lt;/code&gt; method&lt;/li&gt;&lt;/ul&gt;## Caveats / FYI &lt;a name=&quot;fyi&quot;&gt;&lt;/a&gt;&lt;ul&gt;&lt;li&gt;For table extraction, results are highly dependent on OCR quality. By design, tables where no OCR data can be found are not returned.&lt;/li&gt;&lt;li&gt;The library is tailored for usage on documents with white/light background. Effectiveness can not be guaranteed on other type of documents. &lt;/li&gt;&lt;li&gt;Borderless tables extraction is still in alpha stage and might be inconsistent on complex cases.Improvements to the algorithm will be released in future versions.&lt;/li&gt;&lt;/ul&gt;</longdescription>
</pkgmetadata>