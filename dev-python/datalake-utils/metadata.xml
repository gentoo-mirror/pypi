<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription># Data Lake UtilityPackage to manipulate data from/into Amazon S3 using partitions compatible with Apache Hadoop filesystem.At this moment, this package was conceived to handle JSON and Parquet formats. That being said, it expects a Pandas DataFrame.Data will be written into Amazon S3 as a multi-line JSON or Apache Parquet, compressed as GZIP.## FeaturesConvert list of dictionaries...```python[    {&quot;brand&quot;: &quot;Ford&quot;,&quot;model&quot;: &quot;Mustang&quot;,&quot;year&quot;: 1965},    {&quot;brand&quot;: &quot;Pontiac&quot;,&quot;model&quot;: &quot;GTO&quot;,&quot;year&quot;: 1964},    {&quot;brand&quot;: &quot;Lamborghini&quot;,&quot;model&quot;: &quot;Miura&quot;,&quot;year&quot;: 1966}]```...to multi-line JSON compressed as GZIP```text{&quot;brand&quot;: &quot;Ford&quot;,&quot;model&quot;: &quot;Mustang&quot;,&quot;year&quot;: 1965}{&quot;brand&quot;: &quot;Pontiac&quot;,&quot;model&quot;: &quot;GTO&quot;,&quot;year&quot;: 1964}{&quot;brand&quot;: &quot;Lamborghini&quot;,&quot;model&quot;: &quot;Miura&quot;,&quot;year&quot;: 1966}```...or to Apache Parquet compressed as GZIP## How to useThis package does not slice data into partitions at the moment. You must handle slicing of data to write into partitions desired.Check this example below:```pythonfrom datalake_utils.utils import DataLakeimport pandasdata = [    {        &quot;brand&quot;: &quot;Ford&quot;,        &quot;model&quot;: &quot;Mustang&quot;,        &quot;year&quot;: 1965    },    {        &quot;brand&quot;: &quot;Pontiac&quot;,        &quot;model&quot;: &quot;GTO&quot;,        &quot;year&quot;: 1964    },    {        &quot;brand&quot;: &quot;Lamborghini&quot;,        &quot;model&quot;: &quot;Miura&quot;,        &quot;year&quot;: 1966    }]datalake = DataLake(    bucket_name=&quot;vehicles&quot;,    schema=&quot;motor_vehicles&quot;,    table=&quot;cars&quot;,    partitions=[        {            &quot;key&quot;: &quot;decade&quot;,            &quot;value&quot;: &quot;1960s&quot;        }    ],)datalake.append_to_s3(data=pandas.DataFrame(data), file_format=&quot;json&quot;)```It will create an object into Amazon S3 with the following structure:```motor_vehicles/`-- cars/    `-- decade=1960s/        `-- 2c0fea6c-444e-11ed-969f-acde48001122.json.gz```To read all files from partition, do:```pythonretrieved_data = datalake.read_from_s3(file_format=&quot;json&quot;)````retrieved_data` is a Pandas DataFrame object. It is not possible to read a specific file using this function.To convert a Pandas DataFrame into a list of tuples containing a single JSON:```pythondatalake.df_to_tuples(data=retrieved_data)```To delete all files from partition, do:```pythondatalake.delete_from_s3()```</longdescription>
</pkgmetadata>