<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>### About CETLCETL is a Python library that provides a comprehensive set of tools for building and managing data pipelines. It is designed to assist data engineers in handling Extract, Transform, and Load (ETL) tasks more effectively by simplifying the process and reducing the amount of manual labor involved.&lt;br&gt;CETL is particularly useful for Python developers who work with data on a regular basis. It uses popular data containers such as pandas dataframes, JSON objects, and PySpark dataframes to provide a familiar interface for developers. This allows users to easily integrate CETL into their existing data pipelines and workflows.&lt;br&gt;The library is intended to make the ETL process more straightforward by automating many of the technical details involved in data processing and movement. CETL includes a wide range of functions and tools for handling complex data formats, such as CSV, Excel, and JSON files, as well as for working with a variety of data sources, including databases, APIs, and cloud storage services.&lt;br&gt;One of the key benefits of CETL is its ability to handle large datasets, making it suitable for use in high-performance data processing environments. CETL also includes features for data profiling, data validation, data transformation, and data mapping, allowing users to build sophisticated data pipelines that can handle a wide range of data processing tasks.&lt;br&gt;Overall, CETL is a powerful data pipeline tool that can help data engineers to improve their productivity and streamline the ETL process. By providing a comprehensive set of functions and tools for working with data, CETL makes it easier to develop and maintain complex ETL pipelines, reducing the amount of time and effort required to manage data processing tasks.&lt;br&gt;&lt;br&gt;&lt;br&gt;### User Guide#### Example 1GenerateDataFrame is a Python class object designed to represent a transformation step in a data pipeline. This object can be used to generate a dummy dataframe without reading actual data from a file. The main purpose of this object is to assist developers in testing their data processing pipelines.&lt;br&gt;With GenerateDataFrame, developers can quickly and easily create test data that mimics the structure of their actual data. This can be particularly useful when working with large datasets or when data is not readily available. By generating dummy data, developers can test their pipeline's functionality without having to rely on real data sources.&lt;br&gt;GenerateDataFrame is particularly useful in situations where developers need to test their pipeline's ability to handle different types of data and perform various data transformations. This can include testing the pipeline's ability to handle missing data, data outliers, and data formatting issues.&lt;br&gt;Overall, GenerateDataFrame is a powerful tool that can help developers to streamline the testing process and ensure the accuracy and efficiency of their data processing pipelines. By allowing developers to generate dummy data, it provides a quick and easy way to test their pipeline's functionality and identify any potential issues before deploying to production.&lt;br&gt;```pythonfrom cetl import make_pipelinefrom cetl.pandas_modules import generateDataFramepipe = make_pipeline(generateDataFrame())df = pipe.transform(&quot;&quot;)print(df)```|    |   customer_id | first_name   | last_name   | title   ||---:|--------------:|:-------------|:------------|:--------||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    |&lt;br&gt;#### Example 2```pythonfrom cetl import build_pipelinefrom cetl.pandas_modules import generateDataFrame, unionAllfrom cetl.functional_modules import dummyStart, parallelTransformerpipe = build_pipeline(  dummyStart(),                        parallelTransformer([generateDataFrame(), generateDataFrame()]),                         unionAll())df = pipe.transform(&quot;&quot;)print(df)```|    |   customer_id | first_name   | last_name   | title   ||---:|--------------:|:-------------|:------------|:--------||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    ||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    |Alternatively, you can perform the same by using json configuration to the DataPipeline object```pythonfrom cetl import DataPipelinecfg = {&quot;pipeline&quot;:[ {&quot;type&quot;:&quot;dummyStart&quot;, &quot;module_type&quot;:&quot;functional&quot;},                    {&quot;type&quot;:&quot;parallelTransformer&quot;, &quot;transformers&quot;:[                        {&quot;type&quot;:&quot;generateDataFrame&quot;},                        {&quot;type&quot;:&quot;generateDataFrame&quot;}                    ]},                    {&quot;type&quot;:&quot;unionAll&quot;}]}pipe = DataPipeline(cfg)df = pipe.transform(&quot;&quot;)print(df)```|    |   customer_id | first_name   | last_name   | title   ||---:|--------------:|:-------------|:------------|:--------||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    ||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    |&lt;br&gt;#### Example 3: using kafka for data transfer between transformers```pythonfernet_key = &quot;20230315&quot;pipe_topic_name = &quot;kafka_media_test&quot;cfg = { &quot;pipeline&quot;: [   {&quot;type&quot;:&quot;dummyStartEmpty&quot;},                        {&quot;type&quot;:&quot;parallelTransformer&quot;, &quot;transformers&quot;:[                            {&quot;type&quot;:&quot;generateDataFrame&quot;},                            {&quot;type&quot;:&quot;generateDataFrame&quot;},                            {&quot;type&quot;:&quot;generateDataFrame&quot;}                        ]},                        {&quot;type&quot;:&quot;unionAll&quot;}],                            &quot;pipeline_settings&quot;:{   &quot;print_cfg&quot;:1,                                 &quot;print_task_result&quot;:1,                                 &quot;exchange_media&quot;:&quot;kafka&quot;,                                &quot;bootstrap_servers&quot;:[&quot;localhost:9092&quot;],                                &quot;fernet_key&quot;:f&quot;{fernet_key}&quot;,                                &quot;pipe_topic_name&quot;:f&quot;{pipe_topic_name}&quot;}}cfg[&quot;pipeline_settings&quot;][&quot;exchange_media&quot;]=&quot;kafka&quot;kafka_cfg = cfgpipe = DataPipeline(kafka_cfg)result = pipe.transform(&quot;&quot;)# get the output of transformer by task_idtask_id = b&quot;5.unionAll&quot;df = pipe.kafka_media.read_kafka(task_id=task_id)# # get the output of finalprint(df)```|    |   customer_id | first_name   | last_name   | title   ||---:|--------------:|:-------------|:------------|:--------||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    ||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    ||  0 |           111 | peter        | Hong        | Mr.     ||  1 |           222 | YuCheung     | Wong        | Mr.     ||  2 |           333 | Cindy        | Wong        | Mrs.    |&lt;br&gt;### Render the graphNote: please make sure the graphviz executable file is installed.&lt;br&gt;both png file and the svg file will be exported```pythonpipe = pipe.build_digraph()pipe.save_png(&quot;./sample.png&quot;)```#### sample.png&lt;img src=&quot;sample.png&quot;&gt; this version will solve the issue of UnboundLocalError: local variable 'pre_transformer_key' referenced before assignment</longdescription>
</pkgmetadata>