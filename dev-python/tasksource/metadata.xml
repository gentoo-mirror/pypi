<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>## tasksource ![](https://aeiljuispo.cloudimg.io/v7/https://s3.amazonaws.com/moonup/production/uploads/5fc0bcb41160c47d1d43856b/j06-U5e2Tifi2xOnTudqS.jpeg?w=20&amp;h=20&amp;f=face) 500+ curated datasets and preprocessings for effortless scaling of multi-task learning and evaluation.Huggingface Datasets is an excellent library, but it lacks standardization, and datasets often require preprocessing work to be used interchangeably.`tasksource` streamlines interchangeable datasets usage to scale evaluation or multi-task learning.Each dataset is standardized to a `MultipleChoice`, `Classification`, or `TokenClassification` template with canonical fields. We focus on discriminative tasks (= with negative examples or classes) for our annotations but also provide a `SequenceToSequence` template. All implemented preprocessings are in [tasks.py](https://github.com/sileod/tasksource/blob/main/src/tasksource/tasks.py) or [tasks.md](https://github.com/sileod/tasksource/blob/main/tasks.md). A preprocessing is a function that accepts a dataset and returns the standardized dataset. Preprocessing code is concise and human-readable.### Installation and usage:`pip install tasksource````pythonfrom tasksource import list_tasks, load_taskdf = list_tasks() # takes some timefor id in df[df.task_type==&quot;MultipleChoice&quot;].id:    dataset = load_task(id) # all yielded datasets can be used interchangeably```Browse the 500+ curated tasks in tasks.md (200+ MultipleChoice tasks, 200+ Classification tasks), and feel free to request a new task. Datasets are downloaded to $HF_DATASETS_CACHE (like any Hugging Face dataset), so ensure you have more than 100GB of space available.### Pretrained model:Text encoder pretrained on tasksource reached state-of-the-art results: [ðŸ¤—/deberta-v3-base-tasksource-nli](https://hf.co/sileod/deberta-v3-base-tasksource-nli)Tasksource pretraining is notably helpful for RLHF reward modeling.### tasksource-instructThe repo also contains some recasting code to convert tasksource datasets to instructions, providing one of the richest instruction-tuning datasets:[ðŸ¤—/tasksource-instruct-v0](https://hf.co/datasets/tasksource/tasksource-instruct-v0)### tasksource-label-nliWe also recast all classification tasks into entailment detection, to improve entailment-based zero-shot classification detection:[ðŸ¤—/zero-shot-label-nli](https://huggingface.co/datasets/tasksource/zero-shot-label-nli)### Write and use custom preprocessings```pythonfrom tasksource import MultipleChoicecodah = MultipleChoice('question_propmt',choices_list='candidate_answers',    labels='correct_answer_idx',    dataset_name='codah', config_name='codah')    winogrande = MultipleChoice('sentence',['option1','option2'],'answer',    dataset_name='winogrande',config_name='winogrande_xl',    splits=['train','validation',None]) # test labels are not usable    tasks = [winogrande.load(), codah.load()]) #  Aligned datasets (same columns) can be used interchangably  ``` ### Contact and citationFor help integrating tasksource into your experiments, please contact [damien.sileo@inria.fr](mailto:damien.sileo@inria.fr).For more details, refer to this [article:](https://arxiv.org/abs/2301.05948) ```bib@article{sileo2023tasksource,  title={tasksource: Structured Dataset Preprocessing Annotations for Frictionless Extreme Multi-Task Learning and Evaluation},  author={Sileo, Damien},  url= {https://arxiv.org/abs/2301.05948},  journal={arXiv preprint arXiv:2301.05948},  year={2023}}```                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </longdescription>
</pkgmetadata>