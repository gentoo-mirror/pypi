<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/34196005/180642397-1f56d9c7-dee2-48d4-acbf-c3bc62f36150.png&quot; width=&quot;500&quot;&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;    Easiest way of fine-tuning HuggingFace video classification models.&lt;/p&gt;&lt;div align=&quot;center&quot;&gt;    &lt;a href=&quot;https://badge.fury.io/py/video-transformers&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/video-transformers.svg&quot; alt=&quot;pypi version&quot;&gt;&lt;/a&gt;    &lt;a href=&quot;https://pepy.tech/project/video-transformers&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/video-transformers&quot; alt=&quot;total downloads&quot;&gt;&lt;/a&gt;    &lt;a href=&quot;https://twitter.com/fcakyon&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/fcakyon?color=blue&amp;logo=twitter&amp;style=flat&quot; alt=&quot;fcakyon twitter&quot;&gt;&lt;/a&gt;&lt;/div&gt;## üöÄ Features`video-transformers` uses:- ü§ó [accelerate](https://github.com/huggingface/accelerate) for distributed training,- ü§ó [evaluate](https://github.com/huggingface/evaluate) for evaluation,- [pytorchvideo](https://github.com/facebookresearch/pytorchvideo) for dataloadingand supports:- creating and fine-tunining video models using [transformers](https://github.com/huggingface/transformers) and [timm](https://github.com/rwightman/pytorch-image-models) vision models- experiment tracking with [neptune](https://neptune.ai/), [tensorboard](https://www.tensorflow.org/tensorboard) and other trackers- exporting fine-tuned models in [ONNX](https://onnx.ai/) format- pushing fine-tuned models into [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=image-classification&amp;sort=downloads)- loading pretrained models from [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=image-classification&amp;sort=downloads)- Automated [Gradio app](https://gradio.app/), and [space](https://huggingface.co/spaces) creation ## üèÅ Installation- Install `Pytorch`:```bashconda install pytorch=1.11.0 torchvision=0.12.0 cudatoolkit=11.3 -c pytorch```- Install pytorchvideo and transformers from main branch:```bashpip install git+https://github.com/facebookresearch/pytorchvideo.gitpip install git+https://github.com/huggingface/transformers.git```- Install `video-transformers`:```bashpip install video-transformers```## üî• Usage- Prepare video classification dataset in such folder structure (.avi and .mp4 extensions are supported):```bashtrain_root    label_1        video_1        video_2        ...    label_2        video_1        video_2        ...    ...val_root    label_1        video_1        video_2        ...    label_2        video_1        video_2        ...    ...```- Fine-tune Timesformer (from HuggingFace) video classifier:```pythonfrom torch.optim import AdamWfrom video_transformers import VideoModelfrom video_transformers.backbones.transformers import TransformersBackbonefrom video_transformers.data import VideoDataModulefrom video_transformers.heads import LinearHeadfrom video_transformers.trainer import trainer_factoryfrom video_transformers.utils.file import download_ucf6backbone = TransformersBackbone(&quot;facebook/timesformer-base-finetuned-k400&quot;, num_unfrozen_stages=1)download_ucf6(&quot;./&quot;)datamodule = VideoDataModule(    train_root=&quot;ucf6/train&quot;,    val_root=&quot;ucf6/val&quot;,    batch_size=4,    num_workers=4,    num_timesteps=8,    preprocess_input_size=224,    preprocess_clip_duration=1,    preprocess_means=backbone.mean,    preprocess_stds=backbone.std,    preprocess_min_short_side=256,    preprocess_max_short_side=320,    preprocess_horizontal_flip_p=0.5,)head = LinearHead(hidden_size=backbone.num_features, num_classes=datamodule.num_classes)model = VideoModel(backbone, head)optimizer = AdamW(model.parameters(), lr=1e-4)Trainer = trainer_factory(&quot;single_label_classification&quot;)trainer = Trainer(datamodule, model, optimizer=optimizer, max_epochs=8)trainer.fit()```- Fine-tune ConvNeXT (from HuggingFace) + Transformer based video classifier:```pythonfrom torch.optim import AdamWfrom video_transformers import TimeDistributed, VideoModelfrom video_transformers.backbones.transformers import TransformersBackbonefrom video_transformers.data import VideoDataModulefrom video_transformers.heads import LinearHeadfrom video_transformers.necks import TransformerNeckfrom video_transformers.trainer import trainer_factoryfrom video_transformers.utils.file import download_ucf6backbone = TimeDistributed(TransformersBackbone(&quot;facebook/convnext-small-224&quot;, num_unfrozen_stages=1))neck = TransformerNeck(    num_features=backbone.num_features,    num_timesteps=8,    transformer_enc_num_heads=4,    transformer_enc_num_layers=2,    dropout_p=0.1,)download_ucf6(&quot;./&quot;)datamodule = VideoDataModule(    train_root=&quot;ucf6/train&quot;,    val_root=&quot;ucf6/val&quot;,    batch_size=4,    num_workers=4,    num_timesteps=8,    preprocess_input_size=224,    preprocess_clip_duration=1,    preprocess_means=backbone.mean,    preprocess_stds=backbone.std,    preprocess_min_short_side=256,    preprocess_max_short_side=320,    preprocess_horizontal_flip_p=0.5,)head = LinearHead(hidden_size=neck.num_features, num_classes=datamodule.num_classes)model = VideoModel(backbone, head, neck)optimizer = AdamW(model.parameters(), lr=1e-4)Trainer = trainer_factory(&quot;single_label_classification&quot;)trainer = Trainer(    datamodule,    model,    optimizer=optimizer,    max_epochs=8)trainer.fit()```- Fine-tune Resnet18 (from HuggingFace) + GRU based video classifier:```pythonfrom video_transformers import TimeDistributed, VideoModelfrom video_transformers.backbones.transformers import TransformersBackbonefrom video_transformers.data import VideoDataModulefrom video_transformers.heads import LinearHeadfrom video_transformers.necks import GRUNeckfrom video_transformers.trainer import trainer_factoryfrom video_transformers.utils.file import download_ucf6backbone = TimeDistributed(TransformersBackbone(&quot;microsoft/resnet-18&quot;, num_unfrozen_stages=1))neck = GRUNeck(num_features=backbone.num_features, hidden_size=128, num_layers=2, return_last=True)download_ucf6(&quot;./&quot;)datamodule = VideoDataModule(    train_root=&quot;ucf6/train&quot;,    val_root=&quot;ucf6/val&quot;,    batch_size=4,    num_workers=4,    num_timesteps=8,    preprocess_input_size=224,    preprocess_clip_duration=1,    preprocess_means=backbone.mean,    preprocess_stds=backbone.std,    preprocess_min_short_side=256,    preprocess_max_short_side=320,    preprocess_horizontal_flip_p=0.5,)head = LinearHead(hidden_size=neck.hidden_size, num_classes=datamodule.num_classes)model = VideoModel(backbone, head, neck)Trainer = trainer_factory(&quot;single_label_classification&quot;)trainer = Trainer(    datamodule,    model,    max_epochs=8)trainer.fit()```- Perform prediction for a single file or folder of videos:```pythonfrom video_transformers import VideoModelmodel = VideoModel.from_pretrained(model_name_or_path)model.predict(video_or_folder_path=&quot;video.mp4&quot;)&gt;&gt; [{'filename': &quot;video.mp4&quot;, 'predictions': {'class1': 0.98, 'class2': 0.02}}]```## ü§ó Full HuggingFace Integration- Push your fine-tuned model to the hub:```pythonfrom video_transformers import VideoModelmodel = VideoModel.from_pretrained(&quot;runs/exp/checkpoint&quot;)model.push_to_hub('model_name')```- Load any pretrained video-transformer model from the hub:```pythonfrom video_transformers import VideoModelmodel = VideoModel.from_pretrained(&quot;runs/exp/checkpoint&quot;)model.from_pretrained('account_name/model_name')```- Push your model to HuggingFace hub with auto-generated model-cards:```pythonfrom video_transformers import VideoModelmodel = VideoModel.from_pretrained(&quot;runs/exp/checkpoint&quot;)model.push_to_hub('account_name/app_name')```- (Incoming feature) Push your model as a Gradio app to HuggingFace Space:```pythonfrom video_transformers import VideoModelmodel = VideoModel.from_pretrained(&quot;runs/exp/checkpoint&quot;)model.push_to_space('account_name/app_name')```## üìà Multiple tracker support- Tensorboard tracker is enabled by default.- To add Neptune/Layer ... tracking:```pythonfrom video_transformers.tracking import NeptuneTrackerfrom accelerate.tracking import WandBTrackertrackers = [    NeptuneTracker(EXPERIMENT_NAME, api_token=NEPTUNE_API_TOKEN, project=NEPTUNE_PROJECT),    WandBTracker(project_name=WANDB_PROJECT)]trainer = Trainer(    datamodule,    model,    trackers=trackers)```## üï∏Ô∏è ONNX support- Convert your trained models into ONNX format for deployment:```pythonfrom video_transformers import VideoModelmodel = VideoModel.from_pretrained(&quot;runs/exp/checkpoint&quot;)model.to_onnx(quantize=False, opset_version=12, export_dir=&quot;runs/exports/&quot;, export_filename=&quot;model.onnx&quot;)```## ü§ó Gradio support- Convert your trained models into Gradio App for deployment:```pythonfrom video_transformers import VideoModelmodel = VideoModel.from_pretrained(&quot;runs/exp/checkpoint&quot;)model.to_gradio(examples=['video.mp4'], export_dir=&quot;runs/exports/&quot;, export_filename=&quot;app.py&quot;)```## ContributingBefore opening a PR:- Install required development packages:```bashpip install -e .&quot;[dev]&quot;```- Reformat with black and isort:```bashpython -m tests.run_code_style format```</longdescription>
</pkgmetadata>