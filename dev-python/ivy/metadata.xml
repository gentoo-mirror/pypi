<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&gt; üöÄ We are granting pilot access to **Ivy\'s Tracer and Transpiler**&gt; to some users, [join the waitlist](https://console.unify.ai/) if you&gt; want to test them out!&lt;img class=&quot;only-light&quot; width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logo.png?raw=true#gh-light-mode-only&quot;/&gt;------------------------------------------------------------------------&lt;div style=&quot;display: block;&quot; align=&quot;center&quot;&gt;&lt;a href=&quot;https://unify.ai/&quot;&gt;    &lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/website_button.svg&quot;&gt;&lt;/a&gt;&lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;&lt;a href=&quot;https://unify.ai/docs/ivy&quot;&gt;    &lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/docs_button.svg&quot;&gt;&lt;/a&gt;&lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;&lt;a href=&quot;https://unify.ai/demos&quot;&gt;    &lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/examples_button.svg&quot;&gt;&lt;/a&gt;&lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;&lt;a href=&quot;https://unify.ai/docs/ivy/overview/design.html&quot;&gt;    &lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/design_button.svg&quot;&gt;&lt;/a&gt;&lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;&lt;a href=&quot;https://unify.ai/docs/ivy/overview/faq.html&quot;&gt;    &lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/faq_button.svg&quot;&gt;&lt;/a&gt;&lt;/div&gt;------------------------------------------------------------------------# Status&lt;div&gt;    &lt;a href=&quot;https://github.com/unifyai/ivy/issues&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://img.shields.io/github/issues/unifyai/ivy&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/unifyai/ivy/network/members&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://img.shields.io/github/forks/unifyai/ivy&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/unifyai/ivy/stargazers&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://img.shields.io/github/stars/unifyai/ivy&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/unifyai/ivy/pulls&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://pypi.org/project/ivy&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://badge.fury.io/py/ivy.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/unifyai/ivy/actions?query=workflow%3Adocs&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://github.com/unifyai/ivy/actions/workflows/docs.yml/badge.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://github.com/unifyai/ivy/actions?query=workflow%3Atest-ivy&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://github.com/unifyai/ivy/actions/workflows/intelligent-tests.yml/badge.svg&quot;&gt;    &lt;/a&gt;    &lt;a href=&quot;https://discord.gg/sXyFF8tDtm&quot;&gt;        &lt;img class=&quot;dark-light&quot; style=&quot;padding-right: 4px; padding-bottom: 4px;&quot; src=&quot;https://img.shields.io/discord/799879767196958751?color=blue&amp;label=%20&amp;logo=discord&amp;logoColor=white&quot;&gt;    &lt;/a&gt;&lt;/div&gt;&lt;br clear=&quot;all&quot; /&gt;------------------------------------------------------------------------# Unified AI&lt;div style=&quot;display: block;&quot; align=&quot;center&quot;&gt;    &lt;div&gt;    &lt;a href=&quot;https://jax.readthedocs.io&quot;&gt;        &lt;img class=&quot;dark-light&quot; width=&quot;10%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/jax_logo.png&quot;&gt;    &lt;/a&gt;    &lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;    &lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;    &lt;a href=&quot;https://www.tensorflow.org&quot;&gt;        &lt;img class=&quot;dark-light&quot; width=&quot;10%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/tensorflow_logo.png&quot;&gt;    &lt;/a&gt;    &lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;    &lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;    &lt;a href=&quot;https://pytorch.org&quot;&gt;        &lt;img class=&quot;dark-light&quot; width=&quot;10%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/pytorch_logo.png&quot;&gt;    &lt;/a&gt;    &lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;    &lt;img class=&quot;dark-light&quot; width=&quot;5%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/empty.png&quot;&gt;    &lt;a href=&quot;https://numpy.org&quot;&gt;        &lt;img class=&quot;dark-light&quot; width=&quot;10%&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/numpy_logo.png&quot;&gt;    &lt;/a&gt;    &lt;/div&gt;&lt;/div&gt;&lt;br clear=&quot;all&quot; /&gt;------------------------------------------------------------------------Ivy is an open-source machine learning framework thatenables you to:- üî• **Autotune your model**: Automatically find the optimal framework, compiler infrastructure and hardware for your specific use case using `ivy.autotune`.- üîÑ **Convert code into any framework**: Use and build on top of any model, library, or device by converting any code from one framework to another using `ivy.transpile`.- ‚öíÔ∏è **Write framework-agnostic code**: Write your code once in ivy and then choose the most appropriate ML framework as the backend to leverage all the benefits and tools.[Join our growing community](https://discord.com/invite/sXyFF8tDtm) üåç to connect with people using Ivy. **Let\'s** [unify.ai](https://unify.ai) **together ü¶æ**------------------------------------------------------------------------# Getting startedThe best way to get familiar with Ivy is to go through the [Demos](https://unify.ai/docs/ivy/demos/examples_and_demos.html), a good starting point is [Learn The Basics](https://unify.ai/docs/ivy/demos/learn_the_basics.html).The most important notebooks are:- [How to convert your code between frameworks?](https://unify.ai/docs/ivy/demos/learn_the_basics/04_transpile_code.html)- [How to write framework-agnostic code?](https://unify.ai/docs/ivy/demos/learn_the_basics/01_write_ivy_code.html)- Accelerate your development (WIP)- Autotune and optimize models (WIP)------------------------------------------------------------------------## Installing ivyThere are various ways to use Ivy, depending on your preferredenvironment:### Installing using pipThe easiest way to set up Ivy is to install it using pip with thefollowing command:``` bashpip install ivy```or alternatively:``` bashpython3 -m pip install ivy```&lt;details&gt;&lt;summary&gt;Docker&lt;/summary&gt;If you prefer to use containers, we also have pre-built Docker imageswith all the supported frameworks and some relevant packages alreadyinstalled, which you can pull from:``` bashdocker pull unifyai/ivy:latest```If you are working on a GPU device, you can pull from:``` bashdocker pull unifyai/ivy:latest-gpu```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;From Source&lt;/summary&gt;You can also install Ivy from source if you want to take advantage ofthe latest changes, but we can\'t ensure everything will work asexpected. :sweat_smile:``` bashgit clone https://github.com/unifyai/ivy.gitcd ivypip install --user -e .```or alternatively, for the last step:``` bashpython3 -m pip install --user -e .```If you want to set up testing and various frameworks it\'s probably     bestto check out the [Contributing - SettingUp](https://unify.ai/docs/ivy/overview/contributing/setting_up. html#setting-up)page, where OS-specific and IDE-specific instructions and videotutorials to do so are available!&lt;/details&gt;------------------------------------------------------------------------## Using IvyAfter installing Ivy, you can start using it straight away, for example:  &lt;details&gt;   &lt;summary&gt;&lt;b&gt;Transpiling any code from one framework to another&lt;/b&gt;&lt;/summary&gt;   ``` python   import ivy   import torch   import jax   def jax_fn(x):       a = jax.numpy.dot(x, x)       b = jax.numpy.mean(x)       return x * a + b   jax_x = jax.numpy.array([1, 2, 3])   torch_x = torch.tensor([1, 2, 3])   torch_fn = ivy.transpile(jax_fn, source=&quot;jax&quot;, to=&quot;torch&quot;, args=(jax_x,))   ret = torch_fn(torch_x)   ```   &lt;/details&gt;  &lt;details&gt;    &lt;summary&gt;&lt;b&gt;Running your code with any backend&lt;/b&gt;&lt;/summary&gt;   ``` python    import ivy    import torch    import jax    ivy.set_backend(&quot;jax&quot;)    x = jax.numpy.array([1, 2, 3])    y = jax.numpy.array([3, 2, 1])    z = ivy.add(x, y)    ivy.set_backend('torch')    x = torch.tensor([1, 2, 3])    y = torch.tensor([3, 2, 1])    z = ivy.add(x, y)   ```   &lt;/details&gt;------------------------------------------------------------------------# DocumentationYou can find Ivy's documentation in the [Docs page](https://unify.ai/docs/ivy/), which includes:- [Motivation](https://unify.ai/docs/ivy/overview/background.html): This contextualizes the problem Ivy is trying to solve by going over    - The current [ML Explosion](https://unify.ai/docs/ivy/overview/background/ml_explosion.html#ml-explosion).    - Explaining why it is important [to solve this problem](https://unify.ai/docs/ivy/overview/background/why_unify.html#why-unify).    - Explaining how we adhere to existing [standards](https://unify.ai/docs/ivy/overview/background/standardization.html#standardization) to make this happen.- [Related Work](https://unify.ai/docs/ivy/overview/related_work.html): Which paints a picture of the role Ivy plays in the ML stack, comparing it to other existing solutions in terms of functionalities and abstraction level.- [Design](https://unify.ai/docs/ivy/overview/design.html): A user-focused guide about the design decision behind the architecture and the main building blocks of Ivy.- [Deep Dive](https://unify.ai/docs/ivy/overview/deep_dive.html): Which delves deeper into the implementation details of Ivy and is oriented towards potential contributors to the code base.------------------------------------------------------------------------# ExamplesThe [Examples page](https://unify.ai/demos/) features a wide range ofdemos and tutorials showcasing the functionalities of Ivy along withmultiple use cases, but feel free to check out some shorterframework-specific examples here ‚¨áÔ∏è&lt;details&gt;&lt;summary&gt;&lt;b&gt;I'm using PyTorch&amp;ensp;&lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/torch_small_logo.png&quot;&gt;&lt;/b&gt;&lt;/summary&gt;   &lt;blockquote&gt;You can use Ivy to get PyTorch code from:      &lt;details&gt;         &lt;summary&gt;Any model&lt;/summary&gt;         &lt;blockquote&gt;            &lt;details&gt;               &lt;summary&gt;From TensorFlow&lt;/summary&gt;``` pythonimport ivyimport torchimport tensorflow as tf# Get a pretrained keras modeleff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(    include_top=False, weights=&quot;imagenet&quot;, input_shape=(224, 224, 3))# Transpile it into a torch.nn.Module with the corresponding parametersnoise = tf.random.normal(shape=(1, 224, 224, 3))torch_eff_encoder = ivy.transpile(eff_encoder, to=&quot;torch&quot;, args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(torch.nn.Module):    def __init__(self, num_classes=20):        super(Classifier, self).__init__()        self.encoder = torch_eff_encoder        self.fc = torch.nn.Linear(1280, num_classes)    def forward(self, x):        x = self.encoder(x)        return self.fc(x)# Initialize a trainable, customizable, torch.nn.Moduleclassifier = Classifier()ret = classifier(torch.rand((1, 244, 244, 3)))```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From JAX&lt;/summary&gt;``` pythonimport ivyimport jaximport torch# Get a pretrained haiku model# https://unify.ai/demos/scripts/deepmind_perceiver_io.pyfrom deepmind_perceiver_io import key, perceiver_backbone# Transpile it into a torch.nn.Module with the corresponding parametersdummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))params = perceiver_backbone.init(rng=key, images=dummy_input)backbone = ivy.transpile(    perceiver_backbone, to=&quot;torch&quot;, params_v=params, kwargs={&quot;images&quot;: dummy_input})# Build a classifier using the transpiled backboneclass PerceiverIOClassifier(torch.nn.Module):    def __init__(self, num_classes=20):        super(PerceiverIOClassifier, self).__init__()        self.backbone = backbone        self.max_pool = torch.nn.MaxPool2d((512, 1))        self.flatten = torch.nn.Flatten()        self.fc = torch.nn.Linear(1024, num_classes)    def forward(self, x):        x = self.backbone(images=x)        x = self.flatten(self.max_pool(x))        return self.fc(x)# Initialize a trainable, customizable, torch.nn.Moduleclassifier = PerceiverIOClassifier()ret = classifier(torch.rand((1, 3, 224, 224)))```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Any library&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From Tensorflow&lt;/summary&gt;``` pythonimport ivyimport torchimport osos.environ[&quot;SM_FRAMEWORK&quot;] = &quot;tf.keras&quot;import segmentation_models as sm# transpile sm from tensorflow to torchtorch_sm = ivy.transpile(sm, source=&quot;tensorflow&quot;, to=&quot;torch&quot;)# get some image-like arraysoutput = torch.rand((1, 3, 512, 512))target = torch.rand((1, 3, 512, 512))# and use the transpiled version of any function from the library!out = torch_sm.metrics.iou_score(output, target)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From JAX&lt;/summary&gt;``` pythonimport ivyimport raximport torch# transpile rax from jax to torchtorch_rax = ivy.transpile(rax, source=&quot;jax&quot;, to=&quot;torch&quot;)# get some arraysscores = torch.tensor([2.2, 1.3, 5.4])labels = torch.tensor([1.0, 0.0, 0.0])# and use the transpiled version of any function from the library!out = torch_rax.poly1_softmax_loss(scores, labels)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From NumPy&lt;/summary&gt;``` pythonimport ivyimport torchimport madmom# transpile madmon from numpy to torchtorch_madmom = ivy.transpile(madmom, source=&quot;numpy&quot;, to=&quot;torch&quot;)# get some arraysfreqs = torch.arange(20) * 10# and use the transpiled version of any function from the library!out = torch_madmom.audio.filters.hz2midi(freqs)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Any function&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From Tensorflow&lt;/summary&gt;``` pythonimport ivyimport tensorflow as tfimport torchdef loss(predictions, targets):    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))# transpile any function from tf to torchtorch_loss = ivy.transpile(loss, source=&quot;tensorflow&quot;, to=&quot;torch&quot;)# get some arraysp = torch.tensor([3.0, 2.0, 1.0])t = torch.tensor([0.0, 0.0, 0.0])# and use the transpiled version!out = torch_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From JAX&lt;/summary&gt;``` pythonimport ivyimport jax.numpy as jnpimport torchdef loss(predictions, targets):    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))# transpile any function from jax to torchtorch_loss = ivy.transpile(loss, source=&quot;jax&quot;, to=&quot;torch&quot;)# get some arraysp = torch.tensor([3.0, 2.0, 1.0])t = torch.tensor([0.0, 0.0, 0.0])# and use the transpiled version!out = torch_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From NumPy&lt;/summary&gt;``` pythonimport ivyimport numpy as npimport torchdef loss(predictions, targets):    return np.sqrt(np.mean((predictions - targets) ** 2))# transpile any function from numpy to torchtorch_loss = ivy.transpile(loss, source=&quot;numpy&quot;, to=&quot;torch&quot;)# get some arraysp = torch.tensor([3.0, 2.0, 1.0])t = torch.tensor([0.0, 0.0, 0.0])# and use the transpiled version!out = torch_loss(p, t)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;&lt;b&gt;I'm using TensorFlow&amp;ensp;&lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/tf_small_logo.png&quot;&gt;&lt;/b&gt;&lt;/summary&gt;&lt;blockquote&gt;You can use Ivy to get TensorFlow code from:&lt;details&gt;&lt;summary&gt;Any model&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport torchimport timmimport tensorflow as tf# Get a pretrained pytorch modelmlp_encoder = timm.create_model(&quot;mixer_b16_224&quot;, pretrained=True, num_classes=0)# Transpile it into a keras.Model with the corresponding parametersnoise = torch.randn(1, 3, 224, 224)mlp_encoder = ivy.transpile(mlp_encoder, to=&quot;tensorflow&quot;, args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(tf.keras.Model):    def __init__(self):        super(Classifier, self).__init__()        self.encoder = mlp_encoder        self.output_dense = tf.keras.layers.Dense(units=1000, activation=&quot;softmax&quot;)    def call(self, x):        x = self.encoder(x)        return self.output_dense(x)# Transform the classifier and use it as a standard keras.Modelx = tf.random.normal(shape=(1, 3, 224, 224))model = Classifier()ret = model(x)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From JAX&lt;/summary&gt;``` pythonimport ivyimport jaximport tensorflow as tf# Get a pretrained haiku model# https://unify.ai/demos/scripts/deepmind_perceiver_io.pyfrom deepmind_perceiver_io import key, perceiver_backbone# Transpile it into a tf.keras.Model with the corresponding parametersdummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))params = perceiver_backbone.init(rng=key, images=dummy_input)backbone = ivy.transpile(    perceiver_backbone, to=&quot;tensorflow&quot;, params_v=params, args=(dummy_input,))# Build a classifier using the transpiled backboneclass PerceiverIOClassifier(tf.keras.Model):    def __init__(self, num_classes=20):        super(PerceiverIOClassifier, self).__init__()        self.backbone = backbone        self.max_pool = tf.keras.layers.MaxPooling1D(pool_size=512)        self.flatten = tf.keras.layers.Flatten()        self.fc = tf.keras.layers.Dense(num_classes)    def call(self, x):        x = self.backbone(x)        x = self.flatten(self.max_pool(x))        return self.fc(x)# Initialize a trainable, customizable, tf.keras.Modelx = tf.random.normal(shape=(1, 3, 224, 224))classifier = PerceiverIOClassifier()ret = classifier(x)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Any library&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport korniaimport requestsimport numpy as npimport tensorflow as tffrom PIL import Image# transpile kornia from torch to tensorflowtf_kornia = ivy.transpile(kornia, source=&quot;torch&quot;, to=&quot;tensorflow&quot;)# get an imageurl = &quot;http://images.cocodataset.org/train2017/000000000034.jpg&quot;raw_img = Image.open(requests.get(url, stream=True).raw)# convert it to the format expected by korniaimg = np.array(raw_img)img = tf.transpose(tf.constant(img), (2, 0, 1))img = tf.expand_dims(img, 0) / 255# and use the transpiled version of any function from the library!out = tf_kornia.enhance.sharpness(img, 5)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From JAX&lt;/summary&gt;``` pythonimport ivyimport raximport tensorflow as tf# transpile rax from jax to tensorflowtf_rax = ivy.transpile(rax, source=&quot;jax&quot;, to=&quot;tensorflow&quot;)# get some arraysscores = tf.constant([2.2, 1.3, 5.4])labels = tf.constant([1.0, 0.0, 0.0])# and use the transpiled version of any function from the library!out = tf_rax.poly1_softmax_loss(scores, labels)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From NumPy&lt;/summary&gt;``` pythonimport ivyimport madmomimport tensorflow as tf# transpile madmom from numpy to tensorflowtf_madmom = ivy.transpile(madmom, source=&quot;numpy&quot;, to=&quot;tensorflow&quot;)# get some arraysfreqs = tf.range(20) * 10# and use the transpiled version of any function from the library!out = tf_madmom.audio.filters.hz2midi(freqs)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Any function&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport torchimport tensorflow as tfdef loss(predictions, targets):    return torch.sqrt(torch.mean((predictions - targets) ** 2))# transpile any function from torch to tensorflowtf_loss = ivy.transpile(loss, source=&quot;torch&quot;, to=&quot;tensorflow&quot;)# get some arraysp = tf.constant([3.0, 2.0, 1.0])t = tf.constant([0.0, 0.0, 0.0])# and use the transpiled version!out = tf_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From JAX&lt;/summary&gt;``` pythonimport ivyimport jax.numpy as jnpimport tensorflow as tfdef loss(predictions, targets):    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))# transpile any function from jax to tensorflowtf_loss = ivy.transpile(loss, source=&quot;jax&quot;, to=&quot;tensorflow&quot;)# get some arraysp = tf.constant([3.0, 2.0, 1.0])t = tf.constant([0.0, 0.0, 0.0])# and use the transpiled version!out = tf_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From NumPy&lt;/summary&gt;``` pythonimport ivyimport numpy as npimport tensorflow as tfdef loss(predictions, targets):    return np.sqrt(np.mean((predictions - targets) ** 2))# transpile any function from numpy to tensorflowtf_loss = ivy.transpile(loss, source=&quot;numpy&quot;, to=&quot;tensorflow&quot;)# get some arraysp = tf.constant([3.0, 2.0, 1.0])t = tf.constant([0.0, 0.0, 0.0])# and use the transpiled version!out = tf_loss(p, t)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;&lt;b&gt;I'm using Jax&amp;ensp;&lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/jax_small_logo.png&quot;&gt;&lt;/b&gt;&lt;/summary&gt;&lt;blockquote&gt;You can use Ivy to get JAX code from:&lt;details&gt;&lt;summary&gt;Any model&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport timmimport torchimport jaximport haiku as hk# Get a pretrained pytorch modelmlp_encoder = timm.create_model(&quot;mixer_b16_224&quot;, pretrained=True, num_classes=0)# Transpile it into a hk.Module with the corresponding parametersnoise = torch.randn(1, 3, 224, 224)mlp_encoder = ivy.transpile(mlp_encoder, to=&quot;jax&quot;, args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(hk.Module):    def __init__(self, num_classes=1000):        super(Classifier, self).__init__()        self.encoder = mlp_encoder()        self.fc = hk.Linear(output_size=num_classes, with_bias=True)    def __call__(self, x):        x = self.encoder(x)        x = self.fc(x)        return xdef _forward_classifier(x):    module = Classifier()    return module(x)# Transform the classifier and use it as a standard hk.Modulerng_key = jax.random.PRNGKey(42)x = jax.random.uniform(key=rng_key, shape=(1, 3, 224, 224), dtype=jax.numpy.float32)forward_classifier = hk.transform(_forward_classifier)params = forward_classifier.init(rng=rng_key, x=x)ret = forward_classifier.apply(params, None, x)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From TensorFlow&lt;/summary&gt;``` pythonimport ivyimport jaximport haiku as hkimport tensorflow as tf# Get a pretrained keras modeleff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(    include_top=False, weights=&quot;imagenet&quot;, input_shape=(224, 224, 3))# Transpile it into a hk.Module with the corresponding parametersnoise = tf.random.normal(shape=(1, 224, 224, 3))hk_eff_encoder = ivy.transpile(eff_encoder, to=&quot;jax&quot;, args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(hk.Module):    def __init__(self, num_classes=1000):        super(Classifier, self).__init__()        self.encoder = hk_eff_encoder()        self.fc = hk.Linear(output_size=num_classes, with_bias=True)    def __call__(self, x):        x = self.encoder(x)        x = self.fc(x)        return xdef _forward_classifier(x):    module = Classifier()    return module(x)# Transform the classifier and use it as a standard hk.Modulerng_key = jax.random.PRNGKey(42)dummy_x = jax.random.uniform(key=rng_key, shape=(1, 224, 224, 3))forward_classifier = hk.transform(_forward_classifier)params = forward_classifier.init(rng=rng_key, x=dummy_x)ret = forward_classifier.apply(params, None, dummy_x)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Any library&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport korniaimport requestsimport jax.numpy as jnpfrom PIL import Image# transpile kornia from torch to jaxjax_kornia = ivy.transpile(kornia, source=&quot;torch&quot;, to=&quot;jax&quot;)# get an imageurl = &quot;http://images.cocodataset.org/train2017/000000000034.jpg&quot;raw_img = Image.open(requests.get(url, stream=True).raw)# convert it to the format expected by korniaimg = jnp.transpose(jnp.array(raw_img), (2, 0, 1))img = jnp.expand_dims(img, 0) / 255# and use the transpiled version of any function from the library!out = jax_kornia.enhance.sharpness(img, 5)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From TensorFlow&lt;/summary&gt;``` pythonimport ivyimport jaximport osos.environ[&quot;SM_FRAMEWORK&quot;] = &quot;tf.keras&quot;import segmentation_models as sm# transpile sm from tensorflow to jaxjax_sm = ivy.transpile(sm, source=&quot;tensorflow&quot;, to=&quot;jax&quot;)# get some image-like arrayskey = jax.random.PRNGKey(23)key1, key2 = jax.random.split(key)output = jax.random.uniform(key1, (1, 3, 512, 512))target = jax.random.uniform(key2, (1, 3, 512, 512))# and use the transpiled version of any function from the library!out = jax_sm.metrics.iou_score(output, target)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From NumPy&lt;/summary&gt;``` pythonimport ivyimport madmomimport jax.numpy as jnp# transpile madmon from numpy to jaxjax_madmom = ivy.transpile(madmom, source=&quot;numpy&quot;, to=&quot;jax&quot;)# get some arraysfreqs = jnp.arange(20) * 10# and use the transpiled version of any function from the library!out = jax_madmom.audio.filters.hz2midi(freqs)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Any function&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport torchimport jax.numpy as jnpdef loss(predictions, targets):    return torch.sqrt(torch.mean((predictions - targets) ** 2))# transpile any function from torch to jaxjax_loss = ivy.transpile(loss, source=&quot;torch&quot;, to=&quot;jax&quot;)# get some arraysp = jnp.array([3.0, 2.0, 1.0])t = jnp.array([0.0, 0.0, 0.0])# and use the transpiled version!out = jax_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From TensorFlow&lt;/summary&gt;``` pythonimport ivyimport tensorflow as tfimport jax.numpy as jnpdef loss(predictions, targets):    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))# transpile any function from tf to jaxjax_loss = ivy.transpile(loss, source=&quot;tensorflow&quot;, to=&quot;jax&quot;)# get some arraysp = jnp.array([3.0, 2.0, 1.0])t = jnp.array([0.0, 0.0, 0.0])# and use the transpiled version!out = jax_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From NumPy&lt;/summary&gt;``` pythonimport ivyimport numpy as npimport jaximport jax.numpy as jnpjax.config.update('jax_enable_x64', True)def loss(predictions, targets):    return np.sqrt(np.mean((predictions - targets) ** 2))# transpile any function from numpy to jaxjax_loss = ivy.transpile(loss, source=&quot;numpy&quot;, to=&quot;jax&quot;)# get some arraysp = jnp.array([3.0, 2.0, 1.0])t = jnp.array([0.0, 0.0, 0.0])# and use the transpiled version!out = jax_loss(p, t)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;&lt;b&gt;I'm using NumPy&amp;ensp;&lt;img class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/numpy_small_logo.png&quot;&gt;&lt;/b&gt;&lt;/summary&gt;&lt;blockquote&gt;You can use Ivy to get NumPy code from:&lt;details&gt;&lt;summary&gt;Any library&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport korniaimport requestsimport numpy as npfrom PIL import Image# transpile kornia from torch to npnp_kornia = ivy.transpile(kornia, source=&quot;torch&quot;, to=&quot;numpy&quot;)# get an imageurl = &quot;http://images.cocodataset.org/train2017/000000000034.jpg&quot;raw_img = Image.open(requests.get(url, stream=True).raw)# convert it to the format expected by korniaimg = np.transpose(np.array(raw_img), (2, 0, 1))img = np.expand_dims(img, 0) / 255# and use the transpiled version of any function from the library!out = np_kornia.enhance.sharpness(img, 5)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From TensorFlow&lt;/summary&gt;``` pythonimport ivyimport numpy as npimport osos.environ[&quot;SM_FRAMEWORK&quot;] = &quot;tf.keras&quot;import segmentation_models as sm# transpile sm from tensorflow to numpynp_sm = ivy.transpile(sm, source=&quot;tensorflow&quot;, to=&quot;numpy&quot;)# get some image-like arraysoutput = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)target = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)# and use the transpiled version of any function from the library!out = np_sm.metrics.iou_score(output, target)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From Jax&lt;/summary&gt;``` pythonimport ivyimport raximport numpy as np# transpile rax from jax to numpynp_rax = ivy.transpile(rax, source=&quot;jax&quot;, to=&quot;numpy&quot;)# get some arraysscores = np.array([2.2, 1.3, 5.4])labels = np.array([1.0, 0.0, 0.0])# and use the transpiled version of any function from the library!out = np_rax.poly1_softmax_loss(scores, labels)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;Any function&lt;/summary&gt;&lt;blockquote&gt;&lt;details&gt;   &lt;summary&gt;From PyTorch&lt;/summary&gt;``` pythonimport ivyimport torchimport numpy as npdef loss(predictions, targets):    return torch.sqrt(torch.mean((predictions - targets) ** 2))# transpile any function from torch to numpynp_loss = ivy.transpile(loss, source=&quot;torch&quot;, to=&quot;numpy&quot;)# get some arraysp = np.array([3.0, 2.0, 1.0])t = np.array([0.0, 0.0, 0.0])# and use the transpiled version!out = np_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From TensorFlow&lt;/summary&gt;``` pythonimport ivyimport tensorflow as tfimport numpy as npdef loss(predictions, targets):    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))# transpile any function from tf to numpynp_loss = ivy.transpile(loss, source=&quot;tensorflow&quot;, to=&quot;numpy&quot;)# get some arraysp = np.array([3.0, 2.0, 1.0])t = np.array([0.0, 0.0, 0.0])# and use the transpiled version!out = np_loss(p, t)```&lt;/details&gt;&lt;details&gt;   &lt;summary&gt;From JAX&lt;/summary&gt;``` pythonimport ivyimport jax.numpy as jnpimport numpy as npdef loss(predictions, targets):    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))# transpile any function from jax to numpynp_loss = ivy.transpile(loss, source=&quot;jax&quot;, to=&quot;numpy&quot;)# get some arraysp = np.array([3.0, 2.0, 1.0])t = np.array([0.0, 0.0, 0.0])# and use the transpiled version!out = np_loss(p, t)```&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;/blockquote&gt;&lt;/details&gt;&lt;details&gt;&lt;summary&gt;&lt;b&gt;I'm using Ivy&amp;ensp;&lt;img height=&quot;25px&quot; width=&quot;25px&quot; class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/logos/ivy_logo_only.svg&quot;&gt;&lt;/b&gt;&lt;/summary&gt;Or you can use Ivy as a framework, breaking yourself (and your code)free from deciding which community to support, allowing anyone to runyour code in their framework of choice!``` pythonimport ivy# A simple image classification modelclass IvyNet(ivy.Module):    def __init__(        self,        h_w=(32, 32),        input_channels=3,        output_channels=512,        num_classes=2,        data_format=&quot;NCHW&quot;,        device=&quot;cpu&quot;,    ):        self.h_w = h_w        self.input_channels = input_channels        self.output_channels = output_channels        self.num_classes = num_classes        self.data_format = data_format        self.device = device        super().__init__()    def _build(self, *args, **kwargs):        self.extractor = ivy.Sequential(            ivy.Conv2D(self.input_channels, 6, [5, 5], 1, &quot;SAME&quot;, data_format=self.data_format),            ivy.GELU(),            ivy.Conv2D(6, 16, [5, 5], 1, &quot;SAME&quot;, data_format=self.data_format),            ivy.GELU(),            ivy.Conv2D(16, self.output_channels, [5, 5], 1, &quot;SAME&quot;, data_format=self.data_format),            ivy.GELU(),        )        self.classifier = ivy.Sequential(            # Since the padding is &quot;SAME&quot;, this would be image_height x image_width x output_channels            ivy.Linear(self.h_w[0] * self.h_w[1] * self.output_channels, 512),            ivy.GELU(),            ivy.Linear(512, self.num_classes),        )    def _forward(self, x):        x = self.extractor(x)        # flatten all dims except batch dim        x = ivy.flatten(x, start_dim=1, end_dim=-1)        logits = self.classifier(x)        probs = ivy.softmax(logits)        return logits, probs```After building your model in Ivy, you can set your favourite frameworkas the backend to use its operations under the hood!``` pythonivy.set_backend(&quot;torch&quot;)model = IvyNet()x = torch.randn(1, 3, 32, 32)logits, probs = model(x)`````` pythonivy.set_backend(&quot;tensorflow&quot;)model = IvyNet()x = tf.random.uniform(shape=(1, 3, 32, 32))logits, probs = model(x)`````` pythonivy.set_backend(&quot;jax&quot;)model = IvyNet()x = jax.random.uniform(key, shape=(1, 3, 32, 32))logits, probs = model(x)`````` pythonivy.set_backend(&quot;numpy&quot;)model = IvyNet()x = np.random.uniform(size=(1, 3, 32, 32))logits, probs = model(x)```Last but not least, we can also build the training pipeline in pure ivy‚¨áÔ∏è&lt;details&gt;&lt;summary&gt;&lt;a&gt;Let's define some helper functions first&lt;/a&gt;&lt;/summary&gt;``` python# helper function for loading the dataset in batchesdef generate_batches(images, classes, dataset_size, batch_size=32):    targets = {k: v for v, k in enumerate(np.unique(classes))}    y_train = [targets[classes[i]] for i in range(len(classes))]    if batch_size &gt; dataset_size:        raise ivy.utils.exceptions.IvyError(&quot;Use a smaller batch size&quot;)    for idx in range(0, dataset_size, batch_size):        yield ivy.stack(images[idx : min(idx + batch_size, dataset_size)]), ivy.array(            y_train[idx : min(idx + batch_size, dataset_size)]        )# helper function to get the number of current predictionsdef num_correct(preds, labels):    return (preds.argmax() == labels).sum().to_numpy().item()# define a loss functiondef loss_fn(params):    v, model, x, y = params    y_pred, probs = model(x)    return ivy.cross_entropy(y, probs), probs```&lt;/details&gt;&lt;details&gt;&lt;summary&gt;&lt;a&gt;And train this model!&lt;/a&gt;&lt;/summary&gt;``` python# train the model on gpu if it's availabledevice = &quot;cuda:0&quot; if ivy.gpu_is_available() else &quot;cpu&quot;# training hyperparamsoptimizer= ivy.Adam(1e-4)batch_size = 64num_epochs = 20num_classes = 10model = IvyNet(    h_w=(28, 28),    input_channels=1,    output_channels=120,    num_classes=num_classes,    device=device,)model_name = type(model).__name__.lower()# training loopdef train(images, classes, epochs, model, device, num_classes=10, batch_size=32):    # training metrics    epoch_loss = 0.0    running_loss = 0.0    fields = [&quot;epoch&quot;, &quot;epoch_loss&quot;, &quot;training_accuracy&quot;]    metrics = []    dataset_size = len(images)    for epoch in range(epochs):        train_loss, train_correct = 0, 0        train_loop = tqdm(            generate_batches(images, classes, len(images), batch_size=batch_size),            total=dataset_size // batch_size,            position=0,            leave=True,        )        for xbatch, ybatch in train_loop:            if device != &quot;cpu&quot;:                xbatch, ybatch = xbatch.to_device(&quot;gpu:0&quot;), ybatch.to_device(&quot;gpu:0&quot;)            # Since the cross entropy function expects the target classes to be in one-hot encoded format            ybatch_encoded = ivy.one_hot(ybatch, num_classes)            # update model params            loss_probs, grads = ivy.execute_with_gradients(                loss_fn,                (model.v, model, xbatch, ybatch_encoded),            )            model.v = optimizer.step(model.v, grads[&quot;0&quot;])            batch_loss = ivy.to_numpy(loss_probs[0]).mean().item()  # batch mean loss            epoch_loss += batch_loss * xbatch.shape[0]            train_correct += num_correct(loss_probs[1], ybatch)            train_loop.set_description(f&quot;Epoch [{epoch + 1:2d}/{epochs}]&quot;)            train_loop.set_postfix(                running_loss=batch_loss,                accuracy_percentage=(train_correct / dataset_size) * 100,            )        epoch_loss = epoch_loss / dataset_size        training_accuracy = train_correct / dataset_size        metrics.append([epoch, epoch_loss, training_accuracy])        train_loop.write(            f&quot;\nAverage training loss: {epoch_loss:.6f}, Train Correct: {train_correct}&quot;,            end=&quot;\n&quot;,        )    # write metrics for plotting    with open(f&quot;/{model_name}_train_summary.csv&quot;, &quot;w&quot;) as f:        f = csv.writer(f)        f.writerow(fields)        f.writerows(metrics)# assuming the dataset(images and classes) are already prepared in a foldertrain(images, classes, num_epochs, model, device, num_classes = num_classes, batch_size = batch_size)```&lt;/details&gt;&lt;/details&gt;------------------------------------------------------------------------# Diving deeperAlthough the [Docs](https://unify.ai/docs/ivy/) are the best place to learn more, in the next section we will take a look at how Ivy works as both a transpiler and a framework in a bit more of detail to get an idea of why and where to use it.&lt;details&gt;&lt;summary&gt;&lt;b&gt;Ivy as a transpiler&lt;/b&gt;&lt;/summary&gt;Ivy\'s transpiler allows you to use code from any other framework (orfrom any other version of the same framework!) in your own code, by justadding one line of code. Under the hood, Ivy traces a computationalgraph and leverages the frontends and backends to link one framework toanother.This way, Ivy makes all ML-related projects available for you,independently of the framework you want to use to research, develop, ordeploy systems. Feel free to head over to the docs for the full APIreference, but the functions you\'d most likely want to use are:``` python# Traces an efficient fully-functional graph from a function, removing all wrapping and redundant codeivy.trace_graph()# Converts framework-specific code to a different frameworkivy.transpile()# Converts framework-specific code to Ivyivy.unify()```These functions can be used eagerly or lazily. If you pass the necessaryarguments for function tracing, the graph tracing/transpilation step willhappen instantly (eagerly). Otherwise, the graph tracing/transpilationwill happen only when the returned function is first invoked.``` pythonimport ivyimport jaxivy.set_backend(&quot;jax&quot;)# Simple JAX function to transpiledef test_fn(x):    return jax.numpy.sum(x)x1 = ivy.array([1., 2.])`````` python# Arguments are available -&gt; transpilation happens eagerlyeager_graph = ivy.transpile(test_fn, source=&quot;jax&quot;, to=&quot;torch&quot;, args=(x1,))# eager_graph is now torch code and runs efficientlyret = eager_graph(x1)`````` python# Arguments are not available -&gt; transpilation happens lazilylazy_graph = ivy.transpile(test_fn, source=&quot;jax&quot;, to=&quot;torch&quot;)# The transpiled graph is initialized, transpilation will happen hereret = lazy_graph(x1)# lazy_graph is now torch code and runs efficientlyret = lazy_graph(x1)```If you want to learn more, you can find more information in the [Ivy asa transpiler section of thedocs!](https://unify.ai/docs/ivy/overview/design/ivy_as_a_transpiler.html)## When should I use Ivy as a transpiler?If you want to use building blocks published in other frameworks (neuralnetworks, layers, array computing libraries, training pipelines\...),you want to integrate code developed in various frameworks, or maybestraight up move code from one framework to another, the transpiler isdefinitely the tool üîß for the job! As the output of transpilation isnative code in the target framework, you can use the converted code justas if it was code originally developed in that framework, applyingframework-specific optimizations or tools, instantly exposing yourproject to all of the unique perks of a different framework.&lt;/details&gt;&lt;details&gt;&lt;summary&gt;&lt;b&gt;Ivy as a framework&lt;/b&gt;&lt;/summary&gt;The Ivy framework is built on top of various essential components,mainly the [BackendHandler](https://unify.ai/docs/ivy/overview/design/building_blocks.html#backend-handler),which manages what framework is being used behind the scenes and the[Backend FunctionalAPIs](https://unify.ai/docs/ivy/overview/design/building_blocks.html#backend-functional-apis),which provide framework-specific implementations of the Ivy functions.Likewise, classes such as `ivy.Container` or `ivy.Array` are alsoavailable, facilitating the use of structured data and array-likeobjects (learn more about them[here!](https://unify.ai/docs/ivy/overview/design/ivy_as_a_framework.html)).All of the functionalities in Ivy are exposed through the`Ivy functional API` and the `Ivy stateful API`. All functions in the[FunctionalAPI](https://unify.ai/docs/ivy/overview/design/building_blocks.html#ivy-functional-api)are **Framework Agnostic Functions**, which means that we can use themlike this:``` pythonimport ivyimport jax.numpy as jnpimport tensorflow as tfimport numpy as npimport torchdef mse_loss(y, target):    return ivy.mean((y - target)**2)jax_mse   = mse_loss(jnp.ones((5,)), jnp.ones((5,)))tf_mse    = mse_loss(tf.ones((5,)), tf.ones((5,)))np_mse    = mse_loss(np.ones((5,)), np.ones((5,)))torch_mse = mse_loss(torch.ones((5,)), torch.ones((5,)))```In the example above we show how Ivy\'s functions are compatible withtensors from different frameworks. This is the same for ALL Ivyfunctions. They can accept tensors from any framework and return thecorrect result.The [Ivy StatefulAPI](https://unify.ai/docs/ivy/overview/design/ivy_as_a_framework/ivy_stateful_api.html),on the other hand, allows you to define trainable modules and layers,which you can use alone or as a part of any other framework code!``` pythonimport ivyclass Regressor(ivy.Module):    def __init__(self, input_dim, output_dim):        self.input_dim = input_dim        self.output_dim = output_dim        super().__init__()    def _build(self, *args, **kwargs):        self.linear0 = ivy.Linear(self.input_dim, 128)        self.linear1 = ivy.Linear(128, self.output_dim)    def _forward(self, x):        x = self.linear0(x)        x = ivy.functional.relu(x)        x = self.linear1(x)        return x```If we put it all together, we\'ll have something like this. This exampleuses PyTorch as the backend, but this can easily be changed to yourfavorite frameworks, such as TensorFlow, or JAX.``` pythonimport ivyclass Regressor(ivy.Module):    def __init__(self, input_dim, output_dim):        self.input_dim = input_dim        self.output_dim = output_dim        super().__init__()    def _build(self, *args, **kwargs):        self.linear0 = ivy.Linear(self.input_dim, 128)        self.linear1 = ivy.Linear(128, self.output_dim)    def _forward(self, x):        x = self.linear0(x)        x = ivy.functional.relu(x)        x = self.linear1(x)        return xivy.set_backend('torch')  # set backend to PyTorch (or any other backend!)model = Regressor(input_dim=1, output_dim=1)optimizer = ivy.Adam(0.3)n_training_examples = 2000noise = ivy.random.random_normal(shape=(n_training_examples, 1), mean=0, std=0.1)x = ivy.linspace(-6, 3, n_training_examples).reshape((n_training_examples, 1))y = 0.2 * x ** 2 + 0.5 * x + 0.1 + noisedef loss_fn(v, x, target):    pred = model(x, v=v)    return ivy.mean((pred - target) ** 2)for epoch in range(40):    # forward pass    pred = model(x)    # compute loss and gradients    loss, grads = ivy.execute_with_gradients(lambda params: loss_fn(*params), (model.v, x, y))    # update parameters    model.v = optimizer.step(model.v, grads)    # print current loss    print(f'Epoch: {epoch + 1:2d} --- Loss: {ivy.to_numpy(loss).item():.5f}')print('Finished training!')```The model\'s output can be visualized as follows:&lt;div align=&quot;center&quot;&gt;   &lt;img width=&quot;50%&quot; class=&quot;dark-light&quot; src=&quot;https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/regressor_lq.gif&quot;&gt;&lt;/div&gt;As always, you can find more information about [Ivy as a framework inthedocs!](https://unify.ai/docs/ivy/overview/design/ivy_as_a_framework.html)&lt;h2&gt; When should I use Ivy as a framework? &lt;/h2&gt;As Ivy supports multiple backends, writing code in Ivy breaks you freefrom framework limitations. If you want to publish highly flexible codefor everyone to use, independently of the framework they are using, oryou plan to develop ML-related tools and want them to be interoperablewith not only the already existing frameworks, but also with futureframeworks, then Ivy is for you!&lt;/details&gt;------------------------------------------------------------------------# ContributingWe believe that everyone can contribute and make a difference. Whetherit\'s writing code üíª, fixing bugs üêõ, or simply sharing feedback üí¨,your contributions are definitely welcome and appreciated üôåCheck out all of our open tasks, and find out more info in our[Contributingguide](https://unify.ai/docs/ivy/overview/contributing.html) in thedocs!Join our amazing community as a code contributor, and help accelerateour journey to unify all ML frameworks!&lt;a href=&quot;https://github.com/unifyai/ivy/graphs/contributors&quot;&gt;  &lt;img class=&quot;dark-light&quot; src=&quot;https://contrib.rocks/image?repo=unifyai/ivy&amp;anon=0&amp;columns=20&amp;max=100&amp;r=true&quot; /&gt;&lt;/a&gt;------------------------------------------------------------------------# CommunityIn order to achieve the ambitious goal of unifying AI we definitely needas many hands as possible on it! Whether you are a seasoned developer orjust starting out, you\'ll find a place here! Join the Ivy community inour [Discord](https://discord.gg/sXyFF8tDtm) üëæ server, which is theperfect place to ask questions, share ideas, and get help from bothfellow developers and the Ivy Team directly!Also! Feel free to follow us on[Twitter](https://twitter.com/letsunifyai) üê¶ as well, we use it toshare updates, sneak peeks, and all sorts of relevant news, certainly agreat way to stay in the loop üòÑCan\'t wait to see you there!------------------------------------------------------------------------# CitationIf you use Ivy for your work, please don\'t forget to give proper creditby including the accompanying [paper](https://arxiv.org/abs/2102.02886)üìÑ in your references. It\'s a small way to show appreciation and helpto continue to support this and other open source projects üôå    @article{lenton2021ivy,      title={Ivy: Templated deep learning for inter-framework portability},      author={Lenton, Daniel and Pardo, Fabio and Falck, Fabian and James, Stephen and Clark, Ronald},      journal={arXiv preprint arXiv:2102.02886},      year={2021}    }</longdescription>
</pkgmetadata>