<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE pkgmetadata SYSTEM "http://www.gentoo.org/dtd/metadata.dtd">
<pkgmetadata>
	<maintainer type="person">
		<email>gentoo@houseofsuns.org</email>
		<name>Markus Walter</name>
	</maintainer>
	<longdescription>&lt;h1 align=&quot;center&quot;&gt;  phidata&lt;/h1&gt;&lt;p align=&quot;center&quot;&gt;    &lt;em&gt;AI Toolkit for Engineers&lt;/em&gt;&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://python.org/pypi/phidata&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;    &lt;img src=&quot;https://img.shields.io/pypi/v/phidata?color=blue&amp;label=version&quot; alt=&quot;version&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/phidatahq/phidata&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;    &lt;img src=&quot;https://img.shields.io/badge/python-&gt;=3.9-blue&quot; alt=&quot;pythonversion&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/phidatahq/phidata&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;    &lt;img src=&quot;https://pepy.tech/badge/phidata&quot; alt=&quot;downloads&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/phidatahq/phidata/actions/workflows/build.yml&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;    &lt;img src=&quot;https://github.com/phidatahq/phidata/actions/workflows/build.yml/badge.svg&quot; alt=&quot;build-status&quot;&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 align=&quot;center&quot;&gt;  Build LLM applications using production ready templates&lt;/h2&gt;&lt;br /&gt;Phidata is an AI toolkit that provides pre-built templates for LLM apps.## ðŸš€ How it works- Create your LLM app using a template: `phi ws create`- Run your app locally: `phi ws up dev:docker`- Run your app on AWS: `phi ws up prd:aws`For example, run a RAG Chatbot built with FastApi, Streamlit and PgVector in 2 commands:```bashphi ws create -t llm-app -n llm-app  # create the llm-app codebasephi ws up                            # run the llm-app locally```## ðŸ’» Example: Build a RAG LLM AppLet's build a **RAG LLM App** with GPT-4. We'll use PgVector for Knowledge Base and Storage and serve the app using Streamlit and FastApi. Read the full tutorial &lt;a href=&quot;https://docs.phidata.com/how-to/rag-llm-app&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;here&lt;/a&gt;.&gt; Install &lt;a href=&quot;https://docs.docker.com/desktop/install/mac-install/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;docker desktop&lt;/a&gt; to run this app locally.### SetupOpen the `Terminal` and create an `ai` directory with a python virtual environment.```bashmkdir ai &amp;&amp; cd aipython3 -m venv aienvsource aienv/bin/activate```Install phidata```bashpip install phidata```### Create your codebaseCreate your codebase using the `llm-app` template pre-configured with FastApi, Streamlit and PgVector. Use this codebase as a starting point for your LLM product.```bashphi ws create -t llm-app -n llm-app```This will create a folder named `llm-app`&lt;img src=&quot;https://github.com/phidatahq/phidata/assets/22579644/dee7b884-028e-48a0-9074-4deb7a055b97&quot; height=500  alt=&quot;create-llm-app&quot;/&gt;### Serve your LLM App using Streamlit&lt;a href=&quot;https://streamlit.io&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Streamlit&lt;/a&gt; allows us to build micro front-ends for our LLM App and is extremely useful for building basic applications in pure python. Start the `app` group using:```bashphi ws up --group app```&lt;img src=&quot;https://github.com/phidatahq/phidata/assets/22579644/dee35e20-9fe5-4623-af5e-2e8a6ffac58c&quot; height=500  alt=&quot;run-llm-app&quot;/&gt;**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.### Chat with PDFs- Open &lt;a href=&quot;http://localhost:8501&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;localhost:8501&lt;/a&gt; to view streamlit apps that you can customize and make your own.- Click on **Chat with PDFs** in the sidebar- Enter a username and wait for the knowledge base to load.- Choose the `RAG` Conversation type.- Ask &quot;How do I make chicken curry?&quot;- Upload PDFs and ask questions&lt;img width=&quot;800&quot; alt=&quot;chat-with-pdf&quot; src=&quot;https://github.com/phidatahq/phidata/assets/22579644/a8eff0ac-963c-43cb-a784-920bd6713a48&quot;&gt;### Serve your LLM App using FastApiStreamlit is great for building micro front-ends but any production application will be built using a front-end framework like `next.js` backed by a RestApi built using a framework like `FastApi`.Your LLM App comes ready-to-use with FastApi endpoints, start the `api` group using:```bashphi ws up --group api```**Press Enter** to confirm and give a few minutes for the image to download.### View API Endpoints- Open &lt;a href=&quot;http://localhost:8000/docs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;localhost:8000/docs&lt;/a&gt; to view the API Endpoints.- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`- Test the `v1/pdf/conversation/chat` endpoint with `{&quot;message&quot;: &quot;How do I make chicken curry?&quot;}`- The LLM Api comes pre-built with endpoints that you can integrate with your front-end.&lt;img width=&quot;800&quot; alt=&quot;chat-with-pdf&quot; src=&quot;https://github.com/phidatahq/phidata/assets/22579644/17a11146-a49d-4595-9fa0-d6ab15372289&quot;&gt;### Optional: Run JupyterlabA jupyter notebook is a must have for AI development and your `llm-app` comes with a notebook pre-installed with the required dependencies. Enable it by updating the `workspace/settings.py` file:```python {{ title: 'workspace/settings.py'}}...ws_settings = WorkspaceSettings(    ...    # Uncomment the following line    dev_jupyter_enabled=True,...```Start `jupyter` using:```bashphi ws up --group jupyter```**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.### View Jupyterlab UI- Open &lt;a href=&quot;http://localhost:8888&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;localhost:8888&lt;/a&gt; to view the Jupyterlab UI. Password: **admin**- Play around with cookbooks in the `notebooks` folder.### Delete local resourcesPlay around and stop the workspace using:```bashphi ws down```### Run your LLM App on AWSRead how to &lt;a href=&quot;https://docs.phidata.com/guides/llm-app#run-on-aws&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;run your LLM App on AWS here&lt;/a&gt;.### More information:- Read the &lt;a href=&quot;https://docs.phidata.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;documentation&lt;/a&gt;- Read about &lt;a href=&quot;https://docs.phidata.com/intro/basics&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;phidata basics&lt;/a&gt;- Chat with us on &lt;a href=&quot;https://discord.gg/4MtYHHrgA8&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Discord&lt;/a&gt;- Email us at &lt;a href=&quot;mailto:help@phidata.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;help@phidata.com&lt;/a&gt;</longdescription>
</pkgmetadata>